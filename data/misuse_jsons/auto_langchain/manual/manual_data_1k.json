[
    {
        "commit_hash": "2cdfbd18a750cabaff1499fad7473dd91f3c7fa7",
        "index": "6fa1c2ff..5fc4d1bf 100644",
        "commit_message": "Previously, many unit test files started with `enable_v2_behavior`, which would have caused them to run in V2 mode when executing with a V1 test flag. The correct behavior would in fact be to skip such tests when executing with a V1 test flag.\n\nThis fix significantly reduces the total V1 + V2 test load by eliminating redundancy.\n\nPiperOrigin-RevId: 424734850\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "remove API version fix skip test",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class IndexLookupDistributionTest(",
            "",
            "",
            "if __name__ == \"__main__\":",
            "-  tf.compat.v1.enable_v2_behavior()",
            "tf.__internal__.distribute.multi_process_runner.test_main()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=2069038)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=compat))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=v1))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=enable_v2_behavior))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 0,
        "neg_line": [
            "-tf.compat.v1.enable_v2_behavior()"
        ],
        "pos_line": [],
        "core_change": "-tf.compat.v1.enable_v2_behavior()",
        "core_API": "enable_v2_behavior"
    },
    {
        "commit_hash": "7f06bcd0e9176f3262ada4aad3adef2232fe42ef",
        "index": "522b5ed6..72acb5f4 100644",
        "commit_message": "Add docformatter in pre-commit (#1242)\n\n* Add docformatter in pre-commit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Apply suggestions from code review\n\n* Apply suggestions from code review\n\nCo-authored-by: Christian Clauss <cclauss@me.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Christian Clauss <cclauss@me.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "typo fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def load_homography(file_name):",
            "",
            "",
            "def load_image(file_name):",
            "-    \"\"\"Loads the image with OpenCV and converts to torch.Tensor\"\"\"",
            "+    \"\"\"Load the image with OpenCV and converts to torch.Tensor.\"\"\"",
            "if not os.path.isfile(file_name):",
            "raise AssertionError(f\"Invalid file {file_name}\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Loads the image with OpenCV and converts to torch.Tensor\"\"\"), value='\"\"\"Load the image with OpenCV and converts to torch.Tensor.\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3,
        "neg_line": [
            "-\"\"\"Loads the image with OpenCV and converts to torch.Tensor\"\"\""
        ],
        "pos_line": [
            "+\"\"\"Load the image with OpenCV and converts to torch.Tensor.\"\"\""
        ],
        "core_change": "-\"\"\"Loads the image with OpenCV and converts to torch.Tensor\"\"\" +\"\"\"Load the image with OpenCV and converts to torch.Tensor.\"\"\"",
        "core_API": "isfile"
    },
    {
        "commit_hash": "514486739cc732ad05549d81bd48c0aa9e03a0f3",
        "index": "034cc552f..e67615995 100755",
        "commit_message": "Fix CI with change of name of nlp (#7054)\n\n* nlp -> datasets\n\n* More nlp -> datasets\n\n* Woopsie\n\n* More nlp -> datasets\n\n* One last\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "change class",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class TrainerIntegrationTest(unittest.TestCase):",
            "",
            "# Adding one column not used by the model should have no impact",
            "z = np.random.normal(size=(64,)).astype(np.float32)",
            "-        train_dataset = nlp.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})",
            "+        train_dataset = datasets.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})",
            "model = RegressionModel()",
            "trainer = Trainer(model, args, train_dataset=train_dataset)",
            "trainer.train()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4,
        "neg_line": [
            "-train_dataset = nlp.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})"
        ],
        "pos_line": [
            "+train_dataset = datasets.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})"
        ],
        "core_change": "-train_dataset = nlp.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z}) +train_dataset = datasets.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})",
        "core_API": "normal"
    },
    {
        "commit_hash": "02b176c4ce14340d26d42825523f406959c6c202",
        "index": "169f1faeb..78df7911a 100755",
        "commit_message": "Fix torch version comparisons (#18460)\n\nComparisons like\nversion.parse(torch.__version__) > version.parse(\"1.6\")\nare True for torch==1.6.0+cu101 or torch==1.6.0+cpu\n\nversion.parse(version.parse(torch.__version__).base_version) are preferred (and available in pytorch_utils.py\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add condition check for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AlbertEmbeddings(nn.Module):",
            "# position_ids (1, len position emb) is contiguous in memory and exported when serialized",
            "self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))",
            "self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")",
            "-        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):",
            "+        if is_torch_greater_than_1_6:",
            "self.register_buffer(",
            "\"token_type_ids\",",
            "torch.zeros(self.position_ids.size(), dtype=torch.long),"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=version), value='is_torch_greater_than_1_6')",
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=identifier, text=version), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=parse))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__version__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=identifier, text=version))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=parse))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"1.6.0\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 5,
        "neg_line": [
            "-if version.parse(torch.__version__) > version.parse(\"1.6.0\"):"
        ],
        "pos_line": [
            "+if is_torch_greater_than_1_6:"
        ],
        "core_change": "-if version.parse(torch.__version__) > version.parse(\"1.6.0\"): +if is_torch_greater_than_1_6:",
        "core_API": "register_buffer"
    },
    {
        "commit_hash": "5acb5a785b9ed60743e6f687a96bd92dd4e88578",
        "index": "f189ad5b..8160fea3 100644",
        "commit_message": "Encoder abstractions (#518)\n\n* Allow stacked RNNs in PytorchSeq2SeqWrapper\n\n* Modify the zero sequence length logic in PytorchSeq2SeqWrapper to work with stateful RNNs\n\n* WIP: stateful RNNs\n\n* Stateful RNNs\n\n* pylint\n\n* more pylint\n\n* fix the docs\n\n* Address Joel's comments\n\n*  stateful=True that works with GRU\n\n* Deal with sorting in stateful RNNs\n\n* pylint\n\n* mypy\n\n* pylint\n\n* Remove max_batch_size\n\n* Add tests for correctness\n\n* pylint\n\n* initial pass at adding Seq2StackEncoder and adding an EncoderBase\n\n* remove print statements\n\n* docs work\n\n* make sort function return the indices it sorted by\n\n* more work on getting statefulness working correctly\n\n* tweaks to comments, make it clear that wrapping the state in a list is only for statefulness\n\n* finish state update logic and improve docs\n\n* add TODO for review\n\n* fix current tests\n\n* remove abstract Seq2Stack encoder\n\n* use Callable type for base\n\n* add tests for update states\n\n* lint, fix mypy using elipsis in tuple\n\n* fix up ELMo with docs and test\n\n* don't require docs for encoder_base\n\n* add comment for ignored docs\n\n* remove out of date docstring\n\n* remove num_valid from encoder_base, superficial fixes\n\n* use random state in tests, use correct num_valid size\n\n* rename a few things, don't zero out actual tensor in test\n\n* don't split elmo output, fix docs\n\n* actually fix docs\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "no API used",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sort_batch_by_length(tensor: torch.autograd.Variable, sequence_lengths: torc",
            "index_range = Variable(index_range.long())",
            "_, reverse_mapping = permutation_index.sort(0, descending=False)",
            "restoration_indices = index_range.index_select(0, reverse_mapping)",
            "-    return sorted_tensor, sorted_sequence_lengths, restoration_indices",
            "+    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index",
            "",
            "",
            "def get_dropout_mask(dropout_probability: float, tensor_for_masking: torch.autograd.Variable):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'restoration_indices'), position=11, insert_id=42596)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=12, insert_id=42597)",
            "Update(target_node=ASTNode(type=identifier, text=restoration_indices), value='permutation_index')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 6,
        "neg_line": [
            "-return sorted_tensor, sorted_sequence_lengths, restoration_indices"
        ],
        "pos_line": [
            "+return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index"
        ],
        "core_change": "-return sorted_tensor, sorted_sequence_lengths, restoration_indices +return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index",
        "core_API": "long"
    },
    {
        "commit_hash": "c61e22a8e0e393b7d701611437f595656cf16003",
        "index": "b799337c..930c2f4f 100644",
        "commit_message": "Fix unit test failures with torch==1.1.0 (#1840)\n\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_quantile():",
            "",
            "",
            "def test_pi():",
            "-    x = torch.empty(1000).log_normal_(0, 1)",
            "+    x = torch.randn(1000).exp()",
            "assert_equal(pi(x, prob=0.8), quantile(x, probs=[0.1, 0.9]))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=log_normal_), value='exp')",
            "Update(target_node=ASTNode(type=identifier, text=empty), value='randn')",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7,
        "neg_line": [
            "-x = torch.empty(1000).log_normal_(0, 1)"
        ],
        "pos_line": [
            "+x = torch.randn(1000).exp()"
        ],
        "core_change": "-x = torch.empty(1000).log_normal_(0, 1) +x = torch.randn(1000).exp()",
        "core_API": "empty"
    },
    {
        "commit_hash": "484dce11ecb92aab13c29d26b57cbf8c7522ceeb",
        "index": "8f98cb8ac..c36f7287f 100644",
        "commit_message": "[bugfix] TPU + all_gather + SingleTPU shouldn't call xm.all_gather (#6296)\n\n* resolve an issue with TPU\n\n* update\n\n* add changelog\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "add condition check for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TPUAccelerator(Accelerator):",
            "Return:",
            "A tensor of shape (world_size, batch, ...)",
            "\"\"\"",
            "-        return xm.all_gather(tensor, group=group, sync_grads=sync_grads)",
            "+        # todo: Add support for backward with all_gather",
            "+        if torch.distributed.is_initialized():",
            "+            return xm.all_gather(tensor, group=group, sync_grads=sync_grads)",
            "+        return tensor"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('if_statement', None), position=3, insert_id=545019)",
            "Insert(target_node=ASTNode(type=ERROR), node=('return_statement', None), position=4, insert_id=545020)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=545021)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=545022)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=545023)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=545024)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=545025)",
            "Insert(target_node=IN(type=return_statement), node=('identifier', 'tensor'), position=1, insert_id=545026)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=545027)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=545028)",
            "Move(target_node=IN(type=block), node=ASTNode(type=return_statement), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=545029)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=545030)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_initialized'), position=2, insert_id=545031)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=545032)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=545033)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=545034)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=545035)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=545036)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 8,
        "neg_line": [
            "-return xm.all_gather(tensor, group=group, sync_grads=sync_grads)"
        ],
        "pos_line": [
            "+# todo: Add support for backward with all_gather",
            "+if torch.distributed.is_initialized():",
            "+return xm.all_gather(tensor, group=group, sync_grads=sync_grads)",
            "+return tensor"
        ],
        "core_change": "-return xm.all_gather(tensor, group=group, sync_grads=sync_grads) +# todo: Add support for backward with all_gather +if torch.distributed.is_initialized(): +return xm.all_gather(tensor, group=group, sync_grads=sync_grads) +return tensor",
        "core_API": "all_gather"
    },
    {
        "commit_hash": "26dd041c6e45379141302e2d293ab4cd9cf805d4",
        "index": "1e839e767..c73afc096 100644",
        "commit_message": "Add Swin2SR (#19784)\n\n* First draft\n\n* Add more improvements\n\n* Improve forward pass\n\n* Fix layernorm\n\n* Add upscaler\n\n* More improvements\n\n* More improvements\n\n* More improvements\n\n* Improve conversion script\n\n* Add preprocessing\n\n* Make output match original implementation\n\n* Add additional attributes\n\n* Add support for more models\n\n* Support more models\n\n* Add support for real world sr\n\n* Add initial Swin2SRFeatureExtractor\n\n* Add ImageSuperResolutionOutput\n\n* Make more tests pass\n\n* Use BaseModelOutput\n\n* Fix one more test\n\n* Fix more tests\n\n* Fix another test\n\n* Fix all tests\n\n* Rename to Swin2SRImageProcessor\n\n* Fix toctree\n\n* Fix toctree\n\n* Fix rebase\n\n* Improve Swin2SRImageProcessor\n\n* Remove feature extractor file\n\n* Improve model\n\n* Improve conversion script\n\n* Fix integration test\n\n* Fix init\n\n* Fix conversion script\n\n* Address comments\n\n* Improve upsampler\n\n* Add NearestConvUpsampler\n\n* Improve pixel shuffle upsampler\n\n* Improve auxiliary upsampler\n\n* Improve conversion script\n\n* Rename conv_last to final_convolution\n\n* Fix rebase\n\n* Improve upsample module\n\n* Add padding to image processor\n\n* Fix bug\n\n* Update padding\n\n* Remove print statement and fix integration test\n\n* Improve docs\n\n* Add image processor tests\n\n* Convert all checkpoints, fix tests√©\n\n* Remove print statements\n\n* Fix import\n\nCo-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "change class name",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Swinv2SelfAttention(nn.Module):",
            "query_layer = self.transpose_for_scores(mixed_query_layer)",
            "",
            "# cosine attention",
            "-        attention_scores = F.normalize(query_layer, dim=-1) @ F.normalize(key_layer, dim=-1).transpose(-2, -1)",
            "+        attention_scores = nn.functional.normalize(query_layer, dim=-1) @ nn.functional.normalize(",
            "+            key_layer, dim=-1",
            "+        ).transpose(-2, -1)",
            "logit_scale = torch.clamp(self.logit_scale, max=math.log(1.0 / 0.01)).exp()",
            "attention_scores = attention_scores * logit_scale",
            "relative_position_bias_table = self.continuous_position_bias_mlp(self.relative_coords_table).view("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1533278)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1533279)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=F), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1533280)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1533281)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1533282)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=F), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1533283)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 9,
        "neg_line": [
            "-attention_scores = F.normalize(query_layer, dim=-1) @ F.normalize(key_layer, dim=-1).transpose(-2, -1)"
        ],
        "pos_line": [
            "+attention_scores = nn.functional.normalize(query_layer, dim=-1) @ nn.functional.normalize(",
            "+key_layer, dim=-1",
            "+).transpose(-2, -1)"
        ],
        "core_change": "-attention_scores = F.normalize(query_layer, dim=-1) @ F.normalize(key_layer, dim=-1).transpose(-2, -1) +attention_scores = nn.functional.normalize(query_layer, dim=-1) @ nn.functional.normalize( +key_layer, dim=-1 +).transpose(-2, -1)",
        "core_API": "transpose_for_scores"
    },
    {
        "commit_hash": "be348cc33925738825ab40dd6eacdfe4afd4e215",
        "index": "ef282e3..7c610e8 100644",
        "commit_message": "Validate --task speed CPU fix (#10244)\n\n\n",
        "file": "yolov5.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def main(opt):",
            "",
            "else:",
            "weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]",
            "-        opt.half = True  # FP16 for fastest results",
            "+        opt.half = torch.cuda.is_available() and opt.device != 'cpu'  # FP16 for fastest results",
            "if opt.task == 'speed':  # speed benchmarks",
            "# python val.py --task speed --data coco.yaml --batch 1 --weights yolov5n.pt yolov5s.pt...",
            "opt.conf_thres, opt.iou_thres, opt.save_json = 0.25, 0.45, False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('boolean_operator', None), position=2, insert_id=1881295)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=1881296)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1881297)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=1881298)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1881299)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1881300)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1881301)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=1881302)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', \"'cpu'\"), position=2, insert_id=1881303)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1881304)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1881305)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_available'), position=2, insert_id=1881306)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1881307)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1881308)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'opt'), position=0, insert_id=1881309)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1881310)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1881311)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1881312)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1881313)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1881314)",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 10,
        "neg_line": [
            "-opt.half = True  # FP16 for fastest results"
        ],
        "pos_line": [
            "+opt.half = torch.cuda.is_available() and opt.device != 'cpu'  # FP16 for fastest results"
        ],
        "core_change": "-opt.half = True  # FP16 for fastest results +opt.half = torch.cuda.is_available() and opt.device != 'cpu'  # FP16 for fastest results",
        "core_API": "is_available"
    },
    {
        "commit_hash": "faef098054c613e5f504a4b025ce8262e320db4d",
        "index": "edb4a513a..33ecd0c23 100644",
        "commit_message": "Revert fix on module name for hooked functionc\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "revert the fix",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class TorchHook:",
            "if type(native_func) in [types.FunctionType, types.BuiltinFunctionType]:",
            "# 3. Build the hooked function",
            "new_func = self.get_hooked_func(native_func)",
            "-                # 4. Move the native function to its original module",
            "-                # /!\\ Can be different from the torch_module!",
            "-                # Ex: in torch.py `torch.argmax = torch.functional.argmax`",
            "-                # ... So torch.argmax.__module__ is 'torch.functional' != 'torch'",
            "-                setattr(eval(native_func.__module__), f\"native_{func}\", native_func)",
            "+                # 4. Move the native function",
            "+                setattr(torch_module, f\"native_{func}\", native_func)",
            "# 5. Put instead the hooked one",
            "setattr(torch_module, func, new_func)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=eval), value='torch_module')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=eval), position=1)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=native_func))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__module__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 5,
        "AST_diff_line": 10,
        "number": 11,
        "neg_line": [
            "-# 4. Move the native function to its original module",
            "-# /!\\ Can be different from the torch_module!",
            "-# Ex: in torch.py `torch.argmax = torch.functional.argmax`",
            "-# ... So torch.argmax.__module__ is 'torch.functional' != 'torch'",
            "-setattr(eval(native_func.__module__), f\"native_{func}\", native_func)"
        ],
        "pos_line": [
            "+# 4. Move the native function",
            "+setattr(torch_module, f\"native_{func}\", native_func)"
        ],
        "core_change": "-# 4. Move the native function to its original module -# /!\\ Can be different from the torch_module! -# Ex: in torch.py `torch.argmax = torch.functional.argmax` -# ... So torch.argmax.__module__ is 'torch.functional' != 'torch' -setattr(eval(native_func.__module__), f\"native_{func}\", native_func) +# 4. Move the native function +setattr(torch_module, f\"native_{func}\", native_func)",
        "core_API": "get_hooked_func"
    },
    {
        "commit_hash": "a828315185a9dc8b21ec8e5dbead9044caf0d3a2",
        "index": "0ada14cc..e7fe0752 100644",
        "commit_message": "[Enhance] Added dtype flag to geometry module (#820)\n\n* Added docs and tests\n\n* Fixed bug for batch prob generator\n\n* Fixed typo\n\n* Fixed mypy\n\n* Fixed mypy\n\n* Added warning\n\n* Updated tests for geometry\n\n* Added dtype for conversions\n\n* Completed tests\n\n* Fixed homography\n\n* Fixed pinhole camera\n\n* Fixed tests\n\n* Fix geometry testing bugs\n\n* Fixed augmentation tests\n\n* code updated\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_rotation_matrix2d(",
            "",
            "# create output tensor",
            "batch_size: int = center.shape[0]",
            "-    one = torch.tensor(1.).to(center.device)",
            "+    one = torch.tensor(1., device=center.device, dtype=center.dtype)",
            "M: torch.Tensor = torch.zeros(",
            "batch_size, 2, 3, device=center.device, dtype=center.dtype)",
            "M[..., 0:2, 0:2] = scaled_rotation"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=432266)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=432267)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=432268)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=432269)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=432270)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=432271)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=432272)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=432273)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=432274)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'center'), position=0, insert_id=432275)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=432276)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=432277)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 12,
        "neg_line": [
            "-one = torch.tensor(1.).to(center.device)"
        ],
        "pos_line": [
            "+one = torch.tensor(1., device=center.device, dtype=center.dtype)"
        ],
        "core_change": "-one = torch.tensor(1.).to(center.device) +one = torch.tensor(1., device=center.device, dtype=center.dtype)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "619f984c362a2e5fd8f04ae24ea0eb8ce9d9e57a",
        "index": "b1aa96088..b535a9ddb 100644",
        "commit_message": "Option to provide seed to random generators to ensure reproducibility (#1572)\n\n* Option to provide seed to random generators to ensure reproducibility\n\nI added small function in utilities which imports torch, numpy, python\nrandom and sets seed for all of the libraries to ensure reproducibility\nof results.\n\n* Apply recommendations from core contributors on seeding\n\n1. Moved the seeding code to another file\n2. Make deterministic as a parameter for trainer class\n3. Add assertions for seeding numpy\n4. Added warnings\n5. torch.manual_seed should be enough for seeding torch\n\n* Revert \"Apply recommendations from core contributors on seeding\"\n\nThis reverts commit a213c8e6882eec8a9e7408b9418926d2db7c5461.\n\n* Revert \"Revert \"Apply recommendations from core contributors on seeding\"\"\n\nThis reverts commit 59b2da53c62878de7aab0aa3feb3115e105eea06.\n\n* Change in test, for correct seeding\n\n* Allow seed equal to 0\n\n* Allow seed to be uint32.max\n\n* Added deterministic to benchmarks\n\n* Cuda manual seed as in benchmark seeding\n\n* Seeding should be done before model initialization\n\n* cuda manual_seed is not necessary\n\n* Fixing seed test_cpu_lbfgs\n\nOn some seeds seems like lbfgs doesn't converge.\nSo I fixed the seed during testing.\n\n* rebasing issue with old reproducibility.py\n\n* Improved documentation and ability to seed before initializing Train\nclass\n\n* Change in docs\n\n* Removed seed from trainer, update for documentation\n\n* Typo in the docs\n\n* Added seed_everything to _all_\n\n* Fixing old changes\n\n* Model initialization should be earlier then Trainer\n\n* Update pytorch_lightning/trainer/__init__.py\n\nFrom Example to testcode\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Fixing according to the contributors suggestions\n\n* Moving horovod deterministic to Trainer class\n\n* deterministic flag affects horovod docs update\n\n* Improved static typing\n\n* Added deterministic to test runners of horovod\n\nIt is failing on some versions, not very predictable\n\n* static seeds for horovod tests\n\n* Change for reset_seed function in tests\n\n* Seeding horovod using reset_seed from tutils\n\n* Update pytorch_lightning/trainer/__init__.py\n\n* chlog\n\n* Update trainer.py\n\n* change \"testcode\" to \"Example\" in trainer init documentation\n\n* Update pytorch_lightning/trainer/seed.py, first line in comment\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def lightning_loop(MODEL, num_runs=10, num_epochs=10):",
            "early_stop_callback=False,",
            "checkpoint_callback=False,",
            "distributed_backend='dp',",
            "+            deterministic=True,",
            ")",
            "trainer.fit(model)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=581411)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=581412)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'deterministic'), position=0, insert_id=581413)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=581414)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=2, insert_id=581415)",
            "Insert(target_node=IN(type=expression_list), node=('true', 'True'), position=0, insert_id=581416)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=581417)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 7,
        "number": 14,
        "neg_line": [],
        "pos_line": [
            "+deterministic=True,"
        ],
        "core_change": "+deterministic=True,",
        "core_API": "fit"
    },
    {
        "commit_hash": "dcb9ebc26892de4f8002984f64ebfede6c19e329",
        "index": "7220790..221352b 100644",
        "commit_message": "save/load model only for rank==0 (#212)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/212\n\nSave/load can break for distributed training with multiple processes writing to the same file.  Fix by only doing it for rank 0.\n\nReviewed By: hikushalhere, seayoung1112\n\nDifferential Revision: D13704156\n\nfbshipit-source-id: cb468f76ccda93e29735ab7badb130fedf946df9\n\n",
        "file": "pytext.txt.json",
        "label": "yes",
        "comments": "add condition check for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(TrainerBase):",
            "break",
            "sys.stdout.flush()",
            "",
            "-        model.load_state_dict(torch.load(best_model_path))",
            "+        if rank == 0:",
            "+            model.load_state_dict(torch.load(best_model_path))",
            "return model, best_metric",
            "",
            "def _run_epoch("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=887630)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=887631)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=887632)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=887633)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=887634)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'rank'), position=0, insert_id=887635)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=887636)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '0'), position=2, insert_id=887637)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 15,
        "neg_line": [
            "-model.load_state_dict(torch.load(best_model_path))"
        ],
        "pos_line": [
            "+if rank == 0:",
            "+model.load_state_dict(torch.load(best_model_path))"
        ],
        "core_change": "-model.load_state_dict(torch.load(best_model_path)) +if rank == 0: +model.load_state_dict(torch.load(best_model_path))",
        "core_API": "flush"
    },
    {
        "commit_hash": "49f5b931410bc2e56378f20a15e8ac919e0efb88",
        "index": "93a089e2..eedb2efe 100644",
        "commit_message": "Refactor topological part of `engine` module (#10023)\n\n* Refactor topological part of Keras engine.\n\n* Fix imports\n\n* Fix merge mixup.\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "change class name",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def test_preprocess_weights_for_loading_gru_incompatible():",
            "",
            "def assert_not_compatible(src, dest, message):",
            "with pytest.raises(ValueError) as ex:",
            "-            keras.engine.topology.preprocess_weights_for_loading(",
            "+            keras.engine.saving.preprocess_weights_for_loading(",
            "dest, initialize_weights(src).get_weights())",
            "assert message in ex.value.message"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=topology), value='saving')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 16,
        "neg_line": [
            "-keras.engine.topology.preprocess_weights_for_loading("
        ],
        "pos_line": [
            "+keras.engine.saving.preprocess_weights_for_loading("
        ],
        "core_change": "-keras.engine.topology.preprocess_weights_for_loading( +keras.engine.saving.preprocess_weights_for_loading(",
        "core_API": "raises"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "76cd4b830..391322da4 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "change class name",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "Returns:",
            "\"\"\"",
            "",
            "",
            "-class BERTScore(nlp.Metric):",
            "+class BERTScore(datasets.Metric):",
            "def _info(self):",
            "-        return nlp.MetricInfo(",
            "+        return datasets.MetricInfo(",
            "description=_DESCRIPTION,",
            "citation=_CITATION,",
            "homepage=\"https://github.com/Tiiiger/bert_score\",",
            "inputs_description=_KWARGS_DESCRIPTION,",
            "-            features=nlp.Features(",
            "+            features=datasets.Features(",
            "{",
            "-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-                    \"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+                    \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "}",
            "),",
            "codebase_urls=[\"https://github.com/Tiiiger/bert_score\"],"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 17,
        "neg_line": [
            "-class BERTScore(nlp.Metric):",
            "-return nlp.MetricInfo(",
            "-features=nlp.Features(",
            "-\"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-\"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"),"
        ],
        "pos_line": [
            "+class BERTScore(datasets.Metric):",
            "+return datasets.MetricInfo(",
            "+features=datasets.Features(",
            "+\"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+\"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),"
        ],
        "core_change": "-class BERTScore(nlp.Metric): +class BERTScore(datasets.Metric): -return nlp.MetricInfo( +return datasets.MetricInfo( -features=nlp.Features( +features=datasets.Features( -\"predictions\": nlp.Value(\"string\", id=\"sequence\"), -\"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"), +\"predictions\": datasets.Value(\"string\", id=\"sequence\"), +\"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),",
        "core_API": "MetricInfo"
    },
    {
        "commit_hash": "547eb8d12c1b29ad9644160ca5dcb223cc1d0ce6",
        "index": "3e129bc4..d665dfff 100644",
        "commit_message": "[Fix]: fix mask rcnn training stuck problem when there is no positive rois (#3713)\n\n* Fix mask rcnn stuck problem when there is no positive rois\n\n* support non pos inference in cascade methods, link CU-49tawu\n\n* print mmcv version in CI\n\n* use mmcv repo to check wrappers\n\n* change cpu build\n\n* upgrade mmcv requirements and change ci back\n\n* use pre-built whl in CI\n",
        "file": "mmdetection.txt.json",
        "label": "no",
        "comments": "change class",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class CoarseMaskHead(FCNMaskHead):",
            "for i in range(num_fcs):",
            "fc_in_channels = (",
            "last_layer_dim if i == 0 else self.fc_out_channels)",
            "-            self.fcs.append(nn.Linear(fc_in_channels, self.fc_out_channels))",
            "+            self.fcs.append(Linear(fc_in_channels, self.fc_out_channels))",
            "last_layer_dim = self.fc_out_channels",
            "output_channels = self.num_classes * self.output_area",
            "-        self.fc_logits = nn.Linear(last_layer_dim, output_channels)",
            "+        self.fc_logits = Linear(last_layer_dim, output_channels)",
            "",
            "def init_weights(self):",
            "for m in self.fcs.modules():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=Linear), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=Linear), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 18,
        "neg_line": [
            "-self.fcs.append(nn.Linear(fc_in_channels, self.fc_out_channels))",
            "-self.fc_logits = nn.Linear(last_layer_dim, output_channels)"
        ],
        "pos_line": [
            "+self.fcs.append(Linear(fc_in_channels, self.fc_out_channels))",
            "+self.fc_logits = Linear(last_layer_dim, output_channels)"
        ],
        "core_change": "-self.fcs.append(nn.Linear(fc_in_channels, self.fc_out_channels)) +self.fcs.append(Linear(fc_in_channels, self.fc_out_channels)) -self.fc_logits = nn.Linear(last_layer_dim, output_channels) +self.fc_logits = Linear(last_layer_dim, output_channels)",
        "core_API": "append"
    },
    {
        "commit_hash": "6824f52cbfb6303aad67caf93c41cf0faa57f908",
        "index": "ea686cfe1..efda38e4e 100644",
        "commit_message": "Rename config and environment variable for in memory max size (#2454)\n\n* Rename config and env variable IN_MEMORY_MAX_SIZE\n\n* Rename also in documentation\n\n* Fix style\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "style fix",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_load_from_disk_with_default_in_memory(",
            "current_dataset_size = 512  # arrow file size = 512, in-memory dataset size = 148",
            "if max_in_memory_dataset_size == \"default\":",
            "# default = 250 * 2 ** 20",
            "-        max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "+        max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE",
            "else:",
            "-        monkeypatch.setattr(datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)",
            "+        monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", max_in_memory_dataset_size)",
            "if max_in_memory_dataset_size:",
            "expected_in_memory = current_dataset_size < max_in_memory_dataset_size",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1785057)",
            "Update(target_node=ASTNode(type=identifier, text=HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES), value='IN_MEMORY_MAX_SIZE')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=0, insert_id=1785058)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1785059)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=1785060)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\"), value='\"IN_MEMORY_MAX_SIZE\"')",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=config))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 19,
        "neg_line": [
            "-max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "-monkeypatch.setattr(datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)"
        ],
        "pos_line": [
            "+max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE",
            "+monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", max_in_memory_dataset_size)"
        ],
        "core_change": "-max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES +max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE -monkeypatch.setattr(datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size) +monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", max_in_memory_dataset_size)",
        "core_API": "setattr"
    },
    {
        "commit_hash": "569c089b823deb114ecbb4112bde427edd1abaaf",
        "index": "05dbfe0a..0b347ad7 100644",
        "commit_message": "fixes sequence tagger class\n\n",
        "file": "flair.txt.json",
        "label": "no",
        "comments": "remove constraint",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceTagger(flair.nn.DefaultClassifier):",
            "for sentence in batch:",
            "sentence.remove_labels(label_name)",
            "",
            "-            loss = self._calculate_loss(features, gold_labels)",
            "-",
            "if return_loss:",
            "+                loss = self._calculate_loss(features, gold_labels)",
            "overall_loss += loss[0]",
            "label_count += loss[1]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=235366)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=235367)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', 'return_loss'), position=1, insert_id=235368)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=235369)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=235370)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=return_loss))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 21,
        "neg_line": [
            "-loss = self._calculate_loss(features, gold_labels)",
            "-"
        ],
        "pos_line": [
            "+loss = self._calculate_loss(features, gold_labels)"
        ],
        "core_change": "-loss = self._calculate_loss(features, gold_labels) - +loss = self._calculate_loss(features, gold_labels)",
        "core_API": "remove_labels"
    },
    {
        "commit_hash": "4336e1d33818b9f7a9d5757cacc6b5e5d7dc9db8",
        "index": "e0580107..a80b2080 100644",
        "commit_message": "fix unittests for the latest updates\n\n",
        "file": "TTS.txt.json",
        "label": "yes",
        "comments": "add param for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TacotronTrainTest(unittest.TestCase):",
            "optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "for i in range(5):",
            "mel_out, linear_out, align, stop_tokens = model.forward(",
            "-                input, input_lengths, mel_spec)",
            "+                input, input_lengths, mel_spec, speaker_ids)",
            "optimizer.zero_grad()",
            "loss = criterion(mel_out, mel_spec, mel_lengths)",
            "stop_loss = criterion_st(stop_tokens, stop_targets)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1272610)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'speaker_ids'), position=7, insert_id=1272611)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 23,
        "neg_line": [
            "-input, input_lengths, mel_spec)"
        ],
        "pos_line": [
            "+input, input_lengths, mel_spec, speaker_ids)"
        ],
        "core_change": "-input, input_lengths, mel_spec) +input, input_lengths, mel_spec, speaker_ids)",
        "core_API": "Adam"
    },
    {
        "commit_hash": "b1ff0b2ae7d368b7db3a8a8472a29cc195d278d8",
        "index": "578ce0122..341733b8a 100644",
        "commit_message": "Fix bug in examples: double wrap into DataParallel during eval\n\n\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add condition check for type fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def evaluate(args, model, tokenizer, prefix=\"\", test=False):",
            "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)",
            "",
            "# multi-gpu evaluate",
            "-        if args.n_gpu > 1:",
            "+        if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):",
            "model = torch.nn.DataParallel(model)",
            "",
            "# Eval!"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=1545553)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1545554)",
            "Insert(target_node=IN(type=boolean_operator), node=('not_operator', None), position=2, insert_id=1545555)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=1545556)",
            "Insert(target_node=IN(type=not_operator), node=('call', None), position=1, insert_id=1545557)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=1545558)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1545559)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1545560)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'model'), position=1, insert_id=1545561)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1545562)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=1545563)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1545564)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1545565)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1545566)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'DataParallel'), position=2, insert_id=1545567)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1545568)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1545569)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1545570)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 24,
        "neg_line": [
            "-if args.n_gpu > 1:"
        ],
        "pos_line": [
            "+if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):"
        ],
        "core_change": "-if args.n_gpu > 1: +if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):",
        "core_API": "DataParallel"
    },
    {
        "commit_hash": "2b32895c3178a412a701aba09dbad821d8acbd1a",
        "index": "d33bf1d2..6b35f0f1 100644",
        "commit_message": "fix cuda test issues\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestMotionBlur:",
            ") -> torch.Tensor:",
            "return kornia.filters.motion_blur(input, ksize, angle, direction)",
            "",
            "-        img = torch.rand(2, 3, 4, 5)",
            "+        img = torch.rand(2, 3, 4, 5).to(device)",
            "ksize = 5",
            "angle = 65.",
            "direction = .1"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=451106)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=451107)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=451108)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=451109)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=451110)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=451111)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=451112)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 25,
        "neg_line": [
            "-img = torch.rand(2, 3, 4, 5)"
        ],
        "pos_line": [
            "+img = torch.rand(2, 3, 4, 5).to(device)"
        ],
        "core_change": "-img = torch.rand(2, 3, 4, 5) +img = torch.rand(2, 3, 4, 5).to(device)",
        "core_API": "motion_blur"
    },
    {
        "commit_hash": "6ba2231d7227825dfb76e9df161824d04234e69f",
        "index": "972d90b5..f1838ebe 100644",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "no",
        "comments": "update number",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DiTPipelineIntegrationTests(unittest.TestCase):",
            "\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"",
            "f\"/dit/{word}_fp16.npy\"",
            ")",
            "-            assert np.abs((expected_image - image).max()) < 1e-2",
            "+",
            "+            assert np.abs((expected_image - image).max()) < 7.5e-1"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1e-2), value='7.5e-1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 26,
        "neg_line": [
            "-assert np.abs((expected_image - image).max()) < 1e-2"
        ],
        "pos_line": [
            "+",
            "+assert np.abs((expected_image - image).max()) < 7.5e-1"
        ],
        "core_change": "-assert np.abs((expected_image - image).max()) < 1e-2 + +assert np.abs((expected_image - image).max()) < 7.5e-1",
        "core_API": "abs"
    },
    {
        "commit_hash": "a19f102c65b943a668616df5f3b46cfb4376e04c",
        "index": "f8af9deb..ad6b299b 100644",
        "commit_message": "Misc fixes (#88)\n\n* make log_dir if it doesn't exist\n\n* use tqdm for all dataset readers\n\n* save vocab if log_dir exists\n\n* fix logging error\n\n* get around passing batch_first to custom lstms\n\n* fix srl default params\n\n* tentative fix for tensor creation\n\n* use tdqm at correct abstraction in srl reader\n\n* don't do boolean checks on tensors\n\n* use an actual tensor in get_dropout_mask to preserve type\n\n* ensure tensors are on cpu for logging in training loop\n\n* raise if Datasets from readers are empty\n\n* use os.path.join fr logging\n\n* update config\n\n* use incorrect american spelling of labeller\n\n* use more sensible order for bulding things in train\n\n* try different lstm in model\n\n* unpack validation tensors into forward\n\n* fix byteTensor overflow bug in masking\n\n* pylint\n\n* fix merge\n\n* fixes for Matt\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "add condition check for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _Seq2VecWrapper:",
            "def from_params(self, params: Params) -> PytorchSeq2VecWrapper:",
            "if not params.pop('batch_first', True):",
            "raise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")",
            "-        params['batch_first'] = True",
            "+        if self._module_class in self.PYTORCH_MODELS:",
            "+            params['batch_first'] = True",
            "module = self._module_class(**params.as_dict())",
            "return PytorchSeq2VecWrapper(module)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=46940)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=46941)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=46942)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=46943)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=46944)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=46945)",
            "Insert(target_node=IN(type=comparison_operator), node=('in', 'in'), position=1, insert_id=46946)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=2, insert_id=46947)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=46948)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=46949)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_module_class'), position=2, insert_id=46950)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=46951)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=46952)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'PYTORCH_MODELS'), position=2, insert_id=46953)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 30,
        "neg_line": [
            "-params['batch_first'] = True"
        ],
        "pos_line": [
            "+if self._module_class in self.PYTORCH_MODELS:",
            "+params['batch_first'] = True"
        ],
        "core_change": "-params['batch_first'] = True +if self._module_class in self.PYTORCH_MODELS: +params['batch_first'] = True",
        "core_API": "pop"
    },
    {
        "commit_hash": "aa7ff2a1972f3865883e10ba28c5414cdebe8e3b",
        "index": "684e5833..3caac25e 100644",
        "commit_message": "Fixed non-square highres fix generation\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):",
            "return samples",
            "",
            "x = create_random_tensors([opt_C, self.firstphase_height // opt_f, self.firstphase_width // opt_f], seeds=seeds, subseeds=subseeds, subseed_strength=self.subseed_strength, seed_resize_from_h=self.seed_resize_from_h, seed_resize_from_w=self.seed_resize_from_w, p=self)",
            "-        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x))",
            "+        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x, first_phase=True))",
            "",
            "samples = samples[:, :, self.truncate_y//2:samples.shape[2]-self.truncate_y//2, self.truncate_x//2:samples.shape[3]-self.truncate_x//2]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1139521)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1139522)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'first_phase'), position=0, insert_id=1139523)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1139524)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=1139525)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 31,
        "neg_line": [
            "-samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x))"
        ],
        "pos_line": [
            "+samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x, first_phase=True))"
        ],
        "core_change": "-samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x)) +samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x, first_phase=True))",
        "core_API": "sample"
    },
    {
        "commit_hash": "72cc9dd414ee4befbadf9bd3e4804cb2e0908e9c",
        "index": "66056c19..97619eb0 100644",
        "commit_message": "Add exclude to initializer applicator, clean up types (#96)\n\n* Add exclude to initializer applicator, clean up types\n\n* Fix registrable test, and warning in nn.util\n\n* Add back in type ignore statement; I thought it passed...\n\n* Remove brackets\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "change API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sort_batch_by_length(tensor: torch.autograd.Variable, sequence_lengths: torc",
            "sorted_tensor = tensor.index_select(0, permutation_index)",
            "# This is the equivalent of zipping with index, sorting by the original",
            "# sequence lengths and returning the now sorted indices.",
            "-    index_range = Variable(torch.range(0, len(sequence_lengths) - 1).long())",
            "+    index_range = Variable(torch.arange(0, len(sequence_lengths)).long())",
            "_, reverse_mapping = permutation_index.sort(0, descending=False)",
            "restoration_indices = index_range.index_select(0, reverse_mapping)",
            "return sorted_tensor, sorted_sequence_lengths, restoration_indices"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=range), value='arange')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=3)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 32,
        "neg_line": [
            "-index_range = Variable(torch.range(0, len(sequence_lengths) - 1).long())"
        ],
        "pos_line": [
            "+index_range = Variable(torch.arange(0, len(sequence_lengths)).long())"
        ],
        "core_change": "-index_range = Variable(torch.range(0, len(sequence_lengths) - 1).long()) +index_range = Variable(torch.arange(0, len(sequence_lengths)).long())",
        "core_API": "index_select"
    },
    {
        "commit_hash": "09fc038e202c5411c4292d065f4ad04369bd87fc",
        "index": "4a3c2d812..5b67207d3 100644",
        "commit_message": "fix linter and add docs\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "doc fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LabelSmoothing(nn.Module):",
            "self.normalize_length = normalize_length",
            "",
            "def forward(self, x, target):",
            "+        \"\"\"Compute loss between x and target",
            "+",
            "+        :param torch.Tensor x: prediction (batch, seqlen, class)",
            "+        :param torch.Tensor target: target signal masked with self.padding_id (batch, seqlen)",
            "+        :return: scalar float value",
            "+        :rtype torch.Tensor",
            "+        \"\"\"",
            "assert x.size(2) == self.size",
            "batch_size = x.size(0)",
            "x = x.view(-1, self.size)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=175117)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=175118)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Compute loss between x and target\\n\\n        :param torch.Tensor x: prediction (batch, seqlen, class)\\n        :param torch.Tensor target: target signal masked with self.padding_id (batch, seqlen)\\n        :return: scalar float value\\n        :rtype torch.Tensor\\n        \"\"\"'), position=0, insert_id=175119)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 6,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 33,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Compute loss between x and target",
            "+",
            "+:param torch.Tensor x: prediction (batch, seqlen, class)",
            "+:param torch.Tensor target: target signal masked with self.padding_id (batch, seqlen)",
            "+:return: scalar float value",
            "+:rtype torch.Tensor",
            "+\"\"\""
        ],
        "core_change": "+\"\"\"Compute loss between x and target + +:param torch.Tensor x: prediction (batch, seqlen, class) +:param torch.Tensor target: target signal masked with self.padding_id (batch, seqlen) +:return: scalar float value +:rtype torch.Tensor +\"\"\"",
        "core_API": "size"
    },
    {
        "commit_hash": "bcc881c54dcf1d33f3fcfe3f899e5e0045e138a1",
        "index": "0e5100e4e..f50149caa 100644",
        "commit_message": "Gpu fix (#66)\n\n* fixed an ambiguity in sourcing the USE_CUDA and added .cuda to decoder and encoder rnns\n\n* added cuda method call on batches in all pytorch forward prop calls if the env variable is True\n\n* small fix in sourcing USE_CUDA\n\n* added .cuda to some nets that were missing it and to all the correct inputs, enabled cudnn auto-tunner\n\n* only enabling torch.backends.cudnn.benchmark if USE_CUDA is set to true\n\n* added some documentation and removed a line that was only used for debugging\n\n",
        "file": "mindsdb.txt.json",
        "label": "no",
        "comments": "no fix found",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BaseModel(nn.Module):",
            "\"\"\"",
            "logging.error('You must define a forward method for this model')",
            "pass",
            "-",
            "-",
            "-",
            "-",
            "-"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 34,
        "neg_line": [
            "-",
            "-",
            "-",
            "-",
            "-"
        ],
        "pos_line": [],
        "core_change": "- - - - -",
        "core_API": "error"
    },
    {
        "commit_hash": "c6519f29f0512e209906f8265e0d049085670304",
        "index": "bc990c1..3dd2d12 100644",
        "commit_message": "chamfer for empty pointclouds #1174\n\nSummary: Fix divide by zero for empty pointcloud in chamfer. Also for empty batches. In process, needed to regularize num_points_per_cloud for empty batches.\n\nReviewed By: kjchalup\n\nDifferential Revision: D36311330\n\nfbshipit-source-id: 3378ab738bee77ecc286f2110a5c8dc445960340\n\n",
        "file": "pytorch3d.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Pointclouds:",
            "self._compute_packed()",
            "return self._cloud_to_packed_first_idx",
            "",
            "-    def num_points_per_cloud(self):",
            "+    def num_points_per_cloud(self) -> torch.Tensor:",
            "\"\"\"",
            "Return a 1D tensor x with length equal to the number of clouds giving",
            "the number of points in each cloud."
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('->', '->'), position=3, insert_id=912605)",
            "Insert(target_node=ASTNode(type=function_definition), node=('type', None), position=4, insert_id=912606)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=912607)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=912608)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=912609)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=912610)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 35,
        "neg_line": [
            "-def num_points_per_cloud(self):"
        ],
        "pos_line": [
            "+def num_points_per_cloud(self) -> torch.Tensor:"
        ],
        "core_change": "-def num_points_per_cloud(self): +def num_points_per_cloud(self) -> torch.Tensor:",
        "core_API": "_compute_packed"
    },
    {
        "commit_hash": "f0aa242fd2e00c75e24e102f7ec86f009e62a3b3",
        "index": "d301ed8..064ae04 100644",
        "commit_message": "[MRG]Dcgan (#175)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* finish workable version of gan\n\n* add unit test and small refactoring\n\n* add unsupervised super class\n\n* Fix test_dcgan ran too long issue, put default param in unsupervised::generate(input_sample=None)\n\n* remove examples/gan.py from repo\n\n* add missing import\n\n* correct model_trainer signature\n\n",
        "file": "autokeras.txt.json",
        "label": "no",
        "comments": "typo fix",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "from keras.datasets import mnist",
            "from autokeras import ImageClassifier",
            "+import tensorflow",
            "",
            "if __name__ == '__main__':",
            "+    print(tensorflow.__version__)",
            "(x_train, y_train), (x_test, y_test) = mnist.load_data()",
            "-    x_train = x_train.reshape(x_train.shape + (1,))",
            "-    x_test = x_test.reshape(x_test.shape + (1,))",
            "-",
            "+    x_train = x_train.reshape(x_train.shape+(1,))",
            "+    x_test = x_test.reshape(x_test.shape+(1,))",
            "clf = ImageClassifier(verbose=True, augment=False)",
            "clf.fit(x_train, y_train, time_limit=12 * 60 * 60)",
            "clf.final_fit(x_train, y_train, x_test, y_test, retrain=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('import_statement', None), position=2, insert_id=2396186)",
            "Insert(target_node=IN(type=import_statement), node=('import', 'import'), position=0, insert_id=2396187)",
            "Insert(target_node=IN(type=import_statement), node=('dotted_name', None), position=1, insert_id=2396188)",
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=2396189)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorflow'), position=0, insert_id=2396190)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2396191)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2396192)",
            "Insert(target_node=IN(type=call), node=('identifier', 'print'), position=0, insert_id=2396193)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2396194)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2396195)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2396196)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2396197)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensorflow'), position=0, insert_id=2396198)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2396199)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__version__'), position=2, insert_id=2396200)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 37,
        "neg_line": [
            "-x_train = x_train.reshape(x_train.shape + (1,))",
            "-x_test = x_test.reshape(x_test.shape + (1,))",
            "-"
        ],
        "pos_line": [
            "+import tensorflow",
            "+print(tensorflow.__version__)",
            "+x_train = x_train.reshape(x_train.shape+(1,))",
            "+x_test = x_test.reshape(x_test.shape+(1,))"
        ],
        "core_change": "+import tensorflow +print(tensorflow.__version__) -x_train = x_train.reshape(x_train.shape + (1,)) -x_test = x_test.reshape(x_test.shape + (1,)) - +x_train = x_train.reshape(x_train.shape+(1,)) +x_test = x_test.reshape(x_test.shape+(1,))",
        "core_API": "load_data"
    },
    {
        "commit_hash": "8bbc956ff14852e6faf9bc21bba884c34630626f",
        "index": "ee543b1..98fe763 100644",
        "commit_message": "fix bug with misnamed variable in diffusion prior network\n\n",
        "file": "DALLE2-pytorch.txt.json",
        "label": "no",
        "comments": "typo fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DiffusionPriorNetwork(nn.Module):",
            "",
            "null_text_embeds = self.null_text_embeds.to(text_embed.dtype)",
            "",
            "-        text_embeds = torch.where(",
            "+        text_embed = torch.where(",
            "text_keep_mask,",
            "text_embed,",
            "null_text_embeds"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=text_embeds), value='text_embed')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 38,
        "neg_line": [
            "-text_embeds = torch.where("
        ],
        "pos_line": [
            "+text_embed = torch.where("
        ],
        "core_change": "-text_embeds = torch.where( +text_embed = torch.where(",
        "core_API": "to"
    },
    {
        "commit_hash": "c44e4f4e9e19e7eee1ac408129ca69399e3d0ade",
        "index": "f12b9132d..7769fe46d 100644",
        "commit_message": "Fix TF tests for 2.10 (#4956)\n\n* Only create the model once in test_tensorflow\n\n* Unpin TF version\n\n* make style\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TempSeedTest(TestCase):",
            "import tensorflow as tf",
            "from tensorflow.keras import layers",
            "",
            "+        model = layers.Dense(2)",
            "+",
            "def gen_random_output():",
            "-            model = layers.Dense(2)",
            "x = tf.random.uniform((1, 3))",
            "return model(x).numpy()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=3)",
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=4, insert_id=2566747)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=2566748)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'gen_random_output'), position=1, insert_id=2566749)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=parameters), position=2)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=2566750)",
            "Insert(target_node=IN(type=function_definition), node=('block', ''), position=4, insert_id=2566751)",
            "Delete(target_node=ASTNode(type=def, text=def))",
            "Delete(target_node=ASTNode(type=identifier, text=gen_random_output))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=function_definition))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 39,
        "neg_line": [
            "-model = layers.Dense(2)"
        ],
        "pos_line": [
            "+model = layers.Dense(2)",
            "+"
        ],
        "core_change": "+model = layers.Dense(2) + -model = layers.Dense(2)",
        "core_API": "Dense"
    },
    {
        "commit_hash": "c5189bdb019085841dbfeeb457b1f6682c7dbfbf",
        "index": "179dac8..f2f89d5 100644",
        "commit_message": "fix isinstance() validation in pytorch\n\n",
        "file": "tensorboardX.txt.json",
        "label": "yes",
        "comments": "update param for refactor fix",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "def makenp(x, modality=None):",
            "",
            "def pytorch_np(x, modality):",
            "import torch",
            "-    if isinstance(x, torch.autograd.variable.Variable):",
            "+    if isinstance(x, torch.autograd.Variable):",
            "x = x.data",
            "x = x.cpu().numpy()",
            "if modality == 'IMG':"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=variable))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 40,
        "neg_line": [
            "-if isinstance(x, torch.autograd.variable.Variable):"
        ],
        "pos_line": [
            "+if isinstance(x, torch.autograd.Variable):"
        ],
        "core_change": "-if isinstance(x, torch.autograd.variable.Variable): +if isinstance(x, torch.autograd.Variable):",
        "core_API": "cpu"
    },
    {
        "commit_hash": "b9bb417324c0d9013c505dc39c016ab9ca0e23c8",
        "index": "d2f4e29a3..b0e3ca2e1 100644",
        "commit_message": "Fix a typo relative_postion_if_large -> relative_position_if_large (#17366)\n\n\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "add log",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class T5Attention(nn.Module):",
            "is_small = relative_position < max_exact",
            "",
            "# The other half of the buckets are for logarithmically bigger bins in positions up to max_distance",
            "-        relative_postion_if_large = max_exact + (",
            "+        relative_position_if_large = max_exact + (",
            "torch.log(relative_position.float() / max_exact)",
            "/ math.log(max_distance / max_exact)",
            "* (num_buckets - max_exact)",
            ").to(torch.long)",
            "-        relative_postion_if_large = torch.min(",
            "-            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)",
            "+        relative_position_if_large = torch.min(",
            "+            relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)",
            ")",
            "",
            "-        relative_buckets += torch.where(is_small, relative_position, relative_postion_if_large)",
            "+        relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)",
            "return relative_buckets",
            "",
            "def compute_bias(self, query_length, key_length):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=relative_postion_if_large), value='relative_position_if_large')",
            "Update(target_node=ASTNode(type=identifier, text=relative_postion_if_large), value='relative_position_if_large')",
            "Update(target_node=ASTNode(type=identifier, text=relative_postion_if_large), value='relative_position_if_large')",
            "Update(target_node=ASTNode(type=identifier, text=relative_postion_if_large), value='relative_position_if_large')",
            "Update(target_node=ASTNode(type=identifier, text=relative_postion_if_large), value='relative_position_if_large')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 5,
        "number": 41,
        "neg_line": [
            "-relative_postion_if_large = max_exact + (",
            "-relative_postion_if_large = torch.min(",
            "-relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)",
            "-relative_buckets += torch.where(is_small, relative_position, relative_postion_if_large)"
        ],
        "pos_line": [
            "+relative_position_if_large = max_exact + (",
            "+relative_position_if_large = torch.min(",
            "+relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)",
            "+relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)"
        ],
        "core_change": "-relative_postion_if_large = max_exact + ( +relative_position_if_large = max_exact + ( -relative_postion_if_large = torch.min( -relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1) +relative_position_if_large = torch.min( +relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1) -relative_buckets += torch.where(is_small, relative_position, relative_postion_if_large) +relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)",
        "core_API": "log"
    },
    {
        "commit_hash": "9beeabbcedc47908d6e5cb33310396cb59e19375",
        "index": "05d444a84..9334df4b1 100644",
        "commit_message": "Removed unnecessary `_move_optimizer_state` method overrides (#10849)\n\n* Update tpu tp share same logic with ttp\n\n* run test\n\n* Update tpu_spawn.py\n\n* debug\n\n* Add changelog\n\n* Apply suggestions from code review\n\nCo-authored-by: Adrian W√§lchli <aedu.waelchli@gmail.com>\n\n* Update training_type_plugin.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update training_type_plugin.py\n\nCo-authored-by: Adrian W√§lchli <aedu.waelchli@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "change param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TrainingTypePlugin(ABC):",
            "self.lr_schedulers = schedulers",
            "",
            "def _move_optimizer_state(self, device: Optional[torch.device] = None) -> None:",
            "-        \"\"\"Moves the state of the optimizers to the GPU if needed.\"\"\"",
            "-        device = device or self.root_device",
            "+        \"\"\"Moves the state of the optimizers to the appropriate device if needed.\"\"\"",
            "for opt in self.optimizers:",
            "for p, v in opt.state.items():",
            "-                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device)",
            "+                # `self.root_device` would raise error if called outside the spawn process",
            "+                # while training on 8 and more cores.",
            "+                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device or self.root_device)",
            "",
            "def optimizer_state(self, optimizer: Optimizer) -> Dict[str, Tensor]:",
            "\"\"\"Returns state of an optimizer."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Moves the state of the optimizers to the GPU if needed.\"\"\"), value='\"\"\"Moves the state of the optimizers to the appropriate device if needed.\"\"\"')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=boolean_operator), position=7)",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=device))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 7,
        "number": 43,
        "neg_line": [
            "-\"\"\"Moves the state of the optimizers to the GPU if needed.\"\"\"",
            "-device = device or self.root_device",
            "-opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device)"
        ],
        "pos_line": [
            "+\"\"\"Moves the state of the optimizers to the appropriate device if needed.\"\"\"",
            "+# `self.root_device` would raise error if called outside the spawn process",
            "+# while training on 8 and more cores.",
            "+opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device or self.root_device)"
        ],
        "core_change": "-\"\"\"Moves the state of the optimizers to the GPU if needed.\"\"\" -device = device or self.root_device +\"\"\"Moves the state of the optimizers to the appropriate device if needed.\"\"\" -opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device) +# `self.root_device` would raise error if called outside the spawn process +# while training on 8 and more cores. +opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device or self.root_device)",
        "core_API": "items"
    },
    {
        "commit_hash": "a4553e6c6474e19b6b19f98687869773d9c7781e",
        "index": "ad4d456ba..78c1df7d3 100644",
        "commit_message": "Moving pipeline tests from `Narsil` to `hf-internal-testing`. (#14463)\n\n* Moving everything to `hf-internal-testing`.\n\n* Fixing test values.\n\n* Moving to other repo.\n\n* Last touch?\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class ImageSegmentationPipelineTests(unittest.TestCase, metaclass=PipelineTestCa",
            "",
            "import datasets",
            "",
            "-        dataset = datasets.load_dataset(\"Narsil/image_dummy\", \"image\", split=\"test\")",
            "+        dataset = datasets.load_dataset(\"hf-internal-testing/fixtures_image_utils\", \"image\", split=\"test\")",
            "",
            "batch = [",
            "Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\"),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"Narsil/image_dummy\"), value='\"hf-internal-testing/fixtures_image_utils\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 45,
        "neg_line": [
            "-dataset = datasets.load_dataset(\"Narsil/image_dummy\", \"image\", split=\"test\")"
        ],
        "pos_line": [
            "+dataset = datasets.load_dataset(\"hf-internal-testing/fixtures_image_utils\", \"image\", split=\"test\")"
        ],
        "core_change": "-dataset = datasets.load_dataset(\"Narsil/image_dummy\", \"image\", split=\"test\") +dataset = datasets.load_dataset(\"hf-internal-testing/fixtures_image_utils\", \"image\", split=\"test\")",
        "core_API": "load_dataset"
    },
    {
        "commit_hash": "9ba8cd82f09c5242c7981e9449e7ef73567497c9",
        "index": "1457013aa..09b525230 100644",
        "commit_message": "fix: hot fix of two graphs' conflict\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NerNetwork:",
            "return predictions_batch_no_pad",
            "",
            "def shutdown(self):",
            "-        self._sess.close()",
            "\\ No newline at end of file",
            "+        self._sess.close()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=2, insert_id=1925106)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1925107)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=def, text=def), position=0)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=identifier, text=shutdown), position=1)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=parameters), position=2)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=:, text=:), position=3)",
            "Insert(target_node=IN(type=function_definition), node=('ERROR', None), position=4, insert_id=1925108)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=block), position=5)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('ERROR', ' '), position=0, insert_id=1925109)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'No'), position=1, insert_id=1925110)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'newline'), position=2, insert_id=1925111)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'at'), position=3, insert_id=1925112)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'end'), position=4, insert_id=1925113)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'of'), position=5, insert_id=1925114)",
            "Delete(target_node=ASTNode(type=ERROR, text= ))",
            "Delete(target_node=ASTNode(type=identifier, text=No))",
            "Delete(target_node=ASTNode(type=identifier, text=newline))",
            "Delete(target_node=ASTNode(type=identifier, text=at))",
            "Delete(target_node=ASTNode(type=identifier, text=end))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=function_definition))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 47,
        "neg_line": [
            "-self._sess.close()"
        ],
        "pos_line": [
            "+self._sess.close()"
        ],
        "core_change": "-self._sess.close() +self._sess.close()",
        "core_API": "close"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "b02b4e00..5d26e033 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "remove comments",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LayerNorm(torch.nn.Module):",
            "self.beta = torch.nn.Parameter(torch.zeros(dimension))",
            "self.eps = eps",
            "",
            "-    def forward(self, tensor: torch.Tensor):  # pylint: disable=arguments-differ",
            "+    def forward(self, tensor: torch.Tensor):",
            "mean = tensor.mean(-1, keepdim=True)",
            "std = tensor.std(-1, unbiased=False, keepdim=True)",
            "return self.gamma * (tensor - mean) / (std + self.eps) + self.beta"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 49,
        "neg_line": [
            "-def forward(self, tensor: torch.Tensor):  # pylint: disable=arguments-differ"
        ],
        "pos_line": [
            "+def forward(self, tensor: torch.Tensor):"
        ],
        "core_change": "-def forward(self, tensor: torch.Tensor):  # pylint: disable=arguments-differ +def forward(self, tensor: torch.Tensor):",
        "core_API": "Parameter"
    },
    {
        "commit_hash": "57c88c0c7b86fb38715b72f5914bb208c7aa11e7",
        "index": "31f11182..51831add 100644",
        "commit_message": "Fix: `to_hetero` with `GCN` on single node types (#4279)\n\n* fix\n\n* typo\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "add condition check for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GraphConv(MessagePassing):",
            "self.lin.reset_parameters()",
            "",
            "def forward(self, x, edge_index):",
            "+        if isinstance(x, Tensor):",
            "+            x = (x, x)",
            "return self.propagate(edge_index, x=(self.lin(x[0]), x[1]))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=984962)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=984963)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=984964)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=984965)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=984966)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=984967)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=984968)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=984969)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=984970)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=984971)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x'), position=1, insert_id=984972)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=984973)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'Tensor'), position=3, insert_id=984974)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=984975)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=984976)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'x'), position=0, insert_id=984977)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=984978)",
            "Insert(target_node=IN(type=assignment), node=('tuple', None), position=2, insert_id=984979)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=984980)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'x'), position=1, insert_id=984981)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=984982)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'x'), position=3, insert_id=984983)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=984984)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 50,
        "neg_line": [],
        "pos_line": [
            "+if isinstance(x, Tensor):",
            "+x = (x, x)"
        ],
        "core_change": "+if isinstance(x, Tensor): +x = (x, x)",
        "core_API": "reset_parameters"
    },
    {
        "commit_hash": "196a17f332ee4299df5c0ad57a08f65b3568c6b2",
        "index": "3a1857c0..4e919e48 100644",
        "commit_message": "Fix some more scope issues\n\n",
        "file": "tensorpack.txt.json",
        "label": "no",
        "comments": "version fix to fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def resnet_argscope():",
            "with argscope([Conv2D, MaxPooling, BatchNorm], data_format='NCHW'), \\",
            "argscope(Conv2D, use_bias=False), \\",
            "argscope(BatchNorm, use_local_stat=False), \\",
            "-            tf.variable_scope(tf.get_variable_scope(),",
            "-                              custom_getter=maybe_freeze_affine):",
            "+            custom_getter_scope(maybe_freeze_affine):",
            "yield"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='custom_getter_scope')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=maybe_freeze_affine), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=variable_scope))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_variable_scope))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=custom_getter))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 51,
        "neg_line": [
            "-tf.variable_scope(tf.get_variable_scope(),",
            "-custom_getter=maybe_freeze_affine):"
        ],
        "pos_line": [
            "+custom_getter_scope(maybe_freeze_affine):"
        ],
        "core_change": "-tf.variable_scope(tf.get_variable_scope(), -custom_getter=maybe_freeze_affine): +custom_getter_scope(maybe_freeze_affine):",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "81ac45f85c35244831f11f73c09ea10eee4f953a",
        "index": "993e52486..157e65d18 100755",
        "commit_message": "update smddp api to v1.4.0 (#16371)\n\n* update smddp api to v1.4.0\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* address comments\n\n* fix style\n\n* remove unused import\n\n* fix indent\n\n* disable style check for import\n\n* fix space\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            ").to(self.args.device)",
            "",
            "elif is_sagemaker_dp_enabled():",
            "-            model = DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)",
            "+            model = nn.parallel.DistributedDataParallel(",
            "+                model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))]",
            "+            )",
            "elif self.args.local_rank != -1:",
            "kwargs = {}",
            "if self.args.ddp_find_unused_parameters is not None:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1202330)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1202331)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1202332)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'DistributedDataParallel'), position=2, insert_id=1202333)",
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=1202334)",
            "Update(target_node=ASTNode(type=identifier, text=DDP), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=DDP), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1202335)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'parallel'), position=2, insert_id=1202336)",
            "Insert(target_node=ASTNode(type=list), node=('call', None), position=1, insert_id=1202337)",
            "Insert(target_node=IN(type=call), node=('identifier', 'int'), position=0, insert_id=1202338)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1202339)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1202340)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1202341)",
            "Update(target_node=ASTNode(type=identifier, text=dist), value='os')",
            "Update(target_node=ASTNode(type=identifier, text=get_local_rank), value='getenv')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"SMDATAPARALLEL_LOCAL_RANK\"'), position=1, insert_id=1202342)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=broadcast_buffers))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 53,
        "neg_line": [
            "-model = DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)"
        ],
        "pos_line": [
            "+model = nn.parallel.DistributedDataParallel(",
            "+model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))]",
            "+)"
        ],
        "core_change": "-model = DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False) +model = nn.parallel.DistributedDataParallel( +model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))] +)",
        "core_API": "get_local_rank"
    },
    {
        "commit_hash": "51fc8cb7880f07c766dc1cc46a6f4f619dc5626c",
        "index": "7f98ca7..ab43af0 100644",
        "commit_message": "New model in test cases. Fixed test cases.\n",
        "file": "facenet.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Network(object):",
            "\"\"\"",
            "@layer",
            "def softmax(self, target, axis, name=None):",
            "-        max_axis = tf.reduce_max(target, axis, keepdims=True)",
            "+        max_axis = tf.reduce_max(target, axis, keep_dims=True)",
            "target_exp = tf.exp(target-max_axis)",
            "-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)",
            "+        normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)",
            "softmax = tf.div(target_exp, normalize, name)",
            "return softmax"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=keepdims), value='keep_dims')",
            "Update(target_node=ASTNode(type=identifier, text=keepdims), value='keep_dims')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 54,
        "neg_line": [
            "-max_axis = tf.reduce_max(target, axis, keepdims=True)",
            "-normalize = tf.reduce_sum(target_exp, axis, keepdims=True)"
        ],
        "pos_line": [
            "+max_axis = tf.reduce_max(target, axis, keep_dims=True)",
            "+normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)"
        ],
        "core_change": "-max_axis = tf.reduce_max(target, axis, keepdims=True) +max_axis = tf.reduce_max(target, axis, keep_dims=True) -normalize = tf.reduce_sum(target_exp, axis, keepdims=True) +normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)",
        "core_API": "reduce_max"
    },
    {
        "commit_hash": "cd77501a8f09b5b11bf5422b0e24b8316820af77",
        "index": "f01acd5a6..7ab90e6c3 100644",
        "commit_message": "fix error for rnn encoders flatten_parameters\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add condition check for state fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RNN(torch.nn.Module):",
            "if not isinstance(ilens, torch.Tensor):",
            "ilens = torch.tensor(ilens)",
            "xs_pack = pack_padded_sequence(xs_pad, ilens.cpu(), batch_first=True)",
            "-        self.nbrnn.flatten_parameters()",
            "+        if self.training:",
            "+            self.nbrnn.flatten_parameters()",
            "if prev_state is not None and self.nbrnn.bidirectional:",
            "# We assume that when previous state is passed,",
            "# it means that we're streaming the input"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=1334677)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1334678)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=1334679)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1334680)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1334681)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1334682)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1334683)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'training'), position=2, insert_id=1334684)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 55,
        "neg_line": [
            "-self.nbrnn.flatten_parameters()"
        ],
        "pos_line": [
            "+if self.training:",
            "+self.nbrnn.flatten_parameters()"
        ],
        "core_change": "-self.nbrnn.flatten_parameters() +if self.training: +self.nbrnn.flatten_parameters()",
        "core_API": "tensor"
    },
    {
        "commit_hash": "d6cec45801ceb57f25d14f180f196f2a13b1ff26",
        "index": "dc1ca0779..59e9b4793 100644",
        "commit_message": "XLA train step fixes (#17973)\n\n* Copy inputs to train and test step before modifying them, as this breaks things\n\n* Add XLA tests, fix our loss functions to be XLA-compatible\n\n* make fixup\n\n* Update loss computation test to expect vector of per-sample losses\n\n* Patch loss for TFLED\n\n* Patch loss for TFAlbert\n\n* Add a tf_legacy_loss config flag that enables old loss functions\n\n* Stop using config.get() because it's not a dict\n\n* Skip loss computation test for RAG because its loss is very strange and I'm afraid to rewrite it\n\n* make fixup\n\n* Add XLA-compatible RAG loss\n\n* Fix dtype of loss mask for TFAlbert\n\n* Fix test for XLNet too because it overrides the default one\n\n* make fixup\n\n* Fix config test\n\n* No more depending on GPU NaN behaviour\n\n* Add test, avoid potential zero division\n\n* Fix test item assignment\n\n* Fix loss computation masking test\n\n* make fixup\n\n* Fix dtype bugs\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFXLNetModelTest(TFModelTesterMixin, unittest.TestCase):",
            "# Send to model",
            "loss = model(tuple_input[:-1])[0]",
            "",
            "-                self.assertEqual(loss.shape, [loss_size])",
            "+                self.assertEqual(loss.shape.as_list(), expected_loss_size)",
            "",
            "",
            "@require_tf"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2363440)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2363441)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'expected_loss_size'), position=3, insert_id=2363442)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2363443)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2363444)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2363445)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_list'), position=2, insert_id=2363446)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2363447)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2363448)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=loss_size))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 56,
        "neg_line": [
            "-self.assertEqual(loss.shape, [loss_size])"
        ],
        "pos_line": [
            "+self.assertEqual(loss.shape.as_list(), expected_loss_size)"
        ],
        "core_change": "-self.assertEqual(loss.shape, [loss_size]) +self.assertEqual(loss.shape.as_list(), expected_loss_size)",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "154176c9f4e642523536c3fc0913ad4cd8189276",
        "index": "4edc3155..de7cc70b 100644",
        "commit_message": "Add OED estimators and accompanying tests (#1925)\n\n* Update EIG estimators and the accompanying tests\n\n* add back some line spacing\n\n* no need to add pre-release methods\n\n* delete old examples\n\n* remove deleted examples from test list\n\n* stray newline\n\n* remove other irrelevant code\n\n* flake8\n\n* do not use CensoredSigmoidNormal\n\n* remove unnecessary expands\n\n* another flake8\n\n* improve docstrings in glmm\n\n* use reshape in place of contiguous().view\n\n* add docstrings to oed/util.py\n\n* add reason= and deprecation warning\n\n* add a lot of docstrings\n\n* double escape\n\n* try writing warning on one line\n\n* rewrite broadcast_cat to use broadcast_tensors\n\n* rename loss functions\n\n* make use of nn.ParameterDict\n\n* always return an EIG by default\n\n* do not update eig_estimation_benchmarking on this PR\n\n* fix issue with glmm\n\n* change from martin\n\n* add citations\n\n* remove N_seq\n\n* rename mean_field_guide_entropy as mean_field_entropy, allow user to avoid mean-field assumption\n\n* delete eig_estimation_benchmarking\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "change param for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sigmoid_example(design):",
            "torch.tensor([[-1.5, 0.5], [1.5, 0.]])",
            "),",
            "(",
            "-        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor(10.), torch.tensor(1.)),",
            "+        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor([10., 10.]), torch.tensor(1.)),",
            "nz_lm_2p_10_10_1,",
            "torch.tensor([[-1., 0.5], [2.5, -2.]])",
            "),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=1, insert_id=719628)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=719629)",
            "Move(target_node=IN(type=list), node=ASTNode(type=float, text=10.), position=1)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=719630)",
            "Insert(target_node=IN(type=list), node=('float', '10.'), position=3, insert_id=719631)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=719632)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 58,
        "neg_line": [
            "-known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor(10.), torch.tensor(1.)),"
        ],
        "pos_line": [
            "+known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor([10., 10.]), torch.tensor(1.)),"
        ],
        "core_change": "-known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor(10.), torch.tensor(1.)), +known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor([10., 10.]), torch.tensor(1.)),",
        "core_API": "tensor"
    },
    {
        "commit_hash": "44e3e3fb4930298f092f336c2b7add3ebf051928",
        "index": "eb77604fb..8ae5fbbca 100644",
        "commit_message": "prepare for \"__floordiv__ is deprecated  and its behavior will change in a future version of pytorch\" (#20211)\n\n* rounding_mode = \"floor\"  instead of // to prevent behavioral change\n\n* add other TODO\n\n* use `torch_int_div` from pytrch_utils\n\n* same for tests\n\n* fix copies\n\n* style\n\n* use relative imports when needed\n\n* Co-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DetaModel(DetaPreTrainedModel):",
            "scale = 2 * math.pi",
            "",
            "dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)",
            "-        dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)",
            "+        dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)",
            "# batch_size, num_queries, 4",
            "proposals = proposals.sigmoid() * scale",
            "# batch_size, num_queries, 4, 128"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=1175408)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1175409)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1175410)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1175411)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1175412)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'div'), position=2, insert_id=1175413)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=dim_t), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1175414)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=2), position=3)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=//, text=//))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 59,
        "neg_line": [
            "-dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)"
        ],
        "pos_line": [
            "+dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)"
        ],
        "core_change": "-dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats) +dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)",
        "core_API": "arange"
    },
    {
        "commit_hash": "6ed9882ddb2b6249463c855dcca6860161d91f3e",
        "index": "c78e36fdd..1a54353d8 100644",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LxmertAttention(nn.Module):",
            "attention_scores = attention_scores + attention_mask",
            "",
            "# Normalize the attention scores to probabilities.",
            "-        attention_probs = nn.Softmax(dim=-1)(attention_scores)",
            "+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)",
            "",
            "# This is actually dropping out entire tokens to attend to, which might",
            "# seem a bit unusual, but is taken from the original Transformer paper."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1536561)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1536562)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'softmax'), position=2, insert_id=1536563)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'attention_scores'), position=1, insert_id=1536564)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1536565)",
            "Update(target_node=ASTNode(type=identifier, text=Softmax), value='functional')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=attention_scores))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 61,
        "neg_line": [
            "-attention_probs = nn.Softmax(dim=-1)(attention_scores)"
        ],
        "pos_line": [
            "+attention_probs = nn.functional.softmax(attention_scores, dim=-1)"
        ],
        "core_change": "-attention_probs = nn.Softmax(dim=-1)(attention_scores) +attention_probs = nn.functional.softmax(attention_scores, dim=-1)",
        "core_API": "Softmax"
    },
    {
        "commit_hash": "828ca68605cad6d62b20fdbf6903fc13ff7d66c2",
        "index": "b69496f33f..aa39fb78e8 100644",
        "commit_message": "lint fixes (#5332)\n\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def trace(",
            "axis2: int = 1,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    ret = tf.experimental.numpy.trace(",
            "-        x, offset=offset, axis1=axis1, axis2=axis2",
            "-    )",
            "+    ret = tf.experimental.numpy.trace(x, offset=offset, axis1=axis1, axis2=axis2)",
            "return ret"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 63,
        "neg_line": [
            "-ret = tf.experimental.numpy.trace(",
            "-x, offset=offset, axis1=axis1, axis2=axis2",
            "-)"
        ],
        "pos_line": [
            "+ret = tf.experimental.numpy.trace(x, offset=offset, axis1=axis1, axis2=axis2)"
        ],
        "core_change": "-ret = tf.experimental.numpy.trace( -x, offset=offset, axis1=axis1, axis2=axis2 -) +ret = tf.experimental.numpy.trace(x, offset=offset, axis1=axis1, axis2=axis2)",
        "core_API": "trace"
    },
    {
        "commit_hash": "04a17f8550c686e339dfd77ccfdbda9ee168b112",
        "index": "16a4cd080..c11623f21 100644",
        "commit_message": "Doc fixes in preparation for the docstyle PR (#8061)\n\n* Fixes in preparation for doc styling\n\n* More fixes\n\n* Better syntax\n\n* Fixes\n\n* Style\n\n* More fixes\n\n* More fixes\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "fix doc",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFOpenAIGPTDoubleHeadsModel(TFOpenAIGPTPreTrainedModel):",
            "training=False,",
            "):",
            "r\"\"\"",
            "-        mc_token_ids (:obj:`tf.Tensor` or :obj:`Numpy array` of shape :obj:`(batch_size, num_choices)`, `optional`, default to index of the last token of the input)",
            "+        mc_token_ids (:obj:`tf.Tensor` or :obj:`Numpy array` of shape :obj:`(batch_size, num_choices)`, `optional`, default to index of the last token of the input):",
            "Index of the classification token in each input sequence.",
            "Selected in the range ``[0, input_ids.size(-1) - 1]``."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=2, insert_id=2379534)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=\", text=r\"\"\"), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=3)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=4, insert_id=2379535)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=Index), position=5)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=of), position=6)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=the), position=7)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=classification), position=8)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=token), position=9)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=in, text=in), position=10)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=ERROR), position=11)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=attribute), position=12)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=in, text=in), position=13)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=ERROR), position=14)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=string, text=``), position=15)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=., text=.), position=16)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 65,
        "neg_line": [
            "-mc_token_ids (:obj:`tf.Tensor` or :obj:`Numpy array` of shape :obj:`(batch_size, num_choices)`, `optional`, default to index of the last token of the input)"
        ],
        "pos_line": [
            "+mc_token_ids (:obj:`tf.Tensor` or :obj:`Numpy array` of shape :obj:`(batch_size, num_choices)`, `optional`, default to index of the last token of the input):"
        ],
        "core_change": "-mc_token_ids (:obj:`tf.Tensor` or :obj:`Numpy array` of shape :obj:`(batch_size, num_choices)`, `optional`, default to index of the last token of the input) +mc_token_ids (:obj:`tf.Tensor` or :obj:`Numpy array` of shape :obj:`(batch_size, num_choices)`, `optional`, default to index of the last token of the input):",
        "core_API": "size"
    },
    {
        "commit_hash": "335035c91393cbc7ae4488953df2d17b0e6f2735",
        "index": "b5bab9d05..974d3b2cc 100644",
        "commit_message": "[MOD] fix typo, add tests for ldconv\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DynamicConvolution2D(nn.Module):",
            "weight = self.linear_weight(x)  # B x T x kH",
            "weight = F.dropout(weight, self.dropout_rate, training=self.training)",
            "weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k",
            "-        weight_new = torch.zeros(B * H * T * (T + k - 1)).view(B, H, T, T + k - 1).fill_(float('-inf'))",
            "+        weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)",
            "+        weight_new = weight_new.view(B, H, T, T + k - 1).fill_(float('-inf'))",
            "weight_new = weight_new.to(x.device)  # B x H x T x T+k-1",
            "weight_new.as_strided((B, H, T, k), ((T + k - 1) * T * H, (T + k - 1) * T, T + k, 1)).copy_(weight)",
            "weight_new = weight_new.narrow(-1, int((k - 1) / 2), T)  # B x H x T x T(k)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=157976)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=157977)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'weight_new'), position=0, insert_id=157978)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=157979)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=157980)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=157981)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=157982)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=157983)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=157984)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weight'), position=0, insert_id=157985)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=157986)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=157987)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'weight_new'), position=0, insert_id=157988)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 66,
        "neg_line": [
            "-weight_new = torch.zeros(B * H * T * (T + k - 1)).view(B, H, T, T + k - 1).fill_(float('-inf'))"
        ],
        "pos_line": [
            "+weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)",
            "+weight_new = weight_new.view(B, H, T, T + k - 1).fill_(float('-inf'))"
        ],
        "core_change": "-weight_new = torch.zeros(B * H * T * (T + k - 1)).view(B, H, T, T + k - 1).fill_(float('-inf')) +weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype) +weight_new = weight_new.view(B, H, T, T + k - 1).fill_(float('-inf'))",
        "core_API": "linear_weight"
    },
    {
        "commit_hash": "321e63ae8b07070b9f50da743cecd432683f0608",
        "index": "8a0156d93..1b06cf04c 100644",
        "commit_message": "Fixes to import\n\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_ddp_sharded_plugin_correctness_multi_gpu():",
            "run_sharded_correctness(gpus=2, accelerator='ddp_spawn')",
            "",
            "",
            "-@pytest.mark.skipif(",
            "-    LooseVersion(torch.__version__) < LooseVersion(\"1.6.0\"),",
            "-    reason=\"Minimal PT version is set to 1.6\")",
            "+@pytest.mark.skipif(not NATIVE_AMP_AVALAIBLE, reason=\"Requires native AMP\")",
            "@pytest.mark.skipif(platform.system() == \"Windows\",",
            "reason=\"Distributed training is not supported on Windows\")",
            "@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=\"test requires multi-GPU machine\")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('not_operator', None), position=1, insert_id=554955)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=5)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=554956)",
            "Insert(target_node=IN(type=not_operator), node=('identifier', 'NATIVE_AMP_AVALAIBLE'), position=1, insert_id=554957)",
            "Update(target_node=ASTNode(type=string, text=\"Minimal PT version is set to 1.6\"), value='\"Requires native AMP\"')",
            "Delete(target_node=ASTNode(type=identifier, text=LooseVersion))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__version__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=<, text=<))",
            "Delete(target_node=ASTNode(type=identifier, text=LooseVersion))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"1.6.0\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 67,
        "neg_line": [
            "-@pytest.mark.skipif(",
            "-LooseVersion(torch.__version__) < LooseVersion(\"1.6.0\"),",
            "-reason=\"Minimal PT version is set to 1.6\")"
        ],
        "pos_line": [
            "+@pytest.mark.skipif(not NATIVE_AMP_AVALAIBLE, reason=\"Requires native AMP\")"
        ],
        "core_change": "-@pytest.mark.skipif( -LooseVersion(torch.__version__) < LooseVersion(\"1.6.0\"), -reason=\"Minimal PT version is set to 1.6\") +@pytest.mark.skipif(not NATIVE_AMP_AVALAIBLE, reason=\"Requires native AMP\")",
        "core_API": "skipif"
    },
    {
        "commit_hash": "ee950b503eeed5aca3747a4bcf2a40f624b743a0",
        "index": "3e16af67..047c098c 100644",
        "commit_message": "fixed trpo\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Gru(TransformationBase):",
            "",
            "def tf_apply(self, x, sequence_length=None):",
            "x, state = tf.nn.dynamic_rnn(",
            "-            cell=self.cell, inputs=x, sequence_length=sequence_length, dtype=tf.float32,",
            "+            cell=self.cell, inputs=x, sequence_length=sequence_length, initial_state=None,",
            "+            dtype=util.tf_dtype(dtype='float'),",
            "# Weird TensorFlow behavior? (https://github.com/tensorflow/tensorflow/issues/15874)",
            "parallel_iterations=(self.input_spec['shape'][0] + 1)",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=2231482)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=2231483)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'initial_state'), position=0, insert_id=2231484)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2231485)",
            "Insert(target_node=IN(type=keyword_argument), node=('none', 'None'), position=2, insert_id=2231486)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=2231487)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2231488)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='util')",
            "Update(target_node=ASTNode(type=identifier, text=float32), value='tf_dtype')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2231489)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2231490)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2231491)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2231492)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2231493)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'float'\"), position=2, insert_id=2231494)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 68,
        "neg_line": [
            "-cell=self.cell, inputs=x, sequence_length=sequence_length, dtype=tf.float32,"
        ],
        "pos_line": [
            "+cell=self.cell, inputs=x, sequence_length=sequence_length, initial_state=None,",
            "+dtype=util.tf_dtype(dtype='float'),"
        ],
        "core_change": "-cell=self.cell, inputs=x, sequence_length=sequence_length, dtype=tf.float32, +cell=self.cell, inputs=x, sequence_length=sequence_length, initial_state=None, +dtype=util.tf_dtype(dtype='float'),",
        "core_API": "dynamic_rnn"
    },
    {
        "commit_hash": "4565d404e5dada3ee102489e437e908fbe494e75",
        "index": "daf4893..1432c74 100644",
        "commit_message": "Fix grad clip norm for FP16 use cases (#1514)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1514\n\nThe Fairseq FP16 optimizers rely on passing in a `max_grad_norm` of 0 and don‚Äôt expect the model gradient to update (https://www.fburl.com/diffusion/buqtrdb5)  . However in PyText optimizers we rely on the torch utility for clipping gradient norm which does not do this and ends up setting the gradients of the model to 0, thus stopping learning (https://www.fburl.com/diffusion/eg6v07qa) .\n\nTo fix this, Fairseq has its own variant of clip_grad_norm (https://www.fburl.com/diffusion/2q2uwjrf)  and this diff adjusts PyText to rely on that instead of the nn utility.\n\nReviewed By: mwu1993\n\nDifferential Revision: D24427025\n\nfbshipit-source-id: 12849c9ad5c08af355b06357d664b293fcc8a080\n\n",
        "file": "pytext.txt.json",
        "label": "no",
        "comments": "change API call place",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Optimizer(Component):",
            "For those we treat model as max_norm.",
            "eg. optimizer.clip_grad_norm(max_norm)",
            "\"\"\"",
            "-            return torch.nn.utils.clip_grad_norm_(self.params, max_norm)",
            "+            return clip_grad_norm_(self.params, max_norm)",
            "else:",
            "-            return torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)",
            "+            return clip_grad_norm_(model.parameters(), max_norm)",
            "",
            "def pre_export(self, model):",
            "pass"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=clip_grad_norm_), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=clip_grad_norm_), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=utils))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=utils))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 69,
        "neg_line": [
            "-return torch.nn.utils.clip_grad_norm_(self.params, max_norm)",
            "-return torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)"
        ],
        "pos_line": [
            "+return clip_grad_norm_(self.params, max_norm)",
            "+return clip_grad_norm_(model.parameters(), max_norm)"
        ],
        "core_change": "-return torch.nn.utils.clip_grad_norm_(self.params, max_norm) +return clip_grad_norm_(self.params, max_norm) -return torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm) +return clip_grad_norm_(model.parameters(), max_norm)",
        "core_API": "clip_grad_norm"
    },
    {
        "commit_hash": "867a5efa3523fd2bd2fb7705e293f80f4bb15299",
        "index": "327a480f..915786ef 100644",
        "commit_message": "Release 1.8.6rc0 (#660)\n\n* Release 1.8.6rc0\n\n* Update CHANGELOG.md\n\n* Typo Fix\n\n* Changelog Updated\n\n* tl_logging more readable\n\n* Typo Fixed\n\n* https enforced for badges\n\n* RTD badges pointing to Latest Commit\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* Changelog updated\n\n* Update .travis.yml\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update CHANGELOG.md\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "update logging",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Util_Predict_Test(CustomTestCase):",
            "",
            "if __name__ == '__main__':",
            "",
            "-    # tl.logging.set_verbosity(tl.logging.INFO)",
            "+    tf.logging.set_verbosity(tf.logging.DEBUG)",
            "tl.logging.set_verbosity(tl.logging.DEBUG)",
            "",
            "unittest.main()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2262816)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2262817)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2262818)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2262819)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2262820)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2262821)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262822)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_verbosity'), position=2, insert_id=2262823)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2262824)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2262825)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2262826)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2262827)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262828)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logging'), position=2, insert_id=2262829)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2262830)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262831)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'DEBUG'), position=2, insert_id=2262832)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2262833)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262834)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logging'), position=2, insert_id=2262835)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 70,
        "neg_line": [
            "-# tl.logging.set_verbosity(tl.logging.INFO)"
        ],
        "pos_line": [
            "+tf.logging.set_verbosity(tf.logging.DEBUG)"
        ],
        "core_change": "-# tl.logging.set_verbosity(tl.logging.INFO) +tf.logging.set_verbosity(tf.logging.DEBUG)",
        "core_API": "set_verbosity"
    },
    {
        "commit_hash": "35b350b28ca99498edd0d1644ee4dde6247a5409",
        "index": "610bd882..d9df8e98 100644",
        "commit_message": "Fix quantized-inference & Add generic support of checkpoint loading (#2547)\n\n* fix checkpoint loading when it is a dictionary\n\n* fix some issues with saving ckpt & int8 inference\n\n* fix quantized-inference & add generic support of checkpoint loading\n\n* remove int8 hard-coded flag\n\n* fix mlp return tensors\n\n* fix several issue to load checkpoints of GPT-J, GPT-NEOX, and OPT with different TP-size\n\n* add more comments & description for checkpoint-loading module\n\nCo-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "yes",
        "comments": "remove condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeepSpeedSelfAttention(nn.Module):",
            "data_type_fp = torch.half if config.fp16 else torch.float",
            "self.config.layer_id = DeepSpeedSelfAttention.num_layers",
            "DeepSpeedSelfAttention.num_layers = DeepSpeedSelfAttention.num_layers + 1",
            "-        device = torch.cuda.current_device() if config.bigscience_bloom else 'cpu'",
            "+        device = torch.cuda.current_device()  #if config.bigscience_bloom else 'cpu'",
            "qkv_size_per_partition = (self.config.hidden_size // self.config.mp_size) * 3",
            "self.attn_qkvw = nn.Parameter(torch.empty(self.config.hidden_size,",
            "qkv_size_per_partition,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=config))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=bigscience_bloom))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=string, text='cpu'))",
            "Delete(target_node=ASTNode(type=conditional_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 71,
        "neg_line": [
            "-device = torch.cuda.current_device() if config.bigscience_bloom else 'cpu'"
        ],
        "pos_line": [
            "+device = torch.cuda.current_device()  #if config.bigscience_bloom else 'cpu'"
        ],
        "core_change": "-device = torch.cuda.current_device() if config.bigscience_bloom else 'cpu' +device = torch.cuda.current_device()  #if config.bigscience_bloom else 'cpu'",
        "core_API": "current_device"
    },
    {
        "commit_hash": "cd51772bbe4eec0addd4e149b2f47f999020e7a0",
        "index": "d739d8f9..529beab0 100644",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestClosing:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            closing(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-4, rtol=1e-4",
            "+            closing(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-4,",
            "+            rtol=1e-4,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=423216)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 73,
        "neg_line": [
            "-closing(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-atol=1e-4, rtol=1e-4"
        ],
        "pos_line": [
            "+closing(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+expected,",
            "+atol=1e-4,",
            "+rtol=1e-4,"
        ],
        "core_change": "-closing(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected, -atol=1e-4, rtol=1e-4 +closing(tensor, torch.ones_like(structural_element), structuring_element=structural_element), +expected, +atol=1e-4, +rtol=1e-4,",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "f08687f5503b1a59b1f5ce9a4cb0d6983d4d1554",
        "index": "f3d6f03e7..d8fd9e7f6 100644",
        "commit_message": "[RLlib] `rllib train` crashes when using torch PPO/PG/A2C. (#7508)\n\n* Fix.\n\n* Rollback.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchCategorical(TorchDistributionWrapper):",
            "@override(ActionDistribution)",
            "def __init__(self, inputs, model=None, temperature=1.0):",
            "assert temperature > 0.0, \"Categorical `temperature` must be > 0.0!\"",
            "-        super().__init__(inputs / temperature, model)",
            "+        inputs /= temperature",
            "+        super().__init__(inputs, model)",
            "self.dist = torch.distributions.categorical.Categorical(",
            "logits=self.inputs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1125975)",
            "Insert(target_node=IN(type=expression_statement), node=('augmented_assignment', None), position=0, insert_id=1125976)",
            "Insert(target_node=IN(type=augmented_assignment), node=('identifier', 'inputs'), position=0, insert_id=1125977)",
            "Insert(target_node=IN(type=augmented_assignment), node=('/=', '/='), position=1, insert_id=1125978)",
            "Insert(target_node=IN(type=augmented_assignment), node=('identifier', 'temperature'), position=2, insert_id=1125979)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=inputs), position=1)",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=identifier, text=temperature))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 74,
        "neg_line": [
            "-super().__init__(inputs / temperature, model)"
        ],
        "pos_line": [
            "+inputs /= temperature",
            "+super().__init__(inputs, model)"
        ],
        "core_change": "-super().__init__(inputs / temperature, model) +inputs /= temperature +super().__init__(inputs, model)",
        "core_API": "Categorical"
    },
    {
        "commit_hash": "6241ec400c131d0a0e0cf813a198376124f9a064",
        "index": "482885bf0..4ca70ffec 100644",
        "commit_message": "* feat: add SQuAD model for Russian lang (#192)\n\n* feat: add lr decay to squad model\n\n* refactor: cudnn layers refactored\n\n* fix: double transpose output fixed. seq_len init added to bi-gru\n\n* feat: add variational dropout usage\n\n* feat: sequence lengths are added to cudnn rnn layers\n\n* doc: docs for variational dropout\n\n* feat: add cudnn_bi_gru usage\n\n* fix: validation_patience in squad config\n\n* feat: add RU SQuAD dataset and config\n\n* fix: upd squad_ru config\n\n* feat: add lr logging for squad\n\n* feat: add weight decay\n\n* fix: upd squad_ru config and weight_decay parameter\n\n* fix: upd squad configs\n\n* docs: upd squad readme\n\n* test: add squad_ru to tests\n\n* fix: tf_layers compatibility for ner\n\n* fix: ontonotes cudnn fix\n\n* fix: initialization fixed\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "no",
        "comments": "change funtional",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def attention(inputs, state, att_size, mask, scope=\"attention\"):",
            "\"\"\"Computes weighted sum of inputs conditioned on state\"\"\"",
            "with tf.variable_scope(scope):",
            "u = tf.concat([tf.tile(tf.expand_dims(state, axis=1), [1, tf.shape(inputs)[1], 1]), inputs], axis=2)",
            "-        logits = tf.layers.dense(tf.layers.dense(u, att_size, activation=tf.nn.sigmoid), 1, use_bias=False)",
            "+        logits = tf.layers.dense(tf.layers.dense(u, att_size, activation=tf.nn.tanh), 1, use_bias=False)",
            "logits = softmax_mask(tf.squeeze(logits, [2]), mask)",
            "att_weights = tf.expand_dims(tf.nn.softmax(logits), axis=2)",
            "res = tf.reduce_sum(att_weights * inputs, axis=1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sigmoid), value='tanh')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 75,
        "neg_line": [
            "-logits = tf.layers.dense(tf.layers.dense(u, att_size, activation=tf.nn.sigmoid), 1, use_bias=False)"
        ],
        "pos_line": [
            "+logits = tf.layers.dense(tf.layers.dense(u, att_size, activation=tf.nn.tanh), 1, use_bias=False)"
        ],
        "core_change": "-logits = tf.layers.dense(tf.layers.dense(u, att_size, activation=tf.nn.sigmoid), 1, use_bias=False) +logits = tf.layers.dense(tf.layers.dense(u, att_size, activation=tf.nn.tanh), 1, use_bias=False)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "68427a6e..11b3b6fc 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "remove API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args):",
            "bob_decision = Marginal(Search(bob))",
            "",
            "# Here Alice and Bob slightly prefer one location over the other a priori",
            "-    shared_preference = Variable(torch.Tensor([args.preference]))",
            "+    shared_preference = torch.tensor([args.preference])",
            "",
            "bob_depth = args.depth",
            "num_samples = args.num_samples"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 76,
        "neg_line": [
            "-shared_preference = Variable(torch.Tensor([args.preference]))"
        ],
        "pos_line": [
            "+shared_preference = torch.tensor([args.preference])"
        ],
        "core_change": "-shared_preference = Variable(torch.Tensor([args.preference])) +shared_preference = torch.tensor([args.preference])",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "0d3e60c975cf959e23cf55e33c83d08aac083701",
        "index": "719d318ec..2c3109de1 100644",
        "commit_message": "Fixing missing var\n\n",
        "file": "tutorials.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "if torch.backends.cudnn.version() >= 7603:",
            "#",
            "# Channels Last support not limited by existing models, as any model can be converted to Channels Last and propagate format through the graph as soon as input formatted correctly.",
            "#",
            "-input = input.to(memory_format=torch.channels_last)",
            "-model = model.to(memory_format=torch.channels_last)",
            "+",
            "+# Need to be done once, after model initialization (or load)",
            "+model = model.to(memory_format=torch.channels_last) # Replace with your model",
            "+",
            "+# Need to be done for every input",
            "+input = input.to(memory_format=torch.channels_last) # Replace with your input",
            "output = model(input)",
            "",
            "#######################################################################"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=1)"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 77,
        "neg_line": [
            "-input = input.to(memory_format=torch.channels_last)",
            "-model = model.to(memory_format=torch.channels_last)"
        ],
        "pos_line": [
            "+",
            "+# Need to be done once, after model initialization (or load)",
            "+model = model.to(memory_format=torch.channels_last) # Replace with your model",
            "+",
            "+# Need to be done for every input",
            "+input = input.to(memory_format=torch.channels_last) # Replace with your input"
        ],
        "core_change": "-input = input.to(memory_format=torch.channels_last) -model = model.to(memory_format=torch.channels_last) + +# Need to be done once, after model initialization (or load) +model = model.to(memory_format=torch.channels_last) # Replace with your model + +# Need to be done for every input +input = input.to(memory_format=torch.channels_last) # Replace with your input",
        "core_API": "version"
    },
    {
        "commit_hash": "5af44f35186b11b51984800affe9fe371f8d340b",
        "index": "c874463..766d269 100644",
        "commit_message": "fix an error\n\n",
        "file": "tflearn.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def time_distributed(incoming, fn, args=None, scope=None):",
            "else:",
            "x = [fn(x[i], *args) for i in range(timestep)]",
            "x = map(lambda t: tf.reshape(t, [-1, 1]+utils.get_incoming_shape(t)[1:]), x)",
            "-    return tf.concat(1, x)",
            "\\ No newline at end of file",
            "+    return tf.concat(1, x)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=assignment), node=('ERROR', None), position=2, insert_id=2352003)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'file'), position=3, insert_id=2352004)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('ERROR', ' '), position=1, insert_id=2352005)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'No'), position=2, insert_id=2352006)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'newline'), position=3, insert_id=2352007)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'at'), position=4, insert_id=2352008)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'end'), position=5, insert_id=2352009)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'of'), position=6, insert_id=2352010)",
            "Delete(target_node=ASTNode(type=ERROR, text= ))",
            "Delete(target_node=ASTNode(type=identifier, text=No))",
            "Delete(target_node=ASTNode(type=identifier, text=newline))",
            "Delete(target_node=ASTNode(type=identifier, text=at))",
            "Delete(target_node=ASTNode(type=identifier, text=end))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=file))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 79,
        "neg_line": [
            "-return tf.concat(1, x)"
        ],
        "pos_line": [
            "+return tf.concat(1, x)"
        ],
        "core_change": "-return tf.concat(1, x) +return tf.concat(1, x)",
        "core_API": "reshape"
    },
    {
        "commit_hash": "5036f582d175b214710cd88fa577b81a8bffe77d",
        "index": "5a71b8dd..f8c7c89f 100644",
        "commit_message": "fixed mnist superpixels\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Planetoid(Dataset):",
            "# Create unweighted sparse adjacency matrix.",
            "weight = torch.ones(index.size(1))",
            "n = input.size(0)",
            "-        adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n]))",
            "+        adj = SparseTensor(index, weight, torch.Size([n, n]))",
            "",
            "# Bundle graph to data object.",
            "-        self.data = Data(input, adj, position=None, target=target)",
            "+        self.data = Data(input, adj, position=None, target=target.long())",
            "",
            "def __getitem__(self, index):",
            "data = self.data"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='SparseTensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=1088472)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1088473)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1088474)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=target), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1088475)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=1088476)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1088477)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1088478)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sparse))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=FloatTensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 81,
        "neg_line": [
            "-adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n]))",
            "-self.data = Data(input, adj, position=None, target=target)"
        ],
        "pos_line": [
            "+adj = SparseTensor(index, weight, torch.Size([n, n]))",
            "+self.data = Data(input, adj, position=None, target=target.long())"
        ],
        "core_change": "-adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n])) +adj = SparseTensor(index, weight, torch.Size([n, n])) -self.data = Data(input, adj, position=None, target=target) +self.data = Data(input, adj, position=None, target=target.long())",
        "core_API": "ones"
    },
    {
        "commit_hash": "95e4cb95eaca094b35a389f91c5b64378a53b943",
        "index": "6ccf5f615..6dd54d1f8 100644",
        "commit_message": "fixed multiple super\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Tacotron2(TTSInterface, torch.nn.Module):",
            "",
            "def __init__(self, idim, odim, args):",
            "super(Tacotron2, self).__init__()",
            "+        torch.nn.Module.__init__(self)",
            "# store hyperparameters",
            "self.idim = idim",
            "self.odim = odim"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1342091)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1342092)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1342093)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1342094)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1342095)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1342096)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__init__'), position=2, insert_id=1342097)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1342098)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'self'), position=1, insert_id=1342099)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1342100)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1342101)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1342102)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1342103)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1342104)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1342105)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1342106)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 84,
        "neg_line": [],
        "pos_line": [
            "+torch.nn.Module.__init__(self)"
        ],
        "core_change": "+torch.nn.Module.__init__(self)",
        "core_API": "__init__"
    },
    {
        "commit_hash": "86e47b1f532094a09d746ef052ae5bb03f4e1403",
        "index": "1cbe9dd..1a0d29b 100644",
        "commit_message": "Fix universal sentence encoder colab\n\nPiperOrigin-RevId: 387544699\n\n",
        "file": "hub.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "\"import sys\\n\",",
            "\"sys.path.append(f'{os.getcwd()}/SentEval')\\n\",",
            "\"\\n\",",
            "-        \"import tensorflow_hub as hub\\n\",",
            "\"import tensorflow as tf\\n\",",
            "+        \"\\n\",",
            "+        \"# Prevent TF from claiming all GPU memory so there is some left for pytorch.\\n\",",
            "+        \"gpus = tf.config.list_physical_devices('GPU')\\n\",",
            "+        \"if gpus:\\n\",",
            "+        \"  # Memory growth needs to be the same across GPUs.\\n\",",
            "+        \"  for gpu in gpus:\\n\",",
            "+        \"    tf.config.experimental.set_memory_growth(gpu, True)\\n\",",
            "+        \"\\n\",",
            "+        \"import tensorflow_hub as hub\\n\",",
            "\"import tensorflow_text\\n\",",
            "\"import senteval\\n\",",
            "\"import time\\n\","
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=2)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1947733)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=1947734)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=1947735)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=7, insert_id=1947736)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=8, insert_id=1947737)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=9, insert_id=1947738)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=10, insert_id=1947739)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=11, insert_id=1947740)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\\\\n\"'), position=0, insert_id=1947741)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1947742)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\\\\n\"'), position=0, insert_id=1947743)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1947744)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"# Prevent TF from claiming all GPU memory so there is some left for pytorch.\\\\n\"'), position=0, insert_id=1947745)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1947746)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"gpus = tf.config.list_physical_devices(\\'GPU\\')\\\\n\"'), position=0, insert_id=1947747)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1947748)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"if gpus:\\\\n\"'), position=0, insert_id=1947749)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1947750)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"  # Memory growth needs to be the same across GPUs.\\\\n\"'), position=0, insert_id=1947751)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1947752)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"  for gpu in gpus:\\\\n\"'), position=0, insert_id=1947753)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1947754)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"    tf.config.experimental.set_memory_growth(gpu, True)\\\\n\"'), position=0, insert_id=1947755)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1947756)"
        ],
        "plus_line": 9,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 85,
        "neg_line": [
            "-\"import tensorflow_hub as hub\\n\","
        ],
        "pos_line": [
            "+\"\\n\",",
            "+\"# Prevent TF from claiming all GPU memory so there is some left for pytorch.\\n\",",
            "+\"gpus = tf.config.list_physical_devices('GPU')\\n\",",
            "+\"if gpus:\\n\",",
            "+\"  # Memory growth needs to be the same across GPUs.\\n\",",
            "+\"  for gpu in gpus:\\n\",",
            "+\"    tf.config.experimental.set_memory_growth(gpu, True)\\n\",",
            "+\"\\n\",",
            "+\"import tensorflow_hub as hub\\n\","
        ],
        "core_change": "-\"import tensorflow_hub as hub\\n\", +\"\\n\", +\"# Prevent TF from claiming all GPU memory so there is some left for pytorch.\\n\", +\"gpus = tf.config.list_physical_devices('GPU')\\n\", +\"if gpus:\\n\", +\"  # Memory growth needs to be the same across GPUs.\\n\", +\"  for gpu in gpus:\\n\", +\"    tf.config.experimental.set_memory_growth(gpu, True)\\n\", +\"\\n\", +\"import tensorflow_hub as hub\\n\",",
        "core_API": "append"
    },
    {
        "commit_hash": "c9cabd6e98bf06602f7e96be1dc5344eb6ff430a",
        "index": "5df9bd3c1..96220da4e 100644",
        "commit_message": "Fix conflict\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Encoder(torch.nn.Module):",
            "self.embed = Conv2dSubsampling(idim, attention_dim, dropout_rate)",
            "elif input_layer == \"embed\":",
            "self.embed = torch.nn.Sequential(",
            "-                torch.nn.Embedding(idim, attention_dim),",
            "+                torch.nn.Embedding(idim, attention_dim, padding_idx=padding_idx),",
            "pos_enc_class(attention_dim, positional_dropout_rate)",
            ")",
            "elif isinstance(input_layer, torch.nn.Module):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=171801)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=171802)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'padding_idx'), position=0, insert_id=171803)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=171804)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'padding_idx'), position=2, insert_id=171805)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 86,
        "neg_line": [
            "-torch.nn.Embedding(idim, attention_dim),"
        ],
        "pos_line": [
            "+torch.nn.Embedding(idim, attention_dim, padding_idx=padding_idx),"
        ],
        "core_change": "-torch.nn.Embedding(idim, attention_dim), +torch.nn.Embedding(idim, attention_dim, padding_idx=padding_idx),",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "a828315185a9dc8b21ec8e5dbead9044caf0d3a2",
        "index": "fc86bfe6..58100037 100644",
        "commit_message": "[Enhance] Added dtype flag to geometry module (#820)\n\n* Added docs and tests\n\n* Fixed bug for batch prob generator\n\n* Fixed typo\n\n* Fixed mypy\n\n* Fixed mypy\n\n* Added warning\n\n* Updated tests for geometry\n\n* Added dtype for conversions\n\n* Completed tests\n\n* Fixed homography\n\n* Fixed pinhole camera\n\n* Fixed tests\n\n* Fix geometry testing bugs\n\n* Fixed augmentation tests\n\n* code updated\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_checkerboard(h, w, nw):",
            "",
            "",
            "# TODO: Isn't this function duplicated with eye_like?",
            "-def create_eye_batch(batch_size, eye_size):",
            "+def create_eye_batch(batch_size, eye_size, device=None, dtype=None):",
            "\"\"\"Creates a batch of identity matrices of shape Bx3x3",
            "\"\"\"",
            "-    return torch.eye(eye_size).view(",
            "+    return torch.eye(eye_size, device=device, dtype=dtype).view(",
            "1, eye_size, eye_size).expand(batch_size, -1, -1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=432342)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=5, insert_id=432343)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=432344)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=7, insert_id=432345)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'device'), position=0, insert_id=432346)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=432347)",
            "Insert(target_node=IN(type=default_parameter), node=('none', 'None'), position=2, insert_id=432348)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'dtype'), position=0, insert_id=432349)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=432350)",
            "Insert(target_node=IN(type=default_parameter), node=('none', 'None'), position=2, insert_id=432351)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=432352)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=432353)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=432354)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=432355)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=432356)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=432357)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=432358)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=432359)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=432360)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=432361)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 88,
        "neg_line": [
            "-def create_eye_batch(batch_size, eye_size):",
            "-return torch.eye(eye_size).view("
        ],
        "pos_line": [
            "+def create_eye_batch(batch_size, eye_size, device=None, dtype=None):",
            "+return torch.eye(eye_size, device=device, dtype=dtype).view("
        ],
        "core_change": "-def create_eye_batch(batch_size, eye_size): +def create_eye_batch(batch_size, eye_size, device=None, dtype=None): -return torch.eye(eye_size).view( +return torch.eye(eye_size, device=device, dtype=dtype).view(",
        "core_API": "eye"
    },
    {
        "commit_hash": "0070252e186dba391147d72544498eae493dfca1",
        "index": "87e51b036..f0ed0efe5 100644",
        "commit_message": "Move complex tensor related functions to espnet2/enh/layers/complex_utils.py; Refactor complex support code; fix unit tests\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "change condition check for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransformerSeparator(AbsSeparator):",
            "",
            "# if complex spectrum,",
            "if isinstance(input, ComplexTensor) or (",
            "-            is_torch_1_8_plus and torch.is_complex(input)",
            "+            is_torch_1_9_plus and torch.is_complex(input)",
            "):",
            "feature = abs(input)",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=is_torch_1_8_plus), value='is_torch_1_9_plus')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 91,
        "neg_line": [
            "-is_torch_1_8_plus and torch.is_complex(input)"
        ],
        "pos_line": [
            "+is_torch_1_9_plus and torch.is_complex(input)"
        ],
        "core_change": "-is_torch_1_8_plus and torch.is_complex(input) +is_torch_1_9_plus and torch.is_complex(input)",
        "core_API": "is_complex"
    },
    {
        "commit_hash": "31d1f3c8c0c296bbdef9fa1651cfa7995cbed4b1",
        "index": "aa4c67a3..2e0be1d3 100755",
        "commit_message": "final fix\n\n",
        "file": "diffusers.txt.json",
        "label": "no",
        "comments": "math param update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineTesterMixin(unittest.TestCase):",
            "image_slice = image[0, -1, -3:, -3:].cpu()",
            "",
            "assert image.shape == (1, 3, 32, 32)",
            "-        expected_slice = torch.tensor([-0.5712, -0.6215, -0.5953, -0.5438, -0.4775, -0.4539, -0.5172, -0.4872, -0.5105])",
            "+        expected_slice = torch.tensor(",
            "+            [-0.5712, -0.6215, -0.5953, -0.5438, -0.4775, -0.4539, -0.5172, -0.4872, -0.5105]",
            "+        )",
            "assert (image_slice.flatten() - expected_slice).abs().max() < 1e-2",
            "",
            "@slow"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 92,
        "neg_line": [
            "-expected_slice = torch.tensor([-0.5712, -0.6215, -0.5953, -0.5438, -0.4775, -0.4539, -0.5172, -0.4872, -0.5105])"
        ],
        "pos_line": [
            "+expected_slice = torch.tensor(",
            "+[-0.5712, -0.6215, -0.5953, -0.5438, -0.4775, -0.4539, -0.5172, -0.4872, -0.5105]",
            "+)"
        ],
        "core_change": "-expected_slice = torch.tensor([-0.5712, -0.6215, -0.5953, -0.5438, -0.4775, -0.4539, -0.5172, -0.4872, -0.5105]) +expected_slice = torch.tensor( +[-0.5712, -0.6215, -0.5953, -0.5438, -0.4775, -0.4539, -0.5172, -0.4872, -0.5105] +)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "9a2b482b3a0eb2ce58e430c9968b8e471227e775",
        "index": "aef6c9e3..953d9c51 100644",
        "commit_message": "fix dense graph conv test\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "not clear",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DNAConv(MessagePassing):",
            "num_edges = edge_index.size(1)",
            "",
            "edge_index, edge_weight = gcn_norm(edge_index, x.size(self.node_dim),",
            "-                                           edge_weight, self.improved, x.dtype)",
            "+                                           edge_weight, dtype=x.dtype)",
            "",
            "if self.cached:",
            "self._cache = (num_edges, edge_index, edge_weight)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=1021592)",
            "Update(target_node=ASTNode(type=identifier, text=self), value='dtype')",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=self), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1021593)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=improved))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 93,
        "neg_line": [
            "-edge_weight, self.improved, x.dtype)"
        ],
        "pos_line": [
            "+edge_weight, dtype=x.dtype)"
        ],
        "core_change": "-edge_weight, self.improved, x.dtype) +edge_weight, dtype=x.dtype)",
        "core_API": "size"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "360f14e86..8d7a8d0b0 100644",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "API update",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class T5Attention(nn.Module):",
            "position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)",
            "",
            "scores += position_bias",
            "-        attn_weights = F.softmax(scores.float(), dim=-1).type_as(",
            "+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(",
            "scores",
            ")  # (batch_size, n_heads, seq_length, key_length)",
            "-        attn_weights = F.dropout(",
            "+        attn_weights = nn.functional.dropout(",
            "attn_weights, p=self.dropout, training=self.training",
            ")  # (batch_size, n_heads, seq_length, key_length)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1538389)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1538390)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=F), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1538391)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1538392)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1538393)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=F), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1538394)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 94,
        "neg_line": [
            "-attn_weights = F.softmax(scores.float(), dim=-1).type_as(",
            "-attn_weights = F.dropout("
        ],
        "pos_line": [
            "+attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(",
            "+attn_weights = nn.functional.dropout("
        ],
        "core_change": "-attn_weights = F.softmax(scores.float(), dim=-1).type_as( +attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as( -attn_weights = F.dropout( +attn_weights = nn.functional.dropout(",
        "core_API": "softmax"
    },
    {
        "commit_hash": "bc797fd37613f18ddf0fd5122776b4cdcc4922ae",
        "index": "9599bce5b..cc9e84297 100644",
        "commit_message": "[App] Fix multi-node pytorch example CI (#15753)\n\n\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "update doc",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class PyTorchDistributed(L.LightningWork):",
            ")",
            "",
            "",
            "-# 32 GPUs: (8 nodes x 4 v 100)",
            "+# 8 GPUs: (2 nodes x 4 v 100)",
            "compute = L.CloudCompute(\"gpu-fast-multi\")  # 4xV100",
            "component = MultiNode(PyTorchDistributed, num_nodes=2, cloud_compute=compute)",
            "app = L.LightningApp(component)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 95,
        "neg_line": [
            "-# 32 GPUs: (8 nodes x 4 v 100)"
        ],
        "pos_line": [
            "+# 8 GPUs: (2 nodes x 4 v 100)"
        ],
        "core_change": "-# 32 GPUs: (8 nodes x 4 v 100) +# 8 GPUs: (2 nodes x 4 v 100)",
        "core_API": "CloudCompute"
    },
    {
        "commit_hash": "dea62f0fb8b7053e43372d38d2cd2fa54c69290e",
        "index": "19b932a..6faf225 100644",
        "commit_message": "Fix bug with multiple models running in parallels (#43)\n\n* add possibility to select the maximum number of threads per model to be used\n\n* change version\n\nCo-authored-by: morgoth95 <diego.fiori@epfl.ch>\n",
        "file": "nebullvm.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _get_ort_session_options() -> ort.SessionOptions:",
            "if not torch.cuda.is_available():",
            "sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL",
            "sess_options.inter_op_num_threads = 1",
            "-        sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1)",
            "+        sess_options.intra_op_num_threads = max(",
            "+            int(",
            "+                os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\")",
            "+                or torch.get_num_threads()",
            "+            ),",
            "+            1,",
            "+        )",
            "return sess_options"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=654401)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=654402)",
            "Insert(target_node=IN(type=call), node=('identifier', 'int'), position=0, insert_id=654403)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=654404)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=654405)",
            "Insert(target_node=IN(type=argument_list), node=('boolean_operator', None), position=1, insert_id=654406)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=654407)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=654408)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=654409)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=654410)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=654411)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=654412)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=654413)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get'), position=2, insert_id=654414)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=654415)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"NEBULLVM_THREADS_PER_MODEL\"'), position=1, insert_id=654416)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=654417)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'os'), position=0, insert_id=654418)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=654419)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'environ'), position=2, insert_id=654420)"
        ],
        "plus_line": 7,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 97,
        "neg_line": [
            "-sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1)"
        ],
        "pos_line": [
            "+sess_options.intra_op_num_threads = max(",
            "+int(",
            "+os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\")",
            "+or torch.get_num_threads()",
            "+),",
            "+1,",
            "+)"
        ],
        "core_change": "-sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1) +sess_options.intra_op_num_threads = max( +int( +os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\") +or torch.get_num_threads() +), +1, +)",
        "core_API": "is_available"
    },
    {
        "commit_hash": "8ef8ddb91566f17861dc06e714c3bc46c6273291",
        "index": "0103b169..d4d27875 100644",
        "commit_message": "bug fix\n\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "typo fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def save_best_model(model, optimizer, model_loss, best_loss, out_path,",
            "def check_update(model, grad_clip, grad_top):",
            "r'''Check model gradient against unexpected jumps and failures'''",
            "skip_flag = False",
            "-    grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), grad_clip)",
            "+    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)",
            "if np.isinf(grad_norm):",
            "print(\" | > Gradient is INF !!\")",
            "skip_flag = True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=clip_grad_norm), value='clip_grad_norm_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 98,
        "neg_line": [
            "-grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), grad_clip)"
        ],
        "pos_line": [
            "+grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)"
        ],
        "core_change": "-grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), grad_clip) +grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)",
        "core_API": "clip_grad_norm"
    },
    {
        "commit_hash": "3ceb66543deed12d0570e936a4a18a530b93fced",
        "index": "d4118925..cb259dbf 100644",
        "commit_message": "fix quaternion_exp_to_log test errors\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add condition check for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def quaternion_exp_to_log(quaternion: torch.Tensor,",
            ">>> kornia.quaternion_exp_to_log(quaternion)",
            "tensor([0., 0., 0.])",
            "\"\"\"",
            "-    if not torch.is_tensor(quaternion):",
            "+    if not isinstance(quaternion, torch.Tensor):",
            "raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(",
            "type(quaternion)))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 99,
        "neg_line": [
            "-if not torch.is_tensor(quaternion):"
        ],
        "pos_line": [
            "+if not isinstance(quaternion, torch.Tensor):"
        ],
        "core_change": "-if not torch.is_tensor(quaternion): +if not isinstance(quaternion, torch.Tensor):",
        "core_API": "quaternion_exp_to_log"
    },
    {
        "commit_hash": "b82fe7d258a621b953dcb3b78bd03da4fef70a44",
        "index": "db30f188a..ba712a7b0 100644",
        "commit_message": "Replace strided slice with tf.expand_dims (#10078)\n\n* Replace tf.newaxis -> tf.expand_dims\n\n* Fix tests\n\n* Fix tests\n\n* Use reshape when a tensors needs a double expand\n\n* Fix GPT2\n\n* Fix GPT2\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFFlaubertMainLayer(tf.keras.layers.Layer):",
            "tensor_normalized = self.layer_norm2[i](tensor)",
            "tensor = tensor + self.ffns[i](tensor_normalized)",
            "",
            "-            tensor = tensor * mask[..., tf.newaxis]",
            "+            tensor = tensor * tf.expand_dims(mask, axis=-1)",
            "",
            "# Add last hidden state",
            "if inputs[\"output_hidden_states\"]:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2371897)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2371898)",
            "Update(target_node=ASTNode(type=identifier, text=newaxis), value='expand_dims')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2371899)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=mask), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2371900)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2371901)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2371902)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2371903)",
            "Insert(target_node=IN(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=2371904)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=ellipsis, text=...))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 100,
        "neg_line": [
            "-tensor = tensor * mask[..., tf.newaxis]"
        ],
        "pos_line": [
            "+tensor = tensor * tf.expand_dims(mask, axis=-1)"
        ],
        "core_change": "-tensor = tensor * mask[..., tf.newaxis] +tensor = tensor * tf.expand_dims(mask, axis=-1)",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "3c14ff66314f70a337e876b505d2066f5fbb050b",
        "index": "15153a33..721197fe 100644",
        "commit_message": "fix bug in 552c2b3b51c83fc27bdf6df691f1d707e40865f9\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "change API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _replace_global_by_local(kwargs):",
            "if 'collections' in kwargs:",
            "collections = kwargs['collections']",
            "if not collections:",
            "-        collections = set(tf.GraphKeys.GLOBAL_VARIABLES)",
            "+        collections = {tf.GraphKeys.GLOBAL_VARIABLES}",
            "else:",
            "collections = set(collections.copy())",
            "collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('set', None), position=2, insert_id=2273734)",
            "Insert(target_node=IN(type=set), node=('{', '{'), position=0, insert_id=2273735)",
            "Move(target_node=IN(type=set), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=set), node=('}', '}'), position=2, insert_id=2273736)",
            "Delete(target_node=ASTNode(type=identifier, text=set))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 101,
        "neg_line": [
            "-collections = set(tf.GraphKeys.GLOBAL_VARIABLES)"
        ],
        "pos_line": [
            "+collections = {tf.GraphKeys.GLOBAL_VARIABLES}"
        ],
        "core_change": "-collections = set(tf.GraphKeys.GLOBAL_VARIABLES) +collections = {tf.GraphKeys.GLOBAL_VARIABLES}",
        "core_API": "copy"
    },
    {
        "commit_hash": "118ecfd4273b5381aeeb65476a01678c7a96ae3e",
        "index": "6049dc3ed..b214c6d5e 100644",
        "commit_message": "fix for pytorch < 1.6 (#6300)\n\n\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change condition check for null fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class ReformerLayer(nn.Module):",
            "\"\"\"",
            "# randomize seeds",
            "# use cuda generator if available",
            "-        if len(torch.cuda.default_generators) > 0:",
            "+        if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:",
            "# GPU",
            "device_idx = torch.cuda.current_device()",
            "self.feed_forward_seed = torch.cuda.default_generators[device_idx].seed()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=1, insert_id=1878118)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=\", text=\"\"\"), position=0)",
            "Insert(target_node=IN(type=ERROR), node=(')', ')'), position=1, insert_id=1878119)",
            "Insert(target_node=IN(type=ERROR), node=('and', 'and'), position=2, insert_id=1878120)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=comparison_operator), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=4)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('ERROR', None), position=2, insert_id=1878121)",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=identifier, text=device_idx), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=integer, text=0), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=1)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=if_statement))",
            "Delete(target_node=ASTNode(type=ERROR, text=))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 103,
        "neg_line": [
            "-if len(torch.cuda.default_generators) > 0:"
        ],
        "pos_line": [
            "+if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:"
        ],
        "core_change": "-if len(torch.cuda.default_generators) > 0: +if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:",
        "core_API": "current_device"
    },
    {
        "commit_hash": "52491e0198b5cf1fb158a534c3613e2749f6be77",
        "index": "8051287f..f733fbfe 100644",
        "commit_message": "Math fixes for 3d affine transformation & doc updates\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add API call for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_projective_transform(center: torch.Tensor, angles: torch.Tensor) -> torc",
            "",
            "# create rotation matrix",
            "angle_axis_rad: torch.Tensor = K.deg2rad(angles)",
            "-    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3",
            "+    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad) * scales.view(-1, 1, 1)  # Bx3x3",
            "",
            "# define matrix to move forth and back to origin",
            "from_origin_mat = torch.eye(4)[None].repeat(rmat.shape[0], 1, 1).type_as(center)  # Bx4x4"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=4, insert_id=439443)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=439444)",
            "Insert(target_node=IN(type=binary_operator), node=('call', None), position=2, insert_id=439445)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=439446)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=439447)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'scales'), position=0, insert_id=439448)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=439449)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'view'), position=2, insert_id=439450)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=439451)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-1'), position=1, insert_id=439452)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=439453)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=439454)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=439455)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=5, insert_id=439456)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=439457)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 104,
        "neg_line": [
            "-rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3"
        ],
        "pos_line": [
            "+rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad) * scales.view(-1, 1, 1)  # Bx3x3"
        ],
        "core_change": "-rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3 +rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad) * scales.view(-1, 1, 1)  # Bx3x3",
        "core_API": "deg2rad"
    },
    {
        "commit_hash": "28df3f34f56ca210b378ec5e75011cd3d2964a3c",
        "index": "d7a2ed71f..541af9d5f 100644",
        "commit_message": "[RLlib]: Off-Policy Evaluation fixes. (#25899)\n\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FQETorchModel:",
            "q_values, _ = self.q_model({\"obs\": obs}, [], None)",
            "if actions is not None:",
            "actions = torch.tensor(actions, device=self.device, dtype=int)",
            "-            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()",
            "+            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)",
            "return q_values.detach()",
            "",
            "def estimate_v("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('unary_operator', '-1'), position=1, insert_id=1110966)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 105,
        "neg_line": [
            "-q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()"
        ],
        "pos_line": [
            "+q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)"
        ],
        "core_change": "-q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze() +q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)",
        "core_API": "q_model"
    },
    {
        "commit_hash": "29f45a45765e78d68e15113a4203de0548040314",
        "index": "ae09f2eee..06f7f7aa1 100644",
        "commit_message": "pip install espnet[train] (#3755)\n\n* Add pytorch=1.10.0 to CI configuration\n\n* fix:   test/espnet2/bin/test_k2_asr_inference.py\n\n* fix:   espnet2/main_funcs/pack_funcs.py\n\n* pip install espnet[train]\n\n* modified:   ci/install.sh\n\n* fix\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def decode(args):",
            "",
            "# define function for plot prob and att_ws",
            "def _plot_and_save(array, figname, figsize=(6, 4), dpi=150):",
            "+        import matplotlib",
            "+",
            "+        matplotlib.use(\"Agg\")",
            "import matplotlib.pyplot as plt",
            "",
            "shape = array.shape"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=131885)",
            "Insert(target_node=IN(type=block), node=('import_statement', None), position=0, insert_id=131886)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=1, insert_id=131887)",
            "Insert(target_node=IN(type=import_statement), node=('import', 'import'), position=0, insert_id=131888)",
            "Insert(target_node=IN(type=import_statement), node=('dotted_name', None), position=1, insert_id=131889)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=131890)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'matplotlib'), position=0, insert_id=131891)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=131892)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=131893)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'matplotlib'), position=0, insert_id=131894)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=131895)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'use'), position=2, insert_id=131896)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=131897)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"Agg\"'), position=1, insert_id=131898)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=131899)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 106,
        "neg_line": [],
        "pos_line": [
            "+import matplotlib",
            "+",
            "+matplotlib.use(\"Agg\")"
        ],
        "core_change": "+import matplotlib + +matplotlib.use(\"Agg\")",
        "core_API": "use"
    },
    {
        "commit_hash": "f8cf4a19856409b7d31007e384cc5cbdf81fe9ee",
        "index": "5397942e2d..30974ab2eb 100644",
        "commit_message": "[RLlib] Fixed import tensorflow when module not available (#16171)\n\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add condition check for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ModelCatalog:",
            "model_name (str): Name to register the model under.",
            "model_class (type): Python class of the model.",
            "\"\"\"",
            "-        if issubclass(model_class, tf.keras.Model):",
            "-            deprecation_warning(old=\"register_custom_model\", error=False)",
            "+        if tf is not None:",
            "+            if issubclass(model_class, tf.keras.Model):",
            "+                deprecation_warning(old=\"register_custom_model\", error=False)",
            "_global_registry.register(RLLIB_MODEL, model_name, model_class)",
            "",
            "@staticmethod"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 107,
        "neg_line": [
            "-if issubclass(model_class, tf.keras.Model):",
            "-deprecation_warning(old=\"register_custom_model\", error=False)"
        ],
        "pos_line": [
            "+if tf is not None:",
            "+if issubclass(model_class, tf.keras.Model):",
            "+deprecation_warning(old=\"register_custom_model\", error=False)"
        ],
        "core_change": "-if issubclass(model_class, tf.keras.Model): -deprecation_warning(old=\"register_custom_model\", error=False) +if tf is not None: +if issubclass(model_class, tf.keras.Model): +deprecation_warning(old=\"register_custom_model\", error=False)",
        "core_API": "register"
    },
    {
        "commit_hash": "8880f696b6b8368a76296126476ea020fc7c814c",
        "index": "f0f25b0..f6e4dd2 100644",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "yes",
        "comments": "remove condition check for shape fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DLA(nn.Module):",
            "if self.drop_rate > 0.:",
            "x = F.dropout(x, p=self.drop_rate, training=self.training)",
            "x = self.fc(x)",
            "-        if not self.global_pool.is_identity():",
            "-            x = x.flatten(1)  # conv classifier, flatten if pooling isn't pass-through (disabled)",
            "+        x = self.flatten(x)",
            "return x"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=x), value='self')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'x'), position=1, insert_id=1477861)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=global_pool))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=not_operator))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 109,
        "neg_line": [
            "-if not self.global_pool.is_identity():",
            "-x = x.flatten(1)  # conv classifier, flatten if pooling isn't pass-through (disabled)"
        ],
        "pos_line": [
            "+x = self.flatten(x)"
        ],
        "core_change": "-if not self.global_pool.is_identity(): -x = x.flatten(1)  # conv classifier, flatten if pooling isn't pass-through (disabled) +x = self.flatten(x)",
        "core_API": "dropout"
    },
    {
        "commit_hash": "628ee4b5f023d8c31668b901dfc058a8846560eb",
        "index": "3d43345e71..dbc5c6dd69 100644",
        "commit_message": "[RLlib] Bandit tf2 fix (+ add tf2 to test cases). (#24908)\n\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class OnlineLinearRegression(tf.Module if tf else object):",
            "x = tf.squeeze(x, axis=0)",
            "y = y[0]",
            "self.time += 1",
            "-        self.delta_f += y * x",
            "+        self.delta_f += tf.cast(y, tf.float32) * x",
            "self.delta_b += tf.tensordot(x, x, axes=0)",
            "# Can follow an update schedule if not doing sherman morison updates",
            "if self.time % self.update_schedule == 0:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=2136855)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2136856)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2136857)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2136858)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2136859)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2136860)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2136861)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=y), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2136862)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2136863)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2136864)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2136865)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2136866)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2136867)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 110,
        "neg_line": [
            "-self.delta_f += y * x"
        ],
        "pos_line": [
            "+self.delta_f += tf.cast(y, tf.float32) * x"
        ],
        "core_change": "-self.delta_f += y * x +self.delta_f += tf.cast(y, tf.float32) * x",
        "core_API": "squeeze"
    },
    {
        "commit_hash": "741c452551780e110938c8635db496682784be07",
        "index": "bb5d69199..dbc493aa7 100644",
        "commit_message": "Fix disabled grads after call to predict (#6657)\n\n\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(",
            "",
            "results = self.predict_loop.on_predict_epoch_end()",
            "self.predict_loop.on_predict_end()",
            "+",
            "+        # re-enable grads",
            "+        torch.set_grad_enabled(True)",
            "+",
            "return results",
            "",
            "def run_sanity_check(self, ref_model):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=540507)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=540508)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=540509)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=540510)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_grad_enabled'), position=3, insert_id=540511)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=540512)",
            "Insert(target_node=IN(type=argument_list), node=('true', 'True'), position=1, insert_id=540513)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=540514)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'torch'), position=0, insert_id=540515)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 10,
        "number": 112,
        "neg_line": [],
        "pos_line": [
            "+",
            "+# re-enable grads",
            "+torch.set_grad_enabled(True)",
            "+"
        ],
        "core_change": "+ +# re-enable grads +torch.set_grad_enabled(True) +",
        "core_API": "on_predict_epoch_end"
    },
    {
        "commit_hash": "9639a45f1e1a33071d53f709d2fb705d41c8f22f",
        "index": "606e92ab..ff5abaf5 100644",
        "commit_message": "Augmentation Base Refactor (#2117)\n\n* refactor\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Updated augmentation base.\n\n* Removed label & start apply/inverse\n\n* Finished augmentaion base refactor\n\n* container refactoring\n\n* Added missing files\n\n* Added ops\n\n* Update sequential ops\n\n* Almost there\n\n* Fixed computation matrix computation\n\n* Fixed randomcrop\n\n* Fixed erasing\n\n* almost almost\n\n* finished\n\n* Added missing file\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Bug and typing fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Added list typing\n\n* fixed test base\n\n* Fixed typing\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bug fix\n\n* Update kornia/augmentation/_2d/geometric/crop.py\n\nCo-authored-by: Jo√£o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n\n* Fixed build-docs\n\n* Fixed bfloat16 issue on torch1.13.1\n\n* Revert the last commit\n\n* Fixed typing\n\n* Fixed typos\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Typo fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jo√£o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def filter2d(",
            "input = input.view(-1, tmp_kernel.size(0), input.size(-2), input.size(-1))",
            "",
            "# convolve the tensor with the kernel.",
            "-    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)",
            "+    # NOTE: type(...) to fix getting `torch.bfloat16` type.",
            "+    # TODO: @johnnv1, fix it through the Augmentation Base.",
            "+    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1).type(input.dtype)",
            "",
            "if padding == 'same':",
            "out = output.view(b, c, h, w)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=390999)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=391000)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=391001)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=391002)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=391003)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=391004)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=391005)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input'), position=0, insert_id=391006)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=391007)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=391008)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 113,
        "neg_line": [
            "-output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)"
        ],
        "pos_line": [
            "+# NOTE: type(...) to fix getting `torch.bfloat16` type.",
            "+# TODO: @johnnv1, fix it through the Augmentation Base.",
            "+output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1).type(input.dtype)"
        ],
        "core_change": "-output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1) +# NOTE: type(...) to fix getting `torch.bfloat16` type. +# TODO: @johnnv1, fix it through the Augmentation Base. +output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1).type(input.dtype)",
        "core_API": "view"
    },
    {
        "commit_hash": "68686344145651e21240e575818c8347a179a429",
        "index": "16574805ec..620461a3f2 100644",
        "commit_message": "small fixes to remainder dtype handling when modulus=False.\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def remainder(",
            "res_floored = torch.where(res >= 0, torch.floor(res), torch.ceil(res))",
            "diff = res - res_floored",
            "diff, x2 = ivy.promote_types_of_inputs(diff, x2)",
            "-        return torch.mul(diff, x2, out=out)",
            "+        return torch.mul(diff, x2, out=out).to(x1.dtype)",
            "return torch.remainder(x1, x2, out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=338957)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=338958)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=338959)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=338960)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=338961)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=338962)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=338963)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x1'), position=0, insert_id=338964)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=338965)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=338966)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 114,
        "neg_line": [
            "-return torch.mul(diff, x2, out=out)"
        ],
        "pos_line": [
            "+return torch.mul(diff, x2, out=out).to(x1.dtype)"
        ],
        "core_change": "-return torch.mul(diff, x2, out=out) +return torch.mul(diff, x2, out=out).to(x1.dtype)",
        "core_API": "where"
    },
    {
        "commit_hash": "fffb5f445d77d1438a61dbdf32d48f6f3c0672c9",
        "index": "8ef44bb8b..43c5f3021 100644",
        "commit_message": "fix: action mask parameter\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GoalOrientedBotNetwork(TFModel):",
            "name='features')",
            "self._action = tf.placeholder(tf.int32, [1, None],",
            "name='ground_truth_action')",
            "-        self._action_mask = tf.placeholder(tf.float32, [1, None, self.n_actions],",
            "+        self._action_mask = tf.placeholder(tf.float32, [None, None, self.n_actions],",
            "name='action_mask')",
            "",
            "def _build_body(self):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('none', 'None'), position=1, insert_id=1923376)",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 115,
        "neg_line": [
            "-self._action_mask = tf.placeholder(tf.float32, [1, None, self.n_actions],"
        ],
        "pos_line": [
            "+self._action_mask = tf.placeholder(tf.float32, [None, None, self.n_actions],"
        ],
        "core_change": "-self._action_mask = tf.placeholder(tf.float32, [1, None, self.n_actions], +self._action_mask = tf.placeholder(tf.float32, [None, None, self.n_actions],",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "428516056abe41f135133e732a8d44af6ce9a234",
        "index": "4b038ee43..3b1d286c6 100644",
        "commit_message": "[RLlib] SAC Torch (incl. Atari learning) (#7984)\n\n* Policy-classes cleanup and torch/tf unification.\n- Make Policy abstract.\n- Add `action_dist` to call to `extra_action_out_fn` (necessary for PPO torch).\n- Move some methods and vars to base Policy\n  (from TFPolicy): num_state_tensors, ACTION_PROB, ACTION_LOGP and some more.\n\n* Fix `clip_action` import from Policy (should probably be moved into utils altogether).\n\n* - Move `is_recurrent()` and `num_state_tensors()` into TFPolicy (from DynamicTFPolicy).\n- Add config to all Policy c'tor calls (as 3rd arg after obs and action spaces).\n\n* Add `config` to c'tor call to TFPolicy.\n\n* Add missing `config` to c'tor call to TFPolicy in marvil_policy.py.\n\n* Fix test_rollout_worker.py::MockPolicy and BadPolicy classes (Policy base class is now abstract).\n\n* Fix LINT errors in Policy classes.\n\n* Implement StatefulPolicy abstract methods in test cases: test_multi_agent_env.py.\n\n* policy.py LINT errors.\n\n* Create a simple TestPolicy to sub-class from when testing Policies (reduces code in some test cases).\n\n* policy.py\n- Remove abstractmethod from `apply_gradients` and `compute_gradients` (these are not required iff `learn_on_batch` implemented).\n- Fix docstring of `num_state_tensors`.\n\n* Make QMIX torch Policy a child of TorchPolicy (instead of Policy).\n\n* QMixPolicy add empty implementations of abstract Policy methods.\n\n* Store Policy's config in self.config in base Policy c'tor.\n\n* - Make only compute_actions in base Policy's an abstractmethod and provide pass\nimplementation to all other methods if not defined.\n- Fix state_batches=None (most Policies don't have internal states).\n\n* Cartpole tf learning.\n\n* Cartpole tf AND torch learning (in ~ same ts).\n\n* Cartpole tf AND torch learning (in ~ same ts). 2\n\n* Cartpole tf (torch syntax-broken) learning (in ~ same ts). 3\n\n* Cartpole tf AND torch learning (in ~ same ts). 4\n\n* Cartpole tf AND torch learning (in ~ same ts). 5\n\n* Cartpole tf AND torch learning (in ~ same ts). 6\n\n* Cartpole tf AND torch learning (in ~ same ts). Pendulum tf learning.\n\n* WIP.\n\n* WIP.\n\n* SAC torch learning Pendulum.\n\n* WIP.\n\n* SAC torch and tf learning Pendulum and Cartpole after cleanup.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* SAC: Move policy.target_model to policy.device as well.\n\n* Fixes and cleanup.\n\n* Fix data-format of tf keras Conv2d layers (broken for some tf-versions which have data_format=\"channels_first\" as default).\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* Test fixes and LINT.\n\n* Fixes and LINT.\n\nCo-authored-by: Sven Mika <sven@Svens-MacBook-Pro.local>\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EpsilonGreedy(Exploration):",
            "torch.multinomial(random_valid_action_logits, 1), axis=1)",
            "# Pick either random or greedy.",
            "action = torch.where(",
            "-                torch.empty((batch_size, )).uniform_() < epsilon,",
            "+                torch.empty(",
            "+                    (batch_size, )).uniform_().to(self.device) < epsilon,",
            "random_actions, exploit_action)",
            "",
            "return action, action_logp"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1124827)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1124828)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1124829)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1124830)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1124831)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1124832)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1124833)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1124834)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1124835)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1124836)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 116,
        "neg_line": [
            "-torch.empty((batch_size, )).uniform_() < epsilon,"
        ],
        "pos_line": [
            "+torch.empty(",
            "+(batch_size, )).uniform_().to(self.device) < epsilon,"
        ],
        "core_change": "-torch.empty((batch_size, )).uniform_() < epsilon, +torch.empty( +(batch_size, )).uniform_().to(self.device) < epsilon,",
        "core_API": "multinomial"
    },
    {
        "commit_hash": "8f6d73a93ab7ef9a38ca169404a4d3cccfa22d4f",
        "index": "f9104adda..80f9f8626 100644",
        "commit_message": "[sgd] Extend distributed pytorch functionality (#5675)\n\n* raysgd\n\n* apply fn\n\n* double quotes\n\n* removed duplicate TimerStat\n\n* removed duplicate find_free_port\n\n* imports in pytorch_trainer\n\n* init doc\n\n* ray.experimental\n\n* remove resize example\n\n* resnet example\n\n* cifar\n\n* Fix up after kwargs\n\n* data_dir and dataloader_workers args\n\n* formatting\n\n* loss\n\n* init\n\n* update code\n\n* lint\n\n* smoketest\n\n* better_configs\n\n* fix\n\n* fix\n\n* fix\n\n* train_loader\n\n* fixdocs\n\n* ok\n\n* ok\n\n* fix\n\n* fix_update\n\n* fix\n\n* fix\n\n* done\n\n* fix\n\n* fix\n\n* fix\n\n* small\n\n* lint\n\n* fix\n\n* fix\n\n* fix_test\n\n* fix\n\n* validate\n\n* fix\n\n* fi\n\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def test_save_and_restore(ray_start_2_cpus, num_replicas):  # noqa: F811",
            "model_creator,",
            "data_creator,",
            "optimizer_creator,",
            "+        loss_creator=lambda config: nn.MSELoss(),",
            "num_replicas=num_replicas)",
            "trainer2.restore(filename)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=1, insert_id=1508635)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1508636)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=pattern_list), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1508637)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=2, insert_id=1508638)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=num_replicas), position=0)",
            "Insert(target_node=ASTNode(type=pattern_list), node=('identifier', 'loss_creator'), position=6, insert_id=1508639)",
            "Insert(target_node=IN(type=expression_list), node=('lambda', None), position=0, insert_id=1508640)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=1508641)",
            "Insert(target_node=IN(type=lambda), node=('lambda', 'lambda'), position=0, insert_id=1508642)",
            "Insert(target_node=IN(type=lambda), node=('lambda_parameters', None), position=1, insert_id=1508643)",
            "Insert(target_node=IN(type=lambda), node=(':', ':'), position=2, insert_id=1508644)",
            "Insert(target_node=IN(type=lambda), node=('call', None), position=3, insert_id=1508645)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 'config'), position=0, insert_id=1508646)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1508647)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1508648)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1508649)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1508650)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'MSELoss'), position=2, insert_id=1508651)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1508652)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1508653)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 117,
        "neg_line": [],
        "pos_line": [
            "+loss_creator=lambda config: nn.MSELoss(),"
        ],
        "core_change": "+loss_creator=lambda config: nn.MSELoss(),",
        "core_API": "MSELoss"
    },
    {
        "commit_hash": "cd81fbf80030a744b519effb6defb96143a30d18",
        "index": "ee2f4ee2..3dab0e00 100644",
        "commit_message": "Fix bug in dist.Delta.expand() type (#1236)\n\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Delta(TorchDistribution):",
            "",
            "def expand(self, batch_shape):",
            "validate_args = self.__dict__.get('_validate_args')",
            "+        batch_shape = torch.Size(batch_shape)",
            "v = self.v.expand(batch_shape + self.event_shape)",
            "log_density = self.log_density.expand(batch_shape)",
            "return Delta(v, log_density, self.event_dim, validate_args=validate_args)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=739100)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=739101)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'batch_shape'), position=0, insert_id=739102)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=739103)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=739104)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=739105)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=739106)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=739107)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=739108)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Size'), position=2, insert_id=739109)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=739110)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'batch_shape'), position=1, insert_id=739111)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=739112)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 118,
        "neg_line": [],
        "pos_line": [
            "+batch_shape = torch.Size(batch_shape)"
        ],
        "core_change": "+batch_shape = torch.Size(batch_shape)",
        "core_API": "get"
    },
    {
        "commit_hash": "b7045b1938cb58506a6f8dc8274c971f630d9105",
        "index": "10f7355c..17a6b131 100644",
        "commit_message": "fix buffer transfer bug (#2045)\n\n\n",
        "file": "nni.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "",
            "pruner = AGP_Pruner(model, configure_list)",
            "model = pruner.compress()",
            "-",
            "+    model = model.to(device)",
            "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)",
            "for epoch in range(10):",
            "pruner.update_epoch(epoch)",
            "print('# Epoch {} #'.format(epoch))",
            "train(model, device, train_loader, optimizer)",
            "test(model, device, test_loader)",
            "-    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])",
            "+    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)",
            "",
            "",
            "if __name__ == '__main__':"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=667601)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=667602)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'model'), position=0, insert_id=667603)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=667604)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=667605)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=667606)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=667607)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=667608)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'device'), position=9, insert_id=667609)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=0, insert_id=667610)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=667611)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=667612)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=667613)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=667614)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=667615)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 119,
        "neg_line": [
            "-",
            "-pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])"
        ],
        "pos_line": [
            "+model = model.to(device)",
            "+pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)"
        ],
        "core_change": "- +model = model.to(device) -pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28]) +pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)",
        "core_API": "compress"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "a7e308d5..69cfe813 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "change API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBidirectionalLanguageModelTransformer(AllenNlpTestCase):",
            "input_dim=32, hidden_dim=64, num_layers=2",
            ")",
            "",
            "-        mask = torch.ones(3, 6).int()",
            "-        mask[0, 3:] = 0",
            "-        mask[1, 5:] = 0",
            "+        mask = torch.ones(3, 6).bool()",
            "+        mask[0, 3:] = False",
            "+        mask[1, 5:] = False",
            "",
            "forward_mask, backward_mask = transformer_encoder.get_attention_masks(mask)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('false', 'False'), position=2, insert_id=19790)",
            "Insert(target_node=ASTNode(type=assignment), node=('false', 'False'), position=2, insert_id=19791)",
            "Update(target_node=ASTNode(type=identifier, text=int), value='bool')",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 5,
        "number": 120,
        "neg_line": [
            "-mask = torch.ones(3, 6).int()",
            "-mask[0, 3:] = 0",
            "-mask[1, 5:] = 0"
        ],
        "pos_line": [
            "+mask = torch.ones(3, 6).bool()",
            "+mask[0, 3:] = False",
            "+mask[1, 5:] = False"
        ],
        "core_change": "-mask = torch.ones(3, 6).int() -mask[0, 3:] = 0 -mask[1, 5:] = 0 +mask = torch.ones(3, 6).bool() +mask[0, 3:] = False +mask[1, 5:] = False",
        "core_API": "ones"
    },
    {
        "commit_hash": "11a61ab0aa7f93401ffd029d8f1b361d72871722",
        "index": "a8a9eea03..0c91a6538 100644",
        "commit_message": "[release][train] Add `FileLock` to `tensorflow_mnist_example`. (#32712)\n\n* [release][train] Add `FileLock` to `tensorflow_mnist_example`.\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n\n* fix\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n\n---------\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "from ray.air.config import ScalingConfig",
            "",
            "",
            "def mnist_dataset(batch_size: int) -> tf.data.Dataset:",
            "-    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()",
            "+    with FileLock(os.path.expanduser(\"~/.mnist_lock\")):",
            "+        (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()",
            "# The `x` arrays are in uint8 and have values in the [0, 255] range.",
            "# You need to convert them to float32 with values in the [0, 1] range.",
            "x_train = x_train / np.float32(255)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('with_statement', None), position=0, insert_id=1801879)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=1801880)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=1801881)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=1801882)",
            "Move(target_node=IN(type=with_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=1801883)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=1801884)",
            "Insert(target_node=IN(type=call), node=('identifier', 'FileLock'), position=0, insert_id=1801885)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1801886)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1801887)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1801888)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1801889)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1801890)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1801891)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1801892)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1801893)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'expanduser'), position=2, insert_id=1801894)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1801895)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"~/.mnist_lock\"'), position=1, insert_id=1801896)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1801897)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'os'), position=0, insert_id=1801898)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1801899)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'path'), position=2, insert_id=1801900)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 121,
        "neg_line": [
            "-(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()"
        ],
        "pos_line": [
            "+with FileLock(os.path.expanduser(\"~/.mnist_lock\")):",
            "+(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()"
        ],
        "core_change": "-(x_train, y_train), _ = tf.keras.datasets.mnist.load_data() +with FileLock(os.path.expanduser(\"~/.mnist_lock\")): +(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()",
        "core_API": "load_data"
    },
    {
        "commit_hash": "d99245f1ccd6be547702c3e4d4e8f8e54597c569",
        "index": "46bdc50..9ad923b 100644",
        "commit_message": "Fix examples given new tensorflow rnn interface. Update bidirectional rnn to return final state.\n\n",
        "file": "skflow.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def rnn_model(X, y):",
            "# Given encoding of RNN, take encoding of last step (e.g hidden size of the",
            "# neural network of last step) and pass it as features for logistic",
            "# regression over output classes.",
            "-    return skflow.models.logistic_regression(encoding[-1], y)",
            "+    return skflow.models.logistic_regression(encoding, y)",
            "",
            "classifier = skflow.TensorFlowEstimator(model_fn=rnn_model, n_classes=15,",
            "steps=1000, optimizer='Adam', learning_rate=0.01, continue_training=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=encoding), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 122,
        "neg_line": [
            "-return skflow.models.logistic_regression(encoding[-1], y)"
        ],
        "pos_line": [
            "+return skflow.models.logistic_regression(encoding, y)"
        ],
        "core_change": "-return skflow.models.logistic_regression(encoding[-1], y) +return skflow.models.logistic_regression(encoding, y)",
        "core_API": "logistic_regression"
    },
    {
        "commit_hash": "9e5a469c64ca7121d3558f3ddf40b1a3e993ffcc",
        "index": "9553d745..7f79bfca 100644",
        "commit_message": "d-vector handling (#1945)\n\n* Update BaseDatasetConfig\n\n- Add dataset_name\n- Chane name to formatter_name\n\n* Update compute_embedding\n\n- Allow entering dataset by args\n- Use released model by default\n- Use the new key format\n\n* Update loading\n\n* Update recipes\n\n* Update other dep code\n\n* Update tests\n\n* Fixup\n\n* Load multiple embedding files\n\n* Fix argument names in dep code\n\n* Update docs\n\n* Fix argument name\n\n* Fix linter\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "config.save_json(config_path)",
            "command_train = (",
            "f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"",
            "f\"--coqpit.output_path {output_path} \"",
            "-    \"--coqpit.datasets.0.name ljspeech_test \"",
            "+    \"--coqpit.datasets.0.formatter ljspeech_test \"",
            "\"--coqpit.datasets.0.meta_file_train metadata.csv \"",
            "\"--coqpit.datasets.0.meta_file_val metadata.csv \"",
            "\"--coqpit.datasets.0.path tests/data/ljspeech \""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"--coqpit.datasets.0.name ljspeech_test \"), value='\"--coqpit.datasets.0.formatter ljspeech_test \"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 125,
        "neg_line": [
            "-\"--coqpit.datasets.0.name ljspeech_test \""
        ],
        "pos_line": [
            "+\"--coqpit.datasets.0.formatter ljspeech_test \""
        ],
        "core_change": "-\"--coqpit.datasets.0.name ljspeech_test \" +\"--coqpit.datasets.0.formatter ljspeech_test \"",
        "core_API": "save_json"
    },
    {
        "commit_hash": "093e901c979c0f8b0af92f79010b53db1ddcc972",
        "index": "875bcd2..6f85a9c 100644",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main_fun(argv, ctx):",
            "grads = average_gradients(tower_grads)",
            "",
            "# Add a summary to track the learning rate.",
            "-      summaries.append(tf.scalar_summary('learning_rate', lr))",
            "+      summaries.append(tf.summary.scalar('learning_rate', lr))",
            "",
            "# Add histograms for gradients.",
            "for grad, var in grads:",
            "if grad is not None:",
            "summaries.append(",
            "-              tf.histogram_summary(var.op.name + '/gradients', grad))",
            "+              tf.summary.histogram(var.op.name + '/gradients', grad))",
            "",
            "# Apply the gradients to adjust the shared variables.",
            "apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)",
            "",
            "# Add histograms for trainable variables.",
            "for var in tf.trainable_variables():",
            "-        summaries.append(tf.histogram_summary(var.op.name, var))",
            "+        summaries.append(tf.summary.histogram(var.op.name, var))",
            "",
            "# Track the moving averages of all trainable variables.",
            "variable_averages = tf.train.ExponentialMovingAverage("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2213610)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'summaries'), position=0, insert_id=2213611)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2213612)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'append'), position=2, insert_id=2213613)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2213614)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2213615)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2213616)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'scalar'), position=2, insert_id=2213617)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2213618)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'histogram'), position=2, insert_id=2213619)",
            "Update(target_node=ASTNode(type=identifier, text=scalar_summary), value='summary')",
            "Update(target_node=ASTNode(type=identifier, text=histogram_summary), value='summary')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2213620)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2213621)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'histogram'), position=2, insert_id=2213622)",
            "Update(target_node=ASTNode(type=identifier, text=histogram_summary), value='summary')",
            "Delete(target_node=ASTNode(type=identifier, text=summaries))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=append))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 126,
        "neg_line": [
            "-summaries.append(tf.scalar_summary('learning_rate', lr))",
            "-tf.histogram_summary(var.op.name + '/gradients', grad))",
            "-summaries.append(tf.histogram_summary(var.op.name, var))"
        ],
        "pos_line": [
            "+summaries.append(tf.summary.scalar('learning_rate', lr))",
            "+tf.summary.histogram(var.op.name + '/gradients', grad))",
            "+summaries.append(tf.summary.histogram(var.op.name, var))"
        ],
        "core_change": "-summaries.append(tf.scalar_summary('learning_rate', lr)) +summaries.append(tf.summary.scalar('learning_rate', lr)) -tf.histogram_summary(var.op.name + '/gradients', grad)) +tf.summary.histogram(var.op.name + '/gradients', grad)) -summaries.append(tf.histogram_summary(var.op.name, var)) +summaries.append(tf.summary.histogram(var.op.name, var))",
        "core_API": "append"
    },
    {
        "commit_hash": "920b2006bb91831dce89fc0040850982ecf0a3de",
        "index": "0513f26..bdd7fa7 100644",
        "commit_message": "Fix PyText GPU Test (#378)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/378\n\nA patch to Fix GPU Testing.\n\nReviewed By: hikushalhere\n\nDifferential Revision: D14400231\n\nfbshipit-source-id: 0cf058bf887fc11d2a88e17dc0809157f3f9eb68\n\n",
        "file": "pytext.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class Trainer(TrainerBase):",
            "",
            "@timing.time(\"Trainer.test\")",
            "def test(self, test_iter, model, metric_reporter: MetricReporter):",
            "+        if cuda.CUDA_ENABLED:",
            "+            model = model.cuda()",
            "+",
            "model.eval()",
            "with torch.no_grad():",
            "test_metric = self._run_epoch("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1863287)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=1863288)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1863289)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=1863290)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1863291)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1863292)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=0, insert_id=1863293)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1863294)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'CUDA_ENABLED'), position=2, insert_id=1863295)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1863296)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1863297)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'model'), position=0, insert_id=1863298)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1863299)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1863300)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1863301)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1863302)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=0, insert_id=1863303)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1863304)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1863305)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1863306)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1863307)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 127,
        "neg_line": [],
        "pos_line": [
            "+if cuda.CUDA_ENABLED:",
            "+model = model.cuda()",
            "+"
        ],
        "core_change": "+if cuda.CUDA_ENABLED: +model = model.cuda() +",
        "core_API": "time"
    },
    {
        "commit_hash": "989a15d173ea646cdfc5179478859232ac807136",
        "index": "adaf50da8..68efcbd5c 100644",
        "commit_message": "fix _setup_devices in case where there is no torch.distributed package in build (#16821)\n\n* fix _setup_devices in case where there is not torch.distributed\n\n* in training_args_sm.py as well\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TrainingArguments:",
            "@torch_required",
            "def _setup_devices(self) -> \"torch.device\":",
            "logger.info(\"PyTorch: setting up devices\")",
            "-        if torch.distributed.is_initialized() and self.local_rank == -1:",
            "+        if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1:",
            "logger.warning(",
            "\"torch.distributed process group is initialized, but local_rank == -1. \"",
            "\"In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('boolean_operator', None), position=0, insert_id=1201467)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=1201468)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1201469)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1201470)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1201471)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1201472)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1201473)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_available'), position=2, insert_id=1201474)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1201475)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1201476)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1201477)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1201478)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=1201479)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 128,
        "neg_line": [
            "-if torch.distributed.is_initialized() and self.local_rank == -1:"
        ],
        "pos_line": [
            "+if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1:"
        ],
        "core_change": "-if torch.distributed.is_initialized() and self.local_rank == -1: +if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1:",
        "core_API": "info"
    },
    {
        "commit_hash": "673b7d1e5d5d108c4a5975e02de154d8124be0ff",
        "index": "68b5a3d..af31a9e 100755",
        "commit_message": "Fix deprecated functions warnings\n",
        "file": "cnn-text-classification-tf.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.Graph().as_default():",
            "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")",
            "if not os.path.exists(checkpoint_dir):",
            "os.makedirs(checkpoint_dir)",
            "-        saver = tf.train.Saver(tf.all_variables())",
            "+        saver = tf.train.Saver(tf.global_variables())",
            "",
            "# Write vocabulary",
            "vocab_processor.save(os.path.join(out_dir, \"vocab\"))",
            "",
            "# Initialize all variables",
            "-        sess.run(tf.initialize_all_variables())",
            "+        sess.run(tf.global_variables_initializer())",
            "",
            "def train_step(x_batch, y_batch):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1910792)",
            "Update(target_node=ASTNode(type=identifier, text=initialize_all_variables), value='global_variables_initializer')",
            "Update(target_node=ASTNode(type=identifier, text=all_variables), value='global_variables')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1910793)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1910794)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 131,
        "neg_line": [
            "-saver = tf.train.Saver(tf.all_variables())",
            "-sess.run(tf.initialize_all_variables())"
        ],
        "pos_line": [
            "+saver = tf.train.Saver(tf.global_variables())",
            "+sess.run(tf.global_variables_initializer())"
        ],
        "core_change": "-saver = tf.train.Saver(tf.all_variables()) +saver = tf.train.Saver(tf.global_variables()) -sess.run(tf.initialize_all_variables()) +sess.run(tf.global_variables_initializer())",
        "core_API": "Graph"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "5c34c658b..ddc223637 100755",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CLIPTextTransformer(nn.Module):",
            "attentions=encoder_outputs.attentions,",
            ")",
            "",
            "-    def _build_causal_attention_mask(self, bsz, seq_len):",
            "+    def _build_causal_attention_mask(self, bsz, seq_len, dtype):",
            "# lazily create causal attention mask, with full attention between the vision tokens",
            "# pytorch uses additive attention mask; fill with -inf",
            "-        mask = torch.empty(bsz, seq_len, seq_len)",
            "-        mask.fill_(torch.tensor(float(\"-inf\")))",
            "+        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)",
            "+        mask.fill_(torch.tensor(torch.finfo(dtype).min))",
            "mask.triu_(1)  # zero out the lower diagonal",
            "mask = mask.unsqueeze(1)  # expand mask",
            "return mask"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=1195975)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'dtype'), position=7, insert_id=1195976)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1195977)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=1195978)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1195979)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1195980)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=1195981)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1195982)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1195983)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1195984)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1195985)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=float), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1195986)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1195987)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'dtype'), position=1, insert_id=1195988)",
            "Delete(target_node=ASTNode(type=string, text=\"-inf\"))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 132,
        "neg_line": [
            "-def _build_causal_attention_mask(self, bsz, seq_len):",
            "-mask = torch.empty(bsz, seq_len, seq_len)",
            "-mask.fill_(torch.tensor(float(\"-inf\")))"
        ],
        "pos_line": [
            "+def _build_causal_attention_mask(self, bsz, seq_len, dtype):",
            "+mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)",
            "+mask.fill_(torch.tensor(torch.finfo(dtype).min))"
        ],
        "core_change": "-def _build_causal_attention_mask(self, bsz, seq_len): +def _build_causal_attention_mask(self, bsz, seq_len, dtype): -mask = torch.empty(bsz, seq_len, seq_len) -mask.fill_(torch.tensor(float(\"-inf\"))) +mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype) +mask.fill_(torch.tensor(torch.finfo(dtype).min))",
        "core_API": "empty"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "f38c6644..e610e020 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomPerspective(GeometricAugmentationBase2D):",
            "size: Optional[Tuple[int, int]] = None,",
            ") -> Tensor:",
            "return self.apply_transform(",
            "-            input, params=self._params, transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),",
            "-            flags=flags",
            "+            input,",
            "+            params=self._params,",
            "+            transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),",
            "+            flags=flags,",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=402551)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 134,
        "neg_line": [
            "-input, params=self._params, transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),",
            "-flags=flags"
        ],
        "pos_line": [
            "+input,",
            "+params=self._params,",
            "+transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),",
            "+flags=flags,"
        ],
        "core_change": "-input, params=self._params, transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype), -flags=flags +input, +params=self._params, +transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype), +flags=flags,",
        "core_API": "apply_transform"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "58f65819..9ad4e18f 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def get_mini_batch(mini_batch_indices, sequences, seq_lengths, cuda=False):",
            "# get mask for mini-batch",
            "mini_batch_mask = get_mini_batch_mask(mini_batch, sorted_seq_lengths)",
            "",
            "-    # wrap in PyTorch Variables",
            "-    mini_batch = Variable(torch.Tensor(mini_batch))",
            "-    mini_batch_reversed = Variable(torch.Tensor(mini_batch_reversed))",
            "-    mini_batch_mask = Variable(torch.Tensor(mini_batch_mask))",
            "+    # wrap in PyTorch Tensors",
            "+    mini_batch = torch.tensor(mini_batch)",
            "+    mini_batch_reversed = torch.tensor(mini_batch_reversed)",
            "+    mini_batch_mask = torch.tensor(mini_batch_mask)",
            "",
            "# cuda() here because need to cuda() before packing",
            "if cuda:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 21,
        "number": 135,
        "neg_line": [
            "-# wrap in PyTorch Variables",
            "-mini_batch = Variable(torch.Tensor(mini_batch))",
            "-mini_batch_reversed = Variable(torch.Tensor(mini_batch_reversed))",
            "-mini_batch_mask = Variable(torch.Tensor(mini_batch_mask))"
        ],
        "pos_line": [
            "+# wrap in PyTorch Tensors",
            "+mini_batch = torch.tensor(mini_batch)",
            "+mini_batch_reversed = torch.tensor(mini_batch_reversed)",
            "+mini_batch_mask = torch.tensor(mini_batch_mask)"
        ],
        "core_change": "-# wrap in PyTorch Variables -mini_batch = Variable(torch.Tensor(mini_batch)) -mini_batch_reversed = Variable(torch.Tensor(mini_batch_reversed)) -mini_batch_mask = Variable(torch.Tensor(mini_batch_mask)) +# wrap in PyTorch Tensors +mini_batch = torch.tensor(mini_batch) +mini_batch_reversed = torch.tensor(mini_batch_reversed) +mini_batch_mask = torch.tensor(mini_batch_mask)",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "60529ce87..1c6240174 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "logger = logging.getLogger(__name__)",
            "# for the pretrained weights provided with the models",
            "####################################################",
            "XXX_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"xxx-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-pytorch_model.bin\",",
            "-    \"xxx-large-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-pytorch_model.bin\",",
            "+    \"xxx-base-uncased\": \"https://cdn.huggingface.co/xxx-base-uncased-pytorch_model.bin\",",
            "+    \"xxx-large-uncased\": \"https://cdn.huggingface.co/xxx-large-uncased-pytorch_model.bin\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=dictionary), position=2)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689817)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689818)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/xxx-base-uncased-pytorch_model.bin\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/xxx-large-uncased-pytorch_model.bin\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 136,
        "neg_line": [
            "-\"xxx-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-pytorch_model.bin\",",
            "-\"xxx-large-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-pytorch_model.bin\","
        ],
        "pos_line": [
            "+\"xxx-base-uncased\": \"https://cdn.huggingface.co/xxx-base-uncased-pytorch_model.bin\",",
            "+\"xxx-large-uncased\": \"https://cdn.huggingface.co/xxx-large-uncased-pytorch_model.bin\","
        ],
        "core_change": "-\"xxx-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-pytorch_model.bin\", -\"xxx-large-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-pytorch_model.bin\", +\"xxx-base-uncased\": \"https://cdn.huggingface.co/xxx-base-uncased-pytorch_model.bin\", +\"xxx-large-uncased\": \"https://cdn.huggingface.co/xxx-large-uncased-pytorch_model.bin\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "8d1f0aec..859f1ac9 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "yes",
        "comments": "change API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FlopsProfiler(object):",
            "start_time_hook)",
            "",
            "def end_time_hook(module, input, output):",
            "-                torch.cuda.synchronize()",
            "+                get_accelerator().synchronize()",
            "module.__duration__ += time.time() - module.__start_time__",
            "",
            "if not hasattr(module, \"__end_time_hook_handle__\"):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=74886)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_accelerator')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=74887)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=74888)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=74889)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 137,
        "neg_line": [
            "-torch.cuda.synchronize()"
        ],
        "pos_line": [
            "+get_accelerator().synchronize()"
        ],
        "core_change": "-torch.cuda.synchronize() +get_accelerator().synchronize()",
        "core_API": "synchronize"
    },
    {
        "commit_hash": "e172f0087a176b36efe2de8e91efc3e621f36a64",
        "index": "18da7e9..805584f 100644",
        "commit_message": "BatchNorm2D -> BatchNorm2d (#558)\n\n* BatchNorm2D -> BatchNorm2d\n\n* Fix typo\n",
        "file": "tinygrad.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TestOpt(unittest.TestCase):",
            "assert len(GlobalCounters.cache) == 2, \"optimizer didn't fold conv/relu\"",
            "",
            "if __name__ == '__main__':",
            "-  unittest.main()",
            "\\ No newline at end of file",
            "+  unittest.main()"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1520605)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=call), position=0)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 138,
        "neg_line": [
            "-unittest.main()"
        ],
        "pos_line": [
            "+unittest.main()"
        ],
        "core_change": "-unittest.main() +unittest.main()",
        "core_API": "main"
    },
    {
        "commit_hash": "7608b9f677a41cf4704bf1dd6fcca2f2a15825ca",
        "index": "c9f3f8ea..0ad4b0e8 100644",
        "commit_message": "Update GP use the new Parameterized class (#1621)\n\n* add ..nparameterized\n\n* noise constraint positive\n\n* move parameterized to gp\n\n* fix docs\n\n* add helper train and use it\n\n* make scrub tutorials\n\n* make docs\n\n* use full path for SVI TraceELBO\n\n* change test_benckmark too\n\n* resolve merge conflict\n\n* fix bug at svdkl and other docs\n\n* address comments\n\n* raise NotImplementError\n\n* remove name and get_param\n\n* filling missing stuffs\n\n* fix remaining tests\n\n* fix all nits\n\n* update tutorial\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Brownian(Kernel):",
            "",
            "Zt = Z.t()",
            "return torch.where(X.sign() == Zt.sign(),",
            "-                           variance * torch.min(X.abs(), Zt.abs()),",
            "+                           self.variance * torch.min(X.abs(), Zt.abs()),",
            "X.data.new_zeros(X.size(0), Z.size(0)))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=0, insert_id=726425)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=726426)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=726427)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=variance), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 139,
        "neg_line": [
            "-variance * torch.min(X.abs(), Zt.abs()),"
        ],
        "pos_line": [
            "+self.variance * torch.min(X.abs(), Zt.abs()),"
        ],
        "core_change": "-variance * torch.min(X.abs(), Zt.abs()), +self.variance * torch.min(X.abs(), Zt.abs()),",
        "core_API": "t"
    },
    {
        "commit_hash": "8e5e4892af72062a7c3f7fd890caefdcf19bab28",
        "index": "d28b275..4b5ae74 100644",
        "commit_message": "fix seed generator\n\n",
        "file": "lama-cleaner.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PaintByExample(DiffusionInpaintModel):",
            "mask: [H, W, 1] 255 means area to repaint",
            "return: BGR IMAGE",
            "\"\"\"",
            "-        set_seed(config.paint_by_example_seed)",
            "-",
            "output = self.model(",
            "image=PIL.Image.fromarray(image),",
            "mask_image=PIL.Image.fromarray(mask[:, :, -1], mode=\"L\"),",
            "example_image=config.paint_by_example_example_image,",
            "num_inference_steps=config.paint_by_example_steps,",
            "output_type='np.array',",
            "+            generator=torch.manual_seed(config.paint_by_example_seed)",
            ").images[0]",
            "",
            "output = (output * 255).round().astype(\"uint8\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 140,
        "neg_line": [
            "-set_seed(config.paint_by_example_seed)",
            "-"
        ],
        "pos_line": [
            "+generator=torch.manual_seed(config.paint_by_example_seed)"
        ],
        "core_change": "-set_seed(config.paint_by_example_seed) - +generator=torch.manual_seed(config.paint_by_example_seed)",
        "core_API": "model"
    },
    {
        "commit_hash": "44e3e3fb4930298f092f336c2b7add3ebf051928",
        "index": "f8182e8b6..fb7605189 100755",
        "commit_message": "prepare for \"__floordiv__ is deprecated  and its behavior will change in a future version of pytorch\" (#20211)\n\n* rounding_mode = \"floor\"  instead of // to prevent behavioral change\n\n* add other TODO\n\n* use `torch_int_div` from pytrch_utils\n\n* same for tests\n\n* fix copies\n\n* style\n\n* use relative imports when needed\n\n* Co-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BigBirdPegasusBlockSparseAttention(nn.Module):",
            "num_indices_to_gather = indices.shape[-2] * indices.shape[-1]",
            "num_indices_to_pick_from = params.shape[2]",
            "",
            "-        indices_shift = (",
            "-            torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)",
            "-            // num_indices_to_gather",
            "-            * num_indices_to_pick_from",
            "-        )",
            "+        shift = torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)",
            "+        indices_shift = torch_int_div(shift, num_indices_to_gather) * num_indices_to_pick_from",
            "",
            "flattened_indices = indices.view(-1) + indices_shift",
            "flattened_params = params.reshape(-1, params.shape[-2], params.shape[-1])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1175367)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1175368)",
            "Update(target_node=ASTNode(type=identifier, text=indices_shift), value='shift')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'indices_shift'), position=0, insert_id=1175369)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1175370)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=1175371)",
            "Insert(target_node=IN(type=binary_operator), node=('call', None), position=0, insert_id=1175372)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=*, text=*), position=1)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=num_indices_to_pick_from), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'torch_int_div'), position=0, insert_id=1175373)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1175374)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1175375)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'shift'), position=1, insert_id=1175376)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1175377)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=num_indices_to_gather), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1175378)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=//, text=//))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 2,
        "minus_line": 5,
        "AST_diff_line": 23,
        "number": 141,
        "neg_line": [
            "-indices_shift = (",
            "-torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)",
            "-// num_indices_to_gather",
            "-* num_indices_to_pick_from",
            "-)"
        ],
        "pos_line": [
            "+shift = torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)",
            "+indices_shift = torch_int_div(shift, num_indices_to_gather) * num_indices_to_pick_from"
        ],
        "core_change": "-indices_shift = ( -torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device) -// num_indices_to_gather -* num_indices_to_pick_from -) +shift = torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device) +indices_shift = torch_int_div(shift, num_indices_to_gather) * num_indices_to_pick_from",
        "core_API": "arange"
    },
    {
        "commit_hash": "faabe21f84adebf558be4836ebec3d66e8d540d1",
        "index": "9a85d704c..3dc1b4597 100644",
        "commit_message": "fix dataset purge method\nmove publish_obj.exists assert statement before .get\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "use custom method for function",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_benchmark_datasets() -> None:",
            "assert benchmark_report[key_size][\"publish_secs\"] <= timeout",
            "",
            "print(\"purge datasets...\")",
            "-    domain.datasets.purge(skip_checks=True)",
            "+    clean_datasets_on_domain(DOMAIN1_PORT)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=domain), value='clean_datasets_on_domain')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=domain), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=skip_checks), value='DOMAIN1_PORT')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=skip_checks), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=purge))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 142,
        "neg_line": [
            "-domain.datasets.purge(skip_checks=True)"
        ],
        "pos_line": [
            "+clean_datasets_on_domain(DOMAIN1_PORT)"
        ],
        "core_change": "-domain.datasets.purge(skip_checks=True) +clean_datasets_on_domain(DOMAIN1_PORT)",
        "core_API": "purge"
    },
    {
        "commit_hash": "ef023f03599ca18fc137fb79cb737014320a1ed3",
        "index": "cca36c2..556cfb3 100644",
        "commit_message": "Removed mannually adapting the preprocessing layers (#1016)\n\n* remove adapt\n\n* fix\n\n",
        "file": "autokeras.txt.json",
        "label": "no",
        "comments": "add print",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def test_feature_encoder_layer():",
            "",
            "model2 = tf.keras.Model(input_node, hidden_node)",
            "result = model2.predict(data)",
            "+    print(result)",
            "assert result[0][0] == result[2][0]",
            "assert result[0][0] != result[1][0]",
            "assert result[0][1] != result[1][1]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1900526)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1900527)",
            "Insert(target_node=IN(type=call), node=('identifier', 'print'), position=0, insert_id=1900528)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1900529)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1900530)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'result'), position=1, insert_id=1900531)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1900532)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 7,
        "number": 143,
        "neg_line": [],
        "pos_line": [
            "+print(result)"
        ],
        "core_change": "+print(result)",
        "core_API": "Model"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "ee23df0e..df895aa6 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "class name change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class F1Measure(Metric):",
            "raise ConfigurationError(\"A gold label passed to F1Measure contains an id >= {}, \"",
            "\"the number of classes.\".format(num_classes))",
            "if mask is None:",
            "-            mask = ones_like(gold_labels)",
            "+            mask = torch.ones_like(gold_labels)",
            "mask = mask.float()",
            "gold_labels = gold_labels.float()",
            "positive_label_mask = gold_labels.eq(self._positive_label).float()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=37321)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=37322)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=37323)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=ones_like), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 144,
        "neg_line": [
            "-mask = ones_like(gold_labels)"
        ],
        "pos_line": [
            "+mask = torch.ones_like(gold_labels)"
        ],
        "core_change": "-mask = ones_like(gold_labels) +mask = torch.ones_like(gold_labels)",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "9165642dcf446d94a2321ca01d2de04adc744e8a",
        "index": "f599ca47..991fe2a8 100644",
        "commit_message": "edge index bugfix in tu read\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "more return",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def compute_slices(dataset, batch):",
            "y_slice = node_slice if dataset.y.size(0) == num_nodes else graph_slice",
            "slices['y'] = y_slice",
            "",
            "-    return slices",
            "+    return dataset, slices"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('expression_list', None), position=1, insert_id=1076077)",
            "Insert(target_node=IN(type=expression_list), node=('identifier', 'dataset'), position=0, insert_id=1076078)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=1076079)",
            "Move(target_node=IN(type=expression_list), node=ASTNode(type=identifier, text=slices), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 148,
        "neg_line": [
            "-return slices"
        ],
        "pos_line": [
            "+return dataset, slices"
        ],
        "core_change": "-return slices +return dataset, slices",
        "core_API": "size"
    },
    {
        "commit_hash": "7f353e1c0f8621bbd8919cfc5e70a1387770824b",
        "index": "329e43f..f911d54 100644",
        "commit_message": "Fix symbol which no longer exists in tf2 root.\n\nThis was leading to a weird error if the filesystem threw an OpError\nwhen finding out if a path exists.\n\nPiperOrigin-RevId: 303131046\n\n",
        "file": "hub.txt.json",
        "label": "no",
        "comments": "change class name",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def load_module_spec(path):",
            "",
            "Raises:",
            "ValueError: on unexpected values in the module spec.",
            "-    tf.OpError: on file handling exceptions.",
            "+    tf.errors.OpError: on file handling exceptions.",
            "\"\"\"",
            "path = registry.resolver(path)",
            "return registry.loader(path)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1949069)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'errors'), position=2, insert_id=1949070)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 149,
        "neg_line": [
            "-tf.OpError: on file handling exceptions."
        ],
        "pos_line": [
            "+tf.errors.OpError: on file handling exceptions."
        ],
        "core_change": "-tf.OpError: on file handling exceptions. +tf.errors.OpError: on file handling exceptions.",
        "core_API": "resolver"
    },
    {
        "commit_hash": "911786a0eee284be0d5979c5df383ad064dc37c5",
        "index": "ab20740e..7734c5d5 100644",
        "commit_message": "Remove duplicated functional of gp (#1607)\n\n* clean gp\n\n* rearange sgpr\n\n* kernel add -> sum in test_benchmark\n\n* nit\n\n* fix error\n\n* fix error during clean\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args):",
            "# CNN will transform a high dimension image into a low dimension 2D tensors for RBF kernel.",
            "# This kernel accepts inputs are inputs of CNN and gives outputs are covariance matrix of RBF on",
            "# outputs of CNN.",
            "-    kernel = gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)).warp(iwarping_fn=cnn_fn)",
            "+    kernel = gp.kernels.Warp(gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)),",
            "+                             iwarping_fn=cnn_fn)",
            "",
            "# init inducing points (taken randomly from dataset)",
            "Xu = next(iter(train_loader))[0][:args.num_inducing]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=727028)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=727029)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=727030)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=727031)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Warp'), position=2, insert_id=727032)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=727033)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=727034)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=keyword_argument), position=3)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'gp'), position=0, insert_id=727035)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=727036)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'kernels'), position=2, insert_id=727037)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=warp))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 151,
        "neg_line": [
            "-kernel = gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)).warp(iwarping_fn=cnn_fn)"
        ],
        "pos_line": [
            "+kernel = gp.kernels.Warp(gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)),",
            "+iwarping_fn=cnn_fn)"
        ],
        "core_change": "-kernel = gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)).warp(iwarping_fn=cnn_fn) +kernel = gp.kernels.Warp(gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)), +iwarping_fn=cnn_fn)",
        "core_API": "RBF"
    },
    {
        "commit_hash": "84b4c7d1759e7e3475b9f85a73f72b5213593ddb",
        "index": "62feca5d12..893d8e4bc2 100644",
        "commit_message": "fixing torch backend matrix_rank helper to pass array-api test\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def linspace_helper(start, stop, num, axis=None, *, device):",
            "else:",
            "res = [linspace_method(start, stp, num, device=device) for stp in stop]",
            "else:",
            "-        return linspace_method(start, stop, num, device=device)",
            "+        return linspace_method(start, stop, num, dtype=torch.float64, device=device)",
            "res = torch.cat(res, -1).reshape(sos_shape + [num])",
            "if axis is not None:",
            "res = torch.transpose(res, axis, -1)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=282118)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=282119)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=282120)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=282121)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=282122)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=282123)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=282124)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float64'), position=2, insert_id=282125)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 153,
        "neg_line": [
            "-return linspace_method(start, stop, num, device=device)"
        ],
        "pos_line": [
            "+return linspace_method(start, stop, num, dtype=torch.float64, device=device)"
        ],
        "core_change": "-return linspace_method(start, stop, num, device=device) +return linspace_method(start, stop, num, dtype=torch.float64, device=device)",
        "core_API": "cat"
    },
    {
        "commit_hash": "ced4cfdbbf4496ec0d95483e470933b4fed4f95a",
        "index": "fb61e48e..1981fbe9 100644",
        "commit_message": "Allow saving / loading checkpoints from cloud paths (#683)\n\n* Allow saving / loading checkpoints from cloud paths\n\nAllows saving and loading checkpoints directly from cloud paths like\nAmazon S3 (s3://) and Google Cloud Storage (gs://) by using fsspec.\n\nNote: The user will have to install the relevant dependency for each\nprotocol. Otherwise fsspec will fail and specify which dependency is\nmissing.\n\n* Append suffix _fsspec to save/load function names\n\n* Add a lower bound to the fsspec dependency\n\nSkips the 0 major version.\n\n* Add missing changes from refactor\n\n* Use fsspec for remaining artifacts\n\n* Add test case with path requiring fsspec\n\n* Avoid writing logs to file unless output_path is local\n\n* Document the possibility of using paths supported by fsspec\n\n* Fix style and lint\n\n* Add missing lint fixes\n\n* Add type annotations to new functions\n\n* Use Coqpit method for converting config to dict\n\n* Fix type annotation in semi-new function\n\n* Add return type for load_fsspec\n\n* Fix bug where fs not always created\n\n* Restore the experiment removal functionality\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "add custom function",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def save_best_model(model, optimizer, criterion, model_loss, best_loss, out_path",
            "bestmodel_path = \"best_model.pth.tar\"",
            "bestmodel_path = os.path.join(out_path, bestmodel_path)",
            "print(\"\\n > BEST MODEL ({0:.5f}) : {1:}\".format(model_loss, bestmodel_path))",
            "-        torch.save(state, bestmodel_path)",
            "+        save_fsspec(state, bestmodel_path)",
            "return best_loss"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=ERROR), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='save_fsspec')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=save))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 154,
        "neg_line": [
            "-torch.save(state, bestmodel_path)"
        ],
        "pos_line": [
            "+save_fsspec(state, bestmodel_path)"
        ],
        "core_change": "-torch.save(state, bestmodel_path) +save_fsspec(state, bestmodel_path)",
        "core_API": "join"
    },
    {
        "commit_hash": "077fce8b50cd7648a3b52c5fa3db1757a5343365",
        "index": "89b5440e..3b481e59 100644",
        "commit_message": "bugfix for unittests\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class UnittestBase(object):",
            "datetime.now().strftime('%H:%M:%S'), self.__class__.__name__[4:], name",
            "))",
            "sys.stdout.flush()",
            "+        tf.compat.v1.reset_default_graph()",
            "",
            "def finished_test(self, assertion=None):",
            "\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2227208)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2227209)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2227210)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2227211)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2227212)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2227213)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reset_default_graph'), position=2, insert_id=2227214)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2227215)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2227216)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2227217)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2227218)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2227219)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2227220)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2227221)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2227222)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 156,
        "neg_line": [],
        "pos_line": [
            "+tf.compat.v1.reset_default_graph()"
        ],
        "core_change": "+tf.compat.v1.reset_default_graph()",
        "core_API": "now"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "7ca3d60e0..29c50c3c0 100755",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPTNeoXModel(GPTNeoXPreTrainedModel):",
            "# Since we are adding it to the raw scores before the softmax, this is",
            "# effectively the same as removing these entirely.",
            "attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility",
            "-            attention_mask = (1.0 - attention_mask) * -10000.0",
            "+            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min",
            "",
            "# Prepare head mask if needed",
            "# 1.0 in head_mask indicate we keep the head"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=2, insert_id=1196146)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1196147)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196148)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1196149)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1196150)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1196151)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1196152)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196153)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1196154)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1196155)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1196156)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1196157)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1196158)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196159)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1196160)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=float, text=10000.0))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 158,
        "neg_line": [
            "-attention_mask = (1.0 - attention_mask) * -10000.0"
        ],
        "pos_line": [
            "+attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min"
        ],
        "core_change": "-attention_mask = (1.0 - attention_mask) * -10000.0 +attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min",
        "core_API": "to"
    },
    {
        "commit_hash": "63cf61930f7760c56671c0032ae913a6b2be7a27",
        "index": "940df14a..f4798a28 100644",
        "commit_message": "batch and test fixed\n\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "no API used",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PatchAffineShapeEstimator(nn.Module):",
            "\"input shape should be must be [Bx1x{}x{}]. \"",
            "\"Got {}\".format(self.patch_size, self.patch_size, patch.size()))",
            "self.weighting = self.weighting.to(patch.dtype).to(patch.device)",
            "-        grads: torch.Tensor = self.gradient(patch)",
            "+        grads: torch.Tensor = self.gradient(patch) * self.weighting",
            "# unpack the edges",
            "gx: torch.Tensor = grads[:, :, 0]",
            "gy: torch.Tensor = grads[:, :, 1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=4, insert_id=458990)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=458991)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=2, insert_id=458992)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=458993)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=458994)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weighting'), position=2, insert_id=458995)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 159,
        "neg_line": [
            "-grads: torch.Tensor = self.gradient(patch)"
        ],
        "pos_line": [
            "+grads: torch.Tensor = self.gradient(patch) * self.weighting"
        ],
        "core_change": "-grads: torch.Tensor = self.gradient(patch) +grads: torch.Tensor = self.gradient(patch) * self.weighting",
        "core_API": "size"
    },
    {
        "commit_hash": "868cde9a4a0b82d4b721ea072cd0a696884a2edd",
        "index": "9964bfa..1ba4116 100644",
        "commit_message": "fix minor bug\n\n",
        "file": "nebullvm.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def compute_tf_latency(",
            "with tf.device(device):",
            "for _ in range(steps):",
            "starting_time = time.time()",
            "-            _ = model(x)",
            "+            _ = model(*xs)",
            "latencies.append(time.time() - starting_time)",
            "latency = sum(latencies) / steps",
            "return latency, latencies"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('list_splat', None), position=1, insert_id=654720)",
            "Insert(target_node=IN(type=list_splat), node=('*', '*'), position=0, insert_id=654721)",
            "Insert(target_node=IN(type=list_splat), node=('identifier', 'xs'), position=1, insert_id=654722)",
            "Delete(target_node=ASTNode(type=identifier, text=x))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 160,
        "neg_line": [
            "-_ = model(x)"
        ],
        "pos_line": [
            "+_ = model(*xs)"
        ],
        "core_change": "-_ = model(x) +_ = model(*xs)",
        "core_API": "device"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "33e96dd94..365245922 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ≈†a≈°ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class ArxivDataset(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), _FILENAME)",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('arxiv_dataset', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-                )",
            "+                \"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('arxiv_dataset', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"path\": path_to_manual_file})]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'arxiv_dataset\\', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"'), position=1, insert_id=1781579)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('arxiv_dataset', data_dir=...)` that includes a file name {}. Manual download instructions: {})\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_FILENAME))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 162,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('arxiv_dataset', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('arxiv_dataset', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('arxiv_dataset', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format( -path_to_manual_file, _FILENAME, self.manual_download_instructions -) +\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('arxiv_dataset', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
        "core_API": "join"
    },
    {
        "commit_hash": "68e6ab668b30a6014215b94e399151f8c76e471a",
        "index": "fa47289..aac0e44 100755",
        "commit_message": "Hub device mismatch bug fix (#1619)\n\n\n",
        "file": "yolov5.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, classes=None,",
            "merge = False  # use merge-NMS",
            "",
            "t = time.time()",
            "-    output = [torch.zeros(0, 6)] * prediction.shape[0]",
            "+    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]",
            "for xi, x in enumerate(prediction):  # image index, image inference",
            "# Apply constraints",
            "# x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=1300410)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1300411)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1300412)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1300413)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=1300414)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=0), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=6), position=3)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1300415)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1300416)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1300417)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'prediction'), position=0, insert_id=1300418)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1300419)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1300420)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 163,
        "neg_line": [
            "-output = [torch.zeros(0, 6)] * prediction.shape[0]"
        ],
        "pos_line": [
            "+output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]"
        ],
        "core_change": "-output = [torch.zeros(0, 6)] * prediction.shape[0] +output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]",
        "core_API": "time"
    },
    {
        "commit_hash": "87e6e4fe5c7e65cb69e70306f22de6daf16b6e14",
        "index": "24ff39ddb..efd9529cf 100644",
        "commit_message": "Doc styler v2 (#14950)\n\n* New doc styler\n\n* Fix issue with args at the start\n\n* Code sample fixes\n\n* Style code examples in MDX\n\n* Fix more patterns\n\n* Typo\n\n* Typo\n\n* More patterns\n\n* Do without black for now\n\n* Get more info in error\n\n* Docstring style\n\n* Re-enable check\n\n* Quality\n\n* Fix add_end_docstring decorator\n\n* Fix docstring\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def glue_convert_examples_to_features(",
            "output_mode: String indicating the output mode. Either `regression` or `classification`",
            "",
            "Returns:",
            "-        If the `examples` input is a `tf.data.Dataset`, will return a `tf.data.Dataset` containing the",
            "-        task-specific features. If the input is a list of `InputExamples`, will return a list of task-specific",
            "-        `InputFeatures` which can be fed to the model.",
            "+        If the `examples` input is a `tf.data.Dataset`, will return a `tf.data.Dataset` containing the task-specific",
            "+        features. If the input is a list of `InputExamples`, will return a list of task-specific `InputFeatures` which",
            "+        can be fed to the model.",
            "",
            "\"\"\"",
            "warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'the'), position=10, insert_id=2368031)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=binary_operator), position=0)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('binary_operator', None), position=3, insert_id=2368032)",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('ERROR', None), position=1, insert_id=2368033)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'task'), position=0, insert_id=2368034)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=2368035)",
            "Insert(target_node=IN(type=binary_operator), node=('ERROR', None), position=2, insert_id=2368036)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'which'), position=3, insert_id=2368037)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=the), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=input), position=1)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'specific'), position=0, insert_id=2368038)",
            "Insert(target_node=IN(type=ERROR), node=('string', '`InputFeatures`'), position=1, insert_id=2368039)",
            "Delete(target_node=ASTNode(type=identifier, text=the))",
            "Delete(target_node=ASTNode(type=identifier, text=task))",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=identifier, text=specific))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=string, text=`InputFeatures`))",
            "Delete(target_node=ASTNode(type=identifier, text=which))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 164,
        "neg_line": [
            "-If the `examples` input is a `tf.data.Dataset`, will return a `tf.data.Dataset` containing the",
            "-task-specific features. If the input is a list of `InputExamples`, will return a list of task-specific",
            "-`InputFeatures` which can be fed to the model."
        ],
        "pos_line": [
            "+If the `examples` input is a `tf.data.Dataset`, will return a `tf.data.Dataset` containing the task-specific",
            "+features. If the input is a list of `InputExamples`, will return a list of task-specific `InputFeatures` which",
            "+can be fed to the model."
        ],
        "core_change": "-If the `examples` input is a `tf.data.Dataset`, will return a `tf.data.Dataset` containing the -task-specific features. If the input is a list of `InputExamples`, will return a list of task-specific -`InputFeatures` which can be fed to the model. +If the `examples` input is a `tf.data.Dataset`, will return a `tf.data.Dataset` containing the task-specific +features. If the input is a list of `InputExamples`, will return a list of task-specific `InputFeatures` which +can be fed to the model.",
        "core_API": "warn"
    },
    {
        "commit_hash": "5cad4154c36c7837bf6da9cde778ceb9de661d59",
        "index": "bf963d5a9..8d26fa39e 100644",
        "commit_message": "small fix\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "add layer note clear",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ESPnetUASRModel(AbsESPnetModel):",
            "#  e.g. STFT and Feature extract",
            "#       data_loader may send time-domain signal in this case",
            "# speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)",
            "+            speech = F.layer_norm(speech, speech.shape)",
            "feats, feats_lengths = self.frontend(speech, speech_lengths)",
            "else:",
            "# No frontend and no feature extract (usually with pre-extracted feat)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=118668)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=118669)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=118670)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'speech'), position=0, insert_id=118671)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=118672)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=118673)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=118674)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=118675)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'F'), position=0, insert_id=118676)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=118677)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'layer_norm'), position=2, insert_id=118678)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=118679)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'speech'), position=1, insert_id=118680)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=118681)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=118682)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=118683)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'speech'), position=0, insert_id=118684)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=118685)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=118686)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 165,
        "neg_line": [],
        "pos_line": [
            "+speech = F.layer_norm(speech, speech.shape)"
        ],
        "core_change": "+speech = F.layer_norm(speech, speech.shape)",
        "core_API": "layer_norm"
    },
    {
        "commit_hash": "72087f8a178eff6b1890616705f6021cabd8f072",
        "index": "3aa0485f..d1e80df0 100644",
        "commit_message": "Fix DARTS 2nd order (#4385)\n\n\n",
        "file": "nni.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "class DartsTrainer(BaseOneShotTrainer):",
            "p += e * d",
            "",
            "_, loss = self._logits_and_loss(trn_X, trn_y)",
            "-            dalphas.append(torch.autograd.grad(loss, [c.alpha for c in self.nas_modules]))",
            "+            dalphas.append(torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]))",
            "",
            "dalpha_pos, dalpha_neg = dalphas  # dalpha { L_trn(w+) }, # dalpha { L_trn(w-) }",
            "hessian = [(p - n) / (2. * eps) for p, n in zip(dalpha_pos, dalpha_neg)]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=for_in_clause), node=('pattern_list', None), position=1, insert_id=1577520)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', '_'), position=0, insert_id=1577521)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=1, insert_id=1577522)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=identifier, text=c), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 166,
        "neg_line": [
            "-dalphas.append(torch.autograd.grad(loss, [c.alpha for c in self.nas_modules]))"
        ],
        "pos_line": [
            "+dalphas.append(torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]))"
        ],
        "core_change": "-dalphas.append(torch.autograd.grad(loss, [c.alpha for c in self.nas_modules])) +dalphas.append(torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]))",
        "core_API": "_logits_and_loss"
    },
    {
        "commit_hash": "fdb5073ce75db7bba01ff89bfb888c52a12b2430",
        "index": "388975ac21..c6608377db 100644",
        "commit_message": "Fixed subract fn (#4068)\n\nCo-authored-by: abdrahmandiab <abdrahmandiab99@gmail.com>\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def subtract(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    return tf.subtract(x1, x2)",
            "+    return tf.experimental.numpy.subtract(x1, x2)",
            "",
            "",
            "def tan("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1992739)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1992740)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1992741)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1992742)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'numpy'), position=2, insert_id=1992743)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'experimental'), position=2, insert_id=1992744)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 167,
        "neg_line": [
            "-return tf.subtract(x1, x2)"
        ],
        "pos_line": [
            "+return tf.experimental.numpy.subtract(x1, x2)"
        ],
        "core_change": "-return tf.subtract(x1, x2) +return tf.experimental.numpy.subtract(x1, x2)",
        "core_API": "promote_types_of_inputs"
    },
    {
        "commit_hash": "7dc58bd286b1e81ca4d293f05bddff5e93361020",
        "index": "b30909425..dba605f5c 100644",
        "commit_message": "Refactor model summary + generalize example input array (#1773)\n\n* squash\n\nvariant a\n\n\nvariant b\n\n\nadd test\n\n\nrevert rename\n\n\nadd changelog\n\n\ndocs\n\n\nmove changelog entry to top\n\n\nuse hooks\n\n\nwip\n\n\nwipp\n\n\nlayer summary\n\n\nclean up, refactor\n\n\ntype hints\n\n\nrename\n\n\nremove obsolete code\n\n\nrename\n\n\nunused imports\n\n\nsimplify formatting of table and increase readability\n\n\ndoctest\n\n\nsuperclass object\n\n\nupdate examples\n\n\nprint unknown sizes\n\n\nmore docs and doctest\n\n\ntesting\n\n\nunknown layers\n\n\nadd rnn test\n\n\nremove main\n\n\nrestore train mode\n\n\ntest device wip\n\n\ndevice\n\n\nconstant\n\n\nsimplify model forward transfer\n\n\nreturn summary object in method\n\n\nextend tests\n\n\nfix summary for empty module\n\n\nextend tests\n\n\nrefactor and added hook\n\n\nvariant a\n\n\nvariant b\n\n\nadd test\n\n\nrevert rename\n\n\nadd changelog\n\n\ndocs\n\n\nmove changelog entry to top\n\n\nremove hardcoded string\n\n\nsimplify\n\n\ntest unknown shapes and all others\n\n\ncomments for tests\n\n\nfix hparams attribute\n\n* update default\n\n* unused import\n\n* clean up\n\n* replace hardcoded strings\n\n* fix doctest\n\n* fix top/full\n\n* black\n\n* fix rnn test\n\n* fix rnn\n\n* update debugging docs\n\n\nupdate docs\n\n\ntypo\n\n\nupdate docs\n\n\nupdate docs\n\n* add changelog\n\n* extract constant\n\n* setter and getter\n\n* move parity models to test folder\n\n* parameterize mode\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LightningTemplateModel(LightningModule):",
            "self.c_d2 = nn.Linear(in_features=self.hidden_dim,",
            "out_features=self.out_features)",
            "",
            "+        self.example_input_array = torch.zeros(2, 1, 28, 28)",
            "+",
            "def forward(self, x):",
            "\"\"\"",
            "No special modification required for Lightning, define it as you normally would"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=579711)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=579712)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=579713)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=579714)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=579715)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=579716)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=579717)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'example_input_array'), position=2, insert_id=579718)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=579719)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=579720)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=579721)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=579722)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros'), position=2, insert_id=579723)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=579724)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '2'), position=1, insert_id=579725)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=579726)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=579727)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=579728)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '28'), position=5, insert_id=579729)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=6, insert_id=579730)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '28'), position=7, insert_id=579731)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=8, insert_id=579732)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 168,
        "neg_line": [],
        "pos_line": [
            "+self.example_input_array = torch.zeros(2, 1, 28, 28)",
            "+"
        ],
        "core_change": "+self.example_input_array = torch.zeros(2, 1, 28, 28) +",
        "core_API": "Linear"
    },
    {
        "commit_hash": "70b0d4e193ea3d15effebc7cda534b6b9454abef",
        "index": "738981648..a513a8280 100644",
        "commit_message": "Fix compatibility with 1.12 (#17925)\n\n* Fix compatibility with 1.12\n\n* Remove pin from examples requirements\n\n* Update torch scatter version\n\n* Fix compatibility with 1.12\n\n* Remove pin from examples requirements\n\n* Update torch scatter version\n\n* fix torch.onnx.symbolic_opset12 import\n\n* Reject bad version\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class XDropout(torch.autograd.Function):",
            "# Once https://github.com/pytorch/pytorch/issues/78391 is fixed, do something like:",
            "# if opset_version < 12:",
            "#   return torch.onnx.symbolic_opset9.dropout(g, input, dropout_p, train)",
            "-        return torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train)",
            "+        return symbolic_opset12.dropout(g, input, dropout_p, train)",
            "",
            "",
            "# Copied from transformers.models.deberta.modeling_deberta.StableDropout"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=symbolic_opset12), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=onnx))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 169,
        "neg_line": [
            "-return torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train)"
        ],
        "pos_line": [
            "+return symbolic_opset12.dropout(g, input, dropout_p, train)"
        ],
        "core_change": "-return torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train) +return symbolic_opset12.dropout(g, input, dropout_p, train)",
        "core_API": "dropout"
    },
    {
        "commit_hash": "d03223d4d64b89e76b48b00602aba5aa2f817f1e",
        "index": "593111762..33b2d5975 100644",
        "commit_message": "Use packaging to handle versions (#2777)\n\n* Get Python version from platform module\n\n* Set PY_VERSION as version class\n\n* Set PYARROW_VERSION as version class\n\n* Set TORCH_VERSION as version class\n\n* Set TF_VERSION as version class\n\n* Set JAX_VERSION as version class\n\n* Set BEAM_VERSION as version class\n\n* Set RARFILE_VERSION as version class\n\n* Use version class to validate PyArrow version at import\n\n* Use version class in SCRIPTS_VERSION at import\n\n* Use config.PYARROW_VERSION for parquet submodules\n\n* Fix style\n",
        "file": "datasets.txt.json",
        "label": "yes",
        "comments": "change condition check for version fix",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Parquet(datasets.ArrowBasedBuilder):",
            "BUILDER_CONFIG_CLASS = ParquetConfig",
            "",
            "def _info(self):",
            "-        if version.parse(pa.__version__) < version.parse(\"3.0.0\"):",
            "+        if datasets.config.PYARROW_VERSION.major < 3:",
            "raise ImportError(",
            "\"PyArrow >= 3.0.0 is required to used the Parquet dataset builder: pip install --upgrade pyarrow\"",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=0, insert_id=1783786)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('integer', '3'), position=3, insert_id=1783787)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1783788)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=__version__), value='major')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=__version__), position=2)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=pa), value='PYARROW_VERSION')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=pa), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=version), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=parse), value='config')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=version))",
            "Delete(target_node=ASTNode(type=identifier, text=parse))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"3.0.0\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 170,
        "neg_line": [
            "-if version.parse(pa.__version__) < version.parse(\"3.0.0\"):"
        ],
        "pos_line": [
            "+if datasets.config.PYARROW_VERSION.major < 3:"
        ],
        "core_change": "-if version.parse(pa.__version__) < version.parse(\"3.0.0\"): +if datasets.config.PYARROW_VERSION.major < 3:",
        "core_API": "parse"
    },
    {
        "commit_hash": "56437e98a6e673f5b978eebdf2babb28c09e41ad",
        "index": "d9fb6df3d..8a694557e 100644",
        "commit_message": "[bug-fix] Trainer.test points to latest best_model_path (#5161)\n\n* resolve bug\n\n* update code\n\n* add set -e\n\n* Update pytorch_lightning/callbacks/model_checkpoint.py\n\nCo-authored-by: Adrian W√§lchli <aedu.waelchli@gmail.com>\n\n* update test\n\n* Update tests/checkpointing/test_trainer_checkpoint.py\n\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\n\n* Update tests/checkpointing/test_trainer_checkpoint.py\n\nCo-authored-by: Carlos Mochol√≠ <carlossmocholi@gmail.com>\n\n* update on comments\n\n* resolve test\n\n* convert to set\n\n* update\n\n* add error triggering\n\n* update\n\n* update on comments\n\n* update\n\n* resolve import\n\n* update\n\n* update\n\n* Update pytorch_lightning/plugins/rpc_plugin.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* update\n\nCo-authored-by: Adrian W√§lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: Carlos Mochol√≠ <carlossmocholi@gmail.com>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n(cherry picked from commit d5b367871fa3924090ec74bf903bd172bd3e2343)\n\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RPCPlugin(DDPPlugin):",
            "world_size: int) -> None:",
            "os.environ['MASTER_PORT'] = os.getenv('RPC_MASTER_PORT', '15000')",
            "rpc.init_rpc(f\"worker{global_rank}\", rank=global_rank, world_size=world_size)",
            "+        rpc._set_rpc_timeout(self.rpc_timeout_sec)",
            "self.rpc_initialized = True",
            "",
            "def rpc_save_model(self,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=550554)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=550555)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=550556)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=550557)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'rpc'), position=0, insert_id=550558)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=550559)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_set_rpc_timeout'), position=2, insert_id=550560)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=550561)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=550562)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=550563)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=550564)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=550565)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'rpc_timeout_sec'), position=2, insert_id=550566)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 171,
        "neg_line": [],
        "pos_line": [
            "+rpc._set_rpc_timeout(self.rpc_timeout_sec)"
        ],
        "core_change": "+rpc._set_rpc_timeout(self.rpc_timeout_sec)",
        "core_API": "getenv"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "72aab036..a8c20cf5 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SimpleSeq2SeqTest(ModelTestCase):",
            "state = self.model._init_decoder_state(state)",
            "batch_size = state[\"source_mask\"].size()[0]",
            "start_predictions = state[\"source_mask\"].new_full(",
            "-            (batch_size,), fill_value=self.model._start_index",
            "+            (batch_size,), fill_value=self.model._start_index, dtype=torch.long",
            ")",
            "all_top_k_predictions, _ = beam_search.search(",
            "start_predictions, state, self.model.take_step"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=19679)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=19680)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=19681)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=19682)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=19683)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=19684)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19685)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=19686)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 172,
        "neg_line": [
            "-(batch_size,), fill_value=self.model._start_index"
        ],
        "pos_line": [
            "+(batch_size,), fill_value=self.model._start_index, dtype=torch.long"
        ],
        "core_change": "-(batch_size,), fill_value=self.model._start_index +(batch_size,), fill_value=self.model._start_index, dtype=torch.long",
        "core_API": "_init_decoder_state"
    },
    {
        "commit_hash": "8e76796fd0535d8fbc9934e7f6604a1abcfb119e",
        "index": "9445562815..3b1aa83743 100644",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def pg_tf_loss(policy, model, dist_class, train_batch):",
            "logits, _ = model.from_batch(train_batch)",
            "action_dist = dist_class(logits, model)",
            "return -tf.reduce_mean(",
            "-        action_dist.logp(train_batch[SampleBatch.ACTIONS]) *",
            "-        tf.cast(train_batch[Postprocessing.ADVANTAGES], dtype=tf.float32))",
            "+        action_dist.logp(train_batch[SampleBatch.ACTIONS]) * tf.cast(",
            "+            train_batch[Postprocessing.ADVANTAGES], dtype=tf.float32))",
            "",
            "",
            "PGTFPolicy = build_tf_policy("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 173,
        "neg_line": [
            "-action_dist.logp(train_batch[SampleBatch.ACTIONS]) *",
            "-tf.cast(train_batch[Postprocessing.ADVANTAGES], dtype=tf.float32))"
        ],
        "pos_line": [
            "+action_dist.logp(train_batch[SampleBatch.ACTIONS]) * tf.cast(",
            "+train_batch[Postprocessing.ADVANTAGES], dtype=tf.float32))"
        ],
        "core_change": "-action_dist.logp(train_batch[SampleBatch.ACTIONS]) * -tf.cast(train_batch[Postprocessing.ADVANTAGES], dtype=tf.float32)) +action_dist.logp(train_batch[SampleBatch.ACTIONS]) * tf.cast( +train_batch[Postprocessing.ADVANTAGES], dtype=tf.float32))",
        "core_API": "from_batch"
    },
    {
        "commit_hash": "2777264ee8d8b0241c5691120175286cbede76bf",
        "index": "0d90e075..879f374a 100644",
        "commit_message": "`enable_model_cpu_offload` (#2285)\n\n* enable_model_offload PoC\n\nIt's surprisingly more involved than expected, see comments in the PR.\n\n* Rename final_offload_hook\n\n* Invoke the vae forward hook manually.\n\n* Completely remove decoder.\n\n* Style\n\n* apply_forward_hook decorator\n\n* Rename method.\n\n* Style\n\n* Copy enable_model_cpu_offload\n\n* Fix copies.\n\n* Remove comment.\n\n* Fix copies\n\n* Missing import\n\n* Fix doc-builder style.\n\n* Merge main and fix again.\n\n* Add docs\n\n* Fix docs.\n\n* Add a couple of tests.\n\n* style\n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "remove condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionDepth2ImgPipeline(DiffusionPipeline):",
            "`pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module",
            "hooks.",
            "\"\"\"",
            "-        if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):",
            "+        if not hasattr(self.unet, \"_hf_hook\"):",
            "return self.device",
            "for module in self.unet.modules():",
            "if ("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 174,
        "neg_line": [
            "-if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):"
        ],
        "pos_line": [
            "+if not hasattr(self.unet, \"_hf_hook\"):"
        ],
        "core_change": "-if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"): +if not hasattr(self.unet, \"_hf_hook\"):",
        "core_API": "enable_sequential_cpu_offload"
    },
    {
        "commit_hash": "8d3ca018a122d284212248d864d8b7f2fc194aeb",
        "index": "f29a64bd..fc7feb14 100644",
        "commit_message": "Fix CUDA tests on dev (#1610)\n\n* Fix CUDA tests on dev\n\n* fix distn tests; skip  nef test on cuda\n\n* skip jit tests on cuda\n\n* fix test_enum\n\n* skip jit tests on cuda\n\n* fix test exponential gamma\n\n* fix tests in ops/stats\n\n* fix thresholds\n\n* use default seed\n\n* revert num samples\n\n* fix test_eig\n\n* make specific to non-reparam\n\n* fix lint\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_quantile():",
            "z = torch.randn(2000)",
            "",
            "assert_equal(quantile(x, probs=[0., 0.4, 0.5, 1.]), torch.tensor([0., 0.8, 1., 2.]))",
            "-    assert_equal(quantile(y, probs=0.2), torch.tensor(0.2), prec=0.01)",
            "-    assert_equal(quantile(z, probs=0.8413), torch.tensor(1.), prec=0.001)",
            "+    assert_equal(quantile(y, probs=0.2), torch.tensor(0.2), prec=0.02)",
            "+    assert_equal(quantile(z, probs=0.8413), torch.tensor(1.), prec=0.02)",
            "",
            "",
            "def test_pi():"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.01), value='0.02')",
            "Update(target_node=ASTNode(type=float, text=0.001), value='0.02')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 175,
        "neg_line": [
            "-assert_equal(quantile(y, probs=0.2), torch.tensor(0.2), prec=0.01)",
            "-assert_equal(quantile(z, probs=0.8413), torch.tensor(1.), prec=0.001)"
        ],
        "pos_line": [
            "+assert_equal(quantile(y, probs=0.2), torch.tensor(0.2), prec=0.02)",
            "+assert_equal(quantile(z, probs=0.8413), torch.tensor(1.), prec=0.02)"
        ],
        "core_change": "-assert_equal(quantile(y, probs=0.2), torch.tensor(0.2), prec=0.01) -assert_equal(quantile(z, probs=0.8413), torch.tensor(1.), prec=0.001) +assert_equal(quantile(y, probs=0.2), torch.tensor(0.2), prec=0.02) +assert_equal(quantile(z, probs=0.8413), torch.tensor(1.), prec=0.02)",
        "core_API": "randn"
    },
    {
        "commit_hash": "fe081d4f7c5078aef1df677680f10e79f18d6dfc",
        "index": "42880589..2b55cbac 100644",
        "commit_message": "fixing rebase issues\n\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "change name",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TacotronGSTTrainTest(unittest.TestCase):",
            "input_lengths = torch.randint(100, 129, (8, )).long().to(device)",
            "input_lengths[-1] = 128",
            "mel_spec = torch.rand(8, 120, c.audio['num_mels']).to(device)",
            "-        linear_spec = torch.rand(8, 120, c.audio['num_freq']).to(device)",
            "+        linear_spec = torch.rand(8, 120, c.audio['fft_size']).to(device)",
            "mel_lengths = torch.randint(20, 120, (8, )).long().to(device)",
            "mel_lengths[-1] = 120",
            "stop_targets = torch.zeros(8, 120, 1).float().to(device)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='num_freq'), value=\"'fft_size'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 176,
        "neg_line": [
            "-linear_spec = torch.rand(8, 120, c.audio['num_freq']).to(device)"
        ],
        "pos_line": [
            "+linear_spec = torch.rand(8, 120, c.audio['fft_size']).to(device)"
        ],
        "core_change": "-linear_spec = torch.rand(8, 120, c.audio['num_freq']).to(device) +linear_spec = torch.rand(8, 120, c.audio['fft_size']).to(device)",
        "core_API": "randint"
    },
    {
        "commit_hash": "73a6d2ec36ae17f9594436c971896a9eac2243fd",
        "index": "8035922920..5466bfa9ac 100644",
        "commit_message": "small fix to Haiku converter method.\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "update param for refactor fix",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class IvyModule(ivy.Module):",
            "if ivy.array_mode():",
            "a, kw = ivy.args_to_native(*a, **kw)",
            "# noinspection PyUnresolvedReferences",
            "-        params_hk = self._native_module.init(ivy.functional.core.random.RNG, *a, **kw)",
            "+        params_hk = self._native_module.init(ivy.random.RNG, *a, **kw)",
            "params_dict = _hk_flat_map_to_dict(params_hk)",
            "self._hk_params = ivy.Container(params_dict)",
            "param_iterator = self._hk_params.to_iterator()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=functional))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=core))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 177,
        "neg_line": [
            "-params_hk = self._native_module.init(ivy.functional.core.random.RNG, *a, **kw)"
        ],
        "pos_line": [
            "+params_hk = self._native_module.init(ivy.random.RNG, *a, **kw)"
        ],
        "core_change": "-params_hk = self._native_module.init(ivy.functional.core.random.RNG, *a, **kw) +params_hk = self._native_module.init(ivy.random.RNG, *a, **kw)",
        "core_API": "array_mode"
    },
    {
        "commit_hash": "4857546c259042fbc9b2ddc155cd9e6b5bf5d3ff",
        "index": "9288a3c80..c33c470d0",
        "commit_message": "Fix: Failing test in data_modules(dp) (#5924)\n\n* Update test_datamodules.py\n\n* fix code format issue\n\n* fix test restore\n\n* fix code format issue\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "parameterize the variable",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "class ClassificationModel(LightningModule):",
            "return logits",
            "",
            "def configure_optimizers(self):",
            "-        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)",
            "+        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)",
            "return [optimizer], []",
            "",
            "def training_step(self, batch, batch_idx):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=1598314)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1598315)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1598316)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'lr'), position=2, insert_id=1598317)",
            "Delete(target_node=ASTNode(type=float, text=0.01))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 178,
        "neg_line": [
            "-optimizer = torch.optim.Adam(self.parameters(), lr=0.01)"
        ],
        "pos_line": [
            "+optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)"
        ],
        "core_change": "-optimizer = torch.optim.Adam(self.parameters(), lr=0.01) +optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)",
        "core_API": "Adam"
    },
    {
        "commit_hash": "20cb2ed89a128119924fdd1f90ff0f57e34b68ce",
        "index": "89dd7bc0..dca7daed 100644",
        "commit_message": "use `f`-strings (#3557)\n\n* fix f-strings\n\n* update\n\n* update\n\n* update\n\n* fix test\n\n* update\n\n* fix tests\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "remove API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DataParallel(torch.nn.DataParallel):",
            "Batch.from_data_list(data_list[split[i]:split[i + 1]],",
            "follow_batch=self.follow_batch,",
            "exclude_keys=self.exclude_keys).to(",
            "-                                     torch.device('cuda:{}'.format(",
            "-                                         device_ids[i])))",
            "+                                     torch.device(f'cuda:{device_ids[i]}'))",
            "for i in range(len(split) - 1)",
            "]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', \"f'cuda:{device_ids[i]}'\"), position=1, insert_id=992571)",
            "Delete(target_node=ASTNode(type=string, text='cuda:{}'))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=device_ids))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=i))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 179,
        "neg_line": [
            "-torch.device('cuda:{}'.format(",
            "-device_ids[i])))"
        ],
        "pos_line": [
            "+torch.device(f'cuda:{device_ids[i]}'))"
        ],
        "core_change": "-torch.device('cuda:{}'.format( -device_ids[i]))) +torch.device(f'cuda:{device_ids[i]}'))",
        "core_API": "from_data_list"
    },
    {
        "commit_hash": "0227b4a940ceacde43b703d74d03bed2603188e7",
        "index": "6af13d760..0773d0d5f 100644",
        "commit_message": "fix #827\n\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change API call for type fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ProjectedAdaptiveLogSoftmax(nn.Module):",
            "d_emb_i = d_embed // (div_val ** i)",
            "",
            "self.out_projs.append(",
            "-                    nn.Parameter(torch.Tensor(d_proj, d_emb_i))",
            "+                    nn.Parameter(torch.FloatTensor(d_proj, d_emb_i))",
            ")",
            "",
            "self.out_layers.append(nn.Linear(d_emb_i, r_idx-l_idx))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='FloatTensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 180,
        "neg_line": [
            "-nn.Parameter(torch.Tensor(d_proj, d_emb_i))"
        ],
        "pos_line": [
            "+nn.Parameter(torch.FloatTensor(d_proj, d_emb_i))"
        ],
        "core_change": "-nn.Parameter(torch.Tensor(d_proj, d_emb_i)) +nn.Parameter(torch.FloatTensor(d_proj, d_emb_i))",
        "core_API": "append"
    },
    {
        "commit_hash": "0f516e3d86196ca2136f8e8b05bef6d93f2509c1",
        "index": "7cb80d6..ec891f3 100644",
        "commit_message": "fix dropout management in TensorFlowEstimator._predict - get correct list of dropout variables, pass keep_prob = 1.0\n\n",
        "file": "skflow.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorFlowEstimator(BaseEstimator):",
            "raise NotFittedError()",
            "predict_data_feeder = setup_predict_data_feeder(X)",
            "preds = []",
            "-        dropouts = tf.get_collection(DROPOUTS)",
            "-        feed_dict = {prob: 0.0 for prob in dropouts}",
            "+        dropouts = self._graph.get_collection(DROPOUTS)",
            "+        feed_dict = {prob: 1.0 for prob in dropouts}",
            "for data in predict_data_feeder:",
            "feed_dict[self._inp] = data",
            "preds.append(self._session.run("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2171694)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2171695)",
            "Update(target_node=ASTNode(type=float, text=0.0), value='1.0')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_graph'), position=2, insert_id=2171696)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 181,
        "neg_line": [
            "-dropouts = tf.get_collection(DROPOUTS)",
            "-feed_dict = {prob: 0.0 for prob in dropouts}"
        ],
        "pos_line": [
            "+dropouts = self._graph.get_collection(DROPOUTS)",
            "+feed_dict = {prob: 1.0 for prob in dropouts}"
        ],
        "core_change": "-dropouts = tf.get_collection(DROPOUTS) -feed_dict = {prob: 0.0 for prob in dropouts} +dropouts = self._graph.get_collection(DROPOUTS) +feed_dict = {prob: 1.0 for prob in dropouts}",
        "core_API": "get_collection"
    },
    {
        "commit_hash": "b0d33577387bb0397492d0ee0af5b0d15d7c8385",
        "index": "4468dcd5..92a58d1f 100644",
        "commit_message": "Update keras RNG logic to use tf.random.Generator if possible.\n\nThis change also update the RNG behavior for initializer. The seeded initializer will no longer produce same random value across multiple calls. Instead, it will produce different value, and multiple initializer created with same seed will produce same sequences. This change will the make the seeded initializer behavior align between v1 and v2.\n\nKeras was using stateful RNG op in various place when seed is not provided. The recommended approach in v2 is using tf.random.Generator which can be treat as a variable (seed) with stateless RNG op. This change make sure we use this new approach when seed is provided in v2, and also leave a flag to enforce the new approach, which has not been turn on yet. The new approach will be turned on when all the internal tests are fixed. V1 graph mode, the behavior is not change.\n\nPiperOrigin-RevId: 392092094\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "value update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GradientsTest(tf.test.TestCase):",
            "self.assertAllClose(eager_result, function_result)",
            "backprop_result, numeric_result = tf.test.compute_gradient(",
            "m, [inp], delta=1e-3)",
            "-    self.assertAllClose(numeric_result, backprop_result, rtol=1e-2)",
            "+    self.assertAllClose(numeric_result, backprop_result, atol=1e-3)",
            "self.assertAllClose(tf.reshape(numeric_result, [-1]),",
            "-                        tf.reshape(eager_result, [-1]), rtol=1e-2)",
            "+                        tf.reshape(eager_result, [-1]), atol=1e-3)",
            "",
            "def testEmbeddingLookupGradientsHaveKnownShape(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2082406)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2082407)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2082408)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'assertAllClose'), position=2, insert_id=2082409)",
            "Update(target_node=ASTNode(type=identifier, text=rtol), value='atol')",
            "Update(target_node=ASTNode(type=float, text=1e-2), value='1e-3')",
            "Update(target_node=ASTNode(type=identifier, text=rtol), value='atol')",
            "Update(target_node=ASTNode(type=float, text=1e-2), value='1e-3')",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=assertAllClose))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 182,
        "neg_line": [
            "-self.assertAllClose(numeric_result, backprop_result, rtol=1e-2)",
            "-tf.reshape(eager_result, [-1]), rtol=1e-2)"
        ],
        "pos_line": [
            "+self.assertAllClose(numeric_result, backprop_result, atol=1e-3)",
            "+tf.reshape(eager_result, [-1]), atol=1e-3)"
        ],
        "core_change": "-self.assertAllClose(numeric_result, backprop_result, rtol=1e-2) +self.assertAllClose(numeric_result, backprop_result, atol=1e-3) -tf.reshape(eager_result, [-1]), rtol=1e-2) +tf.reshape(eager_result, [-1]), atol=1e-3)",
        "core_API": "assertAllClose"
    },
    {
        "commit_hash": "01c4b22a2fa63d0edd35b2c58e2da5cc663555dd",
        "index": "8b7be3d1..ec6d4417 100644",
        "commit_message": "Fixup `trainer.py` üõ†Ô∏è\n\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "state fixialize log but API fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Trainer:",
            "self.tb_logger.tb_eval_figures(self.total_steps_done, figures)",
            "if audios is not None:",
            "self.tb_logger.tb_eval_audios(self.total_steps_done, audios, self.ap.sample_rate)",
            "+            self.tb_logger.tb_eval_stats(self.total_steps_done, self.keep_avg_eval.avg_values)",
            "",
            "def test_run(self) -> None:",
            "\"\"\"Run test and log the results. Test run must be defined by the model."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1550689)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1550690)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1550691)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1550692)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1550693)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1550694)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tb_eval_stats'), position=2, insert_id=1550695)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1550696)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1550697)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1550698)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=1550699)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1550700)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1550701)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1550702)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tb_logger'), position=2, insert_id=1550703)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1550704)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1550705)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'total_steps_done'), position=2, insert_id=1550706)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1550707)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1550708)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'avg_values'), position=2, insert_id=1550709)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1550710)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1550711)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'keep_avg_eval'), position=2, insert_id=1550712)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 184,
        "neg_line": [],
        "pos_line": [
            "+self.tb_logger.tb_eval_stats(self.total_steps_done, self.keep_avg_eval.avg_values)"
        ],
        "core_change": "+self.tb_logger.tb_eval_stats(self.total_steps_done, self.keep_avg_eval.avg_values)",
        "core_API": "tb_eval_figures"
    },
    {
        "commit_hash": "cc7e0be497a6807c055930b6ea9d8240bef4b2ab",
        "index": "9ec951f1cf..0492af1cad 100644",
        "commit_message": "Fix torch type promotion (#5095)\n\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "remove check for not clear reason",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def bitwise_left_shift(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2, array_api_promotion=True)",
            "-    ivy.assertions.check_all(x2 >= 0, message=\"shifts must be non-negative\")",
            "return torch.bitwise_left_shift(x1, x2, out=out)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=ivy))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=assertions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=check_all))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=x2))",
            "Delete(target_node=ASTNode(type=>=, text=>=))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=message))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"shifts must be non-negative\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 185,
        "neg_line": [
            "-ivy.assertions.check_all(x2 >= 0, message=\"shifts must be non-negative\")"
        ],
        "pos_line": [],
        "core_change": "-ivy.assertions.check_all(x2 >= 0, message=\"shifts must be non-negative\")",
        "core_API": "promote_types_of_inputs"
    },
    {
        "commit_hash": "42cf2eee0501a5d085f4727d409a7b0da2ee6f3e",
        "index": "7d84642e..ec05b0fa 100644",
        "commit_message": "Refactor and add tests in `get_perspective_transform` (#1767)\n\n* use torch.linalg.lstsq\n\n* refactor tests\n\n* add improvements\n\n* add missing import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* improve formula docs\n\n* codespell\n\n* linter\n\n* few fixes\n\n* fix mypy versioning\n\n* disable standalone mymy check\n\n* remove type ignore\n\n* fix mypy version checker\n\n* skip tests for < 1.11.0\n\n* switch back to torch.solve\n\n* doctest fixes\n\n* remove lstsq\n\n* remove import lstsq\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "global variable update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "TEST_DEVICES: Dict[str, torch.device] = get_test_devices()",
            "TEST_DTYPES: Dict[str, torch.dtype] = get_test_dtypes()",
            "",
            "# Combinations of device and dtype to be excluded from testing.",
            "-DEVICE_DTYPE_BLACKLIST = {('cpu', 'float16')}",
            "+# DEVICE_DTYPE_BLACKLIST = {('cpu', 'float16')}",
            "+DEVICE_DTYPE_BLACKLIST = {}",
            "",
            "",
            "@pytest.fixture()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('dictionary', None), position=2, insert_id=402607)",
            "Move(target_node=IN(type=dictionary), node=ASTNode(type={, text={), position=0)",
            "Move(target_node=IN(type=dictionary), node=ASTNode(type=}, text=}), position=1)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text='cpu'))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='float16'))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=set))"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 10,
        "number": 186,
        "neg_line": [
            "-DEVICE_DTYPE_BLACKLIST = {('cpu', 'float16')}"
        ],
        "pos_line": [
            "+# DEVICE_DTYPE_BLACKLIST = {('cpu', 'float16')}",
            "+DEVICE_DTYPE_BLACKLIST = {}"
        ],
        "core_change": "-DEVICE_DTYPE_BLACKLIST = {('cpu', 'float16')} +# DEVICE_DTYPE_BLACKLIST = {('cpu', 'float16')} +DEVICE_DTYPE_BLACKLIST = {}",
        "core_API": "fixture"
    },
    {
        "commit_hash": "7a9021d4f131ee059d49ff9b2d135e6543f75763",
        "index": "8cd494f..388a8d3 100644",
        "commit_message": "fix small issues\n\n",
        "file": "pytorch-CycleGAN-and-pix2pix.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Pix2PixModel(BaseModel):",
            "def backward_D(self):",
            "# Fake",
            "# stop backprop to the generator by detaching fake_B",
            "-        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))",
            "+        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1).data)",
            "pred_fake = self.netD.forward(fake_AB.detach())",
            "self.loss_D_fake = self.criterionGAN(pred_fake, False)",
            "",
            "# Real",
            "real_AB = torch.cat((self.real_A, self.real_B), 1)",
            "pred_real = self.netD.forward(real_AB)",
            "-        self.loss_D_real = self.criterionGAN(self.pred_real, True)",
            "+        self.loss_D_real = self.criterionGAN(pred_real, True)",
            "",
            "# Combined loss",
            "self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=pred_real), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=888059)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=888060)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=888061)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 187,
        "neg_line": [
            "-fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))",
            "-self.loss_D_real = self.criterionGAN(self.pred_real, True)"
        ],
        "pos_line": [
            "+fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1).data)",
            "+self.loss_D_real = self.criterionGAN(pred_real, True)"
        ],
        "core_change": "-fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1)) +fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1).data) -self.loss_D_real = self.criterionGAN(self.pred_real, True) +self.loss_D_real = self.criterionGAN(pred_real, True)",
        "core_API": "query"
    },
    {
        "commit_hash": "96f3b834ff7f3172b45396a6cc57b0124bbdad1a",
        "index": "91764d9..0e33414 100644",
        "commit_message": "Fix typos & supplement descriptions\n\n",
        "file": "examples.txt.json",
        "label": "no",
        "comments": "fix print update",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "args = parser.parse_args()",
            "torch.manual_seed(args.seed)",
            "if torch.cuda.is_available():",
            "if not args.cuda:",
            "-        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")",
            "+        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda.\")",
            "",
            "device = torch.device(\"cuda\" if args.cuda else \"cpu\")",
            "",
            "if args.temperature < 1e-3:",
            "-    parser.error(\"--temperature has to be greater or equal 1e-3\")",
            "+    parser.error(\"--temperature has to be greater or equal 1e-3.\")",
            "",
            "with open(args.checkpoint, 'rb') as f:",
            "model = torch.load(f).to(device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"WARNING: You have a CUDA device, so you should probably run with --cuda\"), value='\"WARNING: You have a CUDA device, so you should probably run with --cuda.\"')",
            "Update(target_node=ASTNode(type=string, text=\"--temperature has to be greater or equal 1e-3\"), value='\"--temperature has to be greater or equal 1e-3.\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 188,
        "neg_line": [
            "-print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")",
            "-parser.error(\"--temperature has to be greater or equal 1e-3\")"
        ],
        "pos_line": [
            "+print(\"WARNING: You have a CUDA device, so you should probably run with --cuda.\")",
            "+parser.error(\"--temperature has to be greater or equal 1e-3.\")"
        ],
        "core_change": "-print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\") +print(\"WARNING: You have a CUDA device, so you should probably run with --cuda.\") -parser.error(\"--temperature has to be greater or equal 1e-3\") +parser.error(\"--temperature has to be greater or equal 1e-3.\")",
        "core_API": "parse_args"
    },
    {
        "commit_hash": "b2e4b091f08f1aaf21855d588c6c8d284baba9eb",
        "index": "d08e60137..59a1ca19a 100755",
        "commit_message": "fix FSDP ShardedGradScaler (#18358)\n\nrenaming it\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add condition check for null fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "transformer_cls_to_wrap = get_module_class_from_name(",
            "model, self.args.fsdp_transformer_layer_cls_to_wrap",
            ")",
            "+                    if transformer_cls_to_wrap is None:",
            "+                        raise Exception(\"Could not find the transformer layer class to wrap in the model.\")",
            "auto_wrap_policy = functools.partial(",
            "transformer_auto_wrap_policy,",
            "# Transformer layer class to wrap"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=1194109)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1194110)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1194111)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1194112)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1194113)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'transformer_cls_to_wrap'), position=0, insert_id=1194114)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=1194115)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=1194116)",
            "Insert(target_node=IN(type=block), node=('raise_statement', None), position=0, insert_id=1194117)",
            "Insert(target_node=IN(type=raise_statement), node=('raise', 'raise'), position=0, insert_id=1194118)",
            "Insert(target_node=IN(type=raise_statement), node=('call', None), position=1, insert_id=1194119)",
            "Insert(target_node=IN(type=call), node=('identifier', 'Exception'), position=0, insert_id=1194120)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1194121)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1194122)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"Could not find the transformer layer class to wrap in the model.\"'), position=1, insert_id=1194123)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1194124)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 189,
        "neg_line": [],
        "pos_line": [
            "+if transformer_cls_to_wrap is None:",
            "+raise Exception(\"Could not find the transformer layer class to wrap in the model.\")"
        ],
        "core_change": "+if transformer_cls_to_wrap is None: +raise Exception(\"Could not find the transformer layer class to wrap in the model.\")",
        "core_API": "partial"
    },
    {
        "commit_hash": "42c5967d5009891721cae0cb291d40cb7a9ea650",
        "index": "860751fe..cecb61a4 100755",
        "commit_message": "agents and models base classes moved, various fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "no",
        "comments": "add custom method to get var",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NAFModel(Model):",
            "# Naf directly outputs V(s)",
            "target_value[action] = target_value_output",
            "",
            "-            target_output_vars = get_variables('target_outputs')",
            "+            target_output_vars = tf.contrib.framework.get_variables('target_outputs')",
            "",
            "with tf.name_scope(\"update\"):",
            "for action in self.action:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2244434)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2244435)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2244436)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=get_variables), position=2)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2244437)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2244438)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'framework'), position=2, insert_id=2244439)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2244440)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2244441)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'contrib'), position=2, insert_id=2244442)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 190,
        "neg_line": [
            "-target_output_vars = get_variables('target_outputs')"
        ],
        "pos_line": [
            "+target_output_vars = tf.contrib.framework.get_variables('target_outputs')"
        ],
        "core_change": "-target_output_vars = get_variables('target_outputs') +target_output_vars = tf.contrib.framework.get_variables('target_outputs')",
        "core_API": "get_variables"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "1ab11e21..02cc4ee5 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "add docs",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpanConstituencyParserTest(ModelTestCase):",
            "# A very annoying edge case: the PTB has several single word sentences.",
            "# when running with a batch size 1, we have to be very careful",
            "# about how we .squeeze/.unsqueeze things to make sure it still runs.",
            "-        text = {\"tokens\": torch.LongTensor([[1]])}",
            "+        text = {\"tokens\": {\"tokens\": torch.LongTensor([[1]])}}",
            "pos_tags = torch.LongTensor([[1]])",
            "spans = torch.LongTensor([[[0, 0]]])",
            "label = torch.LongTensor([[1]])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=pair), node=('string', '\"tokens\"'), position=0, insert_id=22709)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=22710)",
            "Insert(target_node=ASTNode(type=pair), node=('dictionary', None), position=2, insert_id=22711)",
            "Insert(target_node=IN(type=dictionary), node=('{', '{'), position=0, insert_id=22712)",
            "Move(target_node=IN(type=dictionary), node=ASTNode(type=pair), position=1)",
            "Insert(target_node=IN(type=dictionary), node=('}', '}'), position=2, insert_id=22713)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 191,
        "neg_line": [
            "-text = {\"tokens\": torch.LongTensor([[1]])}"
        ],
        "pos_line": [
            "+text = {\"tokens\": {\"tokens\": torch.LongTensor([[1]])}}"
        ],
        "core_change": "-text = {\"tokens\": torch.LongTensor([[1]])} +text = {\"tokens\": {\"tokens\": torch.LongTensor([[1]])}}",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "888686e72b1214148352ac5b69a516c53b74323d",
        "index": "0b86fff7f..39cec18e7 100644",
        "commit_message": "Fix tests on single-GPU machine (#16911)\n\n\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "not ML API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MixedPrecisionBoringFabric(BoringFabric):",
            "[",
            "(\"cpu\", \"16-mixed\", torch.bfloat16),",
            "(\"cpu\", \"bf16-mixed\", torch.bfloat16),",
            "-        pytest.param(\"cuda\", \"16-mixed\", torch.float16, marks=RunIf(min_cuda_gpus=1)),",
            "-        pytest.param(\"cuda\", \"bf16-mixed\", torch.bfloat16, marks=RunIf(min_cuda_gpus=1, bf16_cuda=True)),",
            "+        pytest.param(\"cuda\", \"16-mixed\", torch.float16, marks=RunIf(min_cuda_gpus=2)),",
            "+        pytest.param(\"cuda\", \"bf16-mixed\", torch.bfloat16, marks=RunIf(min_cuda_gpus=2, bf16_cuda=True)),",
            "],",
            ")",
            "def test_amp(accelerator, precision, expected_dtype):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=485666)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pytest'), position=0, insert_id=485667)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=485668)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'param'), position=2, insert_id=485669)",
            "Update(target_node=ASTNode(type=integer, text=1), value='2')",
            "Update(target_node=ASTNode(type=integer, text=1), value='2')",
            "Delete(target_node=ASTNode(type=identifier, text=pytest))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=param))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 192,
        "neg_line": [
            "-pytest.param(\"cuda\", \"16-mixed\", torch.float16, marks=RunIf(min_cuda_gpus=1)),",
            "-pytest.param(\"cuda\", \"bf16-mixed\", torch.bfloat16, marks=RunIf(min_cuda_gpus=1, bf16_cuda=True)),"
        ],
        "pos_line": [
            "+pytest.param(\"cuda\", \"16-mixed\", torch.float16, marks=RunIf(min_cuda_gpus=2)),",
            "+pytest.param(\"cuda\", \"bf16-mixed\", torch.bfloat16, marks=RunIf(min_cuda_gpus=2, bf16_cuda=True)),"
        ],
        "core_change": "-pytest.param(\"cuda\", \"16-mixed\", torch.float16, marks=RunIf(min_cuda_gpus=1)), -pytest.param(\"cuda\", \"bf16-mixed\", torch.bfloat16, marks=RunIf(min_cuda_gpus=1, bf16_cuda=True)), +pytest.param(\"cuda\", \"16-mixed\", torch.float16, marks=RunIf(min_cuda_gpus=2)), +pytest.param(\"cuda\", \"bf16-mixed\", torch.bfloat16, marks=RunIf(min_cuda_gpus=2, bf16_cuda=True)),",
        "core_API": "param"
    },
    {
        "commit_hash": "c6ec8488a13f345fad19969925426aa6ce06a3c2",
        "index": "0649027..f6316a7 100644",
        "commit_message": "fix tensorflow pre-trained model last layer outputshape problem.\n\n",
        "file": "MMdnn.txt.json",
        "label": "yes",
        "comments": "add API call for shape fix",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class tensorflow_extractor(base_extractor):",
            "writer.close()",
            "sess.run(init)",
            "saver = tf.train.Saver()",
            "+            tf.train.export_meta_graph(\"kit.meta\", as_text=True)",
            "saver.restore(sess, path + cls.architecture_map[architecture]['filename'])",
            "save_path = saver.save(sess, path + \"imagenet_{}.ckpt\".format(architecture))",
            "print(\"Model saved in file: %s\" % save_path)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2452758)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2452759)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2452760)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2452761)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2452762)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2452763)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'export_meta_graph'), position=2, insert_id=2452764)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2452765)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"kit.meta\"'), position=1, insert_id=2452766)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2452767)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2452768)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2452769)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2452770)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2452771)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train'), position=2, insert_id=2452772)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_text'), position=0, insert_id=2452773)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2452774)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2452775)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 193,
        "neg_line": [],
        "pos_line": [
            "+tf.train.export_meta_graph(\"kit.meta\", as_text=True)"
        ],
        "core_change": "+tf.train.export_meta_graph(\"kit.meta\", as_text=True)",
        "core_API": "close"
    },
    {
        "commit_hash": "13951099e8959f5fd0900a442985b8f62b14d908",
        "index": "98de4ca4..3059349a 100644",
        "commit_message": "Stabilize autoguide scale parameters via SoftplusTransform (#2767)\n\n* Update to PyTorch nightly\n\n* Update README.md\n\n* Commit to PyTorch nightly on dev branch\n\n* Fix constraint bugs\n\n* Relax torchvision version\n\n* Fix CorrCholesky constraint and test data\n\n* Fix funsor bugs\n\n* Try harder to generate positive data\n\n* xfail; switch to torch_test channel\n\n* Fix versions\n\n* Pin to fixed nightly version\n\n* lint\n\n* xfail some funsor tests\n\n* Remove accidental file\n\n* Use softplus transforms for autoguide scales\n\n* Add transform tests\n\n* Rename stable_positive -> softplus_positive\n\n* Make autoguide constraints configurable\n\n* Address review comments\n\n* lint\n\n* Tweak parameters in inference test\n\n* Regster transforms in docs\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "def test_auto_diagonal_gaussians(auto_class, Elbo):",
            "guide = auto_class(model, rank=1)",
            "else:",
            "guide = auto_class(model)",
            "-    adam = optim.Adam({\"lr\": .001, \"betas\": (0.95, 0.999)})",
            "+    adam = optim.ClippedAdam({\"lr\": .01, \"betas\": (0.95, 0.999),",
            "+                              \"lrd\": 0.1 ** (1 / n_steps)})",
            "svi = SVI(model, guide, adam, loss=Elbo())",
            "",
            "for k in range(n_steps):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Adam), value='ClippedAdam')",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=4, insert_id=1604500)",
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=5, insert_id=1604501)",
            "Update(target_node=ASTNode(type=float, text=.001), value='.01')",
            "Insert(target_node=IN(type=pair), node=('string', '\"lrd\"'), position=0, insert_id=1604502)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=1604503)",
            "Insert(target_node=IN(type=pair), node=('binary_operator', None), position=2, insert_id=1604504)",
            "Insert(target_node=IN(type=binary_operator), node=('float', '0.1'), position=0, insert_id=1604505)",
            "Insert(target_node=IN(type=binary_operator), node=('**', '**'), position=1, insert_id=1604506)",
            "Insert(target_node=IN(type=binary_operator), node=('parenthesized_expression', None), position=2, insert_id=1604507)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1604508)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=1604509)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1604510)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=0, insert_id=1604511)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=1604512)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'n_steps'), position=2, insert_id=1604513)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 194,
        "neg_line": [
            "-adam = optim.Adam({\"lr\": .001, \"betas\": (0.95, 0.999)})"
        ],
        "pos_line": [
            "+adam = optim.ClippedAdam({\"lr\": .01, \"betas\": (0.95, 0.999),",
            "+\"lrd\": 0.1 ** (1 / n_steps)})"
        ],
        "core_change": "-adam = optim.Adam({\"lr\": .001, \"betas\": (0.95, 0.999)}) +adam = optim.ClippedAdam({\"lr\": .01, \"betas\": (0.95, 0.999), +\"lrd\": 0.1 ** (1 / n_steps)})",
        "core_API": "Adam"
    },
    {
        "commit_hash": "37e54d493823cca500f79b85967bfab81f07af2e",
        "index": "22cea57dd..e9c31e96a 100644",
        "commit_message": "fix random seed for stable test\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_frontend_backward_multi_channel(train, use_wpe, use_beamformer):",
            "frontend.train()",
            "else:",
            "frontend.eval()",
            "+    torch.random.manual_seed(14)",
            "x = torch.randn(2, 1000, 2, requires_grad=True)",
            "x_lengths = torch.LongTensor([1000, 980])",
            "y, y_lengths = frontend(x, x_lengths)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=119164)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=119165)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=119166)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=119167)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=119168)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=119169)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=119170)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=119171)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '14'), position=1, insert_id=119172)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=119173)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=119174)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=119175)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'random'), position=2, insert_id=119176)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 195,
        "neg_line": [],
        "pos_line": [
            "+torch.random.manual_seed(14)"
        ],
        "core_change": "+torch.random.manual_seed(14)",
        "core_API": "train"
    },
    {
        "commit_hash": "54497ae347ea1083d754fb65adb287dcb89248d6",
        "index": "a103739f..33e6d1b8 100644",
        "commit_message": "Fix issues suggested by codacy. (#344)\n\n* remove two dangerous default values\n\n* fix mnist tutorial based on codacy\n\n* address hao's comments.\n\n* remove unused y_op\n\n* hao conv.py\n\n* hao prepro.py\n\n* hao files.py\n\n* remove str statement\n\n* hao example mnist\n\n* yapf\n\n* hao cifar10\n\n* hao inceptionv3\n\n* hao ptb tfrecord image processing\n\n* hao tutorials\n\n* str comment\n\n* str docs\n\n* Update README.md\n\n* remove unused code\n\n* minor fix\n\n* small fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "remove as",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Conv1dLayer(Layer):",
            "act = tf.identity",
            "logging.info(\"Conv1dLayer %s: shape:%s stride:%s pad:%s act:%s\" % (self.name, str(shape), str(stride), padding, act.__name__))",
            "",
            "-        with tf.variable_scope(name) as vs:",
            "+        with tf.variable_scope(name):  # as vs:",
            "W = tf.get_variable(name='W_conv1d', shape=shape, initializer=W_init, dtype=D_TYPE, **W_init_args)",
            "self.outputs = tf.nn.convolution(",
            "self.inputs, W, strides=(stride, ), padding=padding, dilation_rate=(dilation_rate, ), data_format=data_format)  # 1.2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=with_item), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=as, text=as))",
            "Delete(target_node=ASTNode(type=identifier, text=vs))",
            "Delete(target_node=ASTNode(type=as_pattern_target))",
            "Delete(target_node=ASTNode(type=as_pattern))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 196,
        "neg_line": [
            "-with tf.variable_scope(name) as vs:"
        ],
        "pos_line": [
            "+with tf.variable_scope(name):  # as vs:"
        ],
        "core_change": "-with tf.variable_scope(name) as vs: +with tf.variable_scope(name):  # as vs:",
        "core_API": "info"
    },
    {
        "commit_hash": "7c292af66f61b1125854218519bf81d494e5b11e",
        "index": "ae7b797e..bca341af 100644",
        "commit_message": "Fix hub (#2687)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2687\n\nReviewed By: alexeib\n\nDifferential Revision: D24095130\n\nPulled By: myleott\n\nfbshipit-source-id: 7d371bccb550ec68b2b9b39dfa4c0718356508d6\n\n",
        "file": "fairseq.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestGradientScaling(unittest.TestCase):",
            "optimizer = FP16Optimizer.build_optimizer(self.namespace_dls, params)",
            "",
            "self.run_iter(model, params, optimizer)",
            "-        self.assertTrue(torch.all(optimizer.fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True))))",
            "+        self.assertTrue(all(",
            "+            torch.all(fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True)))",
            "+            for fp32_params in optimizer.fp32_params.values()",
            "+        ))",
            "",
            "def test_memory_efficient(self):",
            "model = copy.deepcopy(self.model)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'all'), position=0, insert_id=210791)",
            "Insert(target_node=ASTNode(type=call), node=('generator_expression', None), position=1, insert_id=210792)",
            "Insert(target_node=IN(type=generator_expression), node=('(', '('), position=0, insert_id=210793)",
            "Insert(target_node=IN(type=generator_expression), node=('call', None), position=1, insert_id=210794)",
            "Insert(target_node=IN(type=generator_expression), node=('for_in_clause', None), position=2, insert_id=210795)",
            "Insert(target_node=IN(type=generator_expression), node=(')', ')'), position=3, insert_id=210796)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=for_in_clause), node=('for', 'for'), position=0, insert_id=210797)",
            "Insert(target_node=IN(type=for_in_clause), node=('identifier', 'fp32_params'), position=1, insert_id=210798)",
            "Insert(target_node=IN(type=for_in_clause), node=('in', 'in'), position=2, insert_id=210799)",
            "Insert(target_node=IN(type=for_in_clause), node=('call', None), position=3, insert_id=210800)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=210801)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=210802)",
            "Update(target_node=ASTNode(type=identifier, text=eq), value='values')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=210803)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=210804)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'fp32_params'), position=0, insert_id=210805)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=210806)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'eq'), position=2, insert_id=210807)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 197,
        "neg_line": [
            "-self.assertTrue(torch.all(optimizer.fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True))))"
        ],
        "pos_line": [
            "+self.assertTrue(all(",
            "+torch.all(fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True)))",
            "+for fp32_params in optimizer.fp32_params.values()",
            "+))"
        ],
        "core_change": "-self.assertTrue(torch.all(optimizer.fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True)))) +self.assertTrue(all( +torch.all(fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True))) +for fp32_params in optimizer.fp32_params.values() +))",
        "core_API": "build_optimizer"
    },
    {
        "commit_hash": "d70e5ec69de7d2a8d5bd81e1524b87c668d868d0",
        "index": "19f8fc31..bc406155 100644",
        "commit_message": "[Feat] Add tpu-testing in circleci (#787)\n\n* add needed files for tpu-testing in circleci\n\n* adapt script for pytorch nghtlies\n\n* fix shape test\n\n* fix color tests\n\n* add pytorch version to the dockerfile\n\n* fix xla precision and disable torch-xla[1.6,1.7]\n\n* adjust rgb2luv precision\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "test api",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestLuvToRgb(BaseTester):",
            "[0.06325728, 0.78878325, 0.74280596, 0.99514300, 0.47176042]]",
            "]], device=device, dtype=dtype)",
            "",
            "-        assert_allclose(kornia.color.luv_to_rgb(data), expected)",
            "+        assert_allclose(kornia.color.luv_to_rgb(data), expected, rtol=1e-4, atol=1e-4)",
            "",
            "def test_forth_and_back(self, device, dtype):",
            "data = torch.rand(3, 4, 5, device=device, dtype=dtype)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=433592)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=433593)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=433594)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=433595)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rtol'), position=0, insert_id=433596)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=433597)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-4'), position=2, insert_id=433598)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=433599)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=433600)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-4'), position=2, insert_id=433601)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 198,
        "neg_line": [
            "-assert_allclose(kornia.color.luv_to_rgb(data), expected)"
        ],
        "pos_line": [
            "+assert_allclose(kornia.color.luv_to_rgb(data), expected, rtol=1e-4, atol=1e-4)"
        ],
        "core_change": "-assert_allclose(kornia.color.luv_to_rgb(data), expected) +assert_allclose(kornia.color.luv_to_rgb(data), expected, rtol=1e-4, atol=1e-4)",
        "core_API": "luv_to_rgb"
    },
    {
        "commit_hash": "4eb15f14688ec1ecc0992d5ba8ff62d8a4e345da",
        "index": "0949acee..9147e3a2 100644",
        "commit_message": "Fix HALTON sequence utilization in Euler Sampling. Disable Sobol random_type in Euler sampling when time_step is non-constant till TensorFlow 2.2 is released.\n\nPiperOrigin-RevId: 291143718\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EulerSamplingTest(tf.test.TestCase, parameterized.TestCase):",
            "times=times,",
            "num_samples=num_samples,",
            "initial_state=x0,",
            "-            random_type=tff.math.random.RandomType.SOBOL,",
            "+            random_type=tff.math.random.RandomType.HALTON,",
            "time_step=0.01,",
            "-            seed=12134))",
            "+            seed=12134,",
            "+            skip=100,",
            "+            dtype=tf.float32))",
            "",
            "-    self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0)",
            "+    self.assertAllClose(paths.shape, (num_samples, 3, 2), atol=0)",
            "means = np.mean(paths, axis=0)",
            "times = np.reshape(times, [-1, 1])",
            "expected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=2346222)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=7, insert_id=2346223)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2346224)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2346225)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'seed'), position=0, insert_id=2346226)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2346227)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=2, insert_id=2346228)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'skip'), position=0, insert_id=2346229)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2346230)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=2, insert_id=2346231)",
            "Update(target_node=ASTNode(type=identifier, text=seed), value='dtype')",
            "Insert(target_node=IN(type=expression_list), node=('integer', '12134'), position=0, insert_id=2346232)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=2346233)",
            "Insert(target_node=IN(type=expression_list), node=('integer', '100'), position=0, insert_id=2346234)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=2346235)",
            "Insert(target_node=ASTNode(type=ERROR), node=('attribute', None), position=0, insert_id=2346236)",
            "Update(target_node=ASTNode(type=identifier, text=SOBOL), value='HALTON')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2346237)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2346238)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2346239)",
            "Update(target_node=ASTNode(type=integer, text=5), value='3')",
            "Delete(target_node=ASTNode(type=integer, text=12134))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 199,
        "neg_line": [
            "-random_type=tff.math.random.RandomType.SOBOL,",
            "-seed=12134))",
            "-self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0)"
        ],
        "pos_line": [
            "+random_type=tff.math.random.RandomType.HALTON,",
            "+seed=12134,",
            "+skip=100,",
            "+dtype=tf.float32))",
            "+self.assertAllClose(paths.shape, (num_samples, 3, 2), atol=0)"
        ],
        "core_change": "-random_type=tff.math.random.RandomType.SOBOL, +random_type=tff.math.random.RandomType.HALTON, -seed=12134)) +seed=12134, +skip=100, +dtype=tf.float32)) -self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0) +self.assertAllClose(paths.shape, (num_samples, 3, 2), atol=0)",
        "core_API": "assertAllClose"
    },
    {
        "commit_hash": "e01ed1855d292f6fbfe128bda8701eabd151ad73",
        "index": "50194639..32f70650 100644",
        "commit_message": "Logging fix (#661)\n\n* `tf.logging` replaced by: `tl.logging`\n\n* Changelog updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "debug log",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Layer_Shape_Test(unittest.TestCase):",
            "",
            "if __name__ == '__main__':",
            "",
            "-    # tf.logging.set_verbosity(tf.logging.INFO)",
            "-    tf.logging.set_verbosity(tf.logging.DEBUG)",
            "+    # tl.logging.set_verbosity(tl.logging.INFO)",
            "+    tl.logging.set_verbosity(tl.logging.DEBUG)",
            "",
            "unittest.main()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tl')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tl')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 201,
        "neg_line": [
            "-# tf.logging.set_verbosity(tf.logging.INFO)",
            "-tf.logging.set_verbosity(tf.logging.DEBUG)"
        ],
        "pos_line": [
            "+# tl.logging.set_verbosity(tl.logging.INFO)",
            "+tl.logging.set_verbosity(tl.logging.DEBUG)"
        ],
        "core_change": "-# tf.logging.set_verbosity(tf.logging.INFO) -tf.logging.set_verbosity(tf.logging.DEBUG) +# tl.logging.set_verbosity(tl.logging.INFO) +tl.logging.set_verbosity(tl.logging.DEBUG)",
        "core_API": "set_verbosity"
    },
    {
        "commit_hash": "e6a107c14eec6dde40bc3c73c4e2b54dae3996df",
        "index": "e67fc4170..2d5adf73a 100644",
        "commit_message": "Generalize TorchHook to FrameworkHook (#2561)\n\n* create FrameworkHook and move generic hooking functionality over\n\n* PEP-ify import statements all around\n\n* PR comment - fix docstring\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "use custom method to fix the bug",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TorchTensor(AbstractTensor):",
            ")",
            "# This handles case 3: it redirects the command to the appropriate class depending",
            "# of the syft type of the arguments and returns",
            "-            if args_type not in (torch.Tensor, torch.nn.Parameter):",
            "+            if args_type not in FrameworkTensor:",
            "return args_type.handle_func_command(command)",
            "",
            "# build the new command"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('identifier', 'FrameworkTensor'), position=3, insert_id=1457914)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Parameter))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 202,
        "neg_line": [
            "-if args_type not in (torch.Tensor, torch.nn.Parameter):"
        ],
        "pos_line": [
            "+if args_type not in FrameworkTensor:"
        ],
        "core_change": "-if args_type not in (torch.Tensor, torch.nn.Parameter): +if args_type not in FrameworkTensor:",
        "core_API": "handle_func_command"
    },
    {
        "commit_hash": "064efe6c26636ca2e8b6d93cd83aaf890e2f242b",
        "index": "78f76ed8..09d5fad8 100644",
        "commit_message": "Fix BatchNorm TransformModule (#2459)\n\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BatchNorm(TransformModule):",
            "if self.training:",
            "mean, var = y.mean(0), y.var(0)",
            "",
            "-            # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`",
            "-            self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)",
            "-            self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)",
            "+            with torch.no_grad():",
            "+                # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`",
            "+                self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)",
            "+                self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)",
            "",
            "# During test time, use smoothed averages rather than the sample ones",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=3, insert_id=697052)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=697053)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=697054)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=697055)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=697056)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=697057)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=1)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=697058)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=697059)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=697060)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=697061)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=697062)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_grad'), position=2, insert_id=697063)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=697064)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=697065)"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 203,
        "neg_line": [
            "-# NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`",
            "-self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)",
            "-self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)"
        ],
        "pos_line": [
            "+with torch.no_grad():",
            "+# NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`",
            "+self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)",
            "+self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)"
        ],
        "core_change": "-# NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d` -self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum) -self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum) +with torch.no_grad(): +# NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d` +self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum) +self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)",
        "core_API": "mean"
    },
    {
        "commit_hash": "a458c481a37a9a07592b4d6008e9890f2ee43269",
        "index": "c0fbaffcd..5232969a7 100755",
        "commit_message": "fixed a bug about dropout\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "remove API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RNNLM(nn.Module):",
            "",
            "def forward(self, state, x):",
            "h0 = self.embed(x)",
            "-        h1, c1 = self.l1(F.dropout(h0), (state['h1'], state['c1']))",
            "-        h2, c2 = self.l2(F.dropout(h1), (state['h2'], state['c2']))",
            "-        y = self.lo(F.dropout(h2))",
            "+        h1, c1 = self.l1(self.d0(h0), (state['h1'], state['c1']))",
            "+        h2, c2 = self.l2(self.d1(h1), (state['h2'], state['c2']))",
            "+        y = self.lo(self.d2(h2))",
            "state = {'c1': c1, 'h1': h1, 'c2': c2, 'h2': h2}",
            "return state, y"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=F), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=dropout), value='d0')",
            "Update(target_node=ASTNode(type=identifier, text=F), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=dropout), value='d1')",
            "Update(target_node=ASTNode(type=identifier, text=F), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=dropout), value='d2')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 6,
        "number": 205,
        "neg_line": [
            "-h1, c1 = self.l1(F.dropout(h0), (state['h1'], state['c1']))",
            "-h2, c2 = self.l2(F.dropout(h1), (state['h2'], state['c2']))",
            "-y = self.lo(F.dropout(h2))"
        ],
        "pos_line": [
            "+h1, c1 = self.l1(self.d0(h0), (state['h1'], state['c1']))",
            "+h2, c2 = self.l2(self.d1(h1), (state['h2'], state['c2']))",
            "+y = self.lo(self.d2(h2))"
        ],
        "core_change": "-h1, c1 = self.l1(F.dropout(h0), (state['h1'], state['c1'])) -h2, c2 = self.l2(F.dropout(h1), (state['h2'], state['c2'])) -y = self.lo(F.dropout(h2)) +h1, c1 = self.l1(self.d0(h0), (state['h1'], state['c1'])) +h2, c2 = self.l2(self.d1(h1), (state['h2'], state['c2'])) +y = self.lo(self.d2(h2))",
        "core_API": "embed"
    },
    {
        "commit_hash": "69c8c6cdc69fc7afad214a60dbcefc354f54a56d",
        "index": "355e40b6..e16b5f58 100644",
        "commit_message": "fix test\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_link_neighbor_loader_edge_label():",
            "",
            "for batch in loader:",
            "assert batch.edge_label.dtype == torch.long",
            "-        assert torch.all(batch.edge_label[:10] == 2)",
            "+        assert torch.all(batch.edge_label[:10] == 1)",
            "assert torch.all(batch.edge_label[10:] == 0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=2), value='1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 207,
        "neg_line": [
            "-assert torch.all(batch.edge_label[:10] == 2)"
        ],
        "pos_line": [
            "+assert torch.all(batch.edge_label[:10] == 1)"
        ],
        "core_change": "-assert torch.all(batch.edge_label[:10] == 2) +assert torch.all(batch.edge_label[:10] == 1)",
        "core_API": "all"
    },
    {
        "commit_hash": "59f3bb008b3649c126bfec7dfee3e97ee4d12ef8",
        "index": "dd7f1d3..b6dff86 100644",
        "commit_message": "translate bug fix\n\n",
        "file": "examples.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def reportScore(name, scoreTotal, wordsTotal):",
            "def main():",
            "opt = parser.parse_args()",
            "opt.cuda = opt.gpu > -1",
            "-    torch.cuda.set_device(opt.gpu)",
            "+    if opt.cuda:",
            "+        torch.cuda.set_device(opt.gpu)",
            "",
            "translator = onmt.Translator(opt)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=191321)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=191322)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=191323)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=191324)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=191325)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'opt'), position=0, insert_id=191326)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=191327)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=191328)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 208,
        "neg_line": [
            "-torch.cuda.set_device(opt.gpu)"
        ],
        "pos_line": [
            "+if opt.cuda:",
            "+torch.cuda.set_device(opt.gpu)"
        ],
        "core_change": "-torch.cuda.set_device(opt.gpu) +if opt.cuda: +torch.cuda.set_device(opt.gpu)",
        "core_API": "parse_args"
    },
    {
        "commit_hash": "65671dda18c9158480d63978d833aae5dd705671",
        "index": "6819448..6a84345 100755",
        "commit_message": "Fix static padding calculation\n\n",
        "file": "EfficientNet-PyTorch.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Conv2dStaticSamePadding(nn.Conv2d):",
            "pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)",
            "pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)",
            "if pad_h > 0 or pad_w > 0:",
            "-            self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2,",
            "-                                                pad_h - pad_h // 2, pad_h - pad_h // 2))",
            "+            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2,",
            "+                                                pad_h // 2, pad_h - pad_h // 2))",
            "else:",
            "self.static_padding = nn.Identity()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=binary_operator), position=1)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=binary_operator), position=6)",
            "Delete(target_node=ASTNode(type=identifier, text=pad_w))",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=identifier, text=pad_h))",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 209,
        "neg_line": [
            "-self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2,",
            "-pad_h - pad_h // 2, pad_h - pad_h // 2))"
        ],
        "pos_line": [
            "+self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2,",
            "+pad_h // 2, pad_h - pad_h // 2))"
        ],
        "core_change": "-self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2, -pad_h - pad_h // 2, pad_h - pad_h // 2)) +self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, +pad_h // 2, pad_h - pad_h // 2))",
        "core_API": "ZeroPad2d"
    },
    {
        "commit_hash": "2d77a2d6039bc444f2a5801f0c4624cd140c3a23",
        "index": "c434a2a9..654d6638 100644",
        "commit_message": "lazy datasets (#675)\n\n* lazy datasets\n\n* add end-to-end lazy dataset test + fix bugs\n\n* fix notebook tests\n\n* remove accidentally committed ipynb checkpoints\n\n* remove duplicate code in tests\n\n* Iterator -> Iterable\n\n* address PR comments\n\n* fix api docs\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def train_model(params: Params, serialization_dir: str) -> Model:",
            "",
            "logger.info(\"Creating a vocabulary using %s data.\", \", \".join(datasets_for_vocab_creation))",
            "vocab = Vocabulary.from_params(params.pop(\"vocabulary\", {}),",
            "-                                   Dataset([instance for key, dataset in all_datasets.items()",
            "-                                            for instance in dataset.instances",
            "-                                            if key in datasets_for_vocab_creation]))",
            "+                                   (instance for key, dataset in all_datasets.items()",
            "+                                    for instance in dataset",
            "+                                    if key in datasets_for_vocab_creation))",
            "vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))",
            "",
            "model = Model.from_params(vocab, params.pop('model'))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('generator_expression', None), position=3, insert_id=1774367)",
            "Insert(target_node=IN(type=generator_expression), node=('(', '('), position=0, insert_id=1774368)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=identifier, text=instance), position=1)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=for_in_clause), position=2)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=for_in_clause), position=3)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=if_clause), position=4)",
            "Insert(target_node=IN(type=generator_expression), node=(')', ')'), position=5, insert_id=1774369)",
            "Move(target_node=ASTNode(type=for_in_clause), node=ASTNode(type=identifier, text=dataset), position=3)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=instances))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list_comprehension))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 210,
        "neg_line": [
            "-Dataset([instance for key, dataset in all_datasets.items()",
            "-for instance in dataset.instances",
            "-if key in datasets_for_vocab_creation]))"
        ],
        "pos_line": [
            "+(instance for key, dataset in all_datasets.items()",
            "+for instance in dataset",
            "+if key in datasets_for_vocab_creation))"
        ],
        "core_change": "-Dataset([instance for key, dataset in all_datasets.items() -for instance in dataset.instances -if key in datasets_for_vocab_creation])) +(instance for key, dataset in all_datasets.items() +for instance in dataset +if key in datasets_for_vocab_creation))",
        "core_API": "info"
    },
    {
        "commit_hash": "e4648ffef11c3a606c55b738b486007998c25459",
        "index": "33b9cb66..86109e74 100644",
        "commit_message": "Fix multi-speaker init of Tacotron models & tests\n\n",
        "file": "TTS.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SpeedySpeech(BaseTTS):",
            "outputs = {\"model_outputs\": o_de.transpose(1, 2), \"durations_log\": o_dr_log.squeeze(1), \"alignments\": attn}",
            "return outputs",
            "",
            "+    @torch.no_grad()",
            "def inference(self, x, aux_input={\"d_vectors\": None, \"speaker_ids\": None}):  # pylint: disable=unused-argument",
            "\"\"\"",
            "Shapes:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('decorated_definition', None), position=3, insert_id=1549922)",
            "Insert(target_node=IN(type=decorated_definition), node=('decorator', None), position=0, insert_id=1549923)",
            "Move(target_node=IN(type=decorated_definition), node=ASTNode(type=function_definition), position=1)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=1549924)",
            "Insert(target_node=IN(type=decorator), node=('call', None), position=1, insert_id=1549925)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1549926)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1549927)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1549928)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1549929)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_grad'), position=2, insert_id=1549930)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1549931)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1549932)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 213,
        "neg_line": [],
        "pos_line": [
            "+@torch.no_grad()"
        ],
        "core_change": "+@torch.no_grad()",
        "core_API": "transpose"
    },
    {
        "commit_hash": "082aec058451cc629eaf6b55fa5e8a18f4e6fd14",
        "index": "962806e..c6bedce 100644",
        "commit_message": "Fix einsum for keras implementation\n\n",
        "file": "einops.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class KerasBackend(AbstractBackend):",
            "return keras",
            "",
            "def einsum(self, pattern, *x):",
            "-        return self.tf.einsum(pattern, *x)",
            "+        return self.tf.vectorized_map(",
            "+            functools.partial(self.tf.einsum, pattern),",
            "+            *x",
            "+        )",
            "",
            "",
            "class OneFlowBackend(AbstractBackend):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2483790)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2483791)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2483792)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'vectorized_map'), position=2, insert_id=2483793)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2483794)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2483795)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2483796)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=2, insert_id=2483797)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2483798)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2483799)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functools'), position=0, insert_id=2483800)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2483801)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'partial'), position=2, insert_id=2483802)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2483803)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2483804)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=pattern), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2483805)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 214,
        "neg_line": [
            "-return self.tf.einsum(pattern, *x)"
        ],
        "pos_line": [
            "+return self.tf.vectorized_map(",
            "+functools.partial(self.tf.einsum, pattern),",
            "+*x",
            "+)"
        ],
        "core_change": "-return self.tf.einsum(pattern, *x) +return self.tf.vectorized_map( +functools.partial(self.tf.einsum, pattern), +*x +)",
        "core_API": "einsum"
    },
    {
        "commit_hash": "ec1924ee5789b72c31c65932b549c59ccae0cdd6",
        "index": "03f6085e..b853fa5b 100644",
        "commit_message": "additional fix for difference model merging\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run_modelmerger(primary_model_name, secondary_model_name, teritary_model_nam",
            "t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))",
            "theta_1[key] = theta_func1(theta_1[key], t2)",
            "else:",
            "-                    theta_1[key] = 0",
            "+                    theta_1[key] = torch.zeros_like(theta_1[key])",
            "del theta_2, teritary_model",
            "",
            "for key in tqdm.tqdm(theta_0.keys()):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=typed_default_parameter), node=('call', None), position=5, insert_id=1139540)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1139541)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1139542)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1139543)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1139544)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros_like'), position=2, insert_id=1139545)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1139546)",
            "Insert(target_node=IN(type=argument_list), node=('subscript', None), position=1, insert_id=1139547)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1139548)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'theta_1'), position=0, insert_id=1139549)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1139550)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'key'), position=2, insert_id=1139551)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1139552)",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 215,
        "neg_line": [
            "-theta_1[key] = 0"
        ],
        "pos_line": [
            "+theta_1[key] = torch.zeros_like(theta_1[key])"
        ],
        "core_change": "-theta_1[key] = 0 +theta_1[key] = torch.zeros_like(theta_1[key])",
        "core_API": "get"
    },
    {
        "commit_hash": "5792fba00cecac44cc50aa33955f4dca1311c540",
        "index": "a96daaff..23808def 100644",
        "commit_message": "Fix loss computation for empty tensors\n\n",
        "file": "flair.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DefaultClassifier(Classifier):",
            "",
            "def _calculate_loss(self, scores, labels):",
            "",
            "-        if len(labels) == 0: return torch.tensor(0., requires_grad=True), 1",
            "+        if len(labels) == 0: return torch.tensor(0., requires_grad=True, device=flair.device), 1",
            "",
            "if self.multi_label:",
            "labels = torch.tensor([[1 if l in all_labels_for_point else 0 for l in self.label_dictionary.get_items()]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=236909)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=236910)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=236911)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=236912)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=236913)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flair'), position=0, insert_id=236914)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=236915)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=236916)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 216,
        "neg_line": [
            "-if len(labels) == 0: return torch.tensor(0., requires_grad=True), 1"
        ],
        "pos_line": [
            "+if len(labels) == 0: return torch.tensor(0., requires_grad=True, device=flair.device), 1"
        ],
        "core_change": "-if len(labels) == 0: return torch.tensor(0., requires_grad=True), 1 +if len(labels) == 0: return torch.tensor(0., requires_grad=True, device=flair.device), 1",
        "core_API": "tensor"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "cb738ec0..3657657d 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "yes",
        "comments": "change API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EmbeddingLayer(nn.Module):",
            "torch.empty(weight_shape[0],",
            "weight_shape[1],",
            "dtype=dtype,",
            "-                        device=torch.cuda.current_device()))",
            "+                        device=get_accelerator().current_device_name()))",
            "",
            "def forward(self, input):",
            "return F.embedding(input, self.weight)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=74856)",
            "Update(target_node=ASTNode(type=identifier, text=current_device), value='current_device_name')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_accelerator')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=74857)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=74858)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=74859)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 217,
        "neg_line": [
            "-device=torch.cuda.current_device()))"
        ],
        "pos_line": [
            "+device=get_accelerator().current_device_name()))"
        ],
        "core_change": "-device=torch.cuda.current_device())) +device=get_accelerator().current_device_name()))",
        "core_API": "empty"
    },
    {
        "commit_hash": "9559873d135de78734a835d9725f2c0dabe4ace7",
        "index": "b104230bf8..c3a816709b 100644",
        "commit_message": "[rllib] tuple space shouldn't assume elements are all the same size (#2637)\n\n* fix\n\n* lint\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "change API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MultiActionDistribution(ActionDistribution):",
            "",
            "def logp(self, x):",
            "\"\"\"The log-likelihood of the action distribution.\"\"\"",
            "-        split_list = self.reshaper.split_tensor(x)",
            "+        split_list = tf.split(x, len(self.input_lens), axis=1)",
            "for i, distribution in enumerate(self.child_distributions):",
            "# Remove extra categorical dimension",
            "if isinstance(distribution, Categorical):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2154108)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2154109)",
            "Update(target_node=ASTNode(type=identifier, text=reshaper), value='split')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2154110)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=x), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2154111)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=3, insert_id=2154112)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=2154113)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2154114)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=2154115)",
            "Insert(target_node=IN(type=call), node=('identifier', 'len'), position=0, insert_id=2154116)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2154117)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2154118)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=2154119)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=2154120)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input_lens'), position=2, insert_id=2154121)",
            "Delete(target_node=ASTNode(type=identifier, text=split_tensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 218,
        "neg_line": [
            "-split_list = self.reshaper.split_tensor(x)"
        ],
        "pos_line": [
            "+split_list = tf.split(x, len(self.input_lens), axis=1)"
        ],
        "core_change": "-split_list = self.reshaper.split_tensor(x) +split_list = tf.split(x, len(self.input_lens), axis=1)",
        "core_API": "split_tensor"
    },
    {
        "commit_hash": "6407280486749791c4753e755fe3e6d0f19ca345",
        "index": "28fd7aa4..de4e6be2 100644",
        "commit_message": "fixed stochastic policy shapes, naf under construction\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CategoricalOneHotPolicy(StochasticPolicy):",
            "def __init__(self, network, session, state, random, action_count=1, scope='policy'):",
            "with tf.variable_scope(scope):",
            "action_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')",
            "+            action_layer = tf.reshape(action_layer, [-1, action_count])",
            "+",
            "distribution = tf.nn.softmax(action_layer)",
            "sample = tf.multinomial(distribution, 1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2246072)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2246073)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'action_layer'), position=0, insert_id=2246074)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2246075)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2246076)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2246077)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2246078)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2246079)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2246080)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reshape'), position=2, insert_id=2246081)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2246082)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'action_layer'), position=1, insert_id=2246083)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2246084)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=3, insert_id=2246085)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2246086)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2246087)",
            "Insert(target_node=IN(type=list), node=('unary_operator', '-1'), position=1, insert_id=2246088)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=2246089)",
            "Insert(target_node=IN(type=list), node=('identifier', 'action_count'), position=3, insert_id=2246090)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=2246091)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 219,
        "neg_line": [],
        "pos_line": [
            "+action_layer = tf.reshape(action_layer, [-1, action_count])",
            "+"
        ],
        "core_change": "+action_layer = tf.reshape(action_layer, [-1, action_count]) +",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "e746639503c7c81c10f4c789616a21b089e058a9",
        "index": "32709d5d..6a1c1e72 100644",
        "commit_message": "Support FP16 training (#520)\n\n* add fp16 support\n\n* fpn donot need bn normalize\n\n* refactor wrapped bn\n\n* fix bug of retinanet\n\n* add fp16 ssd300 voc, cascade r50, cascade mask r50\n\n* fix bug in cascade rcnn testing\n\n* add support to fix bn training\n\n* add fix bn cfg\n\n* delete fixbn cfg, mv fixbn fp16 to a new branch\n\n* fix cascade mask fp16 bug in test\n\n* fix bug in cascade mask rcnn fp16 test\n\n* add more fp16 cfgs\n\n* add fp16 fast-r50 and faster-dconv-r50\n\n* add fp16 test, minor fix\n\n* clean code\n\n* fix config work_dir name\n\n* add patch func, refactor code\n\n* fix format\n\n* clean code\n\n* move convert rois to single_level_extractor\n\n* fix bug for cascade mask, the seg mask is ndarray\n\n* refactor code, add two decorator force_fp32 and auto_fp16\n\n* add fp16_enable attribute\n\n* add more comment, fix format and test assertion\n\n* fix pep8 format error\n\n* format commont and api\n\n* rename distribute to distributed, fix dict copy\n\n* rename function name\n\n* move function, add comment\n\n* remove unused parameter\n\n* mv decorators into decorators.py, hook related functions to hook\n\n* add auto_fp16 to forward of semantic head\n\n* add auto_fp16 to all heads and fpn\n\n* add docstrings and minor bug fix\n\n* simple refactoring\n\n* bug fix for patching forward method\n\n* roi extractor in fp32 mode\n\n* fix flake8 error\n\n* fix ci error\n\n* add fp16 support to ga head\n\n* remove parallel test assert\n\n* minor fix\n\n* add comment in build_optimizer\n\n* fix typo in comment\n\n* fix typo enable --> enabling\n\n* udpate README\n\n",
        "file": "mmdetection.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class SingleRoIExtractor(nn.Module):",
            "out_size = self.roi_layers[0].out_size",
            "num_levels = len(feats)",
            "target_lvls = self.map_roi_levels(rois, num_levels)",
            "-        roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels,",
            "-                                           out_size, out_size).fill_(0)",
            "+        roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels,",
            "+                                       out_size, out_size)",
            "for i in range(num_levels):",
            "inds = target_lvls == i",
            "if inds.any():"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('subscript', None), position=0, insert_id=1856811)",
            "Update(target_node=ASTNode(type=identifier, text=FloatTensor), value='new_zeros')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='feats')",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1856812)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=1856813)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1856814)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=fill_))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 221,
        "neg_line": [
            "-roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels,",
            "-out_size, out_size).fill_(0)"
        ],
        "pos_line": [
            "+roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels,",
            "+out_size, out_size)"
        ],
        "core_change": "-roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels, -out_size, out_size).fill_(0) +roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels, +out_size, out_size)",
        "core_API": "map_roi_levels"
    },
    {
        "commit_hash": "d1e57166109428f3462e848d18db613a681f429f",
        "index": "aa5cde3..df8ed67 100644",
        "commit_message": "train with multi-gpu half test bug fix #99\n\n",
        "file": "yolov5.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test(data,",
            "else:  # called by train.py",
            "training = True",
            "device = next(model.parameters()).device  # get model device",
            "-        half = device.type != 'cpu'  # half precision only supported on CUDA",
            "+        half = device.type != 'cpu' and torch.cuda.device_count() == 1  # half precision only supported on single-GPU",
            "if half:",
            "model.half()  # to FP16"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('boolean_operator', None), position=9, insert_id=1304236)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1304237)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=1304238)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=1304239)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1304240)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '1'), position=2, insert_id=1304241)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1304242)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1304243)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1304244)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1304245)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device_count'), position=2, insert_id=1304246)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1304247)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1304248)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1304249)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1304250)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1304251)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 222,
        "neg_line": [
            "-half = device.type != 'cpu'  # half precision only supported on CUDA"
        ],
        "pos_line": [
            "+half = device.type != 'cpu' and torch.cuda.device_count() == 1  # half precision only supported on single-GPU"
        ],
        "core_change": "-half = device.type != 'cpu'  # half precision only supported on CUDA +half = device.type != 'cpu' and torch.cuda.device_count() == 1  # half precision only supported on single-GPU",
        "core_API": "parameters"
    },
    {
        "commit_hash": "918a97f3470cb103604061133fda581df4f8256f",
        "index": "5f4b082..712a298 100644",
        "commit_message": "Fix issue with torchvision 0.11.0\n",
        "file": "RobustVideoMatting.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MobileNetV3LargeEncoder(MobileNetV3):",
            ")",
            "",
            "if pretrained:",
            "-            self.load_state_dict(load_state_dict_from_url(",
            "+            self.load_state_dict(torch.hub.load_state_dict_from_url(",
            "'https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth'))",
            "",
            "del self.avgpool"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1130300)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1130301)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1130302)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=load_state_dict_from_url), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1130303)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1130304)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hub'), position=2, insert_id=1130305)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 223,
        "neg_line": [
            "-self.load_state_dict(load_state_dict_from_url("
        ],
        "pos_line": [
            "+self.load_state_dict(torch.hub.load_state_dict_from_url("
        ],
        "core_change": "-self.load_state_dict(load_state_dict_from_url( +self.load_state_dict(torch.hub.load_state_dict_from_url(",
        "core_API": "load_state_dict"
    },
    {
        "commit_hash": "25c50e8e1d7b51e000f8e6b07616e2bb5b7a77f2",
        "index": "00c9c9057..6fb2e3f88 100644",
        "commit_message": "fixed zeros matrix definition\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def make_non_pad_mask(lengths):",
            "\"\"\"",
            "bs = int(len(lengths))",
            "maxlen = int(max(lengths))",
            "-    mask = torch.zeros(bs, maxlen).byte()",
            "+    mask = torch.zeros(bs, maxlen, dtype=torch.uint8)",
            "for i, l in enumerate(lengths):",
            "mask[i, :l] = 1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=179747)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=bs), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=maxlen), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=179748)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=179749)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=6)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=179750)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=179751)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=179752)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=179753)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=byte), value='uint8')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=byte), position=2)",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 224,
        "neg_line": [
            "-mask = torch.zeros(bs, maxlen).byte()"
        ],
        "pos_line": [
            "+mask = torch.zeros(bs, maxlen, dtype=torch.uint8)"
        ],
        "core_change": "-mask = torch.zeros(bs, maxlen).byte() +mask = torch.zeros(bs, maxlen, dtype=torch.uint8)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "cc27ac1a874a1d47d59cc0373ba6c55751f6aa2c",
        "index": "0eefac892..236551d27 100755",
        "commit_message": "Fix BeitForMaskedImageModeling (#13275)\n\n* First pass\n\n* Fix docs of bool_masked_pos\n\n* Add integration script\n\n* Fix docstring\n\n* Add integration test for BeitForMaskedImageModeling\n\n* Remove file\n\n* Fix docs\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BeitForMaskedImageModeling(BeitPreTrainedModel):",
            "",
            "outputs = self.beit(",
            "pixel_values,",
            "+            bool_masked_pos=bool_masked_pos,",
            "head_mask=head_mask,",
            "output_attentions=output_attentions,",
            "output_hidden_states=output_hidden_states,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('keyword_argument', None), position=6, insert_id=1212504)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=7, insert_id=1212505)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'bool_masked_pos'), position=0, insert_id=1212506)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1212507)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'bool_masked_pos'), position=2, insert_id=1212508)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 5,
        "number": 225,
        "neg_line": [],
        "pos_line": [
            "+bool_masked_pos=bool_masked_pos,"
        ],
        "core_change": "+bool_masked_pos=bool_masked_pos,",
        "core_API": "beit"
    },
    {
        "commit_hash": "a807c0149de4678ffa5352bf54cb0e9badbe6349",
        "index": "6d5e824f7..030500725 100644",
        "commit_message": "fix mt task collect_stats at stage 9\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "remove constraint",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Embedding(AbsFrontend):",
            "assert check_argument_types()",
            "super().__init__()",
            "self.embed_dim = embed_dim",
            "-        self.padding = padding",
            "self.embed_scale = 1.0 if no_embed_scale else math.sqrt(embed_dim)",
            "-        self.embed = torch.nn.Embedding(input_size, embed_dim, padding_idx=padding)",
            "+        self.embed = torch.nn.Embedding(input_size, embed_dim)",
            "",
            "def forward(",
            "self, input: torch.Tensor, input_lengths: torch.Tensor"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=padding))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=padding))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=padding_idx))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=padding))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 226,
        "neg_line": [
            "-self.padding = padding",
            "-self.embed = torch.nn.Embedding(input_size, embed_dim, padding_idx=padding)"
        ],
        "pos_line": [
            "+self.embed = torch.nn.Embedding(input_size, embed_dim)"
        ],
        "core_change": "-self.padding = padding -self.embed = torch.nn.Embedding(input_size, embed_dim, padding_idx=padding) +self.embed = torch.nn.Embedding(input_size, embed_dim)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "75c910ab3cf62ad9226fb2cd8715757aa6ce3488",
        "index": "a7411eddf5..f689893c72 100644",
        "commit_message": "Fixed failing tests for astype\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Finfo:",
            "# -------------------#",
            "",
            "",
            "-def astype(x: torch.Tensor, dtype: torch.dtype, *, copy: bool = True) -> torch.Tensor:",
            "+def astype(",
            "+    x: torch.Tensor, dtype: torch.dtype, /, *, copy: bool = True",
            "+) -> torch.Tensor:",
            "dtype = ivy.as_native_dtype(dtype)",
            "if isinstance(dtype, str):",
            "dtype = ivy.as_native_dtype(dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('positional_separator', None), position=5, insert_id=339411)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=339412)",
            "Insert(target_node=IN(type=positional_separator), node=('/', '/'), position=0, insert_id=339413)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 3,
        "number": 227,
        "neg_line": [
            "-def astype(x: torch.Tensor, dtype: torch.dtype, *, copy: bool = True) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def astype(",
            "+x: torch.Tensor, dtype: torch.dtype, /, *, copy: bool = True",
            "+) -> torch.Tensor:"
        ],
        "core_change": "-def astype(x: torch.Tensor, dtype: torch.dtype, *, copy: bool = True) -> torch.Tensor: +def astype( +x: torch.Tensor, dtype: torch.dtype, /, *, copy: bool = True +) -> torch.Tensor:",
        "core_API": "as_native_dtype"
    },
    {
        "commit_hash": "2322eb8e2f9765cb73f59b324cc46a0e9cfe803f",
        "index": "ac11e7ae7..6dd62d270 100644",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFXGLMPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2361645)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2361646)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361647)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2361648)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2361649)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2361650)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2361651)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2361652)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2361653)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2361654)",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 229,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "-\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), -\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), +\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "2e56a4793e2cf9ee39226c1daf1f54bef1005ce7",
        "index": "64e7c29..10d8a07 100644",
        "commit_message": "rename log_softmax, support dim, fix onnx Softmax\n\n",
        "file": "tinygrad.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpeedyResNet:",
            "nn.Linear(512, num_classes, bias=False)",
            "]",
            "",
            "-  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of logsoftmax",
            "-  def __call__(self, x): return x.sequential(self.net).logsoftmax()",
            "+  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of log_softmax",
            "+  def __call__(self, x): return x.sequential(self.net).log_softmax()",
            "",
            "from extra.jit import TinyJit",
            "@TinyJit"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=logsoftmax), value='log_softmax')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 231,
        "neg_line": [
            "-# note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of logsoftmax",
            "-def __call__(self, x): return x.sequential(self.net).logsoftmax()"
        ],
        "pos_line": [
            "+# note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of log_softmax",
            "+def __call__(self, x): return x.sequential(self.net).log_softmax()"
        ],
        "core_change": "-# note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of logsoftmax -def __call__(self, x): return x.sequential(self.net).logsoftmax() +# note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of log_softmax +def __call__(self, x): return x.sequential(self.net).log_softmax()",
        "core_API": "Linear"
    },
    {
        "commit_hash": "a9ce1d4f199adebc8ad5ce5e659951b2c81b3c69",
        "index": "866e1aa4..2f76469a 100644",
        "commit_message": "bug fix for tacotron and tests update\n\n",
        "file": "TTS.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TacotronTrainTest(unittest.TestCase):",
            "optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "for i in range(5):",
            "mel_out, linear_out, align, stop_tokens = model.forward(",
            "-                input, mel_spec)",
            "+                input, input_lengths, mel_spec)",
            "assert stop_tokens.data.max() <= 1.0",
            "assert stop_tokens.data.min() >= 0.0",
            "optimizer.zero_grad()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'input_lengths'), position=3, insert_id=1273306)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1273307)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 233,
        "neg_line": [
            "-input, mel_spec)"
        ],
        "pos_line": [
            "+input, input_lengths, mel_spec)"
        ],
        "core_change": "-input, mel_spec) +input, input_lengths, mel_spec)",
        "core_API": "Adam"
    },
    {
        "commit_hash": "6cf72a9b1ee3125c5ccefcf031401ef8d49d8fe8",
        "index": "33505c81..621b5c17 100644",
        "commit_message": "Fix slow tests (#1210)\n\n* fix tests\n\n* Fix more\n\n* more\n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "remove condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):",
            "",
            "prev_sample = sample + derivative * dt",
            "",
            "-        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")",
            "+        device = model_output.device",
            "if device.type == \"mps\":",
            "# randn does not work reproducibly on mps",
            "noise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=model_output))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"cpu\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=conditional_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 234,
        "neg_line": [
            "-device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")"
        ],
        "pos_line": [
            "+device = model_output.device"
        ],
        "core_change": "-device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\") +device = model_output.device",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "214c84a2359b8021e3d2f170d18e734b78538d18",
        "index": "2b74541..798748d 100644",
        "commit_message": "Disable use of timm nn.Linear wrapper since AMP autocast + torchscript use appears fixed\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def _create_fc(num_features, num_classes, use_conv=False):",
            "elif use_conv:",
            "fc = nn.Conv2d(num_features, num_classes, 1, bias=True)",
            "else:",
            "-        # NOTE: using my Linear wrapper that fixes AMP + torchscript casting issue",
            "-        fc = Linear(num_features, num_classes, bias=True)",
            "+        fc = nn.Linear(num_features, num_classes, bias=True)",
            "return fc"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1477429)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1477430)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1477431)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Linear), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 236,
        "neg_line": [
            "-# NOTE: using my Linear wrapper that fixes AMP + torchscript casting issue",
            "-fc = Linear(num_features, num_classes, bias=True)"
        ],
        "pos_line": [
            "+fc = nn.Linear(num_features, num_classes, bias=True)"
        ],
        "core_change": "-# NOTE: using my Linear wrapper that fixes AMP + torchscript casting issue -fc = Linear(num_features, num_classes, bias=True) +fc = nn.Linear(num_features, num_classes, bias=True)",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "3fe6f3b3eb0c641cf546d9da37928d5afd2570c9",
        "index": "a7a787e28..37765d502 100644",
        "commit_message": "[RLlib] 2 bug fixes: Bandit registration not working if torch not installed. Env checker for MA envs. (#22821)\n\n\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OnlineLinearRegression(nn.Module):",
            "batch_dots = batch_dots.reshape([B, C])",
            "return batch_dots",
            "",
            "-    def forward(self, x, sample_theta=False):",
            "+    def forward(self, x: TensorType, sample_theta: bool = False):",
            "\"\"\"Predict scores on input batch using the underlying linear model.",
            "",
            "Args:",
            "-            x (torch.Tensor): Input feature tensor of shape",
            "-                (batch_size, feature_dim)",
            "-            sample_theta (bool): Whether to sample the weights from its",
            "+            x: Input feature tensor of shape (batch_size, feature_dim)",
            "+            sample_theta: Whether to sample the weights from its",
            "posterior distribution to perform Thompson Sampling as per",
            "http://proceedings.mlr.press/v28/agrawal13.pdf .",
            "\"\"\""
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Predict scores on input batch using the underlying linear model.\n\nArgs:\n            x (torch.Tensor): Input feature tensor of shape\n                (batch_size, feature_dim)\n            sample_theta (bool): Whether to sample the weights from its\nposterior distribution to perform Thompson Sampling as per\nhttp://proceedings.mlr.press/v28/agrawal13.pdf .\n\"\"\"), value='\"\"\"Predict scores on input batch using the underlying linear model.\\n\\nArgs:\\n            x: Input feature tensor of shape (batch_size, feature_dim)\\n            sample_theta: Whether to sample the weights from its\\nposterior distribution to perform Thompson Sampling as per\\nhttp://proceedings.mlr.press/v28/agrawal13.pdf .\\n\"\"\"')",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=3, insert_id=1113496)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=6, insert_id=1113497)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=identifier, text=x), position=0)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=1113498)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=1113499)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=sample_theta), position=0)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=1113500)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=1113501)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type==, text==), position=3)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=false, text=False), position=4)",
            "Insert(target_node=IN(type=type), node=('identifier', 'TensorType'), position=0, insert_id=1113502)",
            "Insert(target_node=IN(type=type), node=('identifier', 'bool'), position=0, insert_id=1113503)",
            "Delete(target_node=ASTNode(type=default_parameter))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 14,
        "number": 237,
        "neg_line": [
            "-def forward(self, x, sample_theta=False):",
            "-x (torch.Tensor): Input feature tensor of shape",
            "-(batch_size, feature_dim)",
            "-sample_theta (bool): Whether to sample the weights from its"
        ],
        "pos_line": [
            "+def forward(self, x: TensorType, sample_theta: bool = False):",
            "+x: Input feature tensor of shape (batch_size, feature_dim)",
            "+sample_theta: Whether to sample the weights from its"
        ],
        "core_change": "-def forward(self, x, sample_theta=False): +def forward(self, x: TensorType, sample_theta: bool = False): -x (torch.Tensor): Input feature tensor of shape -(batch_size, feature_dim) -sample_theta (bool): Whether to sample the weights from its +x: Input feature tensor of shape (batch_size, feature_dim) +sample_theta: Whether to sample the weights from its",
        "core_API": "reshape"
    },
    {
        "commit_hash": "4de68a42a0df89df01a999ee48f361f24c8c19d4",
        "index": "d03fa55a..6ebdc660 100644",
        "commit_message": "Improves API docs and docstring consistency (#4244)\n\n* refactor py2md\n\n* improve py2md, warn if backticks missing\n\n* ensure backticks consistent\n\n* remove docstring help test\n\n* fixes and handle more edge cases\n\n* add failing test for pydoc-markdown bug\n\n* update pydoc-markdown\n\n* fix some links\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceAccuracy(Metric):",
            "A tensor of predictions of shape (batch_size, k, sequence_length).",
            "gold_labels : `torch.Tensor`, required.",
            "A tensor of integer class label of shape (batch_size, sequence_length).",
            "-        mask : `torch.BoolTensor`, optional (default = None).",
            "+        mask : `torch.BoolTensor`, optional (default = `None`).",
            "A masking tensor the same size as `gold_labels`.",
            "\"\"\"",
            "predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', '`None`'), position=2, insert_id=18143)",
            "Delete(target_node=ASTNode(type=none, text=None))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 240,
        "neg_line": [
            "-mask : `torch.BoolTensor`, optional (default = None)."
        ],
        "pos_line": [
            "+mask : `torch.BoolTensor`, optional (default = `None`)."
        ],
        "core_change": "-mask : `torch.BoolTensor`, optional (default = None). +mask : `torch.BoolTensor`, optional (default = `None`).",
        "core_API": "detach_tensors"
    },
    {
        "commit_hash": "fdc2e643c3224b20d000f61ef5360ff1b9fa4629",
        "index": "de99ac3ed..6a045a01b 100644",
        "commit_message": "added cbs to notebooks, made copy-paste error fix in generation_utils (#16246)\n\n\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class GenerationMixin:",
            "continue  # don't waste resources running the code we don't need",
            "",
            "next_token_logits = outputs.logits[:, -1, :]",
            "-",
            "-            # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`",
            "-            # cannot be generated both before and after the `nn.functional.log_softmax` operation.",
            "-            next_token_logits = outputs.logits[:, -1, :]",
            "# hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`",
            "# cannot be generated both before and after the `nn.functional.log_softmax` operation.",
            "next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=next_token_logits))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=outputs))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=logits))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 241,
        "neg_line": [
            "-",
            "-# hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`",
            "-# cannot be generated both before and after the `nn.functional.log_softmax` operation.",
            "-next_token_logits = outputs.logits[:, -1, :]"
        ],
        "pos_line": [],
        "core_change": "- -# hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id` -# cannot be generated both before and after the `nn.functional.log_softmax` operation. -next_token_logits = outputs.logits[:, -1, :]",
        "core_API": "adjust_logits_during_generation"
    },
    {
        "commit_hash": "8182f5168fda52069305749351bc24c0002ff91f",
        "index": "67cd0bf5..e7c57529 100644",
        "commit_message": "Fixup `utils` for the trainer\n\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "remove debug",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def get_commit_hash():",
            "return commit",
            "",
            "",
            "-def create_experiment_folder(root_path, model_name, debug):",
            "+def create_experiment_folder(root_path, model_name):",
            "\"\"\"Create a folder with the current date and time\"\"\"",
            "date_str = datetime.datetime.now().strftime(\"%B-%d-%Y_%I+%M%p\")",
            "-    if debug:",
            "-        commit_hash = \"debug\"",
            "-    else:",
            "-        commit_hash = get_commit_hash()",
            "+    commit_hash = get_commit_hash()",
            "output_folder = os.path.join(root_path, model_name + \"-\" + date_str + \"-\" + commit_hash)",
            "os.makedirs(output_folder, exist_ok=True)",
            "print(\" > Experiment folder: {}\".format(output_folder))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=5)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=debug))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=debug))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=commit_hash))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"debug\"))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=else_clause))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 242,
        "neg_line": [
            "-def create_experiment_folder(root_path, model_name, debug):",
            "-if debug:",
            "-commit_hash = \"debug\"",
            "-else:",
            "-commit_hash = get_commit_hash()"
        ],
        "pos_line": [
            "+def create_experiment_folder(root_path, model_name):",
            "+commit_hash = get_commit_hash()"
        ],
        "core_change": "-def create_experiment_folder(root_path, model_name, debug): +def create_experiment_folder(root_path, model_name): -if debug: -commit_hash = \"debug\" -else: -commit_hash = get_commit_hash() +commit_hash = get_commit_hash()",
        "core_API": "now"
    },
    {
        "commit_hash": "41e6055bf56f045abe72e7442f5b51da1c7bdb1f",
        "index": "ac559bc9..61bdb363 100644",
        "commit_message": "test fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def k_hop_subgraph(node_idx, num_hops, edge_index, relabel_nodes=False,",
            "node_mask[subsets[-1]] = True",
            "torch.index_select(node_mask, 0, row, out=edge_mask)",
            "subsets.append(col[edge_mask])",
            "-    subset = torch.cat(subsets).unique(sorted=False)",
            "+    subset = torch.cat(subsets).unique()",
            "# Add `node_idx` to the beginning of `subset`.",
            "subset = subset[subset != node_idx]",
            "subset = torch.cat([torch.tensor([node_idx], device=row.device), subset])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=sorted))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 243,
        "neg_line": [
            "-subset = torch.cat(subsets).unique(sorted=False)"
        ],
        "pos_line": [
            "+subset = torch.cat(subsets).unique()"
        ],
        "core_change": "-subset = torch.cat(subsets).unique(sorted=False) +subset = torch.cat(subsets).unique()",
        "core_API": "index_select"
    },
    {
        "commit_hash": "c2005786d7a9b6e62897e1a4a8aac9fe2a042e9f",
        "index": "fd7ccb1..2b24bfe 100644",
        "commit_message": "fix slow sd test\n\n",
        "file": "lama-cleaner.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_runway_sd_1_5_negative_prompt(sd_device, strategy, sampler):",
            "def test_cv2(strategy, cv2_flag, cv2_radius):",
            "model = ModelManager(",
            "name=\"cv2\",",
            "-        device=device,",
            "+        device=torch.device(device),",
            ")",
            "cfg = get_config(strategy, cv2_flag=cv2_flag, cv2_radius=cv2_radius)",
            "assert_equal("
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=484058)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=484059)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=484060)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=484061)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=484062)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=device), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=484063)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=484064)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=484065)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 244,
        "neg_line": [
            "-device=device,"
        ],
        "pos_line": [
            "+device=torch.device(device),"
        ],
        "core_change": "-device=device, +device=torch.device(device),",
        "core_API": "device"
    },
    {
        "commit_hash": "3e9f164d02a0b07bb5fcf11188a1f28b90ed2a9e",
        "index": "fa790030..df7c39a0 100644",
        "commit_message": "Upgrade shufflenet; fix paramsetter for restore\n\n",
        "file": "tensorpack.txt.json",
        "label": "no",
        "comments": "parameterize the parameter",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "summary.add_moving_summary(self.cost)",
            "",
            "def _get_optimizer(self):",
            "-        lr = tf.get_variable('learning_rate', initializer=1e-3, trainable=False)",
            "+        lr = tf.get_variable('learning_rate', initializer=self.learning_rate, trainable=False)",
            "opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)",
            "return optimizer.apply_grad_processors(",
            "opt, [gradproc.GlobalNormClip(10), gradproc.SummaryGradient()])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=2289355)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2289356)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2289357)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'learning_rate'), position=2, insert_id=2289358)",
            "Delete(target_node=ASTNode(type=float, text=1e-3))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 245,
        "neg_line": [
            "-lr = tf.get_variable('learning_rate', initializer=1e-3, trainable=False)"
        ],
        "pos_line": [
            "+lr = tf.get_variable('learning_rate', initializer=self.learning_rate, trainable=False)"
        ],
        "core_change": "-lr = tf.get_variable('learning_rate', initializer=1e-3, trainable=False) +lr = tf.get_variable('learning_rate', initializer=self.learning_rate, trainable=False)",
        "core_API": "add_moving_summary"
    },
    {
        "commit_hash": "93af1b57f093b610bac08dc54f8c164f79fcd2e5",
        "index": "2e71f9c..82406e6 100644",
        "commit_message": "Fixed naming of the fully connected layer\n",
        "file": "facenet.txt.json",
        "label": "no",
        "comments": "change name",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Network(object):",
            "weights = self.make_var('weights', shape=[dim, num_out])",
            "biases = self.make_var('biases', [num_out])",
            "op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b",
            "-            fc = op(feed_in, weights, biases, name=scope.name)",
            "+            #fc = op(feed_in, weights, biases, name=scope.name)",
            "+            fc = op(feed_in, weights, biases, name=name)",
            "return fc"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=identifier, text=name), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=scope))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 246,
        "neg_line": [
            "-fc = op(feed_in, weights, biases, name=scope.name)"
        ],
        "pos_line": [
            "+#fc = op(feed_in, weights, biases, name=scope.name)",
            "+fc = op(feed_in, weights, biases, name=name)"
        ],
        "core_change": "-fc = op(feed_in, weights, biases, name=scope.name) +#fc = op(feed_in, weights, biases, name=scope.name) +fc = op(feed_in, weights, biases, name=name)",
        "core_API": "make_var"
    },
    {
        "commit_hash": "c45342e39d66b45ed5faa11b7679b8d1b8b633d4",
        "index": "e27005c5dc..720da386b1 100644",
        "commit_message": "Updated code to mesh with get_weights returning a dict and new tf code (#187)\n\n* Updated code to mesh with get_weights returning a dict and new tf code\n\n* Added tf.global_variables_initalizer to hyperopt example as well\n\n* Small fix.\n\n* Small name change.\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def train_cnn_and_compute_accuracy(params, steps, train_images, train_labels, va",
            "# Do the training and evaluation.",
            "with tf.Session() as sess:",
            "# Initialize the network weights.",
            "-    sess.run(tf.initialize_all_variables())",
            "+    sess.run(tf.global_variables_initializer())",
            "for i in range(1, steps + 1):",
            "# Fetch the next batch of data.",
            "image_batch = get_batch(train_images, i, batch_size)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=initialize_all_variables), value='global_variables_initializer')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 250,
        "neg_line": [
            "-sess.run(tf.initialize_all_variables())"
        ],
        "pos_line": [
            "+sess.run(tf.global_variables_initializer())"
        ],
        "core_change": "-sess.run(tf.initialize_all_variables()) +sess.run(tf.global_variables_initializer())",
        "core_API": "Session"
    },
    {
        "commit_hash": "d1dd5d578e0cbdee651eeb125dcb78e36b9cf322",
        "index": "c91874b03..aef230c4b 100644",
        "commit_message": "[RLlib] Fix PyTorch A3C / A2C loss function using mixed reduced sum / mean (#11449)\n\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def actor_critic_loss(policy, model, dist_class, train_batch):",
            "values = model.value_function()",
            "dist = dist_class(logits, model)",
            "log_probs = dist.logp(train_batch[SampleBatch.ACTIONS])",
            "-    policy.entropy = dist.entropy().mean()",
            "+    policy.entropy = dist.entropy().sum()",
            "policy.pi_err = -train_batch[Postprocessing.ADVANTAGES].dot(",
            "log_probs.reshape(-1))",
            "-    policy.value_err = nn.functional.mse_loss(",
            "-        values.reshape(-1), train_batch[Postprocessing.VALUE_TARGETS])",
            "+    policy.value_err = torch.sum(",
            "+        torch.pow(",
            "+            values.reshape(-1) - train_batch[Postprocessing.VALUE_TARGETS],",
            "+            2.0))",
            "overall_err = sum([",
            "policy.pi_err,",
            "policy.config[\"vf_loss_coeff\"] * policy.value_err,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1120847)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1120848)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1120849)",
            "Update(target_node=ASTNode(type=identifier, text=mean), value='sum')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1120850)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1120851)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sum'), position=2, insert_id=1120852)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1120853)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1120854)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=nn), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=functional), value='pow')",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1120855)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1120856)",
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '2.0'), position=3, insert_id=1120857)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1120858)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=subscript), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mse_loss))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 251,
        "neg_line": [
            "-policy.entropy = dist.entropy().mean()",
            "-policy.value_err = nn.functional.mse_loss(",
            "-values.reshape(-1), train_batch[Postprocessing.VALUE_TARGETS])"
        ],
        "pos_line": [
            "+policy.entropy = dist.entropy().sum()",
            "+policy.value_err = torch.sum(",
            "+torch.pow(",
            "+values.reshape(-1) - train_batch[Postprocessing.VALUE_TARGETS],",
            "+2.0))"
        ],
        "core_change": "-policy.entropy = dist.entropy().mean() +policy.entropy = dist.entropy().sum() -policy.value_err = nn.functional.mse_loss( -values.reshape(-1), train_batch[Postprocessing.VALUE_TARGETS]) +policy.value_err = torch.sum( +torch.pow( +values.reshape(-1) - train_batch[Postprocessing.VALUE_TARGETS], +2.0))",
        "core_API": "value_function"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "8c5b149e..7719eef0 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class DeepSpeedDataLoader(object):",
            "else:",
            "if data_sampler is None:",
            "data_sampler = RandomSampler(dataset)",
            "-                device_count = torch.cuda.device_count()",
            "+                device_count = get_accelerator().device_count()",
            "batch_size *= device_count",
            "",
            "if num_local_io_workers is None:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1816318)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_accelerator')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1816319)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816320)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816321)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 252,
        "neg_line": [
            "-device_count = torch.cuda.device_count()"
        ],
        "pos_line": [
            "+device_count = get_accelerator().device_count()"
        ],
        "core_change": "-device_count = torch.cuda.device_count() +device_count = get_accelerator().device_count()",
        "core_API": "device_count"
    },
    {
        "commit_hash": "cbed500fe6f28b80b30ef8aebdef41198268f289",
        "index": "9df42ac..dd6f4d2 100644",
        "commit_message": "Fixed typo in test\n\n",
        "file": "autokeras.txt.json",
        "label": "no",
        "comments": "typo fix",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class CategoricalToNumerical(preprocessor.Preprocessor):",
            "\"column_names\": config[\"column_names\"],",
            "}",
            "obj = cls(**init_config)",
            "-        obj.layer = keras_layers.MultiCategoryEncoding(config[\"encoding\"])",
            "-        obj.layer.build(None)",
            "+        obj.layer = preprocessors.deserialize(config[\"layer\"])",
            "for encoding_layer, vocab in zip(",
            "obj.layer.encoding_layers, config[\"encoding_vocab\"]",
            "):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=keras_layers), value='preprocessors')",
            "Update(target_node=ASTNode(type=identifier, text=MultiCategoryEncoding), value='deserialize')",
            "Update(target_node=ASTNode(type=string, text=\"encoding\"), value='\"layer\"')",
            "Delete(target_node=ASTNode(type=identifier, text=obj))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=layer))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=build))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 253,
        "neg_line": [
            "-obj.layer = keras_layers.MultiCategoryEncoding(config[\"encoding\"])",
            "-obj.layer.build(None)"
        ],
        "pos_line": [
            "+obj.layer = preprocessors.deserialize(config[\"layer\"])"
        ],
        "core_change": "-obj.layer = keras_layers.MultiCategoryEncoding(config[\"encoding\"]) -obj.layer.build(None) +obj.layer = preprocessors.deserialize(config[\"layer\"])",
        "core_API": "MultiCategoryEncoding"
    },
    {
        "commit_hash": "c317aaa3de289fc0a69e6b1001ffc586bb4d9c9a",
        "index": "28555d2f..74249bd4 100644",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "use customized API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def transform_bbox(trans_mat: torch.Tensor, boxes: torch.Tensor, mode: str = \"xy",
            "boxes[..., -2] = boxes[..., 0] + boxes[..., -2]  # x + w",
            "boxes[..., -1] = boxes[..., 1] + boxes[..., -1]  # y + h",
            "",
            "-    transformed_boxes: torch.Tensor = kornia.transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2))",
            "+    transformed_boxes: torch.Tensor = transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2))",
            "transformed_boxes = transformed_boxes.view_as(boxes)",
            "",
            "if mode == 'xywh':"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=transform_points), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=kornia))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 254,
        "neg_line": [
            "-transformed_boxes: torch.Tensor = kornia.transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2))"
        ],
        "pos_line": [
            "+transformed_boxes: torch.Tensor = transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2))"
        ],
        "core_change": "-transformed_boxes: torch.Tensor = kornia.transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2)) +transformed_boxes: torch.Tensor = transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2))",
        "core_API": "transform_points"
    },
    {
        "commit_hash": "41c99fbf385a8c875fb6181ce7301e4bc218535b",
        "index": "556cfb3..7547d10 100644",
        "commit_message": "Use preprocessing layers for categorical encoding (#1090)\n\n* removed sigmoid layer\n\n* added lookup\n\n* bug fix\n",
        "file": "autokeras.txt.json",
        "label": "no",
        "comments": "print fix",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def test_feature_encoder_layer():",
            "",
            "model2 = tf.keras.Model(input_node, hidden_node)",
            "result = model2.predict(data)",
            "-    print(result)",
            "+    model2.predict(data2)",
            "assert result[0][0] == result[2][0]",
            "assert result[0][0] != result[1][0]",
            "assert result[0][1] != result[1][1]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2472151)",
            "Update(target_node=ASTNode(type=identifier, text=print), value='model2')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=print), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2472152)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'predict'), position=2, insert_id=2472153)",
            "Update(target_node=ASTNode(type=identifier, text=result), value='data2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 255,
        "neg_line": [
            "-print(result)"
        ],
        "pos_line": [
            "+model2.predict(data2)"
        ],
        "core_change": "-print(result) +model2.predict(data2)",
        "core_API": "Model"
    },
    {
        "commit_hash": "e9abf07ec3cb77f86cf8817aedbff4f5bfb2cc05",
        "index": "d61e0f6a40..c794467657 100644",
        "commit_message": "fix tensorflow asinh failure\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def asin(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:",
            "return tf.asin(x)",
            "",
            "",
            "-def asinh(",
            "-        x: Union[tf.Tensor, tf.Variable]",
            "-) -> Union[tf.Tensor, tf.Variable]:",
            "-    x = tf.cast(x, tf.float32)",
            "+def asinh(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:",
            "return tf.asinh(x)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=2006615)",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cast))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 257,
        "neg_line": [
            "-def asinh(",
            "-x: Union[tf.Tensor, tf.Variable]",
            "-) -> Union[tf.Tensor, tf.Variable]:",
            "-x = tf.cast(x, tf.float32)"
        ],
        "pos_line": [
            "+def asinh(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:"
        ],
        "core_change": "-def asinh( -x: Union[tf.Tensor, tf.Variable] -) -> Union[tf.Tensor, tf.Variable]: -x = tf.cast(x, tf.float32) +def asinh(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:",
        "core_API": "asin"
    },
    {
        "commit_hash": "e9014fb4245d8408ff763a4f67b8a2365c8b6f3e",
        "index": "34af8f05..639f892b 100644",
        "commit_message": "Fix typing issue in meters when resuming FP16 training (#1132)\n\nSummary:\nWhen we save checkpoints, we move all CUDA tensors to CPU. This includes meter values (e.g., grad norm). Upon reloading the checkpoint, these meter values remain on the CPU, but subsequent meter values are likely to be on GPU, thus raising an exception (PyTorch doesn't support operations between CPU and CUDA tensors). In the case of FP16 training, you get a slightly different exception due to trying to add float16 tensors on CPU, but it's the same underlying cause.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1132\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D20850925\n\nPulled By: myleott\n\nfbshipit-source-id: df12b051f2eae3566a1f4cd1b621ed1c8fdf0050\n\n",
        "file": "fairseq.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StopwatchMeter(Meter):",
            "if self.start_time is not None:",
            "delta = time.perf_counter() - self.start_time",
            "self.sum = self.sum + delta",
            "-            self.n = self.n + n",
            "+            self.n = type_as(self.n, n) + n",
            "",
            "def reset(self):",
            "self.sum = 0  # cumulative time during which stopwatch was active"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=214389)",
            "Insert(target_node=IN(type=call), node=('identifier', 'type_as'), position=0, insert_id=214390)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=214391)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=214392)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=214393)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'n'), position=3, insert_id=214394)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=214395)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 258,
        "neg_line": [
            "-self.n = self.n + n"
        ],
        "pos_line": [
            "+self.n = type_as(self.n, n) + n"
        ],
        "core_change": "-self.n = self.n + n +self.n = type_as(self.n, n) + n",
        "core_API": "perf_counter"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "02c4a2a28..183a45437 100644",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RagTokenForGeneration(RagPreTrainedModel):",
            "n_docs = n_docs if n_docs is not None else self.config.n_docs",
            "",
            "# RAG-token marginalization",
            "-        seq_logprobs = torch.nn.functional.log_softmax(seq_logits, dim=-1).view(",
            "+        seq_logprobs = nn.functional.log_softmax(seq_logits, dim=-1).view(",
            "seq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.size(-1)",
            ")",
            "doc_logprobs = torch.log_softmax(doc_scores, dim=1)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 259,
        "neg_line": [
            "-seq_logprobs = torch.nn.functional.log_softmax(seq_logits, dim=-1).view("
        ],
        "pos_line": [
            "+seq_logprobs = nn.functional.log_softmax(seq_logits, dim=-1).view("
        ],
        "core_change": "-seq_logprobs = torch.nn.functional.log_softmax(seq_logits, dim=-1).view( +seq_logprobs = nn.functional.log_softmax(seq_logits, dim=-1).view(",
        "core_API": "log_softmax"
    },
    {
        "commit_hash": "b95ec190af7794f40562dff1082b59c4b4590590",
        "index": "37a6d7d..3b919b8 100644",
        "commit_message": "followups to D37592429\n\nSummary: Fixing comments on D37592429 (https://github.com/facebookresearch/pytorch3d/commit/0dce883241ae638b9fa824f34fca9590d5f0782c)\n\nReviewed By: shapovalov\n\nDifferential Revision: D37752367\n\nfbshipit-source-id: 40aa7ee4dc0c5b8b7a84a09d13a3933a9e3afedd\n\n",
        "file": "pytorch3d.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HarmonicTimeEncoder(GlobalEncoderBase, torch.nn.Module):",
            "time = frame_timestamp / self.time_divisor",
            "return self._harmonic_embedding(time)  # pyre-ignore: 29",
            "",
            "-    def calc_squared_encoding_norm(self):",
            "-        return 0.0",
            "+    def calculate_squared_encoding_norm(self) -> Optional[torch.Tensor]:",
            "+        return None"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=calc_squared_encoding_norm), value='calculate_squared_encoding_norm')",
            "Insert(target_node=ASTNode(type=function_definition), node=('->', '->'), position=3, insert_id=910426)",
            "Insert(target_node=ASTNode(type=function_definition), node=('type', None), position=4, insert_id=910427)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=910428)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Optional'), position=0, insert_id=910429)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=910430)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=910431)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=910432)",
            "Insert(target_node=ASTNode(type=return_statement), node=('none', 'None'), position=1, insert_id=910433)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=910434)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=910435)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=910436)",
            "Delete(target_node=ASTNode(type=float, text=0.0))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 261,
        "neg_line": [
            "-def calc_squared_encoding_norm(self):",
            "-return 0.0"
        ],
        "pos_line": [
            "+def calculate_squared_encoding_norm(self) -> Optional[torch.Tensor]:",
            "+return None"
        ],
        "core_change": "-def calc_squared_encoding_norm(self): -return 0.0 +def calculate_squared_encoding_norm(self) -> Optional[torch.Tensor]: +return None",
        "core_API": "_harmonic_embedding"
    },
    {
        "commit_hash": "1beddcdfb78a89eb44898882b8ecf5f91e104542",
        "index": "646f9bd0..bfd6cad8 100644",
        "commit_message": "a few more doc fixes (#4078)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ElmoLstm(_EncoderBase):",
            "",
            "# Returns",
            "",
            "-        A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size),",
            "-        where the num_layers dimension represents the LSTM output from that layer.",
            "+        `torch.Tensor`",
            "+            A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size),",
            "+            where the num_layers dimension represents the LSTM output from that layer.",
            "\"\"\"",
            "batch_size, total_sequence_length = mask.size()",
            "stacked_sequence_output, final_states, restoration_indices = self.sort_and_run_forward("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=18447)",
            "Move(target_node=ASTNode(type=block), node=ASTNode(type=ERROR), position=1)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '`torch.Tensor`'), position=0, insert_id=18448)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 263,
        "neg_line": [
            "-A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size),",
            "-where the num_layers dimension represents the LSTM output from that layer."
        ],
        "pos_line": [
            "+`torch.Tensor`",
            "+A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size),",
            "+where the num_layers dimension represents the LSTM output from that layer."
        ],
        "core_change": "-A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size), -where the num_layers dimension represents the LSTM output from that layer. +`torch.Tensor` +A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size), +where the num_layers dimension represents the LSTM output from that layer.",
        "core_API": "size"
    },
    {
        "commit_hash": "da79ae9501960c4f895e7732d0ae7129703998b0",
        "index": "c4a4c957c..d148e6775 100644",
        "commit_message": "[train][docs] update docstrings/quickstarts to work when `use_gpu=True` (#31692)\n\nFixes Trainer docstrings and quickstarts to work when use_gpu=True.\n\nSigned-off-by: Matthew Deng <matt@anyscale.com>\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HorovodTrainer(DataParallelTrainer):",
            "),",
            ")",
            "train_dataset = ray.data.from_items([{\"x\": x, \"y\": x + 1} for x in range(32)])",
            "-        scaling_config = ScalingConfig(num_workers=3)",
            "-        # If using GPUs, use the below scaling config instead.",
            "-        # scaling_config = ScalingConfig(num_workers=3, use_gpu=True)",
            "+        scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)",
            "trainer = HorovodTrainer(",
            "train_loop_per_worker=train_loop_per_worker,",
            "scaling_config=scaling_config,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1103947)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1103948)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'use_gpu'), position=0, insert_id=1103949)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1103950)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'use_gpu'), position=2, insert_id=1103951)"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 5,
        "number": 265,
        "neg_line": [
            "-scaling_config = ScalingConfig(num_workers=3)",
            "-# If using GPUs, use the below scaling config instead.",
            "-# scaling_config = ScalingConfig(num_workers=3, use_gpu=True)"
        ],
        "pos_line": [
            "+scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)"
        ],
        "core_change": "-scaling_config = ScalingConfig(num_workers=3) -# If using GPUs, use the below scaling config instead. -# scaling_config = ScalingConfig(num_workers=3, use_gpu=True) +scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)",
        "core_API": "from_items"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "0b65e197..0bd44f56 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "remove API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestStackedSelfAttention(AllenNlpTestCase):",
            "feedforward_hidden_dim=5,",
            "num_layers=3,",
            "num_attention_heads=3)",
            "-        inputs = Variable(torch.randn([3, 5, 9]))",
            "+        inputs = torch.randn([3, 5, 9])",
            "encoder_output = encoder(inputs, None)",
            "assert list(encoder_output.size()) == [3, 5, 12]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 266,
        "neg_line": [
            "-inputs = Variable(torch.randn([3, 5, 9]))"
        ],
        "pos_line": [
            "+inputs = torch.randn([3, 5, 9])"
        ],
        "core_change": "-inputs = Variable(torch.randn([3, 5, 9])) +inputs = torch.randn([3, 5, 9])",
        "core_API": "randn"
    },
    {
        "commit_hash": "5ecce832170355f068c8e6e5b655513807e2f338",
        "index": "f2edde0..5f25a18 100644",
        "commit_message": "PyTorch 1.4 compat\n\nSummary: Restore compatibility with PyTorch 1.4 and 1.5, and a few lint fixes.\n\nReviewed By: patricklabatut\n\nDifferential Revision: D30048115\n\nfbshipit-source-id: ee05efa7c625f6079fb06a3cc23be93e48df9433\n\n",
        "file": "pytorch3d.txt.json",
        "label": "yes",
        "comments": "change condition check for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def qr(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:  # pragma: no cove",
            "\"\"\"",
            "Like torch.linalg.qr.",
            "\"\"\"",
            "-    if hasattr(torch.linalg, \"qr\"):",
            "+    if hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\"):",
            "# PyTorch version >= 1.9",
            "return torch.linalg.qr(A)",
            "return torch.qr(A)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=918275)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=918276)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=918277)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'hasattr'), position=0, insert_id=918278)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=918279)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=918280)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'torch'), position=1, insert_id=918281)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=918282)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"linalg\"'), position=3, insert_id=918283)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=918284)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 267,
        "neg_line": [
            "-if hasattr(torch.linalg, \"qr\"):"
        ],
        "pos_line": [
            "+if hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\"):"
        ],
        "core_change": "-if hasattr(torch.linalg, \"qr\"): +if hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\"):",
        "core_API": "qr"
    },
    {
        "commit_hash": "12c1b5b8f448d652f5e1fa0f069b9569f4540948",
        "index": "7bf7bde03..363122147 100644",
        "commit_message": "fix test (#9669)\n\n\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def prepare_bart_inputs_dict(",
            "if decoder_attention_mask is None:",
            "decoder_attention_mask = decoder_input_ids.ne(config.pad_token_id)",
            "if head_mask is None:",
            "-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads)",
            "+        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)",
            "if decoder_head_mask is None:",
            "-        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads)",
            "+        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)",
            "return {",
            "\"input_ids\": input_ids,",
            "\"decoder_input_ids\": decoder_input_ids,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1224229)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1224230)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1224231)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ones'), position=2, insert_id=1224232)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1224233)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1224234)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1224235)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1224236)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1224237)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1224238)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_device'), position=2, insert_id=1224239)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1224240)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1224241)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_device'), position=2, insert_id=1224242)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ones))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 268,
        "neg_line": [
            "-head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads)",
            "-decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads)"
        ],
        "pos_line": [
            "+head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)",
            "+decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)"
        ],
        "core_change": "-head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads) +head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device) -decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads) +decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)",
        "core_API": "ne"
    },
    {
        "commit_hash": "726aba089d12503249d824bbaf4070f47d0fe44d",
        "index": "bee36c39..4e968aef 100755",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "no",
        "comments": "assertion test",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PNDMSchedulerTest(SchedulerCommonTest):",
            "scheduler_config = self.get_scheduler_config(steps_offset=1)",
            "scheduler = scheduler_class(**scheduler_config)",
            "scheduler.set_timesteps(10)",
            "-        assert np.equal(",
            "+        assert torch.equal(",
            "scheduler.timesteps,",
            "-            np.array([901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]),",
            "-        ).all()",
            "+            torch.LongTensor(",
            "+                [901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]",
            "+            ),",
            "+        )",
            "",
            "def test_betas(self):",
            "for beta_start, beta_end in zip([0.0001, 0.001], [0.002, 0.02]):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=np), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=np), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=array), value='LongTensor')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=all))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 9,
        "number": 269,
        "neg_line": [
            "-assert np.equal(",
            "-np.array([901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]),",
            "-).all()"
        ],
        "pos_line": [
            "+assert torch.equal(",
            "+torch.LongTensor(",
            "+[901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]",
            "+),",
            "+)"
        ],
        "core_change": "-assert np.equal( +assert torch.equal( -np.array([901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]), -).all() +torch.LongTensor( +[901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1] +), +)",
        "core_API": "get_scheduler_config"
    },
    {
        "commit_hash": "bf3eaa9264e579e9cc84a641012154fcd803c06c",
        "index": "0f35f58e8..a509800ed 100644",
        "commit_message": "[RLlib] Dreamer fixes and reinstate Dreamer test. (#17821)\n\nCo-authored-by: sven1977 <svenmika1977@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add param for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DreamerModel(TorchModelV2, nn.Module):",
            "and policy to obtain action.",
            "\"\"\"",
            "if state is None:",
            "-            self.initial_state()",
            "+            self.state = self.get_initial_state(batch_size=obs.shape[0])",
            "else:",
            "self.state = state",
            "post = self.state[:4]"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=1116093)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1116094)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=initial_state), value='state')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1116095)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1116096)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1116097)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_initial_state'), position=2, insert_id=1116098)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1116099)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'batch_size'), position=0, insert_id=1116100)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1116101)",
            "Insert(target_node=IN(type=keyword_argument), node=('subscript', None), position=2, insert_id=1116102)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=1116103)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1116104)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=1116105)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1116106)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'obs'), position=0, insert_id=1116107)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1116108)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=1116109)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 270,
        "neg_line": [
            "-self.initial_state()"
        ],
        "pos_line": [
            "+self.state = self.get_initial_state(batch_size=obs.shape[0])"
        ],
        "core_change": "-self.initial_state() +self.state = self.get_initial_state(batch_size=obs.shape[0])",
        "core_API": "initial_state"
    },
    {
        "commit_hash": "7e4e016eb61c56f364c29e0a8138506e979ddd47",
        "index": "0a19844e8..cbeb03476 100644",
        "commit_message": "multiple bug fixes\n\n",
        "file": "PySyft.txt.json",
        "label": "yes",
        "comments": "add condition check for null fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Plan(Serializable):",
            "# prevent circular dependency",
            "# syft relative",
            "from ...core.node.vm.vm import VirtualMachine  # noqa: F401",
            "+        if self.local_executor is not None:",
            "+            # this is necessary for syfts nn.module, because the plan contains state from the module",
            "+            # in order to use this state, we first need to send the model, and then execute te plan",
            "+            return self.local_executor(**kwargs)",
            "",
            "alice = VirtualMachine(name=\"plan_executor\")",
            "alice_client: client.Client = alice.get_client()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=1447445)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1447446)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1447447)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1447448)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1447449)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1447450)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1447451)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1447452)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1447453)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=1447454)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1447455)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1447456)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'local_executor'), position=2, insert_id=1447457)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1447458)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=1447459)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1447460)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1447461)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1447462)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1447463)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'local_executor'), position=2, insert_id=1447464)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1447465)",
            "Insert(target_node=IN(type=argument_list), node=('dictionary_splat', None), position=1, insert_id=1447466)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1447467)",
            "Insert(target_node=IN(type=dictionary_splat), node=('**', '**'), position=0, insert_id=1447468)",
            "Insert(target_node=IN(type=dictionary_splat), node=('identifier', 'kwargs'), position=1, insert_id=1447469)"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 271,
        "neg_line": [],
        "pos_line": [
            "+if self.local_executor is not None:",
            "+# this is necessary for syfts nn.module, because the plan contains state from the module",
            "+# in order to use this state, we first need to send the model, and then execute te plan",
            "+return self.local_executor(**kwargs)"
        ],
        "core_change": "+if self.local_executor is not None: +# this is necessary for syfts nn.module, because the plan contains state from the module +# in order to use this state, we first need to send the model, and then execute te plan +return self.local_executor(**kwargs)",
        "core_API": "local_executor"
    },
    {
        "commit_hash": "48269070d23ad8a4c6f31bc6847c358aac182ad1",
        "index": "42011249..51c861a2 100644",
        "commit_message": "more fixes\n\n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GradTTS(DiffusionPipeline):",
            "mu_y = mu_y.transpose(1, 2)",
            "",
            "# Sample latent representation from terminal distribution N(mu_y, I)",
            "-        z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature",
            "+        z = mu_y + torch.randn(mu_y.shape, device=mu_y.device, generator=generator) / temperature",
            "",
            "xt = z * y_mask",
            "h = 1.0 / num_inference_steps"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=randn_like), value='randn')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=112480)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=112481)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=6, insert_id=112482)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=mu_y), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=112483)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=112484)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'generator'), position=0, insert_id=112485)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=112486)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'generator'), position=2, insert_id=112487)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 272,
        "neg_line": [
            "-z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature"
        ],
        "pos_line": [
            "+z = mu_y + torch.randn(mu_y.shape, device=mu_y.device, generator=generator) / temperature"
        ],
        "core_change": "-z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature +z = mu_y + torch.randn(mu_y.shape, device=mu_y.device, generator=generator) / temperature",
        "core_API": "transpose"
    },
    {
        "commit_hash": "995c204337d16a6146a433cee360e5a5bfbc9a6f",
        "index": "faa8031d..7d46d766 100644",
        "commit_message": "Data2vec prelim (#2929)\n\nSummary:\nPreliminaries for data2vec release, include some minor improvements and bug fixes\n\nMost important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929\n\nReviewed By: wnhsu\n\nDifferential Revision: D33649708\n\nPulled By: alexeib\n\nfbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb\n\n",
        "file": "fairseq.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NanDetector:",
            "gradients = {}",
            "for name, param in self.named_parameters:",
            "if param.grad is not None:",
            "-                grad_norm = torch.norm(param.grad.data, p=2, dtype=torch.float32)",
            "+                grad_norm = torch.norm(param.grad.data.float(), p=2)",
            "norm[name] = grad_norm.item()",
            "if torch.isnan(grad_norm).any() or torch.isinf(grad_norm).any():",
            "gradients[name] = param.grad.data"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=argument_list), position=3)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=204324)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=204325)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=204326)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=204327)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=204328)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=204329)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=204330)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 273,
        "neg_line": [
            "-grad_norm = torch.norm(param.grad.data, p=2, dtype=torch.float32)"
        ],
        "pos_line": [
            "+grad_norm = torch.norm(param.grad.data.float(), p=2)"
        ],
        "core_change": "-grad_norm = torch.norm(param.grad.data, p=2, dtype=torch.float32) +grad_norm = torch.norm(param.grad.data.float(), p=2)",
        "core_API": "norm"
    },
    {
        "commit_hash": "4d2056722aabbfe138d16abf88ed77b471b058ea",
        "index": "2e36c80..21c0313 100644",
        "commit_message": "Mixup and prefetcher improvements\n* Do mixup in custom collate fn if prefetcher enabled, reduces performance impact\n* Move mixup code to own file\n* Add arg to disable prefetcher\n* Fix no cuda transfer when prefetcher off\n* Random erasing when prefetcher off wasn't changed to match new args, fixed\n* Default random erasing to off (prob = 0.) for train\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "yes",
        "comments": "add condition check for null fix",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "def create_loader(",
            "# of samples per-process, will slightly alter validation results",
            "sampler = OrderedDistributedSampler(dataset)",
            "",
            "+    if collate_fn is None:",
            "+        collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate",
            "+",
            "loader = torch.utils.data.DataLoader(",
            "dataset,",
            "batch_size=batch_size,",
            "shuffle=sampler is None and is_training,",
            "num_workers=num_workers,",
            "sampler=sampler,",
            "-        collate_fn=fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate,",
            "+        collate_fn=collate_fn,",
            "drop_last=is_training,",
            ")",
            "if use_prefetcher:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=call), position=5)",
            "Insert(target_node=ASTNode(type=ERROR), node=('if', 'if'), position=6, insert_id=1772240)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=ERROR), position=7)",
            "Insert(target_node=ASTNode(type=ERROR), node=('comparison_operator', None), position=0, insert_id=1772241)",
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=1, insert_id=1772242)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'collate_fn'), position=2, insert_id=1772243)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=conditional_expression), position=5)",
            "Insert(target_node=ASTNode(type=ERROR), node=('=', '='), position=6, insert_id=1772244)",
            "Update(target_node=ASTNode(type=identifier, text=loader), value='collate_fn')",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=identifier, text=loader), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=1772245)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=1772246)",
            "Insert(target_node=ASTNode(type=conditional_expression), node=('ERROR', None), position=4, insert_id=1772247)",
            "Insert(target_node=ASTNode(type=conditional_expression), node=('identifier', 'loader'), position=5, insert_id=1772248)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'collate_fn'), position=2, insert_id=1772249)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 274,
        "neg_line": [
            "-collate_fn=fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate,"
        ],
        "pos_line": [
            "+if collate_fn is None:",
            "+collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate",
            "+",
            "+collate_fn=collate_fn,"
        ],
        "core_change": "+if collate_fn is None: +collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate + -collate_fn=fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate, +collate_fn=collate_fn,",
        "core_API": "DataLoader"
    },
    {
        "commit_hash": "c74f40cbe86047da9723cf4d68f6baddec4a3f6f",
        "index": "d6b8e6d7..508beb7b 100644",
        "commit_message": "Fix build test. (#1057)\n\n* 2.2.1 release\n\n* Fix build test.\n\n* apply yapf\n\n* ping yapf to 0.28.0\n\n* fix build\n\n* use yapf 0.29\n\n* fix yapf\n\n* include tests in make format.\n\n* ping autoflake and isort version.\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def set_gpu_fraction(gpu_fraction=0.3):",
            "",
            "",
            "def train_epoch(",
            "-        network, X, y, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None, batch_size=100, shuffle=True",
            "+    network, X, y, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None, batch_size=100, shuffle=True",
            "):",
            "\"\"\"Training a given non time-series network by the given cost function, training data, batch_size etc.",
            "for one epoch."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 276,
        "neg_line": [
            "-network, X, y, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None, batch_size=100, shuffle=True"
        ],
        "pos_line": [
            "+network, X, y, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None, batch_size=100, shuffle=True"
        ],
        "core_change": "-network, X, y, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None, batch_size=100, shuffle=True +network, X, y, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None, batch_size=100, shuffle=True",
        "core_API": "Adam"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "c821cb81..3c7b0c47 100644",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "no",
        "comments": "log update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def fpn_map_rois_to_levels(boxes):",
            "Be careful that the returned tensor could be empty.",
            "\"\"\"",
            "sqrtarea = tf.sqrt(tf_area(boxes))",
            "-    level = tf.to_int32(tf.floor(",
            "-        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))))",
            "+    level = tf.cast(tf.floor(",
            "+        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))), tf.int32)",
            "",
            "# RoI levels range from 2~5 (not 6)",
            "level_ids = ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_int32), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278879)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278880)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278881)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278882)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2278883)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 277,
        "neg_line": [
            "-level = tf.to_int32(tf.floor(",
            "-4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))))"
        ],
        "pos_line": [
            "+level = tf.cast(tf.floor(",
            "+4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))), tf.int32)"
        ],
        "core_change": "-level = tf.to_int32(tf.floor( -4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2)))) +level = tf.cast(tf.floor( +4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))), tf.int32)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "e13e63a811438c250c1760cbcbcbe6c034a8570d",
        "index": "bdffcd1..575a7c3 100644",
        "commit_message": "bugfix in cotcurv laplacian loss. closes #551 (#553)\n\nSummary: Pull Request resolved: https://github.com/facebookresearch/pytorch3d/pull/553\n\nReviewed By: theschnitz\n\nDifferential Revision: D26257591\n\nPulled By: gkioxari\n\nfbshipit-source-id: 899a3f733a77361e8572b0900a34b55764ff08f2\n\n",
        "file": "pytorch3d.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def mesh_laplacian_smoothing(meshes, method: str = \"uniform\"):",
            "elif method == \"cot\":",
            "loss = L.mm(verts_packed) * norm_w - verts_packed",
            "elif method == \"cotcurv\":",
            "-        loss = (L.mm(verts_packed) - verts_packed) * norm_w",
            "+        loss = (L.mm(verts_packed) - L_sum * verts_packed) * norm_w",
            "loss = loss.norm(dim=1)",
            "",
            "loss = loss * weights"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('binary_operator', None), position=2, insert_id=920389)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'L_sum'), position=0, insert_id=920390)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=920391)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=verts_packed), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 278,
        "neg_line": [
            "-loss = (L.mm(verts_packed) - verts_packed) * norm_w"
        ],
        "pos_line": [
            "+loss = (L.mm(verts_packed) - L_sum * verts_packed) * norm_w"
        ],
        "core_change": "-loss = (L.mm(verts_packed) - verts_packed) * norm_w +loss = (L.mm(verts_packed) - L_sum * verts_packed) * norm_w",
        "core_API": "mm"
    },
    {
        "commit_hash": "73b2221b5eb4fd21802e6bf41e21d5df8ef9bf2c",
        "index": "6b83aa08..d953e1cd 100644",
        "commit_message": "Update DARTS trainer and fix docstring issues (#1772)\n\n\n",
        "file": "nni.txt.json",
        "label": "no",
        "comments": "not clear",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def apply_fixed_architecture(model, fixed_arc_path, device=None):",
            "architecture = FixedArchitecture(model, fixed_arc)",
            "architecture.to(device)",
            "architecture.reset()",
            "+    return architecture"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=4, insert_id=1439494)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1439495)",
            "Insert(target_node=IN(type=return_statement), node=('identifier', 'architecture'), position=1, insert_id=1439496)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 3,
        "number": 279,
        "neg_line": [],
        "pos_line": [
            "+return architecture"
        ],
        "core_change": "+return architecture",
        "core_API": "to"
    },
    {
        "commit_hash": "8abdaf77e422f87f1cd0471fcd6c7c38447facda",
        "index": "4d646e14..e050f0ab 100644",
        "commit_message": "bug fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AsyncMultiGPUTrainer(MultiGPUTrainer,",
            "",
            "self._setup_predictor_factory(predict_tower)",
            "self._average_gradient = average_gradient",
            "+        assert tf.test.is_gpu_available()",
            "",
            "def _setup(self):",
            "super(AsyncMultiGPUTrainer, self)._setup()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('assert', 'assert'), position=3, insert_id=2309153)",
            "Insert(target_node=ASTNode(type=ERROR), node=('call', None), position=4, insert_id=2309154)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'def'), position=5, insert_id=2309155)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2309156)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2309157)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2309158)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2309159)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_gpu_available'), position=2, insert_id=2309160)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2309161)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2309162)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2309163)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2309164)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'test'), position=2, insert_id=2309165)",
            "Delete(target_node=ASTNode(type=def, text=def))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 14,
        "number": 280,
        "neg_line": [],
        "pos_line": [
            "+assert tf.test.is_gpu_available()"
        ],
        "core_change": "+assert tf.test.is_gpu_available()",
        "core_API": "_setup_predictor_factory"
    },
    {
        "commit_hash": "481aa29299f87fe0bf3666a82818ab7232595fd2",
        "index": "9e224153..085bffad 100644",
        "commit_message": "Fix Autoformer to compatible with RandomOneShot strategy (#4987)\n\n\n",
        "file": "nni.txt.json",
        "label": "no",
        "comments": "add value to list",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_hub_oneshot(space_type, strategy_type):",
            "NDS_SPACES = ['amoeba', 'darts', 'pnas', 'enas', 'nasnet']",
            "if strategy_type == 'proxyless':",
            "if 'width' in space_type or 'depth' in space_type or \\",
            "-                any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3']):",
            "+                any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3', 'autoformer']):",
            "pytest.skip('The space has used unsupported APIs.')",
            "if strategy_type in ['darts', 'gumbel'] and space_type == 'mobilenetv3':",
            "pytest.skip('Skip as it consumes too much memory.')"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=4, insert_id=656371)",
            "Insert(target_node=ASTNode(type=list), node=('string', \"'autoformer'\"), position=5, insert_id=656372)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 281,
        "neg_line": [
            "-any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3']):"
        ],
        "pos_line": [
            "+any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3', 'autoformer']):"
        ],
        "core_change": "-any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3']): +any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3', 'autoformer']):",
        "core_API": "startswith"
    },
    {
        "commit_hash": "b62aee4a8ed3420be90a1aea6f90345992a3b858",
        "index": "5e742b05..04f9278d 100644",
        "commit_message": "fixed self-loop bug in gcn\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GCNConv(MessagePassing):",
            "x = torch.matmul(x, self.weight)",
            "",
            "if not self.cached or self.cached_result is None:",
            "-            edge_index, norm = GCNConv.norm(edge_index,",
            "-                                            x.size(0), edge_weight,",
            "+            edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,",
            "self.improved, x.dtype)",
            "self.cached_result = edge_index, norm"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 282,
        "neg_line": [
            "-edge_index, norm = GCNConv.norm(edge_index,",
            "-x.size(0), edge_weight,"
        ],
        "pos_line": [
            "+edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,"
        ],
        "core_change": "-edge_index, norm = GCNConv.norm(edge_index, -x.size(0), edge_weight, +edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,",
        "core_API": "matmul"
    },
    {
        "commit_hash": "703d694a226cdde01bfee8274b92705872a7129b",
        "index": "6a2f11b60..46f762bcc 100644",
        "commit_message": "fix for asr_mix\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add API call for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CTC(torch.nn.Module):",
            "if self.ctc_type == \"builtin\":",
            "olens = to_device(ys_hat, torch.LongTensor([len(s) for s in ys]))",
            "hlens = hlens.long()",
            "+            ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix",
            "self.loss = self.loss_fn(ys_hat, ys_pad, hlens, olens)",
            "else:",
            "self.loss = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=141112)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=141113)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'ys_pad'), position=0, insert_id=141114)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=141115)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=141116)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=141117)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=141118)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=141119)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=141120)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cat'), position=2, insert_id=141121)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=141122)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'ys'), position=1, insert_id=141123)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=141124)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 283,
        "neg_line": [],
        "pos_line": [
            "+ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix"
        ],
        "core_change": "+ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "41ec490b6d4f69bd2d23fb500c1fd502eb715a76",
        "index": "275a617a..b0bd5d62 100644",
        "commit_message": "Fixes calibration and adds example scripts (#2431)\n\n* Adds calibration to binary and category output feature schema.\n\n* Adds type annotations for create_calibration_module.\n\n* Fixes initialization of calibration module for category features.\n\n* First pass at forest cover and mushroom edibility.\n\n* Fixed brier plot.\n\n* Adds forest cover visualizations.\n\n* Reduce epochs to 1 and default transformer params.\n\n* Adds calibration as an output feature key which should not be nested inside decoder.\n\n* Moved output_features below input_features.\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\n",
        "file": "ludwig.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CategoryOutputFeature(CategoryFeatureMixin, OutputFeature):",
            "# hidden: shape [batch_size, size of final fully connected layer]",
            "return {LOGITS: self.decoder_obj(hidden), PROJECTION_INPUT: hidden}",
            "",
            "-    def create_calibration_module(self, feature) -> torch.nn.Module:",
            "+    def create_calibration_module(self, feature: CategoryOutputFeatureConfig) -> torch.nn.Module:",
            "\"\"\"Creates the appropriate calibration module based on the feature config.",
            "",
            "Today, only one type of calibration (\"temperature_scaling\") is available, but more options may be supported in",
            "the future.",
            "\"\"\"",
            "-        if feature.get(\"calibration\"):",
            "+        if feature.calibration:",
            "calibration_cls = calibration.get_calibration_cls(CATEGORY, \"temperature_scaling\")",
            "return calibration_cls(num_classes=self.num_classes)",
            "return None"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=3, insert_id=599142)",
            "Update(target_node=ASTNode(type=identifier, text=get), value='calibration')",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=identifier, text=feature), position=0)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=599143)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=599144)",
            "Insert(target_node=IN(type=type), node=('identifier', 'CategoryOutputFeatureConfig'), position=0, insert_id=599145)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"calibration\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 284,
        "neg_line": [
            "-def create_calibration_module(self, feature) -> torch.nn.Module:",
            "-if feature.get(\"calibration\"):"
        ],
        "pos_line": [
            "+def create_calibration_module(self, feature: CategoryOutputFeatureConfig) -> torch.nn.Module:",
            "+if feature.calibration:"
        ],
        "core_change": "-def create_calibration_module(self, feature) -> torch.nn.Module: +def create_calibration_module(self, feature: CategoryOutputFeatureConfig) -> torch.nn.Module: -if feature.get(\"calibration\"): +if feature.calibration:",
        "core_API": "decoder_obj"
    },
    {
        "commit_hash": "0655e4a2bd89ccc20f4f1157f65b3e5a61f140e1",
        "index": "9aa44963..95b164c8 100644",
        "commit_message": "fixed lstm, added gru, other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "no",
        "comments": "change value",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PiecewiseConstant(Parameter):",
            "self.values = values",
            "",
            "def get_parameter_value(self):",
            "-        if self.unit == 'timestep':",
            "+        if self.unit == 'timesteps':",
            "step = Module.retrieve_tensor(name='timestep')",
            "-        elif self.unit == 'episode':",
            "+        elif self.unit == 'episodes':",
            "step = Module.retrieve_tensor(name='episode')",
            "",
            "+        # step = tf.Print(step, (step,))",
            "+",
            "parameter = tf.train.piecewise_constant(",
            "x=step, boundaries=self.boundaries, values=self.values",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=comparison_operator), position=1)",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=ERROR), position=2)",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=identifier, text=step), position=3)",
            "Insert(target_node=ASTNode(type=if_statement), node=('comparison_operator', None), position=1, insert_id=2231597)",
            "Update(target_node=ASTNode(type=string, text='timestep'), value=\"'episodes'\")",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=string, text='timestep'), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=2231598)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=2231599)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', \"'timesteps'\"), position=2, insert_id=2231600)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2231601)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2231602)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'unit'), position=2, insert_id=2231603)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=unit))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=string, text='episode'))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 285,
        "neg_line": [
            "-if self.unit == 'timestep':",
            "-elif self.unit == 'episode':"
        ],
        "pos_line": [
            "+if self.unit == 'timesteps':",
            "+elif self.unit == 'episodes':",
            "+# step = tf.Print(step, (step,))",
            "+"
        ],
        "core_change": "-if self.unit == 'timestep': +if self.unit == 'timesteps': -elif self.unit == 'episode': +elif self.unit == 'episodes': +# step = tf.Print(step, (step,)) +",
        "core_API": "retrieve_tensor"
    },
    {
        "commit_hash": "a37e77e96184170960d4373c58d7e7d84cff3a71",
        "index": "d9e774bf..f2caa82d 100644",
        "commit_message": "fix tests on ubuntu\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_gcn_conv():",
            "assert out2.size() == (4, 32)",
            "assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6)",
            "",
            "-    torch.jit.script(conv.jittable())",
            "-",
            "t = '(Tensor, Tensor, OptTensor) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "assert jit(x, edge_index).tolist() == out1.tolist()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=jit))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=script))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=conv))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=jittable))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 286,
        "neg_line": [
            "-torch.jit.script(conv.jittable())",
            "-"
        ],
        "pos_line": [],
        "core_change": "-torch.jit.script(conv.jittable()) -",
        "core_API": "size"
    },
    {
        "commit_hash": "1e07b6b334c0b94e06ca9860641bd2e16aff90fb",
        "index": "ab8d2b7e..cacfacef 100644",
        "commit_message": "[Flax SD finetune] Fix dtype (#1038)\n\nfix jnp dtype\n",
        "file": "diffusers.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=total_train_batch_size, drop_last=True",
            ")",
            "",
            "-    weight_dtype = torch.float32",
            "+    weight_dtype = jnp.float32",
            "if args.mixed_precision == \"fp16\":",
            "-        weight_dtype = torch.float16",
            "+        weight_dtype = jnp.float16",
            "elif args.mixed_precision == \"bf16\":",
            "-        weight_dtype = torch.bfloat16",
            "+        weight_dtype = jnp.bfloat16",
            "",
            "# Load models and create wrapper for stable diffusion",
            "tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='jnp')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='jnp')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='jnp')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 287,
        "neg_line": [
            "-weight_dtype = torch.float32",
            "-weight_dtype = torch.float16",
            "-weight_dtype = torch.bfloat16"
        ],
        "pos_line": [
            "+weight_dtype = jnp.float32",
            "+weight_dtype = jnp.float16",
            "+weight_dtype = jnp.bfloat16"
        ],
        "core_change": "-weight_dtype = torch.float32 +weight_dtype = jnp.float32 -weight_dtype = torch.float16 +weight_dtype = jnp.float16 -weight_dtype = torch.bfloat16 +weight_dtype = jnp.bfloat16",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "e593cad25d6831623e6a2b6d34bcb04adcbe00f9",
        "index": "3843a36c1..1be8116a6 100644",
        "commit_message": "Added reversed operations to RST (#4508)\n\n* initialized replicated sharing tensor\n\n* removed crypto provider for now\n\n* added reconstruction implementation and sharing from native tensor\n\n* cleaning\n\n* cleaning\n\n* cleaning\n\n* added more tests\n\n* added private sharing\n\n* added public addetion and general addetion\n\n* added subtraction\n\n* added negative results and refactors\n\n* fix mod_to_real and switch priavte public\n\n* cleaning\n\n* communication rounds optimization\n\n* more communication rounds optimization\n\n* cleaning\n\n* added edge cases to tests\n\n* cleaning\n\n* more cleaning\n\n* even more cleaning\n\n* even more cleaning\n\n* arrange workers fix\n\n* add and sub with operator\n\n* added public multiplication\n\n* cleaning\n\n* more cleaning\n\n* more cleaning\n\n* added private multiplication without correlated randomness\n\n* formatting\n\n* added matrix multiplication\n\n* added public matrix multiplication\n\n* cleaning\n\n* cleaning\n\n* added shape, apply to shares\n\n* added correlated randomness\n\n* przs edits\n\n* cleaning\n\n* support for bigger ring size\n\n* convolution support wootwoot!!üéâüéâ\n\n* cleaning\n\n* more cleaning\n\n* black box principle\n\n* negative tests\n\n* cleaning\n\n* added support for 1-xor 2-consecutive arithmetic 3- arbitrary ring_size\n\n* cleaning\n\n* cleaning\n\n* more cleaning, information hiding\n\n* falcon_tensor.py, falcon.py, falcon_helper\n\n* moved xor to falcon_tensor.py\n\n* clean\n\n* removed non-tensor operations from RST\n\n* merge master\n\n* cleaning\n\n* fixed black version\n\n* fixing black\n\n* added reversed operations to RST\n\nCo-authored-by: Muhammed Abogazia <abogaziah@users.noreply.github.com>\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ReplicatedSharingTensor(AbstractTensor):",
            "return self.__private_multiplication_operation(secret, mul)",
            "",
            "__mul__ = mul",
            "+    __rmul__ = mul",
            "",
            "def matmul(self, value):",
            "return self.__switch_public_private(value, self.__public_matmul, self.__private_matmul)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=787419)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=787420)",
            "Insert(target_node=IN(type=assignment), node=('identifier', '__rmul__'), position=0, insert_id=787421)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=787422)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'mul'), position=2, insert_id=787423)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 5,
        "number": 290,
        "neg_line": [],
        "pos_line": [
            "+__rmul__ = mul"
        ],
        "core_change": "+__rmul__ = mul",
        "core_API": "__private_multiplication_operation"
    },
    {
        "commit_hash": "9a718e29855713a51877237b2dcc25e39c234c82",
        "index": "117c4776..d0d812ce 100644",
        "commit_message": "Various fixes (#2127)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2127\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21550962\n\nPulled By: myleott\n\nfbshipit-source-id: ddbe3f287f170862378e0702fc378a4fe400793a\n\n",
        "file": "fairseq.txt.json",
        "label": "no",
        "comments": "remove version fixcheck",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestJitLSTMModel(unittest.TestCase):",
            "scripted_model = torch.jit.script(model)",
            "self._test_save_and_load(scripted_model)",
            "",
            "-    @unittest.skipIf(",
            "-        torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\"",
            "-    )",
            "def test_assert_jit_vs_nonjit_(self):",
            "task, parser = get_dummy_task_and_parser()",
            "LSTMModel.add_args(parser)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=function_definition), position=3)",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=unittest))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=skipIf))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__version__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=<, text=<))",
            "Delete(target_node=ASTNode(type=string, text=\"1.6.0\"))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text=\"Targeting OSS scriptability for the 1.6 release\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type=decorated_definition))"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 293,
        "neg_line": [
            "-@unittest.skipIf(",
            "-torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\"",
            "-)"
        ],
        "pos_line": [],
        "core_change": "-@unittest.skipIf( -torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\" -)",
        "core_API": "script"
    },
    {
        "commit_hash": "b434c479e7be787c49be2df381011bed3dc8f070",
        "index": "42215f8e1..1793f3e7b 100644",
        "commit_message": "Quantisation (#5706)\n\n* empty\n\n* sq\n\n* obs\n\n\n* int\n\n* ts\n\n* helpers\n\n* chlog\n\n* yapf\n\n* avg\n\n* dupl\n\n* Apply suggestions from code review\n\nCo-authored-by: Nicki Skafte <skaftenicki@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol√≠ <carlossmocholi@gmail.com>\n\n* fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol√≠ <carlossmocholi@gmail.com>\n\n* fixes\n\n* note\n\n* warn\n\n* 45\n\n* link\n\nCo-authored-by: Carlos Mochol√≠ <carlossmocholi@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol√≠ <carlossmocholi@gmail.com>\n\n* yapf\n\n* flake8\n\n* Apply suggestions from code review\n\n* Apply suggestions from code review\n\nCo-authored-by: Nicki Skafte <skaftenicki@gmail.com>\nCo-authored-by: Carlos Mochol√≠ <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def test_result_reduce_ddp(result_cls):",
            "pytest.param(5, False, 0, id='nested_list_predictions'),",
            "pytest.param(6, False, 0, id='dict_list_predictions'),",
            "pytest.param(7, True, 0, id='write_dict_predictions'),",
            "-        pytest.param(",
            "-            0,",
            "-            True,",
            "-            1,",
            "-            id='full_loop_single_gpu',",
            "-            marks=pytest.mark.skipif(torch.cuda.device_count() < 1, reason=\"test requires single-GPU machine\")",
            "-        )",
            "+        pytest.param(0, True, 1, id='full_loop_single_gpu', marks=pytest.mark.skipif(**_SKIPIF_ARGS_NO_GPU))",
            "]",
            ")",
            "def test_result_obj_predictions(tmpdir, test_option, do_train, gpus):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('dictionary_splat', None), position=1, insert_id=1844732)",
            "Insert(target_node=IN(type=dictionary_splat), node=('**', '**'), position=0, insert_id=1844733)",
            "Insert(target_node=IN(type=dictionary_splat), node=('identifier', '_SKIPIF_ARGS_NO_GPU'), position=1, insert_id=1844734)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device_count))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=<, text=<))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=reason))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"test requires single-GPU machine\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 7,
        "AST_diff_line": 22,
        "number": 294,
        "neg_line": [
            "-pytest.param(",
            "-0,",
            "-True,",
            "-1,",
            "-id='full_loop_single_gpu',",
            "-marks=pytest.mark.skipif(torch.cuda.device_count() < 1, reason=\"test requires single-GPU machine\")",
            "-)"
        ],
        "pos_line": [
            "+pytest.param(0, True, 1, id='full_loop_single_gpu', marks=pytest.mark.skipif(**_SKIPIF_ARGS_NO_GPU))"
        ],
        "core_change": "-pytest.param( -0, -True, -1, -id='full_loop_single_gpu', -marks=pytest.mark.skipif(torch.cuda.device_count() < 1, reason=\"test requires single-GPU machine\") -) +pytest.param(0, True, 1, id='full_loop_single_gpu', marks=pytest.mark.skipif(**_SKIPIF_ARGS_NO_GPU))",
        "core_API": "param"
    },
    {
        "commit_hash": "b2225436f140868d4990cc735c5904fa633611d4",
        "index": "0302c980..3bf043f3 100644",
        "commit_message": "flops counter formatting fix (#3837)\n\n\n",
        "file": "nni.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def count_flops_params(model, x, custom_ops=None, verbose=True, mode='default'):",
            "print(f'FLOPs total: {profiler.sum_flops()}')",
            "print(f'#Params total: {profiler.sum_params()}')",
            "",
            "-    return profiler.sum_flops(), profiler.sum_params(), profiler.results",
            "\\ No newline at end of file",
            "+    return profiler.sum_flops(), profiler.sum_params(), profiler.results"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=2, insert_id=663881)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=663882)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('ERROR', ' '), position=1, insert_id=663883)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'No'), position=2, insert_id=663884)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'newline'), position=3, insert_id=663885)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'at'), position=4, insert_id=663886)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'end'), position=5, insert_id=663887)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'of'), position=6, insert_id=663888)",
            "Insert(target_node=IN(type=expression_statement), node=('identifier', 'file'), position=0, insert_id=663889)",
            "Move(target_node=ASTNode(type=expression_list), node=ASTNode(type=attribute), position=4)",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=ERROR, text= ))",
            "Delete(target_node=ASTNode(type=identifier, text=No))",
            "Delete(target_node=ASTNode(type=identifier, text=newline))",
            "Delete(target_node=ASTNode(type=identifier, text=at))",
            "Delete(target_node=ASTNode(type=identifier, text=end))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=file))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 295,
        "neg_line": [
            "-return profiler.sum_flops(), profiler.sum_params(), profiler.results"
        ],
        "pos_line": [
            "+return profiler.sum_flops(), profiler.sum_params(), profiler.results"
        ],
        "core_change": "-return profiler.sum_flops(), profiler.sum_params(), profiler.results +return profiler.sum_flops(), profiler.sum_params(), profiler.results",
        "core_API": "sum_flops"
    },
    {
        "commit_hash": "8503cc755050c6ed5bc771e3244c29b71be1841e",
        "index": "06823e7fe..f69d8f15c 100644",
        "commit_message": "Fix torch device issues (#20304)\n\n* fix device issue\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeformableDetrModelIntegrationTests(unittest.TestCase):",
            "results = feature_extractor.post_process_object_detection(",
            "outputs, threshold=0.3, target_sizes=[image.size[::-1]]",
            ")[0]",
            "-        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382])",
            "+        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382]).to(torch_device)",
            "expected_labels = [17, 17, 75, 75, 63]",
            "-        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841])",
            "+        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]).to(torch_device)",
            "",
            "self.assertEqual(len(results[\"scores\"]), 5)",
            "self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1184246)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1184247)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1184248)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1184249)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1184250)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1184251)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1184252)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1184253)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1184254)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'torch_device'), position=1, insert_id=1184255)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1184256)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1184257)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1184258)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1184259)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'torch_device'), position=1, insert_id=1184260)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1184261)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 296,
        "neg_line": [
            "-expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382])",
            "-expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841])"
        ],
        "pos_line": [
            "+expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382]).to(torch_device)",
            "+expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]).to(torch_device)"
        ],
        "core_change": "-expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382]) +expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382]).to(torch_device) -expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]) +expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]).to(torch_device)",
        "core_API": "post_process_object_detection"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "5af1c107..5d8a396f 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Critic(object):",
            "n = InputLayer(self.s, name='in')",
            "n = DenseLayer(n, n_units=30, act=tf.nn.relu6, W_init=tf.random_uniform_initializer(0, 0.01), name='hidden')",
            "# n = DenseLayer(n, n_units=5, act=tf.nn.relu, W_init=tf.random_uniform_initializer(0, 0.01), name='hidden2')",
            "-            n = DenseLayer(n, n_units=1, act=tf.identity, name='V')",
            "+            n = DenseLayer(n, n_units=1, act=None, name='V')",
            "self.v = n.outputs",
            "",
            "with tf.variable_scope('squared_TD_error'):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('none', 'None'), position=2, insert_id=2262247)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 297,
        "neg_line": [
            "-n = DenseLayer(n, n_units=1, act=tf.identity, name='V')"
        ],
        "pos_line": [
            "+n = DenseLayer(n, n_units=1, act=None, name='V')"
        ],
        "core_change": "-n = DenseLayer(n, n_units=1, act=tf.identity, name='V') +n = DenseLayer(n, n_units=1, act=None, name='V')",
        "core_API": "random_uniform_initializer"
    },
    {
        "commit_hash": "0a408c4ceb030bdc25597b77a52361e49b8bbafd",
        "index": "a0792c1..29b4e85 100644",
        "commit_message": "Fix wrong replacement\n\n",
        "file": "nebullvm.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorflowBackendInferenceLearner(TensorflowBaseInferenceLearner):",
            "metadata = LearnerMetadata.read(path)",
            "network_parameters = ModelParams(**metadata.network_parameters)",
            "input_tfms = metadata.input_tfms",
            "-        model = nebullvm.operations.inference_learners.utils.load_model(",
            "+        model = tf.keras.models.load_model(",
            "path / TENSORFLOW_BACKEND_FILENAMES[\"tf_model\"]",
            ")",
            "device = Device(metadata.device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=inference_learners), value='models')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=inference_learners), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=nebullvm), value='tf')",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2123494)",
            "Update(target_node=ASTNode(type=identifier, text=operations), value='keras')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=utils))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 298,
        "neg_line": [
            "-model = nebullvm.operations.inference_learners.utils.load_model("
        ],
        "pos_line": [
            "+model = tf.keras.models.load_model("
        ],
        "core_change": "-model = nebullvm.operations.inference_learners.utils.load_model( +model = tf.keras.models.load_model(",
        "core_API": "read"
    },
    {
        "commit_hash": "2cbe29a7fa8bc63d57f97b03ba726676425e160b",
        "index": "f52f5850c..96a8893ef 100644",
        "commit_message": "[RLlib] Curiosity minor fixes, do-overs, and testing. (#10143)\n\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "FileType = Any",
            "# Represents the result dict returned by Trainer.train().",
            "ResultDict = dict",
            "",
            "+# A tf or torch local optimizer object.",
            "+LocalOptimizer = Union[\"tf.keras.optimizers.Optimizer\",",
            "+                       \"torch.optim.Optimizer\"]",
            "+",
            "# Dict of tensors returned by compute gradients on the policy, e.g.,",
            "# {\"td_error\": [...], \"learner_stats\": {\"vf_loss\": ..., ...}}, for multi-agent,",
            "# {\"policy1\": {\"learner_stats\": ..., }, \"policy2\": ...}."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1615700)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1615701)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'LocalOptimizer'), position=0, insert_id=1615702)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1615703)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=2, insert_id=1615704)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Union'), position=0, insert_id=1615705)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1615706)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"tf.keras.optimizers.Optimizer\"'), position=2, insert_id=1615707)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=1615708)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"torch.optim.Optimizer\"'), position=4, insert_id=1615709)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=1615710)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 11,
        "number": 300,
        "neg_line": [],
        "pos_line": [
            "+# A tf or torch local optimizer object.",
            "+LocalOptimizer = Union[\"tf.keras.optimizers.Optimizer\",",
            "+\"torch.optim.Optimizer\"]",
            "+"
        ],
        "core_change": "+# A tf or torch local optimizer object. +LocalOptimizer = Union[\"tf.keras.optimizers.Optimizer\", +\"torch.optim.Optimizer\"] +",
        "core_API": "train"
    },
    {
        "commit_hash": "4ad67b4bbcf57cff3410d4a548450e53d390e56c",
        "index": "a2b4996..bbd2b9b 100644",
        "commit_message": "Refactor triton buffer to use CLBuffer of cuda runtime (#524)\n\n* Refactor triton buffer to use CLBuffer of runtime\n\n* Fix opencl GT0\n",
        "file": "tinygrad.txt.json",
        "label": "no",
        "comments": "method define",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class CLImage:",
            "",
            "class CLBuffer:",
            "def __init__(self, size): self.cl = cuda.mem_alloc(size)",
            "-  def copyin(self, b:np.ndarray): cuda.memcpy_htod_async(self.cl, b)",
            "+  def copyin(self, b:np.ndarray, stream:Optional[cuda.Stream]=None): cuda.memcpy_htod_async(self.cl, b, stream)",
            "def copyout(self, a:np.ndarray): cuda.memcpy_dtoh(a, self.cl)",
            "",
            "class CLProgram:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=1875082)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=5, insert_id=1875083)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('identifier', 'stream'), position=0, insert_id=1875084)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=1875085)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=1875086)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=1875087)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('none', 'None'), position=4, insert_id=1875088)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=1875089)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Optional'), position=0, insert_id=1875090)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1875091)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=1875092)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1875093)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1875094)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'stream'), position=5, insert_id=1875095)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=0, insert_id=1875096)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1875097)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Stream'), position=2, insert_id=1875098)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 301,
        "neg_line": [
            "-def copyin(self, b:np.ndarray): cuda.memcpy_htod_async(self.cl, b)"
        ],
        "pos_line": [
            "+def copyin(self, b:np.ndarray, stream:Optional[cuda.Stream]=None): cuda.memcpy_htod_async(self.cl, b, stream)"
        ],
        "core_change": "-def copyin(self, b:np.ndarray): cuda.memcpy_htod_async(self.cl, b) +def copyin(self, b:np.ndarray, stream:Optional[cuda.Stream]=None): cuda.memcpy_htod_async(self.cl, b, stream)",
        "core_API": "mem_alloc"
    },
    {
        "commit_hash": "888340d17ed91eeee1b576cda36f13f0ef3e5459",
        "index": "deab2bb4c..89fee6d21 100644",
        "commit_message": "Fix RMSLE metric (#3188)\n\n* fix rmsle\n\n* Updated test to match rmsle fix\n\n* Updated RMSLE example result to match functional\n\n* chlog\n\n* add randomized test\n\n* fix pep8\n\nCo-authored-by: Jirka Borovec <jirka@pytorchlightning.ai>\nCo-authored-by: Nicki Skafte <skaftenicki@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rmsle(",
            ">>> x = torch.tensor([0., 1, 2, 3])",
            ">>> y = torch.tensor([0., 1, 2, 2])",
            ">>> rmsle(x, y)",
            "-        tensor(0.0207)",
            "+        tensor(0.1438)",
            "",
            "\"\"\"",
            "-    rmsle = mse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)",
            "+    rmsle = rmse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)",
            "return rmsle"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mse), value='rmse')",
            "Update(target_node=ASTNode(type=float, text=0.0207), value='0.1438')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 302,
        "neg_line": [
            "-tensor(0.0207)",
            "-rmsle = mse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)"
        ],
        "pos_line": [
            "+tensor(0.1438)",
            "+rmsle = rmse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)"
        ],
        "core_change": "-tensor(0.0207) +tensor(0.1438) -rmsle = mse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction) +rmsle = rmse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "0d90e35f3b14bd242a64a7a2af744a258e7d0298",
        "index": "fdd624b2..3a1471fa 100644",
        "commit_message": "More unit test fixes\n\n",
        "file": "fairseq.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def add_dataset_args(parser, train=False, gen=False):",
            "",
            "def add_distributed_training_args(parser):",
            "group = parser.add_argument_group('Distributed training')",
            "-    group.add_argument('--distributed-world-size', default=1, type=int, metavar='N',",
            "-                       help='total number of GPUs across all nodes, default: 1 GPU')",
            "+    group.add_argument('--distributed-world-size', type=int, metavar='N',",
            "+                       default=torch.cuda.device_count(),",
            "+                       help='total number of GPUs across all nodes (default: all visible GPUs)')",
            "group.add_argument('--distributed-rank', default=0, type=int,",
            "help='rank of the current worker')",
            "group.add_argument('--distributed-backend', default='nccl', type=str,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=argument_list), position=3)",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=argument_list), position=6)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=8, insert_id=225232)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'default'), position=0, insert_id=225233)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=225234)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=225235)",
            "Update(target_node=ASTNode(type=string, text='total number of GPUs across all nodes, default: 1 GPU'), value=\"'total number of GPUs across all nodes (default: all visible GPUs)'\")",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=225236)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=225237)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=225238)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=225239)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device_count'), position=2, insert_id=225240)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=225241)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=225242)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=225243)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=225244)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=225245)",
            "Delete(target_node=ASTNode(type=identifier, text=default))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 303,
        "neg_line": [
            "-group.add_argument('--distributed-world-size', default=1, type=int, metavar='N',",
            "-help='total number of GPUs across all nodes, default: 1 GPU')"
        ],
        "pos_line": [
            "+group.add_argument('--distributed-world-size', type=int, metavar='N',",
            "+default=torch.cuda.device_count(),",
            "+help='total number of GPUs across all nodes (default: all visible GPUs)')"
        ],
        "core_change": "-group.add_argument('--distributed-world-size', default=1, type=int, metavar='N', -help='total number of GPUs across all nodes, default: 1 GPU') +group.add_argument('--distributed-world-size', type=int, metavar='N', +default=torch.cuda.device_count(), +help='total number of GPUs across all nodes (default: all visible GPUs)')",
        "core_API": "add_argument_group"
    },
    {
        "commit_hash": "3e6167c51df23b7629d7830e81e8cf4ea52032fc",
        "index": "71c741831..a97def4e9 100644",
        "commit_message": "Fixed format in some files\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DCCRNSeparator(AbsSeparator):",
            "self.flatten_parameters()",
            "",
            "def forward(",
            "-        self,",
            "-        input: Union[torch.Tensor, ComplexTensor],",
            "+        self,",
            "+        input: Union[torch.Tensor, ComplexTensor],",
            "ilens: torch.Tensor,",
            "additional: Optional[Dict] = None,",
            ") -> Tuple[List[Union[torch.Tensor, ComplexTensor]], torch.Tensor, OrderedDict]:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 304,
        "neg_line": [
            "-self,",
            "-input: Union[torch.Tensor, ComplexTensor],"
        ],
        "pos_line": [
            "+self,",
            "+input: Union[torch.Tensor, ComplexTensor],"
        ],
        "core_change": "-self, -input: Union[torch.Tensor, ComplexTensor], +self, +input: Union[torch.Tensor, ComplexTensor],",
        "core_API": "flatten_parameters"
    },
    {
        "commit_hash": "55487f33b135849cdda49bcc75778976d94a2575",
        "index": "dde26943..4b2132f7 100644",
        "commit_message": "Added dtype parameter to zeros_like and ones_like (#5062)\n\n* Fixed checking input masks in Layer.compute_mask\n\n* Added dtype parameter to zeros_like and ones_like\n\n* Fix existing docstring for ones_like and zeros_like\n\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def ones_like(x, name=None):",
            "[ 1.,  1.,  1.]], dtype=float32)",
            "```",
            "\"\"\"",
            "-    return tf.ones_like(x, name=name)",
            "+    return tf.ones_like(x, dtype=dtype, name=name)",
            "",
            "",
            "def random_uniform_variable(shape, low, high, dtype=None,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2114717)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2114718)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2114719)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2114720)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=2114721)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 305,
        "neg_line": [
            "-return tf.ones_like(x, name=name)"
        ],
        "pos_line": [
            "+return tf.ones_like(x, dtype=dtype, name=name)"
        ],
        "core_change": "-return tf.ones_like(x, name=name) +return tf.ones_like(x, dtype=dtype, name=name)",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "c215878f11d81808dfe4721795f0c105200e6601",
        "index": "7bf249e..6ed528a 100644",
        "commit_message": "YOLOv5 Apple Metal Performance Shader (MPS) support (#7878)\n\n* Apple Metal Performance Shader (MPS) device support\n\nFollowing https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\n\nShould work with Apple M1 devices with PyTorch nightly installed with command `--device mps`. Usage examples:\n```bash\npython train.py --device mps\npython detect.py --device mps\npython val.py --device mps\n```\n\n* Update device strategy to fix MPS issue\n",
        "file": "yolov5.txt.json",
        "label": "no",
        "comments": "rename param for method",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Ensemble(nn.ModuleList):",
            "return y, None  # inference, train output",
            "",
            "",
            "-def attempt_load(weights, map_location=None, inplace=True, fuse=True):",
            "+def attempt_load(weights, device=None, inplace=True, fuse=True):",
            "from models.yolo import Detect, Model",
            "",
            "# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "-        ckpt = torch.load(attempt_download(w), map_location=map_location)  # load",
            "-        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model",
            "+        ckpt = torch.load(attempt_download(w))",
            "+        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model",
            "model.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode",
            "",
            "# Compatibility updates"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=map_location), value='device')",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1294083)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1294084)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1294085)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=1294086)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1294087)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='to')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1294088)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=1294089)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1294090)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=map_location))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=map_location))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 306,
        "neg_line": [
            "-def attempt_load(weights, map_location=None, inplace=True, fuse=True):",
            "-ckpt = torch.load(attempt_download(w), map_location=map_location)  # load",
            "-ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model"
        ],
        "pos_line": [
            "+def attempt_load(weights, device=None, inplace=True, fuse=True):",
            "+ckpt = torch.load(attempt_download(w))",
            "+ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model"
        ],
        "core_change": "-def attempt_load(weights, map_location=None, inplace=True, fuse=True): +def attempt_load(weights, device=None, inplace=True, fuse=True): -ckpt = torch.load(attempt_download(w), map_location=map_location)  # load -ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model +ckpt = torch.load(attempt_download(w)) +ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model",
        "core_API": "load"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "d4daa779..e396d5fd 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "remove API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from allennlp.common.params import Params",
            "",
            "class TestStackedBidirectionalLstm(AllenNlpTestCase):",
            "def test_stacked_bidirectional_lstm_completes_forward_pass(self):",
            "-        input_tensor = torch.autograd.Variable(torch.rand(4, 5, 3))",
            "+        input_tensor = torch.rand(4, 5, 3)",
            "input_tensor[1, 4:, :] = 0.",
            "input_tensor[2, 2:, :] = 0.",
            "input_tensor[3, 1:, :] = 0."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=autograd))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 307,
        "neg_line": [
            "-input_tensor = torch.autograd.Variable(torch.rand(4, 5, 3))"
        ],
        "pos_line": [
            "+input_tensor = torch.rand(4, 5, 3)"
        ],
        "core_change": "-input_tensor = torch.autograd.Variable(torch.rand(4, 5, 3)) +input_tensor = torch.rand(4, 5, 3)",
        "core_API": "Variable"
    },
    {
        "commit_hash": "c19b8e4ae04ac90e27f3ef95b78e9d7478fc7c5f",
        "index": "d9c3f494c..b6127d278 100644",
        "commit_message": "fixing CTRL tests and OpenAI GPT tests\n\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFCTRLMainLayer(tf.keras.layers.Layer):",
            "token_type_embeds = 0",
            "position_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])",
            "",
            "-        inputs_embeds = self.w(input_ids)",
            "+        inputs_embeds = self.w(input_ids, mode='embedding')",
            "# x = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded",
            "seq_len = input_shape[-1]",
            "mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2385919)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2385920)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'mode'), position=0, insert_id=2385921)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2385922)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'embedding'\"), position=2, insert_id=2385923)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 309,
        "neg_line": [
            "-inputs_embeds = self.w(input_ids)"
        ],
        "pos_line": [
            "+inputs_embeds = self.w(input_ids, mode='embedding')"
        ],
        "core_change": "-inputs_embeds = self.w(input_ids) +inputs_embeds = self.w(input_ids, mode='embedding')",
        "core_API": "reshape"
    },
    {
        "commit_hash": "f6ed1c8712e5b69ea35bcad3bd33d39d0d0b554c",
        "index": "0eec41f1..ed33402f 100644",
        "commit_message": "Fix issue with non-canonical TF version name format.\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _preprocess_conv3d_input(x, data_format):",
            "A tensor.",
            "\"\"\"",
            "# tensorflow doesn't support float64 for conv layer before 1.8.0",
            "-    if (dtype(x) == 'float64'",
            "-            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):",
            "+    if (dtype(x) == 'float64' and",
            "+            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):",
            "x = tf.cast(x, 'float32')",
            "tf_data_format = 'NDHWC'",
            "if data_format == 'channels_first':"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=1, insert_id=2111870)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=2111871)",
            "Insert(target_node=IN(type=subscript), node=('call', None), position=0, insert_id=2111872)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2111873)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=2111874)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2111875)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2111876)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2111877)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2111878)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'split'), position=2, insert_id=2111879)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2111880)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'-'\"), position=1, insert_id=2111881)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 310,
        "neg_line": [
            "-if (dtype(x) == 'float64'",
            "-and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):"
        ],
        "pos_line": [
            "+if (dtype(x) == 'float64' and",
            "+StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):"
        ],
        "core_change": "-if (dtype(x) == 'float64' -and StrictVersion(tf.__version__) < StrictVersion('1.8.0')): +if (dtype(x) == 'float64' and +StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):",
        "core_API": "split"
    },
    {
        "commit_hash": "b7f6fd9449e2170da9ff6ac1ef4ab24f5b76ca2e",
        "index": "b0d07ca51..74083b116 100644",
        "commit_message": "Fix xtreme s metrics (#3957)\n\n* make wer more robust\n\n* [XTREME-S Metrics] Add babel again\n\n* fix\n\n* make style\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "add value",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class XtremeS(datasets.Metric):",
            "tokenize=tokenize,",
            "use_effective_order=use_effective_order,",
            ")",
            "-        elif self.config_name in [\"fleurs-asr\", \"mls\", \"voxpopuli\"]:",
            "+        elif self.config_name in [\"fleurs-asr\", \"mls\", \"voxpopuli\", \"babel\"]:",
            "concatenate_texts = wer_kwargs.pop(\"concatenate_texts\", False)",
            "return wer_and_cer(predictions, references, concatenate_texts, self.config_name)",
            "else:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=6, insert_id=1779662)",
            "Insert(target_node=ASTNode(type=list), node=('string', '\"babel\"'), position=7, insert_id=1779663)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 311,
        "neg_line": [
            "-elif self.config_name in [\"fleurs-asr\", \"mls\", \"voxpopuli\"]:"
        ],
        "pos_line": [
            "+elif self.config_name in [\"fleurs-asr\", \"mls\", \"voxpopuli\", \"babel\"]:"
        ],
        "core_change": "-elif self.config_name in [\"fleurs-asr\", \"mls\", \"voxpopuli\"]: +elif self.config_name in [\"fleurs-asr\", \"mls\", \"voxpopuli\", \"babel\"]:",
        "core_API": "pop"
    },
    {
        "commit_hash": "6c78c778a9578a71e8065643765de14d69614bad",
        "index": "8aa3d4d9..43f7b6ac 100644",
        "commit_message": "add fix for torch 1.0 on RTD (#1591)\n\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "remove constraint for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from __future__ import absolute_import, division, print_function",
            "",
            "+import os",
            "+",
            "import torch",
            "",
            "-assert torch.__version__.startswith('1.')",
            "+if 'READTHEDOCS' not in os.environ:",
            "+    # RTD is running 0.4.1 due to a memory issue with pytorch 1.0",
            "+    assert torch.__version__.startswith('1.')",
            "",
            "",
            "def patch_dependency(target, root_module=torch):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('import_statement', None), position=1, insert_id=728221)",
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=728222)",
            "Insert(target_node=IN(type=import_statement), node=('import', 'import'), position=0, insert_id=728223)",
            "Insert(target_node=IN(type=import_statement), node=('dotted_name', None), position=1, insert_id=728224)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=728225)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=728226)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=728227)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=728228)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'os'), position=0, insert_id=728229)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', \"'READTHEDOCS'\"), position=0, insert_id=728230)",
            "Insert(target_node=IN(type=comparison_operator), node=('not in', 'not'), position=1, insert_id=728231)",
            "Insert(target_node=IN(type=comparison_operator), node=('not in', 'in'), position=2, insert_id=728232)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=3, insert_id=728233)",
            "Move(target_node=IN(type=block), node=ASTNode(type=assert_statement), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'os'), position=0, insert_id=728234)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=728235)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'environ'), position=2, insert_id=728236)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 312,
        "neg_line": [
            "-assert torch.__version__.startswith('1.')"
        ],
        "pos_line": [
            "+import os",
            "+",
            "+if 'READTHEDOCS' not in os.environ:",
            "+# RTD is running 0.4.1 due to a memory issue with pytorch 1.0",
            "+assert torch.__version__.startswith('1.')"
        ],
        "core_change": "+import os + -assert torch.__version__.startswith('1.') +if 'READTHEDOCS' not in os.environ: +# RTD is running 0.4.1 due to a memory issue with pytorch 1.0 +assert torch.__version__.startswith('1.')",
        "core_API": "startswith"
    },
    {
        "commit_hash": "4a53e8e9e405779cc9f01c11c4d866b3fb6738e2",
        "index": "04b55b7b6..ba94baaa7 100644",
        "commit_message": "Fix DataCollatorForWholeWordMask again (#8397)\n\n\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):",
            "mask_labels = []",
            "for e in examples:",
            "ref_tokens = []",
            "-            for id in e[\"input_ids\"].tolist():",
            "+            for id in tolist(e[\"input_ids\"]):",
            "token = self.tokenizer._convert_id_to_token(id)",
            "ref_tokens.append(token)",
            "",
            "# For Chinese tokens, we need extra inf to mark sub-word, e.g [Âñú,Ê¨¢]-> [ÂñúÔºå##Ê¨¢]",
            "if \"chinese_ref\" in e:",
            "-                ref_pos = e[\"chinese_ref\"].tolist()",
            "+                ref_pos = tolist(e[\"chinese_ref\"])",
            "len_seq = e[\"input_ids\"].size(0)",
            "for i in range(len_seq):",
            "if i in ref_pos:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'tolist'), position=0, insert_id=1230342)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1230343)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1230344)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1230345)",
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'tolist'), position=0, insert_id=1230346)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1230347)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1230348)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1230349)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tolist))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tolist))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 313,
        "neg_line": [
            "-for id in e[\"input_ids\"].tolist():",
            "-ref_pos = e[\"chinese_ref\"].tolist()"
        ],
        "pos_line": [
            "+for id in tolist(e[\"input_ids\"]):",
            "+ref_pos = tolist(e[\"chinese_ref\"])"
        ],
        "core_change": "-for id in e[\"input_ids\"].tolist(): +for id in tolist(e[\"input_ids\"]): -ref_pos = e[\"chinese_ref\"].tolist() +ref_pos = tolist(e[\"chinese_ref\"])",
        "core_API": "_convert_id_to_token"
    },
    {
        "commit_hash": "6a02df3aa882ae3468ead98829ef177025b3b8ab",
        "index": "527cd2a0..ee71d37a 100644",
        "commit_message": "Fix bug in get_or_create_layer migration utility that produced regularization losses of the wrong rank, causing failures on model fit.\n\nPiperOrigin-RevId: 414066868\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "functional change",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class _EagerVariableStore(tf.Module):",
            "layer = create_layer_method()",
            "self._layers[name] = layer",
            "if isinstance(layer, base_layer.Layer):",
            "-        self._regularizers[name] = lambda: layer.losses",
            "+        self._regularizers[name] = lambda: tf.math.reduce_sum(layer.losses)",
            "return self._layers[name]",
            "",
            "def add_regularizer(self, var, regularizer):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=lambda), node=('call', None), position=2, insert_id=2074448)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2074449)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2074450)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2074451)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2074452)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reduce_sum'), position=2, insert_id=2074453)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2074454)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2074455)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2074456)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2074457)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2074458)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 314,
        "neg_line": [
            "-self._regularizers[name] = lambda: layer.losses"
        ],
        "pos_line": [
            "+self._regularizers[name] = lambda: tf.math.reduce_sum(layer.losses)"
        ],
        "core_change": "-self._regularizers[name] = lambda: layer.losses +self._regularizers[name] = lambda: tf.math.reduce_sum(layer.losses)",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "6f8e367ae942f8892f7824860efff86ac64db6ae",
        "index": "b1e47131c..3b04ccbb7 100644",
        "commit_message": "Fix Padded Batch Error 12282 (#12487)\n\nThis fixes the padded batch [issue](https://github.com/huggingface/transformers/issues/12282). The error was generated due to the maximum sequence length of the attention mask not matching the padded sequence length of the hidden_states. `np.allclose` now passes with a 1e-2 absolute tolerance.\n\nThis change fixes\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFWav2Vec2MainLayer(tf.keras.layers.Layer):",
            "if inputs[\"attention_mask\"] is not None:",
            "# compute real output lengths according to convolution formula",
            "output_lengths = self._get_feat_extract_output_lengths(tf.reduce_sum(inputs[\"attention_mask\"], -1))",
            "-            attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)",
            "+",
            "+            attention_mask = tf.sequence_mask(",
            "+                output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype",
            "+            )",
            "",
            "hidden_states = self.feature_projection(hidden_states, training=inputs[\"training\"])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2369733)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2369734)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'maxlen'), position=0, insert_id=2369735)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2369736)",
            "Insert(target_node=IN(type=keyword_argument), node=('subscript', None), position=2, insert_id=2369737)",
            "Insert(target_node=IN(type=subscript), node=('call', None), position=0, insert_id=2369738)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2369739)",
            "Insert(target_node=IN(type=subscript), node=('integer', '1'), position=2, insert_id=2369740)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2369741)",
            "Insert(target_node=IN(type=call), node=('identifier', 'shape_list'), position=0, insert_id=2369742)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2369743)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2369744)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'hidden_states'), position=1, insert_id=2369745)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2369746)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 315,
        "neg_line": [
            "-attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)"
        ],
        "pos_line": [
            "+",
            "+attention_mask = tf.sequence_mask(",
            "+output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype",
            "+)"
        ],
        "core_change": "-attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype) + +attention_mask = tf.sequence_mask( +output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype +)",
        "core_API": "_get_feat_extract_output_lengths"
    },
    {
        "commit_hash": "65bb86eb432e1437658cca2d920f8514bf867de5",
        "index": "ec531152..db6fe67b 100644",
        "commit_message": "Fix batch log pdf masked view (#322)\n\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "functional change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DiagNormal(Distribution):",
            "# when the data is a ragged tensor. also useful for KL annealing. this entire logic",
            "# will likely be done in a better/cleaner way in the future",
            "if log_pdf_mask is not None:",
            "-            # TODO fix this to broadcasting as below, e.g. by instead:",
            "-            # log_pxs *= log_pdf_mask  # Then continue with broadcasting logic below.",
            "-            return torch.sum(log_pdf_mask * log_pxs, -1)",
            "+            log_pxs = log_pxs * log_pdf_mask",
            "batch_log_pdf = torch.sum(log_pxs, -1)",
            "batch_log_pdf_shape = x.size()[:-1] + (1,)",
            "return batch_log_pdf.contiguous().view(batch_log_pdf_shape)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=762730)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=762731)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='log_pxs')",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=762732)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Move(target_node=ASTNode(type=identifier, text=log_pdf_mask), node=ASTNode(type=binary_operator), position=2)",
            "Move(target_node=ASTNode(type=*, text=*), node=ASTNode(type=binary_operator), position=3)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sum))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 317,
        "neg_line": [
            "-# TODO fix this to broadcasting as below, e.g. by instead:",
            "-# log_pxs *= log_pdf_mask  # Then continue with broadcasting logic below.",
            "-return torch.sum(log_pdf_mask * log_pxs, -1)"
        ],
        "pos_line": [
            "+log_pxs = log_pxs * log_pdf_mask"
        ],
        "core_change": "-# TODO fix this to broadcasting as below, e.g. by instead: -# log_pxs *= log_pdf_mask  # Then continue with broadcasting logic below. -return torch.sum(log_pdf_mask * log_pxs, -1) +log_pxs = log_pxs * log_pdf_mask",
        "core_API": "sum"
    },
    {
        "commit_hash": "8880f696b6b8368a76296126476ea020fc7c814c",
        "index": "e85112e..f810eb8 100644",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "no",
        "comments": "remove constraint",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class MobileNetV3(nn.Module):",
            "",
            "def forward(self, x):",
            "x = self.forward_features(x)",
            "-        if not self.global_pool.is_identity():",
            "-            x = x.flatten(1)",
            "+        x = self.flatten(x)",
            "if self.drop_rate > 0.:",
            "x = F.dropout(x, p=self.drop_rate, training=self.training)",
            "return self.classifier(x)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=x), value='self')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'x'), position=1, insert_id=1478048)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=global_pool))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=not_operator))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 320,
        "neg_line": [
            "-if not self.global_pool.is_identity():",
            "-x = x.flatten(1)"
        ],
        "pos_line": [
            "+x = self.flatten(x)"
        ],
        "core_change": "-if not self.global_pool.is_identity(): -x = x.flatten(1) +x = self.flatten(x)",
        "core_API": "forward_features"
    },
    {
        "commit_hash": "c5905cfa50c2f3f89849f7603d2ed0ce5aab099b",
        "index": "cb2ca031..d703f327 100644",
        "commit_message": "linter fixes\n\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ParallelWaveganGenerator(torch.nn.Module):",
            "",
            "def apply_weight_norm(self):",
            "def _apply_weight_norm(m):",
            "-            if isinstance(m, torch.nn.Conv1d) or isinstance(",
            "-                    m, torch.nn.Conv2d):",
            "+            if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d)):",
            "torch.nn.utils.weight_norm(m)",
            "# print(f\"Weight norm is applied to {m}.\")"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('call', None), position=1, insert_id=1268753)",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=isinstance), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1268754)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=m), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('tuple', None), position=3, insert_id=1268755)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1268756)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=1268757)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=attribute), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=attribute), position=3)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=or, text=or))",
            "Delete(target_node=ASTNode(type=identifier, text=isinstance))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=m))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=boolean_operator))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 321,
        "neg_line": [
            "-if isinstance(m, torch.nn.Conv1d) or isinstance(",
            "-m, torch.nn.Conv2d):"
        ],
        "pos_line": [
            "+if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d)):"
        ],
        "core_change": "-if isinstance(m, torch.nn.Conv1d) or isinstance( -m, torch.nn.Conv2d): +if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d)):",
        "core_API": "weight_norm"
    },
    {
        "commit_hash": "50f82e12823ef8844b45a0dd864a78eea80de879",
        "index": "31630b17f..b8fd916ec 100644",
        "commit_message": "Fix docstrings for TF BLIP (#22618)\n\n* Fix docstrings for TFBLIP\n\n* Fix missing line in TF port!\n\n* Use values from torch tests now other bugs fixed\n\n* Use values from torch tests now other bugs fixed\n\n* Fix doctest string\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFBlipModelIntegrationTest(unittest.TestCase):",
            "out_itm = model(**inputs)",
            "out = model(**inputs, use_itm_head=False, training=False)",
            "",
            "-        expected_scores = tf.convert_to_tensor([[0.9798, 0.0202]])",
            "+        expected_scores = tf.convert_to_tensor([[0.0029, 0.9971]])",
            "self.assertTrue(np.allclose(tf.nn.softmax(out_itm[0]).numpy(), expected_scores, rtol=1e-3, atol=1e-3))",
            "-        self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5053]]), rtol=1e-3, atol=1e-3))",
            "+        self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5162]]), rtol=1e-3, atol=1e-3))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2356793)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2356794)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2356795)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'convert_to_tensor'), position=2, insert_id=2356796)",
            "Update(target_node=ASTNode(type=float, text=0.9798), value='0.0029')",
            "Update(target_node=ASTNode(type=float, text=0.0202), value='0.9971')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=float, text=0.5053), value='0.5162')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=convert_to_tensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 322,
        "neg_line": [
            "-expected_scores = tf.convert_to_tensor([[0.9798, 0.0202]])",
            "-self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5053]]), rtol=1e-3, atol=1e-3))"
        ],
        "pos_line": [
            "+expected_scores = tf.convert_to_tensor([[0.0029, 0.9971]])",
            "+self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5162]]), rtol=1e-3, atol=1e-3))"
        ],
        "core_change": "-expected_scores = tf.convert_to_tensor([[0.9798, 0.0202]]) +expected_scores = tf.convert_to_tensor([[0.0029, 0.9971]]) -self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5053]]), rtol=1e-3, atol=1e-3)) +self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5162]]), rtol=1e-3, atol=1e-3))",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "eb75f7d33830c06273cb5886b458e1a85fde041c",
        "index": "dd39028..70d0896 100644",
        "commit_message": "fix small issues in the code refactoring\n\n",
        "file": "pytorch-CycleGAN-and-pix2pix.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BaseModel():",
            "save_filename = '%s_net_%s.pth' % (which_epoch, name)",
            "save_path = os.path.join(self.save_dir, save_filename)",
            "net = getattr(self, 'net' + name)",
            "-                net.load_state_dict(torch.load(save_path))",
            "+                net.module.load_state_dict(torch.load(save_path))",
            "",
            "# print network information",
            "def print_networks(self, verbose):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=888022)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=888023)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=net), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'module'), position=2, insert_id=888024)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 323,
        "neg_line": [
            "-net.load_state_dict(torch.load(save_path))"
        ],
        "pos_line": [
            "+net.module.load_state_dict(torch.load(save_path))"
        ],
        "core_change": "-net.load_state_dict(torch.load(save_path)) +net.module.load_state_dict(torch.load(save_path))",
        "core_API": "join"
    },
    {
        "commit_hash": "66eb98c1575963c589b1bb88c7c66f9716aa52ba",
        "index": "4af552b..ce9b5eb 100644",
        "commit_message": "Fix dynamic info extraction for stable diffusion\n\n",
        "file": "nebullvm.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SpeedsterRootOp(Operation):",
            ") -> List[BaseInferenceLearner]:",
            "if self.orig_latency_measure_op.get_result() is not None:",
            "model_outputs = self.orig_latency_measure_op.get_result()[0]",
            "-            if isinstance(model, Module):",
            "+            if isinstance(model, torch.nn.Module):",
            "optimization_op = self.torch_optimization_op",
            "elif isinstance(model, tf.Module) and model is not None:",
            "optimization_op = self.tensorflow_optimization_op"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1428117)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1428118)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1428119)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Module), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1428120)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1428121)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1428122)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 324,
        "neg_line": [
            "-if isinstance(model, Module):"
        ],
        "pos_line": [
            "+if isinstance(model, torch.nn.Module):"
        ],
        "core_change": "-if isinstance(model, Module): +if isinstance(model, torch.nn.Module):",
        "core_API": "get_result"
    },
    {
        "commit_hash": "c215878f11d81808dfe4721795f0c105200e6601",
        "index": "202a957..b0d98cc 100644",
        "commit_message": "YOLOv5 Apple Metal Performance Shader (MPS) support (#7878)\n\n* Apple Metal Performance Shader (MPS) device support\n\nFollowing https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\n\nShould work with Apple M1 devices with PyTorch nightly installed with command `--device mps`. Usage examples:\n```bash\npython train.py --device mps\npython detect.py --device mps\npython val.py --device mps\n```\n\n* Update device strategy to fix MPS issue\n",
        "file": "yolov5.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run(",
            "):",
            "# PyTorch model",
            "im = torch.zeros((batch_size, 3, *imgsz))  # BCHW image",
            "-    model = attempt_load(weights, map_location=torch.device('cpu'), inplace=True, fuse=False)",
            "+    model = attempt_load(weights, device=torch.device('cpu'), inplace=True, fuse=False)",
            "_ = model(im)  # inference",
            "model.info()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=map_location), value='device')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 325,
        "neg_line": [
            "-model = attempt_load(weights, map_location=torch.device('cpu'), inplace=True, fuse=False)"
        ],
        "pos_line": [
            "+model = attempt_load(weights, device=torch.device('cpu'), inplace=True, fuse=False)"
        ],
        "core_change": "-model = attempt_load(weights, map_location=torch.device('cpu'), inplace=True, fuse=False) +model = attempt_load(weights, device=torch.device('cpu'), inplace=True, fuse=False)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "618dca0895f5f4ede19f5feebb064648e128e12e",
        "index": "8c71e85c..9e928250 100644",
        "commit_message": "Fix AnchorHead in_channels (#1506)\n\n* test that all configs can be loaded\n\n* Use in_channels correctly in anchor_head and guided_anchor_head\n\n* Fix lint errors. Only tests a subset of configs\n\n* remove local config\n\n* fix yapf\n\n* Remove slower tests\n\n* Remove debug code\n\n* trigger travis\n\n",
        "file": "mmdetection.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class GuidedAnchorHead(AnchorHead):",
            "",
            "def _init_layers(self):",
            "self.relu = nn.ReLU(inplace=True)",
            "-        self.conv_loc = nn.Conv2d(self.feat_channels, 1, 1)",
            "-        self.conv_shape = nn.Conv2d(self.feat_channels, self.num_anchors * 2,",
            "-                                    1)",
            "+        self.conv_loc = nn.Conv2d(self.in_channels, 1, 1)",
            "+        self.conv_shape = nn.Conv2d(self.in_channels, self.num_anchors * 2, 1)",
            "self.feature_adaption = FeatureAdaption(",
            "-            self.feat_channels,",
            "+            self.in_channels,",
            "self.feat_channels,",
            "kernel_size=3,",
            "deformable_groups=self.deformable_groups)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1427182)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1427183)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1427184)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Conv2d'), position=2, insert_id=1427185)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1427186)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1427187)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1427188)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'in_channels'), position=2, insert_id=1427189)",
            "Update(target_node=ASTNode(type=identifier, text=feat_channels), value='in_channels')",
            "Update(target_node=ASTNode(type=identifier, text=feat_channels), value='in_channels')",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Conv2d))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=feat_channels))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 20,
        "number": 327,
        "neg_line": [
            "-self.conv_loc = nn.Conv2d(self.feat_channels, 1, 1)",
            "-self.conv_shape = nn.Conv2d(self.feat_channels, self.num_anchors * 2,",
            "-1)",
            "-self.feat_channels,"
        ],
        "pos_line": [
            "+self.conv_loc = nn.Conv2d(self.in_channels, 1, 1)",
            "+self.conv_shape = nn.Conv2d(self.in_channels, self.num_anchors * 2, 1)",
            "+self.in_channels,"
        ],
        "core_change": "-self.conv_loc = nn.Conv2d(self.feat_channels, 1, 1) -self.conv_shape = nn.Conv2d(self.feat_channels, self.num_anchors * 2, -1) +self.conv_loc = nn.Conv2d(self.in_channels, 1, 1) +self.conv_shape = nn.Conv2d(self.in_channels, self.num_anchors * 2, 1) -self.feat_channels, +self.in_channels,",
        "core_API": "ReLU"
    },
    {
        "commit_hash": "5dda1735fda047f4242d28f91e6e457b9760d52d",
        "index": "774b3502..ac1a8a9b 100644",
        "commit_message": "Inference support for `mps` device (#355)\n\n* Initial support for mps in Stable Diffusion pipeline.\n\n* Initial \"warmup\" implementation when using mps.\n\n* Make some deterministic tests pass with mps.\n\n* Disable training tests when using mps.\n\n* SD: generate latents in CPU then move to device.\n\nThis is especially important when using the mps device, because\ngenerators are not supported there. See for example\nhttps://github.com/pytorch/pytorch/issues/84288.\n\nIn addition, the other pipelines seem to use the same approach: generate\nthe random samples then move to the appropriate device.\n\nAfter this change, generating an image in MPS produces the same result\nas when using the CPU, if the same seed is used.\n\n* Remove prints.\n\n* Pass AutoencoderKL test_output_pretrained with mps.\n\nSampling from `posterior` must be done in CPU.\n\n* Style\n\n* Do not use torch.long for log op in mps device.\n\n* Perform incompatible padding ops in CPU.\n\nUNet tests now pass.\nSee https://github.com/pytorch/pytorch/issues/84535\n\n* Style: fix import order.\n\n* Remove unused symbols.\n\n* Remove MPSWarmupMixin, do not apply automatically.\n\nWe do apply warmup in the tests, but not during normal use.\nThis adopts some PR suggestions by @patrickvonplaten.\n\n* Add comment for mps fallback to CPU step.\n\n* Add README_mps.md for mps installation and use.\n\n* Apply `black` to modified files.\n\n* Restrict README_mps to SD, show measures in table.\n\n* Make PNDM indexing compatible with mps.\n\nAddresses #239.\n\n* Do not use float64 when using LDMScheduler.\n\nFixes #358.\n\n* Fix typo identified by @patil-suraj\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* Adapt example to new output style.\n\n* Restore 1:1 results reproducibility with CompVis.\n\nHowever, mps latents need to be generated in CPU because generators\ndon't work in the mps device.\n\n* Move PyTorch nightly to requirements.\n\n* Adapt `test_scheduler_outputs_equivalence` ton MPS.\n\n* mps: skip training tests instead of ignoring silently.\n\n* Make VQModel tests pass on mps.\n\n* mps ddim tests: warmup, increase tolerance.\n\n* ScoreSdeVeScheduler indexing made mps compatible.\n\n* Make ldm pipeline tests pass using warmup.\n\n* Style\n\n* Simplify casting as suggested in PR.\n\n* Add Known Issues to readme.\n\n* `isort` import order.\n\n* Remove _mps_warmup helpers from ModelMixin.\n\nAnd just make changes to the tests.\n\n* Skip tests using unittest decorator for consistency.\n\n* Remove temporary var.\n\n* Remove spurious blank space.\n\n* Remove unused symbol.\n\n* Remove README_mps.\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com> \n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UNet2DConditionModel(ModelMixin, ConfigMixin):",
            "if not torch.is_tensor(timesteps):",
            "timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)",
            "elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:",
            "-            timesteps = timesteps[None].to(sample.device)",
            "+            timesteps = timesteps.to(dtype=torch.float32)",
            "+            timesteps = timesteps[None].to(device=sample.device)",
            "",
            "# broadcast to batch dimension in a way that's compatible with ONNX/Core ML",
            "timesteps = timesteps.expand(sample.shape[0])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=105043)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=105044)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=105045)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=105046)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=105047)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'timesteps'), position=0, insert_id=105048)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=105049)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'timesteps'), position=0, insert_id=105050)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=105051)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=105052)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=105053)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=105054)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=105055)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=105056)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=105057)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=105058)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=105059)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=105060)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=105061)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=105062)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=105063)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=105064)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 328,
        "neg_line": [
            "-timesteps = timesteps[None].to(sample.device)"
        ],
        "pos_line": [
            "+timesteps = timesteps.to(dtype=torch.float32)",
            "+timesteps = timesteps[None].to(device=sample.device)"
        ],
        "core_change": "-timesteps = timesteps[None].to(sample.device) +timesteps = timesteps.to(dtype=torch.float32) +timesteps = timesteps[None].to(device=sample.device)",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "cf1b8c34ccbb29bdb2fb38ebf13baf94ec7027b5",
        "index": "279614371..6228cb51f 100644",
        "commit_message": "Fix donut image processor (#20625)\n\n* fix donut image processor\n\n* Update test values\n\n* Apply lower bound on resizing size\n\n* Add in missing size param\n\n* Resolve resize channel_dimension bug\n\n* Update src/transformers/image_transforms.py\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DonutModelIntegrationTest(unittest.TestCase):",
            "self.assertEqual(len(outputs.scores), 11)",
            "self.assertTrue(",
            "torch.allclose(",
            "-                outputs.scores[0][0, :3], torch.tensor([5.3153, -3.5276, 13.4781], device=torch_device), atol=1e-4",
            "+                outputs.scores[0][0, :3], torch.tensor([5.6019, -3.5070, 13.7123], device=torch_device), atol=1e-4",
            ")",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=5.3153), value='5.6019')",
            "Update(target_node=ASTNode(type=float, text=13.4781), value='13.7123')",
            "Update(target_node=ASTNode(type=float, text=3.5276), value='3.5070')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 330,
        "neg_line": [
            "-outputs.scores[0][0, :3], torch.tensor([5.3153, -3.5276, 13.4781], device=torch_device), atol=1e-4"
        ],
        "pos_line": [
            "+outputs.scores[0][0, :3], torch.tensor([5.6019, -3.5070, 13.7123], device=torch_device), atol=1e-4"
        ],
        "core_change": "-outputs.scores[0][0, :3], torch.tensor([5.3153, -3.5276, 13.4781], device=torch_device), atol=1e-4 +outputs.scores[0][0, :3], torch.tensor([5.6019, -3.5070, 13.7123], device=torch_device), atol=1e-4",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "d38d22bf827896e8d8a8ec608c6c4585c52c1856",
        "index": "6b084851..297823fe 100644",
        "commit_message": "keep the '-summary' suffix\n\n",
        "file": "tensorpack.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def add_moving_summary(*args, **kwargs):",
            "ema_ops.append(ema_op)",
            "with tf.name_scope(None):",
            "# cannot add it into colocate group -- will force everything to cpus",
            "-            tf.summary.scalar(name, ema_op)    # write the EMA value as a summary",
            "+            tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "if coll is not None:",
            "for op in ema_ops:",
            "# TODO a new collection to summary every step?"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=2294543)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=name), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2294544)",
            "Insert(target_node=IN(type=binary_operator), node=('string', \"'-summary'\"), position=2, insert_id=2294545)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 332,
        "neg_line": [
            "-tf.summary.scalar(name, ema_op)    # write the EMA value as a summary"
        ],
        "pos_line": [
            "+tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary"
        ],
        "core_change": "-tf.summary.scalar(name, ema_op)    # write the EMA value as a summary +tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
        "core_API": "append"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "85a9f8c8..3f495c55 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BilinearSimilarity(SimilarityFunction):",
            "self.reset_parameters()",
            "",
            "def reset_parameters(self):",
            "-        torch.nn.init.xavier_uniform(self._weight_matrix)",
            "+        torch.nn.init.xavier_uniform_(self._weight_matrix)",
            "self._bias.data.fill_(0)",
            "",
            "@overrides"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=xavier_uniform), value='xavier_uniform_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 333,
        "neg_line": [
            "-torch.nn.init.xavier_uniform(self._weight_matrix)"
        ],
        "pos_line": [
            "+torch.nn.init.xavier_uniform_(self._weight_matrix)"
        ],
        "core_change": "-torch.nn.init.xavier_uniform(self._weight_matrix) +torch.nn.init.xavier_uniform_(self._weight_matrix)",
        "core_API": "reset_parameters"
    },
    {
        "commit_hash": "f8798c1c13bece96a0fd92d9b7e7bd69576fa34d",
        "index": "62dcc8e0..b758a110 100755",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UniformRaySampler(RaySampler):",
            "self._calc_ray_params(cameras, points_2d_camera)",
            "",
            "",
            "-def sample_lengths(num_rays: int, num_ray_points: int, device: Device, dtype: torch.dtype, irregular=False) -> Tensor:",
            "+def sample_lengths(",
            "+    num_rays: int, num_ray_points: int, device: Device, dtype: torch.dtype, irregular: bool = False",
            "+) -> Tensor:",
            "if num_ray_points <= 1:",
            "raise ValueError('Number of ray points must be greater than 1')",
            "if not irregular:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=9, insert_id=388428)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=irregular), position=0)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=388429)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=388430)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type==, text==), position=3)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=false, text=False), position=4)",
            "Insert(target_node=IN(type=type), node=('identifier', 'bool'), position=0, insert_id=388431)",
            "Delete(target_node=ASTNode(type=default_parameter))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 8,
        "number": 334,
        "neg_line": [
            "-def sample_lengths(num_rays: int, num_ray_points: int, device: Device, dtype: torch.dtype, irregular=False) -> Tensor:"
        ],
        "pos_line": [
            "+def sample_lengths(",
            "+num_rays: int, num_ray_points: int, device: Device, dtype: torch.dtype, irregular: bool = False",
            "+) -> Tensor:"
        ],
        "core_change": "-def sample_lengths(num_rays: int, num_ray_points: int, device: Device, dtype: torch.dtype, irregular=False) -> Tensor: +def sample_lengths( +num_rays: int, num_ray_points: int, device: Device, dtype: torch.dtype, irregular: bool = False +) -> Tensor:",
        "core_API": "_calc_ray_params"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "8d0fa11e5..aa445726a 100644",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class AdaptiveEmbedding(nn.Module):",
            "",
            "inp_i = inp_flat.index_select(0, indices_i) - l_idx",
            "emb_i = self.emb_layers[i](inp_i)",
            "-                emb_i = F.linear(emb_i, self.emb_projs[i])",
            "+                emb_i = nn.functional.linear(emb_i, self.emb_projs[i])",
            "",
            "emb_flat.index_copy_(0, indices_i, emb_i)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1538421)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1538422)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=F), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1538423)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 335,
        "neg_line": [
            "-emb_i = F.linear(emb_i, self.emb_projs[i])"
        ],
        "pos_line": [
            "+emb_i = nn.functional.linear(emb_i, self.emb_projs[i])"
        ],
        "core_change": "-emb_i = F.linear(emb_i, self.emb_projs[i]) +emb_i = nn.functional.linear(emb_i, self.emb_projs[i])",
        "core_API": "index_select"
    },
    {
        "commit_hash": "fa8337de966f77749a1fe80abd86f68f80146515",
        "index": "8488bc70..1d554d2c 100644",
        "commit_message": "fix and use corr2d without tf cast\n\n",
        "file": "d2l-en.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class Residual(tf.keras.Model):  #@save",
            "if self.conv3 is not None:",
            "X = self.conv3(X)",
            "Y += X",
            "-        return tf.keras.activations.relu(Y + X)",
            "+        return tf.keras.activations.relu(Y)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=Y), position=1)",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=identifier, text=X))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 337,
        "neg_line": [
            "-return tf.keras.activations.relu(Y + X)"
        ],
        "pos_line": [
            "+return tf.keras.activations.relu(Y)"
        ],
        "core_change": "-return tf.keras.activations.relu(Y + X) +return tf.keras.activations.relu(Y)",
        "core_API": "conv3"
    },
    {
        "commit_hash": "6b205928c95cfc5f6af32f8a71b2a3d4d92d58cb",
        "index": "dd119377..e8828580 100644",
        "commit_message": "fix empty partitions in tests/integration_tests/test_preprocessing.py::test_dask_known_divisions (#2310)\n\n\n",
        "file": "ludwig.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_with_split(backend, csv_filename, tmpdir):",
            "def test_dask_known_divisions(feature_fn, csv_filename, tmpdir):",
            "import dask.dataframe as dd",
            "",
            "-    num_examples = NUM_EXAMPLES",
            "-",
            "input_features = [feature_fn(os.path.join(tmpdir, \"generated_output\"))]",
            "output_features = [category_feature(vocab_size=5, reduce_input=\"sum\")]",
            "-    data_csv = generate_data(",
            "-        input_features, output_features, os.path.join(tmpdir, csv_filename), num_examples=num_examples",
            "-    )",
            "-    data_df = dd.from_pandas(pd.read_csv(data_csv), npartitions=10)",
            "+",
            "+    # num_examples=100 and npartitions=2 to ensure the test is not flaky, by having non-empty post-split datasets.",
            "+    data_csv = generate_data(input_features, output_features, os.path.join(tmpdir, csv_filename), num_examples=100)",
            "+    data_df = dd.from_pandas(pd.read_csv(data_csv), npartitions=2)",
            "assert data_df.known_divisions",
            "",
            "config = {"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('integer', '100'), position=2, insert_id=1796121)",
            "Update(target_node=ASTNode(type=integer, text=10), value='2')",
            "Delete(target_node=ASTNode(type=identifier, text=num_examples))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=NUM_EXAMPLES))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=num_examples))"
        ],
        "plus_line": 3,
        "minus_line": 5,
        "AST_diff_line": 8,
        "number": 338,
        "neg_line": [
            "-num_examples = NUM_EXAMPLES",
            "-",
            "-data_csv = generate_data(",
            "-input_features, output_features, os.path.join(tmpdir, csv_filename), num_examples=num_examples",
            "-)",
            "-data_df = dd.from_pandas(pd.read_csv(data_csv), npartitions=10)"
        ],
        "pos_line": [
            "+",
            "+# num_examples=100 and npartitions=2 to ensure the test is not flaky, by having non-empty post-split datasets.",
            "+data_csv = generate_data(input_features, output_features, os.path.join(tmpdir, csv_filename), num_examples=100)",
            "+data_df = dd.from_pandas(pd.read_csv(data_csv), npartitions=2)"
        ],
        "core_change": "-num_examples = NUM_EXAMPLES - -data_csv = generate_data( -input_features, output_features, os.path.join(tmpdir, csv_filename), num_examples=num_examples -) -data_df = dd.from_pandas(pd.read_csv(data_csv), npartitions=10) + +# num_examples=100 and npartitions=2 to ensure the test is not flaky, by having non-empty post-split datasets. +data_csv = generate_data(input_features, output_features, os.path.join(tmpdir, csv_filename), num_examples=100) +data_df = dd.from_pandas(pd.read_csv(data_csv), npartitions=2)",
        "core_API": "join"
    },
    {
        "commit_hash": "c088e2a6e29b6913cd8124f04ea6cd8655da2421",
        "index": "03cb59fd..d5f62090 100644",
        "commit_message": "fix doc build. make MergeAllSummaries a Triggerable\n\n",
        "file": "tensorpack.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "sys.path.insert(0, os.path.abspath('../'))",
            "os.environ['TENSORPACK_DOC_BUILDING'] = '1'",
            "",
            "",
            "-MOCK_MODULES = ['scipy',",
            "-                #'tensorflow', 'tensorflow.contrib',",
            "-                #'tensorflow.python.ops',",
            "-                #'tensorflow.contrib.framework',",
            "-                #'tensorflow.python',",
            "-                #'tensorflow.python.training',",
            "+MOCK_MODULES = ['scipy', 'tabulate',",
            "'sklearn.datasets', 'sklearn',",
            "'scipy.misc', 'h5py', 'nltk',",
            "'cv2', 'scipy.io', 'dill', 'zmq', 'subprocess32', 'lmdb',"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=ERROR), position=15)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=4, insert_id=2459637)",
            "Insert(target_node=ASTNode(type=ERROR), node=('string', \"'tabulate'\"), position=5, insert_id=2459638)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=6, insert_id=2459639)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 0,
        "minus_line": 5,
        "AST_diff_line": 5,
        "number": 339,
        "neg_line": [
            "-MOCK_MODULES = ['scipy',",
            "-#'tensorflow', 'tensorflow.contrib',",
            "-#'tensorflow.python.ops',",
            "-#'tensorflow.contrib.framework',",
            "-#'tensorflow.python',",
            "-#'tensorflow.python.training',"
        ],
        "pos_line": [
            "+MOCK_MODULES = ['scipy', 'tabulate',"
        ],
        "core_change": "-MOCK_MODULES = ['scipy', -#'tensorflow', 'tensorflow.contrib', -#'tensorflow.python.ops', -#'tensorflow.contrib.framework', -#'tensorflow.python', -#'tensorflow.python.training', +MOCK_MODULES = ['scipy', 'tabulate',",
        "core_API": "insert"
    },
    {
        "commit_hash": "d3ba34ee7e147804d694bf5955d74156105a4262",
        "index": "3a10d71..56215cc 100644",
        "commit_message": "Fix Mobilenet V3 model name for sotabench. Minor res2net cleanup.\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "for m in model_list:",
            "data_root=os.environ.get('IMAGENET_DIR', './imagenet')",
            ")",
            "",
            "+    torch.cuda.empty_cache()",
            "+"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=0, insert_id=1863910)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=for_statement), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=expression_statement), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=ERROR), position=2)",
            "Insert(target_node=IN(type=ERROR), node=('expression_statement', None), position=3, insert_id=1863911)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1863912)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1863913)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1863914)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1863915)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1863916)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'empty_cache'), position=2, insert_id=1863917)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1863918)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1863919)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1863920)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1863921)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1863922)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 341,
        "neg_line": [],
        "pos_line": [
            "+torch.cuda.empty_cache()",
            "+"
        ],
        "core_change": "+torch.cuda.empty_cache() +",
        "core_API": "get"
    },
    {
        "commit_hash": "af8425b749439eec00a886e635827a65b302a54d",
        "index": "d8cb4d296..95b78a525 100644",
        "commit_message": "Refactoring the TF activations functions (#7150)\n\n* Refactoring the activations functions into a common file\n\n* Apply style\n\n* remove unused import\n\n* fix tests\n\n* Fix tests.\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFMLP(tf.keras.layers.Layer):",
            "nx = config.n_embd",
            "self.c_fc = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_fc\")",
            "self.c_proj = TFConv1D(nx, n_state, initializer_range=config.initializer_range, name=\"c_proj\")",
            "-        self.act = gelu",
            "+        self.act = get_tf_activation(\"gelu\")",
            "self.dropout = tf.keras.layers.Dropout(config.resid_pdrop)",
            "",
            "def call(self, x, training=False):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2380109)",
            "Update(target_node=ASTNode(type=identifier, text=gelu), value='get_tf_activation')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=gelu), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2380110)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2380111)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"gelu\"'), position=1, insert_id=2380112)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2380113)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 342,
        "neg_line": [
            "-self.act = gelu"
        ],
        "pos_line": [
            "+self.act = get_tf_activation(\"gelu\")"
        ],
        "core_change": "-self.act = gelu +self.act = get_tf_activation(\"gelu\")",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "d364fdbb269891d4c81d94b37d130eab34dcc1eb",
        "index": "3e9a48d1..36fcc445 100644",
        "commit_message": "Reland BT enablement on fairseq - fairseq change (#4513)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/fairseq/pull/4513\nWith some fixes to torchscript using dual copies.\nReland this diff.\n\nReviewed By: erichan1\n\nDifferential Revision: D37371293\n\nfbshipit-source-id: 4fcfc4083955b6f5fc4ef8600f1b517b6ba69aae\n\n",
        "file": "fairseq.txt.json",
        "label": "no",
        "comments": "add custom API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestExportModels(unittest.TestCase):",
            "_test_save_and_load(scripted)",
            "",
            "@unittest.skipIf(",
            "-        torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\"",
            "+        version_check(),",
            "+        \"Targeting OSS scriptability for the 1.13.0.dev20220613 release\",",
            ")",
            "def test_export_transformer_no_token_pos_emb(self):",
            "task, parser = get_dummy_task_and_parser()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=203499)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=203500)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=203501)",
            "Update(target_node=ASTNode(type=string, text=\"1.6.0\"), value='\"Targeting OSS scriptability for the 1.13.0.dev20220613 release\"')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=string, text=\"1.6.0\"), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='version_check')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=203502)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=203503)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__version__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=<, text=<))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=string, text=\"Targeting OSS scriptability for the 1.6 release\"))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 346,
        "neg_line": [
            "-torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\""
        ],
        "pos_line": [
            "+version_check(),",
            "+\"Targeting OSS scriptability for the 1.13.0.dev20220613 release\","
        ],
        "core_change": "-torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\" +version_check(), +\"Targeting OSS scriptability for the 1.13.0.dev20220613 release\",",
        "core_API": "skipIf"
    },
    {
        "commit_hash": "aa8bb815a743d3d14fa07c62434b5cdcee87b938",
        "index": "10953269..22e86ee4 100644",
        "commit_message": "fix mozilla/TTS#685\n\n",
        "file": "TTS.txt.json",
        "label": "yes",
        "comments": "remove param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TacotronAbstract(ABC, nn.Module):",
            "def _backward_pass(self, mel_specs, encoder_outputs, mask):",
            "\"\"\" Run backwards decoder \"\"\"",
            "decoder_outputs_b, alignments_b, _ = self.decoder_backward(",
            "-            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask,",
            "-            self.speaker_embeddings_projected)",
            "+            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask)",
            "decoder_outputs_b = decoder_outputs_b.transpose(1, 2).contiguous()",
            "return decoder_outputs_b, alignments_b"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=speaker_embeddings_projected))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 347,
        "neg_line": [
            "-encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask,",
            "-self.speaker_embeddings_projected)"
        ],
        "pos_line": [
            "+encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask)"
        ],
        "core_change": "-encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask, -self.speaker_embeddings_projected) +encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask)",
        "core_API": "decoder_backward"
    },
    {
        "commit_hash": "b6495c2b0562fd7d14592a96fb0ab6efe37ee114",
        "index": "26f3ee86..f78e8967 100644",
        "commit_message": "fix test ddp initialize (#2100)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "model = MMDataParallel(model, device_ids=[0])",
            "outputs = single_gpu_test(model, data_loader, args.show)",
            "else:",
            "-        model = MMDistributedDataParallel(model.cuda())",
            "+        model = MMDistributedDataParallel(",
            "+            model.cuda(),",
            "+            device_ids=[torch.cuda.current_device()],",
            "+            broadcast_buffers=False)",
            "outputs = multi_gpu_test(model, data_loader, args.tmpdir,",
            "args.gpu_collect)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=640981)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=640982)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=640983)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=640984)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=6, insert_id=640985)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device_ids'), position=0, insert_id=640986)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=640987)",
            "Insert(target_node=IN(type=keyword_argument), node=('list', None), position=2, insert_id=640988)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'broadcast_buffers'), position=0, insert_id=640989)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=640990)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=640991)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=640992)",
            "Insert(target_node=IN(type=list), node=('call', None), position=1, insert_id=640993)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=640994)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=640995)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=640996)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=640997)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640998)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'current_device'), position=2, insert_id=640999)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=641000)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=641001)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=641002)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=641003)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 348,
        "neg_line": [
            "-model = MMDistributedDataParallel(model.cuda())"
        ],
        "pos_line": [
            "+model = MMDistributedDataParallel(",
            "+model.cuda(),",
            "+device_ids=[torch.cuda.current_device()],",
            "+broadcast_buffers=False)"
        ],
        "core_change": "-model = MMDistributedDataParallel(model.cuda()) +model = MMDistributedDataParallel( +model.cuda(), +device_ids=[torch.cuda.current_device()], +broadcast_buffers=False)",
        "core_API": "cuda"
    },
    {
        "commit_hash": "0afa5755b2212a783f861f03006a73f883100e81",
        "index": "a85f89d0..b90537bb 100644",
        "commit_message": "Fixes warnings and add compatibility stub in torch solve (#1235)\n\n* add a compatibiliy stub for torch solve\n\n* change missing _torch_solve_cast calls\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bump pytorch version to 1.7.1\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "change API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_tps_transform(points_src: torch.Tensor, points_dst: torch.Tensor) -> Tup",
            "l_matrix: torch.Tensor = torch.cat((k_matrix, p_matrix), -1)",
            "l_matrix = torch.cat((l_matrix, p_matrix_t), 1)",
            "",
            "-    weights, _ = torch.solve(dest_with_zeros, l_matrix)",
            "+    weights, _ = _torch_solve_cast(dest_with_zeros, l_matrix)",
            "kernel_weights: torch.Tensor = weights[:, :-3]",
            "affine_weights: torch.Tensor = weights[:, -3:]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_solve_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=solve))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 349,
        "neg_line": [
            "-weights, _ = torch.solve(dest_with_zeros, l_matrix)"
        ],
        "pos_line": [
            "+weights, _ = _torch_solve_cast(dest_with_zeros, l_matrix)"
        ],
        "core_change": "-weights, _ = torch.solve(dest_with_zeros, l_matrix) +weights, _ = _torch_solve_cast(dest_with_zeros, l_matrix)",
        "core_API": "cat"
    },
    {
        "commit_hash": "b562b6611fb53dae9bcffcaaf44d944539ae22de",
        "index": "37d4a50e..51ace485 100644",
        "commit_message": "Allow directly passing text embeddings to Stable Diffusion Pipeline for prompt weighting (#2071)\n\n* add text embeds to sd\n\n* add text embeds to sd\n\n* finish tests\n\n* finish\n\n* finish\n\n* make style\n\n* fix tests\n\n* make style\n\n* make style\n\n* up\n\n* better docs\n\n* fix\n\n* fix\n\n* new try\n\n* up\n\n* up\n\n* finish\n",
        "file": "diffusers.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionImageVariationPipeline(DiffusionPipeline):",
            "image_embeddings = image_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)",
            "",
            "if do_classifier_free_guidance:",
            "-            uncond_embeddings = torch.zeros_like(image_embeddings)",
            "+            negative_prompt_embeds = torch.zeros_like(image_embeddings)",
            "",
            "# For classifier free guidance, we need to do two forward passes.",
            "# Here we concatenate the unconditional and text embeddings into a single batch",
            "# to avoid doing two forward passes",
            "-            image_embeddings = torch.cat([uncond_embeddings, image_embeddings])",
            "+            image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])",
            "",
            "return image_embeddings"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uncond_embeddings), value='negative_prompt_embeds')",
            "Update(target_node=ASTNode(type=identifier, text=uncond_embeddings), value='negative_prompt_embeds')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 350,
        "neg_line": [
            "-uncond_embeddings = torch.zeros_like(image_embeddings)",
            "-image_embeddings = torch.cat([uncond_embeddings, image_embeddings])"
        ],
        "pos_line": [
            "+negative_prompt_embeds = torch.zeros_like(image_embeddings)",
            "+image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])"
        ],
        "core_change": "-uncond_embeddings = torch.zeros_like(image_embeddings) +negative_prompt_embeds = torch.zeros_like(image_embeddings) -image_embeddings = torch.cat([uncond_embeddings, image_embeddings]) +image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])",
        "core_API": "view"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "35ff838dec..0c145757c7 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AsyncSamplesOptimizerTest(unittest.TestCase):",
            "",
            "def _make_envs(self):",
            "def make_sess():",
            "-            return tf.Session(config=tf.ConfigProto(device_count={\"CPU\": 2}))",
            "+            return tf1.Session(config=tf1.ConfigProto(device_count={\"CPU\": 2}))",
            "",
            "local = RolloutWorker(",
            "env_creator=lambda _: gym.make(\"CartPole-v0\"),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 351,
        "neg_line": [
            "-return tf.Session(config=tf.ConfigProto(device_count={\"CPU\": 2}))"
        ],
        "pos_line": [
            "+return tf1.Session(config=tf1.ConfigProto(device_count={\"CPU\": 2}))"
        ],
        "core_change": "-return tf.Session(config=tf.ConfigProto(device_count={\"CPU\": 2})) +return tf1.Session(config=tf1.ConfigProto(device_count={\"CPU\": 2}))",
        "core_API": "Session"
    },
    {
        "commit_hash": "3922a2497e3a4f351ea6dae7d535c4e8801ff274",
        "index": "6cf93fa7a..9daf4124b 100644",
        "commit_message": "TF ALBERT + TF Utilities + Fix warnings\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "add anotation",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AlbertModel(AlbertPreTrainedModel):",
            "inner_group_idx = int(layer - group_idx * self.config.inner_group_num)",
            "self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)",
            "",
            "+    @add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING)",
            "def forward(",
            "self,",
            "input_ids=None,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('decorator', None), position=0, insert_id=1242807)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=1242808)",
            "Insert(target_node=IN(type=decorator), node=('call', None), position=1, insert_id=1242809)",
            "Insert(target_node=IN(type=call), node=('identifier', 'add_start_docstrings_to_callable'), position=0, insert_id=1242810)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1242811)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1242812)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'ALBERT_INPUTS_DOCSTRING'), position=1, insert_id=1242813)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1242814)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 8,
        "number": 352,
        "neg_line": [],
        "pos_line": [
            "+@add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING)"
        ],
        "core_change": "+@add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING)",
        "core_API": "prune_heads"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "25dd987f..e2705d35 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSelfAttentiveSpanExtractor:",
            "extractor._global_attention._module.weight.data.fill_(0.0)",
            "extractor._global_attention._module.bias.data.fill_(0.0)",
            "",
            "-        indices = torch.LongTensor([[[1, 3],",
            "-                                     [2, 4]],",
            "-                                    [[0, 2],",
            "-                                     [3, 4]]]) # smaller span tests masking.",
            "+        indices = torch.LongTensor(",
            "+            [[[1, 3], [2, 4]], [[0, 2], [3, 4]]]",
            "+        )  # smaller span tests masking.",
            "span_representations = extractor(sequence_tensor, indices)",
            "assert list(span_representations.size()) == [2, 2, input_dim]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 353,
        "neg_line": [
            "-indices = torch.LongTensor([[[1, 3],",
            "-[2, 4]],",
            "-[[0, 2],",
            "-[3, 4]]]) # smaller span tests masking."
        ],
        "pos_line": [
            "+indices = torch.LongTensor(",
            "+[[[1, 3], [2, 4]], [[0, 2], [3, 4]]]",
            "+)  # smaller span tests masking."
        ],
        "core_change": "-indices = torch.LongTensor([[[1, 3], -[2, 4]], -[[0, 2], -[3, 4]]]) # smaller span tests masking. +indices = torch.LongTensor( +[[[1, 3], [2, 4]], [[0, 2], [3, 4]]] +)  # smaller span tests masking.",
        "core_API": "fill_"
    },
    {
        "commit_hash": "a403b8ef0ab2c6fdefc91faf66e0fc6394ff9737",
        "index": "e02c1c8f..4f2ca671 100644",
        "commit_message": "fix input shape in get_rotation_matrix2d\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "remove param for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Tester(unittest.TestCase):",
            "# generate input data",
            "batch_size = 1",
            "center = torch.zeros(batch_size, 2)",
            "-        angle = torch.ones(batch_size, 1)",
            "-        scale = torch.ones(batch_size, 1)",
            "+        angle = torch.ones(batch_size)",
            "+        scale = torch.ones(batch_size)",
            "",
            "center = utils.tensor_to_gradcheck_var(center)  # to var",
            "angle = utils.tensor_to_gradcheck_var(angle)  # to var"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=477882)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=477883)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=477884)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ones'), position=2, insert_id=477885)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ones))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 357,
        "neg_line": [
            "-angle = torch.ones(batch_size, 1)",
            "-scale = torch.ones(batch_size, 1)"
        ],
        "pos_line": [
            "+angle = torch.ones(batch_size)",
            "+scale = torch.ones(batch_size)"
        ],
        "core_change": "-angle = torch.ones(batch_size, 1) -scale = torch.ones(batch_size, 1) +angle = torch.ones(batch_size) +scale = torch.ones(batch_size)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "dd151a765fd9435ca343a230a6d5a523896692f1",
        "index": "80394b4da8..f8c4c25b72 100644",
        "commit_message": "Fixed issues with trace in Torch frontend\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_torch_trace(",
            "dtype_and_values,",
            "as_variable,",
            "num_positional_args,",
            "+    with_out,",
            "native_array,",
            "fw,",
            "):",
            "input_dtype, value = dtype_and_values",
            "-    # if \"float16\" in input_dtype:",
            "-    #    input_dtype = ivy.FloatDtype(\"float32\")  # Float16 is unsupported for trace.",
            "helpers.test_frontend_function(",
            "input_dtypes=input_dtype,",
            "as_variable_flags=as_variable,",
            "-        with_out=False,",
            "+        with_out=with_out,",
            "num_positional_args=num_positional_args,",
            "native_array_flags=native_array,",
            "fw=fw,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'with_out'), position=7, insert_id=329528)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=329529)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'with_out'), position=2, insert_id=329530)",
            "Delete(target_node=ASTNode(type=false, text=False))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 4,
        "number": 359,
        "neg_line": [
            "-# if \"float16\" in input_dtype:",
            "-#    input_dtype = ivy.FloatDtype(\"float32\")  # Float16 is unsupported for trace.",
            "-with_out=False,"
        ],
        "pos_line": [
            "+with_out,",
            "+with_out=with_out,"
        ],
        "core_change": "+with_out, -# if \"float16\" in input_dtype: -#    input_dtype = ivy.FloatDtype(\"float32\")  # Float16 is unsupported for trace. -with_out=False, +with_out=with_out,",
        "core_API": "FloatDtype"
    },
    {
        "commit_hash": "f71873c5fc59b49f456a81fb8e11cb9019db33b0",
        "index": "079696c24..d34449fa5 100755",
        "commit_message": "[deepspeed] check whether model is NLP one instead of counting on input type (#21800)\n\n* trying to figure out whether model is NLP\n\n* drop my changes and apply easier fix\n\n* trying to handle all int input types\n\n* fix logic\n\n---------\n\nCo-authored-by: Stas Bekman <stas@stason.org>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change condition check for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "return type(data)(self._prepare_input(v) for v in data)",
            "elif isinstance(data, torch.Tensor):",
            "kwargs = {\"device\": self.args.device}",
            "-            if self.deepspeed and data.dtype != torch.int64:",
            "-                # NLP models inputs are int64 and those get adjusted to the right dtype of the",
            "+            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):",
            "+                # NLP models inputs are int/uint and those get adjusted to the right dtype of the",
            "# embedding. Other models such as wav2vec2's inputs are already float and thus",
            "# may need special handling to match the dtypes of the model",
            "kwargs.update({\"dtype\": self.args.hf_deepspeed_config.dtype()})"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('parenthesized_expression', None), position=2, insert_id=1175308)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1175309)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=1175310)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1175311)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=1175312)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=1175313)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=2, insert_id=1175314)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1175315)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1175316)",
            "Update(target_node=ASTNode(type=identifier, text=data), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=dtype), value='is_floating_point')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1175317)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'data'), position=1, insert_id=1175318)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1175319)",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='is_complex')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1175320)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'data'), position=1, insert_id=1175321)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1175322)",
            "Delete(target_node=ASTNode(type=!=, text=!=))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 360,
        "neg_line": [
            "-if self.deepspeed and data.dtype != torch.int64:",
            "-# NLP models inputs are int64 and those get adjusted to the right dtype of the"
        ],
        "pos_line": [
            "+if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):",
            "+# NLP models inputs are int/uint and those get adjusted to the right dtype of the"
        ],
        "core_change": "-if self.deepspeed and data.dtype != torch.int64: -# NLP models inputs are int64 and those get adjusted to the right dtype of the +if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)): +# NLP models inputs are int/uint and those get adjusted to the right dtype of the",
        "core_API": "_prepare_input"
    },
    {
        "commit_hash": "62d71d62766ed6fab221fcc2f5f1abc07863a921",
        "index": "901333d92..27d195f08 100644",
        "commit_message": "fix: fix import of intents\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "no",
        "comments": "customized method ",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class HybridCodeNetworkModel(TFModel):",
            "self.obs_size = params['obs_size']",
            "",
            "def _build_graph(self):",
            "-        tf.reset_default_graph()",
            "-",
            "self._add_placeholders()",
            "",
            "# build body"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=1924679)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=reset_default_graph))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 361,
        "neg_line": [
            "-tf.reset_default_graph()",
            "-"
        ],
        "pos_line": [],
        "core_change": "-tf.reset_default_graph() -",
        "core_API": "reset_default_graph"
    },
    {
        "commit_hash": "24124709ca2c0e62d868d360e2fbdb73320db3d8",
        "index": "c1b71d629..9ba39f0a3 100644",
        "commit_message": "Fix torch device issues (#20584)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeformableDetrImageProcessor(BaseImageProcessor):",
            "img_w = torch.Tensor([i[1] for i in target_sizes])",
            "else:",
            "img_h, img_w = target_sizes.unbind(1)",
            "-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)",
            "+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)",
            "boxes = boxes * scale_fct[:, None, :]",
            "",
            "results = []"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1181864)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1181865)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1181866)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1181867)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1181868)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1181869)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1181870)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'boxes'), position=0, insert_id=1181871)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1181872)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1181873)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 362,
        "neg_line": [
            "-scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)"
        ],
        "pos_line": [
            "+scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)"
        ],
        "core_change": "-scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1) +scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "e4aaca1f..5600e1a9 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "change API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpanBasedF1Test(AllenNlpTestCase):",
            "gold_indices = [self.vocab.get_token_index(x, \"tags\") for x in bio_tags]",
            "gold_tensor = torch.tensor([gold_indices], device=device)",
            "prediction_tensor = torch.rand([1, 6, self.vocab.get_vocab_size(\"tags\")], device=device)",
            "-        mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device=device)",
            "+        mask = torch.BoolTensor(",
            "+            [[True, True, True, True, True, True, True, True, True]], device=device",
            "+        )",
            "",
            "# Make prediction so that it is exactly correct.",
            "for i, tag_index in enumerate(gold_indices):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='BoolTensor')",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=20127)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=4, insert_id=20128)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=6, insert_id=20129)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=9, insert_id=20130)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=11, insert_id=20131)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=14, insert_id=20132)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=16, insert_id=20133)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=18, insert_id=20134)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=19, insert_id=20135)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 363,
        "neg_line": [
            "-mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device=device)"
        ],
        "pos_line": [
            "+mask = torch.BoolTensor(",
            "+[[True, True, True, True, True, True, True, True, True]], device=device",
            "+)"
        ],
        "core_change": "-mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device=device) +mask = torch.BoolTensor( +[[True, True, True, True, True, True, True, True, True]], device=device +)",
        "core_API": "get_token_index"
    },
    {
        "commit_hash": "a62e68a3eb8c3397bc656acf86cbb75b2afecf2c",
        "index": "dd8c66d8..251d62ba 100644",
        "commit_message": "Remove prefix for first tower in replicated mode. Support inference now.\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "add condition check for null fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TowerContext(object):",
            "self._ctxs = []",
            "if len(self._name):",
            "if self.has_own_variables:",
            "-                # open new variable scopes",
            "-                self._ctxs.append(tf.variable_scope(self._name))",
            "+                if self.vs_name:",
            "+                    self._ctxs.append(tf.variable_scope(self.vs_name))",
            "else:",
            "# use existing variable scope",
            "reuse = self.index > 0 or (not self.is_training)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=2301347)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2301348)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=2301349)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2301350)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2301351)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2301352)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2301353)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'vs_name'), position=2, insert_id=2301354)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=_name), value='vs_name')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 364,
        "neg_line": [
            "-# open new variable scopes",
            "-self._ctxs.append(tf.variable_scope(self._name))"
        ],
        "pos_line": [
            "+if self.vs_name:",
            "+self._ctxs.append(tf.variable_scope(self.vs_name))"
        ],
        "core_change": "-# open new variable scopes -self._ctxs.append(tf.variable_scope(self._name)) +if self.vs_name: +self._ctxs.append(tf.variable_scope(self.vs_name))",
        "core_API": "append"
    },
    {
        "commit_hash": "1e0aadd5884959333c81685e6d6e31f46d5dda00",
        "index": "af54cdc..93c3324 100644",
        "commit_message": "Bug fix in update norm calculation\n\n",
        "file": "apex.txt.json",
        "label": "no",
        "comments": "rename method",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistributedFusedLAMB(torch.optim.Optimizer):",
            "l2_norm = torch.zeros(size=[self._model_params_num], dtype=torch.float32, device='cuda')",
            "local_contrib_l2_norm = multi_tensor_applier(self.multi_tensor_l2norm, self._overflow_buf, [self._contrib_update_frag_for_norm], True)[1] ** 2",
            "l2_norm.masked_scatter_(self._model_param_is_contrib, local_contrib_l2_norm)",
            "-        torch.distributed.allreduce(l2_norm, group=self._ag_pg[0])",
            "+        torch.distributed.all_reduce(l2_norm, group=self._ag_pg[0])",
            "return l2_norm.masked_select(self._model_param_is_contrib)",
            "",
            "def _pipeline_step(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=allreduce), value='all_reduce')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 366,
        "neg_line": [
            "-torch.distributed.allreduce(l2_norm, group=self._ag_pg[0])"
        ],
        "pos_line": [
            "+torch.distributed.all_reduce(l2_norm, group=self._ag_pg[0])"
        ],
        "core_change": "-torch.distributed.allreduce(l2_norm, group=self._ag_pg[0]) +torch.distributed.all_reduce(l2_norm, group=self._ag_pg[0])",
        "core_API": "zeros"
    },
    {
        "commit_hash": "28f91343d3f2ba3a8bc9ebd4b2c87bf2d9acab2c",
        "index": "749428fa..cd8792d3 100644",
        "commit_message": "Fixes shape issue in `_BinaryPostprocessing` (#2094)\n\n* Fixes shape issue in\n\n* adds regression test\n\nCo-authored-by: Geoffrey Angus <geoffrey@predibase.com>\n",
        "file": "ludwig.txt.json",
        "label": "yes",
        "comments": "add param for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _BinaryPostprocessing(torch.nn.Module):",
            "predictions = [self.bool2str.get(pred, self.bool2str[0]) for pred in predictions]",
            "",
            "probs = preds[self.probabilities_key]",
            "-        probs = torch.dstack(1 - probs, probs)",
            "+        probs = torch.stack([1 - probs, probs], dim=-1)",
            "",
            "return {",
            "self.predictions_key: predictions,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=dstack), value='stack')",
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=1, insert_id=603224)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=603225)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=603226)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=603227)",
            "Move(target_node=IN(type=list), node=ASTNode(type=binary_operator), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=probs), position=3)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=603228)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=603229)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=603230)",
            "Insert(target_node=IN(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=603231)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 367,
        "neg_line": [
            "-probs = torch.dstack(1 - probs, probs)"
        ],
        "pos_line": [
            "+probs = torch.stack([1 - probs, probs], dim=-1)"
        ],
        "core_change": "-probs = torch.dstack(1 - probs, probs) +probs = torch.stack([1 - probs, probs], dim=-1)",
        "core_API": "get"
    },
    {
        "commit_hash": "8ba73adbad0415f56378eb77f8d28ea7969fb87c",
        "index": "697ace9..c5d1a99 100644",
        "commit_message": "revert \"chore(core): update launch backend to subprocess (#158)\" (#176)\n\nThis reverts commit 9ac889 to fix training bug\n",
        "file": "YOLOX.txt.json",
        "label": "yes",
        "comments": "change API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if __name__ == \"__main__\":",
            "exp = get_exp(args.exp_file, args.name)",
            "exp.merge(args.opts)",
            "",
            "-    num_gpu = get_num_devices() if args.devices is None else args.devices",
            "-    assert num_gpu <= get_num_devices()",
            "+    num_gpu = torch.cuda.device_count() if args.devices is None else args.devices",
            "+    assert num_gpu <= torch.cuda.device_count()",
            "",
            "dist_url = \"auto\" if args.dist_url is None else args.dist_url",
            "launch("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1307722)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1307723)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1307724)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1307725)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1307726)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device_count'), position=2, insert_id=1307727)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1307728)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1307729)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device_count'), position=2, insert_id=1307730)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1307731)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1307732)",
            "Update(target_node=ASTNode(type=identifier, text=get_num_devices), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=get_num_devices), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1307733)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1307734)",
            "Update(target_node=ASTNode(type=identifier, text=get_num_devices), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=get_num_devices), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1307735)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1307736)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 368,
        "neg_line": [
            "-num_gpu = get_num_devices() if args.devices is None else args.devices",
            "-assert num_gpu <= get_num_devices()"
        ],
        "pos_line": [
            "+num_gpu = torch.cuda.device_count() if args.devices is None else args.devices",
            "+assert num_gpu <= torch.cuda.device_count()"
        ],
        "core_change": "-num_gpu = get_num_devices() if args.devices is None else args.devices -assert num_gpu <= get_num_devices() +num_gpu = torch.cuda.device_count() if args.devices is None else args.devices +assert num_gpu <= torch.cuda.device_count()",
        "core_API": "merge"
    },
    {
        "commit_hash": "16f5fa434902a3066ef11eb799c9900a824b4295",
        "index": "7077f6b4..b56dcd28 100644",
        "commit_message": "Fix tests failing on CUDA (#1834)\n\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "change API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_log_prob_eta1(d):",
            "assert (lps_less_ladj - lps_less_ladj.min()).abs().sum() < 1e-4",
            "",
            "",
            "-@pytest.mark.parametrize(\"eta\", [.1, .5, 1, 2, 5])",
            "+@pytest.mark.parametrize(\"eta\", [.1, .5, 1., 2., 5.])",
            "def test_log_prob_d2(eta):",
            "-    dist = LKJCorrCholesky(2, torch.DoubleTensor([eta]))",
            "+    dist = LKJCorrCholesky(2, torch.tensor([eta]))",
            "test_dist = TransformedDistribution(Beta(eta, eta), AffineTransform(loc=-1., scale=2.0))",
            "",
            "samples = dist.sample(torch.Size([100]))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('float', '1.'), position=5, insert_id=721231)",
            "Insert(target_node=ASTNode(type=list), node=('float', '2.'), position=8, insert_id=721232)",
            "Insert(target_node=ASTNode(type=list), node=('float', '5.'), position=10, insert_id=721233)",
            "Update(target_node=ASTNode(type=identifier, text=DoubleTensor), value='tensor')",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=integer, text=5))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 370,
        "neg_line": [
            "-@pytest.mark.parametrize(\"eta\", [.1, .5, 1, 2, 5])",
            "-dist = LKJCorrCholesky(2, torch.DoubleTensor([eta]))"
        ],
        "pos_line": [
            "+@pytest.mark.parametrize(\"eta\", [.1, .5, 1., 2., 5.])",
            "+dist = LKJCorrCholesky(2, torch.tensor([eta]))"
        ],
        "core_change": "-@pytest.mark.parametrize(\"eta\", [.1, .5, 1, 2, 5]) +@pytest.mark.parametrize(\"eta\", [.1, .5, 1., 2., 5.]) -dist = LKJCorrCholesky(2, torch.DoubleTensor([eta])) +dist = LKJCorrCholesky(2, torch.tensor([eta]))",
        "core_API": "min"
    },
    {
        "commit_hash": "2c6c46d59991f9b67e74223b8b1d0b0906d926a6",
        "index": "6872d83..d44439e 100644",
        "commit_message": "fix a bug in DDPG.py.\n\n",
        "file": "Reinforcement-learning-with-tensorflow.txt.json",
        "label": "no",
        "comments": "rename the variable",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Critic(object):",
            "self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)",
            "",
            "with tf.variable_scope('a_grad'):",
            "-            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)",
            "+            self.a_grads = tf.gradients(self.q, self.a)[0]   # tensor of gradients of each sample (None, a_dim)",
            "",
            "if self.replacement['name'] == 'hard':",
            "self.t_replace_counter = 0"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2157176)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2157177)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2157178)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=a), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 372,
        "neg_line": [
            "-self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)"
        ],
        "pos_line": [
            "+self.a_grads = tf.gradients(self.q, self.a)[0]   # tensor of gradients of each sample (None, a_dim)"
        ],
        "core_change": "-self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim) +self.a_grads = tf.gradients(self.q, self.a)[0]   # tensor of gradients of each sample (None, a_dim)",
        "core_API": "AdamOptimizer"
    },
    {
        "commit_hash": "16f8d3ecfca4b94cc6d80274bb9be3e3231031cd",
        "index": "dbee0003..ec47b9bc 100644",
        "commit_message": "[PyTorch] Set net.eval and net.train explicitly (#1110)\n\n* [PyTorch] Set net.eval and net.train explicitly\n\n* [PyTorch] Set net.eval if isinstance nn.Module\n\n* [PyTorch] Set net.train if isinstance nn.Module\n\n* [Fix] Custom Dropout, Closes #1093\n\n* 2 spaces before inline comments\n\n* 2 spaces before inline comments\n\n* 2 spaces before inline comments\n",
        "file": "d2l-en.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def corr2d(X, K):  #@save",
            "",
            "# Defined in file: ./chapter_convolutional-neural-networks/lenet.md",
            "def evaluate_accuracy_gpu(net, data_iter, device=None): #@save",
            "+    net.eval()  # Set the model to evaluation mode",
            "if not device:",
            "device = next(iter(net.parameters())).device",
            "metric = d2l.Accumulator(2)  # num_corrected_examples, num_examples"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=62383)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=62384)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=62385)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=62386)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=62387)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'net'), position=0, insert_id=62388)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=62389)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'eval'), position=2, insert_id=62390)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=62391)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=62392)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 11,
        "number": 375,
        "neg_line": [],
        "pos_line": [
            "+net.eval()  # Set the model to evaluation mode"
        ],
        "core_change": "+net.eval()  # Set the model to evaluation mode",
        "core_API": "eval"
    },
    {
        "commit_hash": "7611877df1223a8b3549550d3373e05a11d45298",
        "index": "bf9ea7f6..ffe9a3c4 100644",
        "commit_message": "[feat] updated patch augmentation (#1095)\n\n* updated patch augmentation\n\n* Fixed lint\n\n* Renamed forward_batchwise and forward_patchwise\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VideoSequential(ImageSequential):",
            "# Size of T",
            "frame_num = input.size(self._temporal_channel)",
            "# Got param generation shape to (B, C, H, W). Ignoring T.",
            "-        batch_shape = self.__infer_channel_exclusive_batch_shape__(input)",
            "+        batch_shape = self.__infer_channel_exclusive_batch_shape__(input, self._temporal_channel)",
            "input = self._input_shape_convert_in(input)",
            "input = input.reshape(-1, *batch_shape[1:])",
            "if not self.same_on_frame:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=423423)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=423424)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=423425)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=423426)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_temporal_channel'), position=2, insert_id=423427)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 376,
        "neg_line": [
            "-batch_shape = self.__infer_channel_exclusive_batch_shape__(input)"
        ],
        "pos_line": [
            "+batch_shape = self.__infer_channel_exclusive_batch_shape__(input, self._temporal_channel)"
        ],
        "core_change": "-batch_shape = self.__infer_channel_exclusive_batch_shape__(input) +batch_shape = self.__infer_channel_exclusive_batch_shape__(input, self._temporal_channel)",
        "core_API": "size"
    },
    {
        "commit_hash": "d8b5beb0b0a5cb3ec3ea20e9fff415057dcf25f6",
        "index": "4eb4440..07f6321 100755",
        "commit_message": "Fix2 `select_device()` for Multi-GPU (#6461)\n\n* Fix2 select_device() for Multi-GPU\n\n* Cleanup\n\n* Cleanup\n\n* Simplify error message\n\n* Improve assert\n\n* Update torch_utils.py\n",
        "file": "yolov5.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def create_dataloader(path, imgsz, batch_size, stride, single_cls=False, hyp=Non",
            "prefix=prefix)",
            "",
            "batch_size = min(batch_size, len(dataset))",
            "-    nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])  # number of workers",
            "+    nd = torch.cuda.device_count()  # number of CUDA devices",
            "+    nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers",
            "sampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)",
            "loader = DataLoader if image_weights else InfiniteDataLoader  # only DataLoader allows for attribute updates",
            "return loader(dataset,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'nd'), position=5, insert_id=1881809)",
            "Insert(target_node=ASTNode(type=ERROR), node=('=', '='), position=6, insert_id=1881810)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'torch'), position=7, insert_id=1881811)",
            "Insert(target_node=ASTNode(type=ERROR), node=('.', '.'), position=8, insert_id=1881812)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'cuda'), position=9, insert_id=1881813)",
            "Insert(target_node=ASTNode(type=ERROR), node=('.', '.'), position=10, insert_id=1881814)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'device_count'), position=11, insert_id=1881815)",
            "Insert(target_node=ASTNode(type=ERROR), node=('parameters', None), position=12, insert_id=1881816)",
            "Insert(target_node=ASTNode(type=parameters), node=('tuple_pattern', None), position=2, insert_id=1881817)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=3, insert_id=1881818)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=1881819)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=1, insert_id=1881820)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'max'), position=6, insert_id=1881821)",
            "Insert(target_node=IN(type=tuple_pattern), node=('(', '('), position=0, insert_id=1881822)",
            "Insert(target_node=IN(type=tuple_pattern), node=('identifier', 'nd'), position=1, insert_id=1881823)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=tuple_pattern), node=('ERROR', None), position=3, insert_id=1881824)",
            "Insert(target_node=IN(type=tuple_pattern), node=(')', ')'), position=4, insert_id=1881825)",
            "Insert(target_node=IN(type=ERROR), node=('integer', '1'), position=0, insert_id=1881826)",
            "Delete(target_node=ASTNode(type=identifier, text=DEVICE_COUNT))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 377,
        "neg_line": [
            "-nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])  # number of workers"
        ],
        "pos_line": [
            "+nd = torch.cuda.device_count()  # number of CUDA devices",
            "+nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers"
        ],
        "core_change": "-nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])  # number of workers +nd = torch.cuda.device_count()  # number of CUDA devices +nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers",
        "core_API": "cpu_count"
    },
    {
        "commit_hash": "d5c0effcfbe4fc905e22c2c1d5123ff150205581",
        "index": "6f6b5951b..d5fb3b0b4 100644",
        "commit_message": "Docs 4/n (#15628)\n\n* remove source-lit\n\n* docs\n\n* docs\n\n* docs\n\n* docs\n\n* ic\n\n* deploy\n\n* deploy\n\n* deploy\n\n* deploy\n\n* deploy\n\n* deploy\n\n* Apply suggestions from code review\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* make build run\n\nCo-authored-by: Jirka Borovec <6035284+Borda@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Rick Izzo <rick@grid.ai>\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LitStreamlit(L.app.components.ServeStreamlit):",
            "audio.seek(0)",
            "st.audio(audio)",
            "",
            "-app = L.LightningApp(LitStreamlit())",
            "+app = L.LightningApp(StreamlitApp())"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LitStreamlit), value='StreamlitApp')"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 378,
        "neg_line": [
            "-app = L.LightningApp(LitStreamlit())"
        ],
        "pos_line": [
            "+app = L.LightningApp(StreamlitApp())"
        ],
        "core_change": "-app = L.LightningApp(LitStreamlit()) +app = L.LightningApp(StreamlitApp())",
        "core_API": "seek"
    },
    {
        "commit_hash": "95bdb06bdb93b05c495f683e28ede1e428a904fe",
        "index": "caee56f74..700840908 100644",
        "commit_message": "fixed init\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "change API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiHeadedAttention(BaseMultiHeadedAttention):",
            "",
            "def __init__(self, q_dim, k_dim, v_dim, n_head, n_feat, dropout_rate=0.0):",
            "\"\"\"Initialize multi head attention module.\"\"\"",
            "-        super(MultiHeadedAttention, self).__init__()",
            "+        torch.nn.Module.__init__(self)",
            "assert n_feat % n_head == 0",
            "# We assume d_v always equals d_k",
            "self.d_k = n_feat // n_head"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=154025)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=self), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=154026)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=154027)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=154028)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=154029)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=154030)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=154031)",
            "Delete(target_node=ASTNode(type=identifier, text=super))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=MultiHeadedAttention))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 379,
        "neg_line": [
            "-super(MultiHeadedAttention, self).__init__()"
        ],
        "pos_line": [
            "+torch.nn.Module.__init__(self)"
        ],
        "core_change": "-super(MultiHeadedAttention, self).__init__() +torch.nn.Module.__init__(self)",
        "core_API": "__init__"
    },
    {
        "commit_hash": "91b1bf8aa7ea2f28dd045366639885cdc134c4c8",
        "index": "57fc2fd7a..02df9abd0 100644",
        "commit_message": "POC crypten.nn.module in plans (#3531)\n\n* parent db61f12afe51b3a216c53db80813b8d8ac378055\nauthor George Muraru <murarugeorgec@gmail.com> 1589458543 +0300\ncommitter George Muraru <murarugeorgec@gmail.com> 1592429175 +0300\n\nInitial POC for models in plans\n\n* Format linter\n\n* Flake\n\n* Add tests for OnnxModel\n\n* Add decrypt as method to overload\n\n* Add tests compared to jail\n\n* Review suggestions\n\n* Remove protobuf for Onnx and linter check\n\n* Remove TODO indent\n\n* Put wrap back\n\n* Remove OnnxModel from tests\n\n* Remove duplicated test\n\n* Put comment back\n\n* Flake8 fix\n\n* Fix test serde\n\n* Add tests to increase cov\n\n* Changes from crypten - prevent enc of shapes\n\n* Add no coverage\n\n* Remove Onnx Model\n\n* Fix syft-proto\n\n* Fix review\n\n* Fix review some more\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "add module",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "if dependency_check.crypten_available:",
            "",
            "framework_packages[\"crypten\"] = crypten",
            "framework_tensors.append(crypten.mpc.MPCTensor)",
            "+    framework_tensors.append(crypten.nn.Module)",
            "+",
            "",
            "framework_tensors = tuple(framework_tensors)",
            "FrameworkTensorType = Union[framework_tensors]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1451283)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1451284)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1451285)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1451286)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'framework_tensors'), position=0, insert_id=1451287)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1451288)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'append'), position=2, insert_id=1451289)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1451290)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1451291)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1451292)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1451293)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1451294)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1451295)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'crypten'), position=0, insert_id=1451296)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1451297)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1451298)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 381,
        "neg_line": [],
        "pos_line": [
            "+framework_tensors.append(crypten.nn.Module)",
            "+"
        ],
        "core_change": "+framework_tensors.append(crypten.nn.Module) +",
        "core_API": "append"
    },
    {
        "commit_hash": "0e108ce4dbd35db181b64ee84a3f1e13575f5a61",
        "index": "3ef6635769..0e4bbd4a86 100644",
        "commit_message": "fixing some imports etc.\n\nsome leftover from commit 44af78a09c306d764aec5985de4cf0e054316f28\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "update API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def array(object_in, dtype: Optional[str] = None, dev: Optional[str] = None):",
            "dev = default_device(dev)",
            "dtype = dtype_from_str(default_dtype(dtype, object_in))",
            "if isinstance(object_in, np.ndarray):",
            "-        return _torch.Tensor(object_in).to(dev_from_str(dev))",
            "+        return torch.Tensor(object_in).to(dev_from_str(dev))",
            "if dtype is not None:",
            "-        return _torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))",
            "-    elif isinstance(object_in, _torch.Tensor):",
            "+        return torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))",
            "+    elif isinstance(object_in, torch.Tensor):",
            "return object_in.to(dev_from_str(dev))",
            "else:",
            "-        return _torch.tensor(object_in, device=dev_from_str(dev))",
            "+        return torch.tensor(object_in, device=dev_from_str(dev))",
            "",
            "asarray = array"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_torch), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=_torch), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=_torch), value='return')",
            "Insert(target_node=ASTNode(type=attribute), node=('ERROR', None), position=1, insert_id=374406)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'torch'), position=0, insert_id=374407)",
            "Update(target_node=ASTNode(type=identifier, text=_torch), value='torch')",
            "Delete(target_node=ASTNode(type=identifier, text=return))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 8,
        "number": 384,
        "neg_line": [
            "-return _torch.Tensor(object_in).to(dev_from_str(dev))",
            "-return _torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))",
            "-elif isinstance(object_in, _torch.Tensor):",
            "-return _torch.tensor(object_in, device=dev_from_str(dev))"
        ],
        "pos_line": [
            "+return torch.Tensor(object_in).to(dev_from_str(dev))",
            "+return torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))",
            "+elif isinstance(object_in, torch.Tensor):",
            "+return torch.tensor(object_in, device=dev_from_str(dev))"
        ],
        "core_change": "-return _torch.Tensor(object_in).to(dev_from_str(dev)) +return torch.Tensor(object_in).to(dev_from_str(dev)) -return _torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev)) -elif isinstance(object_in, _torch.Tensor): +return torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev)) +elif isinstance(object_in, torch.Tensor): -return _torch.tensor(object_in, device=dev_from_str(dev)) +return torch.tensor(object_in, device=dev_from_str(dev))",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "e437eee76b9c63044bcf48164a4582ac34ffd3a8",
        "index": "f6b9d6bc..6f466053 100644",
        "commit_message": "[Fix] 3d augmentations return wrong transformation matrix (#665)\n\n* Transformation matrix and tests fixed\n\n* doctest fixed\n\n* Autoformat\n\nCo-authored-by: Feher Gergo (CC-AD/EPE-Bp) <fhe2bp@bosch.com>\nCo-authored-by: Feher Gergo (CC-AD/EPE-Bp) <bosch.feher@extaccount.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def compute_dflip_transformation3d(input: torch.Tensor, params: Dict[str, torch.",
            "d: int = input.shape[-3]",
            "flip_mat: torch.Tensor = torch.tensor([[1, 0, 0, 0],",
            "[0, 1, 0, 0],",
            "-                                           [0, 0, -1, d],",
            "+                                           [0, 0, -1, d - 1],",
            "[0, 0, 0, 1]])",
            "",
            "trans_mat[to_flip] = flip_mat.type_as(input)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('binary_operator', None), position=7, insert_id=437036)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=d), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=437037)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=437038)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 385,
        "neg_line": [
            "-[0, 0, -1, d],"
        ],
        "pos_line": [
            "+[0, 0, -1, d - 1],"
        ],
        "core_change": "-[0, 0, -1, d], +[0, 0, -1, d - 1],",
        "core_API": "tensor"
    },
    {
        "commit_hash": "16f4cc9ff08f38debd0086a0a489b3e16e208c30",
        "index": "dca3df6a0..dca3d5392 100644",
        "commit_message": "Shubhamagarwal92 master (#1349)\n\n* SA: for #958: set torch cuda device when finding root\n\n* SA: for #958: removing root gpu hack in trainer/evaluation_loop\n\n* SA: setting torch cuda device\n\n* comment line too long\n\n* check if root gpu exists or available\n\n* Incorporating suggestions on #1094\n\n* since root gpu returns none instead of -1 for cpu\n\n* undo changes\n\n* fixed dp memory thing\n\nCo-authored-by: Shubham Agarwal <shubhamagarwal92@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(",
            "self.gpus = gpus",
            "self.data_parallel_device_ids = parse_gpu_ids(self.gpus)",
            "self.root_gpu = determine_root_gpu_device(self.data_parallel_device_ids)",
            "+        self.root_device = torch.device(\"cpu\")",
            "",
            "# tpu state flags",
            "self.use_tpu = False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('attribute', None), position=9, insert_id=585744)",
            "Insert(target_node=ASTNode(type=ERROR), node=('=', '='), position=10, insert_id=585745)",
            "Insert(target_node=ASTNode(type=ERROR), node=('call', None), position=11, insert_id=585746)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=585747)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=585748)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'root_device'), position=2, insert_id=585749)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=585750)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=585751)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=585752)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=585753)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=585754)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=585755)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"cpu\"'), position=1, insert_id=585756)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=585757)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 14,
        "number": 387,
        "neg_line": [],
        "pos_line": [
            "+self.root_device = torch.device(\"cpu\")"
        ],
        "core_change": "+self.root_device = torch.device(\"cpu\")",
        "core_API": "device"
    },
    {
        "commit_hash": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
        "index": "705f3f7a..d9843953 100644",
        "commit_message": "Merging vision into main (#4800)\n\n* An initial VilBERT model for NLVR2 (#4423)\n\n* Some initial work; lots left to do\n\n* Initial test mostly passing, though things are still a bit of a mess\n\n* tests are passing with small fixtures\n\n* remove prints\n\n* Test more stuff\n\n* PathLike\n\n* Make vilbert pass tests\n\n* PR comments\n\n* call float before log\n\n* add CI\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* Initializing a VilBERT model from a pre-trained transformer (#4495)\n\n* saving state\n\n* Code is running, though it is returning zero gradients (but not None)\n\n* initial test passing, still working on albert\n\n* albert works, but bert-base-uncased still gives zero gradients\n\n* Loading of weights should now work\n\n* black, flake, mypy\n\n* remove drop and mask functionality from reader\n\n* make comment better\n\n* fix tests\n\n* flake\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* new data loading (#4497)\n\n* first implementation\n\n* update docstrings\n\n* fixes\n\n* fix sharding logic\n\n* clean up DatasetReader\n\n* fix samplers\n\n* fixes\n\n* fixes\n\n* patch models for now\n\n* more fixes\n\n* fix linting error\n\n* fix model test case\n\n* some fixes\n\n* fix linting err\n\n* updates\n\n* rename dataloader -> data_loader\n\n* fixes\n\n* more JoinableQueue\n\n* set daemon=True\n\n* fixes\n\n* fix\n\n* fixes\n\n* fix\n\n* update shuffle logic\n\n* load instances right away when not lazy\n\n* add tqdm when num_workers <= 0\n\n* apply_token_indexers\n\n* fix bug causing high mem usage\n\n* address some of @dirkgr's comments\n\n* fix lazy\n\n* use sensible default for max_batches_in_mem\n\n* ensure workers terminated on err\n\n* fix\n\n* start adding some tests\n\n* more tests\n\n* add some more tests\n\n* address most of Matt's comments\n\n* update PyTorchDataLoader test\n\n* get rid of lazy option\n\n* fix linting\n\n* update docs, change max_batches_per_epoch to max_instances_per_epcoh\n\n* update CHANGELOG\n\n* fix drop_last validation\n\n* fix py2md test fixture\n\n* handle drop_last\n\n* update docs\n\n* implement sharding for most readers\n\n* fix worker init fn\n\n* limit tqdm output\n\n* fixes\n\n* ensure vision CI runs on each commit (#4582)\n\n* ensure vision CI runs on each commit\n\n* fix\n\n* try fix CHANGELOG check\n\n* ensure models check runs on right branch\n\n* Formatting updates for new version of black (#4607)\n\n* reformat for new version of black (#4605)\n\n* reformat for new version of black\n\n* pin black\n\n* reformat for black\n\n* fix\n\n* rename 'node_rank' to 'global_rank' in dataset reader 'DistributedInfo' (#4608)\n\n* rename 'node_rank' to 'global_rank'\n\n* Clarify doc comments\n\n* fix line length\n\n* remove duplicate padding calculations in collate fn (#4617)\n\n* fix len calculation for new data loader (#4618)\n\n* fix len calculation for new data loader\n\n* add test\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* make existing readers work with multi-process loading (#4597)\n\n* make existing readers work with multi-process loading\n\n* add 'overrides' decorator\n\n* call apply_token_indexers in predictor\n\n* clean up\n\n* fix tests\n\n* Add MultiTaskModel (#4601)\n\n* Initial design of the multi-task model\n\n* PR comments, more implementation\n\n* changelog and docs fix\n\n* More tests, and fixes for those tests\n\n* mypy and make test less flaky\n\n* Update allennlp/models/multitask.py\n\n* Update allennlp/models/multitask.py\n\nCo-authored-by: Dirk Groeneveld <groeneveld@gmail.com>\n\n* Update allennlp/models/multitask.py\n\nCo-authored-by: James Barry <james.barry26@mail.dcu.ie>\n\n* respect active heads in get_metrics\n\n* Clean up changelog\n\n* black (apparently github UI doesn't add newlines?)\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\nCo-authored-by: Dirk Groeneveld <groeneveld@gmail.com>\nCo-authored-by: James Barry <james.barry26@mail.dcu.ie>\n\n* Detectron NLVR2 (#4481)\n\n* Passes a batch of detectron images to the model in the correct format\n\n* Loads a model and runs inference on it\n\n* Some initial work; lots left to do\n\n* Initial test mostly passing, though things are still a bit of a mess\n\n* tests are passing with small fixtures\n\n* remove prints\n\n* More configurable reader\n\n* add image_root and feature extraction to detectron model\n\n* Use general detectron cfg functions\n\n* Adds TensorField\n\n* Fix detectron dependency\n\n* Adds a detectron processor that we can use in dataset readers\n\n* Test more stuff\n\n* PathLike\n\n* Make vilbert pass tests\n\n* PR comments\n\n* call float before log\n\n* add CI\n\n* PathLike\n\n* Adds another NLVR2 reader\n\n* add region feature and grid feature configuration json and attrtibute to cfg file\n\n* change detectron_utils based on https://github.com/vedanuj/grid-feats-vqa/blob/master/extract_feature.py\n\n* add bottom up and top down roi head into detectron2 based on allennlp/models/detectron.py\n\n* Fix padding in TensorField\n\n* Fix field construction\n\n* Adds ability to read an arbitrary file\n\n* More type annotations\n\n* Remove old reader, add test for new one\n\n* Use the right kind of field\n\n* Run Jiasen's configs as tests\n\n* We don't need this field\n\n* Removes detectron reader\n\n* Remove detectron reader and field\n\n* Unify ArrayField and TensorField\n\n* Making sure that no merge will go cleanly from now on\n\n* Clean up the new output from the detectron processor a bit\n\n* Fix Detectron2 version as v0.2\n\n* saving state\n\n* Code is running, though it is returning zero gradients (but not None)\n\n* initial test passing, still working on albert\n\n* albert works, but bert-base-uncased still gives zero gradients\n\n* Note\n\n* Formatting\n\n* Adds Registrable base classes for image operations\n\n* Adds a real example of a image2image module\n\n* Run the new code (without implementation) in the nlvr2 reader\n\n* Solve some issue involving circular imports\n\n* add new modules for vilbert\n\n* add parameters for detectron image loader.\n\n* push current code on implementing proposal generator.\n\n* push current progress on proposal generator\n\n* Update FasterRCNNProposalGenerator & Merge Detectron2 config\n\n* Loading of weights should now work\n\n* black, flake, mypy\n\n* Run detectron pipeline pieces one at a time\n\nThis is unfinished and will not run this way.\n\n* Fix the data format for the backbone\n\n* Handle image sizes separately\n\n* remove drop and mask functionality from reader\n\n* make comment better\n\n* remove proposal_embedder, and finish proposal generator\n\n* working on grid embedder\n\n* added simple test for resnet backbone, which passes\n\n* Got proposal generator test passing\n\n* Change default number of detections per image: 100 => 36\n\n* Fix detectron config hierarchy: test_detectron_per_image\n\n* Make number of detections configurable & Add test\n\n* rename ProposalGenerator to RegionDetector\n\n* try to fix makefile\n\n* another attempt at makefile\n\n* quotes in the pip command...\n\n* added a simple test for the dataset reader, made it pass\n\n* add feature caching to the dataset reader\n\n* another try with the makefile\n\n* a better temporary fix for installing detectron\n\n* writing files before committing is good...\n\n* fix tests\n\n* fix (at least part of) the vilbert tests\n\n* ok, this makefile change should actually work\n\n* add torchvision, try to remove eager import of detectron code\n\n* flake\n\n* cleanup\n\n* more cleanup\n\n* mypy, flake\n\n* add back code I shouldn't have removed\n\n* black\n\n* test and flake fixes\n\n* fix region_detector for multiple images and add feature and coords padding\n\n* fix imports\n\n* restore null grid embedder\n\n* add back (todo) null region detector\n\n* Bring back import changes, to fix circular imports caused by NLVR2\nreader\n\n* region detector test passing\n\n* model test finally passing\n\n* update torchvision version\n\n* add vqav2 dataset\n\n* add gpu support for detectron feature extraction\n\n* add lmdbCache to cache feature into lmdb database\n\n* fix typo\n\n* update vqa jsonnet\n\n* fix url adding by cat\n\n* Fixes type annotation\n\n* Fixes borked error message\n\n* New feature cache\n\n* Formatting\n\n* Fix the tensor cache\n\n* Be explicit about our dependencies\n\n* Use the new tensor cache\n\n* Adds a test using the tensor cache\n\n* Run NLVR dataprep on GPU\n\n* Tqdm when finding images\n\n* Fixes padding in array field\n\n* Adjust max_length when truncating in PretrainedTransformerTokenizer\n\n* Fewer print statements\n\n* remove VQA from this branch and copy default vilbert parameters.\n\n* Sanjay's vision features cache script (#4633)\n\n* Use LMDB cache in NLVR2 dataset reader; fix a few typos\n\n* Standalone script for caching image features\n\n* Removing reference to LMDB cache in NLVR2 dataset reader\n\n* Adding back asterisk in nlvr2 dataset reader\n\n* Fixing one variable name mistake\n\n* Decreasing batch size and making a few cuda-related changes\n\n* Loading images in batches to avoid GPU OOM error\n\n* Pedantic changes for consistency\n\n* Run the pre-processing with the models and not the data loading\n\n* Filter out paths of images already cached\n\n* Add image extensions other than png\n\n* Fixes import error\n\n* Makes the vision features script work alongside other scripts or training runs\n\nCo-authored-by: sanjays <sanjays@ip-10-0-0-157.us-west-2.compute.internal>\nCo-authored-by: sanjays <sanjays@ip-10-1-10-157.us-west-2.compute.internal>\nCo-authored-by: Sanjay Subramanian <sanjays@allennlp-server1.corp.ai2>\nCo-authored-by: Sanjay Subramanian <sanjays_ssubramanian@hotmail.com>\n\n* Adds missing imports\n\n* Makes TensorCache into a real MutableMapping\n\n* Formatting\n\n* Changelog\n\n* Fix typecheck\n\n* Makes the NLVR2 reader work with Pete's new code\n\n* Fix type annotation\n\n* Formatting\n\n* Backwards compatibility\n\n* Fix tests\n\n* Fix broken config\n\n* Update grid embedder test\n\n* Fix vilbert_from_huggingface configuration\n\n* Don't run the vilbert_from_huggingface test anymore\n\n* Remove unused test fixtures\n\n* Fix the region detector test\n\n* Fix vilbert-from-huggingface and bring it back\n\n* Fuck the linter\n\n* Run the region detector test on GPU\n\n* Run more stuff on GPU\n\nThe CPU test runner doesn't have enough memory.\n\n* Depend on newer version of Detectron\n\n* Reinstall Detectron before running tests\n\n* Just force CUDA to be on, instead of reinstalling Detecton2\n\n* Detectron needs CUDA_HOME to be set during install\n\nAt least this thing fails quickly.\n\n* Try a different way of wrangling the detectron installer\n\n* Bring back amp\n\n* Trying to make tests faster, and passing\n\n* use two regions, to make tests pass\n\n* black\n\n* Documentation for TensorCache\n\n* Documentation for the NLVR2 dataset reader\n\n* Rename ArrayField to TensorField\n\nCo-authored-by: Matt Gardner <mattg@allenai.org>\nCo-authored-by: jiasenlu <jiasenlu@gatech.edu>\nCo-authored-by: Jaemin Cho <heythisischo@gmail.com>\nCo-authored-by: jiasenlu <echosenm@gmail.com>\nCo-authored-by: sanjays <sanjays@ip-10-0-0-157.us-west-2.compute.internal>\nCo-authored-by: sanjays <sanjays@ip-10-1-10-157.us-west-2.compute.internal>\nCo-authored-by: Sanjay Subramanian <sanjays@allennlp-server1.corp.ai2>\nCo-authored-by: Sanjay Subramanian <sanjays_ssubramanian@hotmail.com>\n\n* This should have been part of the previously merged PR\n\n* Transformer toolkit (#4577)\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* Attention scoring functions\n\n* merging output and self output\n\n* utility to replicate layers, further cleanup\n\n* adding sinusoidal positional encoding\n\n* adding activation layer\n\n* adding base class for generic loading of pretrained weights\n\n* further generalizing, adding tests\n\n* updates\n\n* adding bimodal encoder, kwargs in from_pretrained_module\n\n* vilbert using transformer toolkit\n\n* fixing test function\n\n* changing to torch.allclose\n\n* fixing attention score api\n\n* bug fix in bimodal output\n\n* changing to older attention modules\n\n* _construct_default_mapping returns mapping\n\n* adding kwargs to _get_input_arguments, adding examples\n\n* using cached_transformers\n\n* making transformer_encoder more general\n\n* added get_relevant_module, loading by name\n\n* fixing constructor name\n\n* undoing failure after merge\n\n* misc minor changes\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* Transformer toolkit: BiModalEncoder now has separate `num_attention_heads` for both modalities (#4728)\n\n* separate num_attention_heads for both modalities, default arguments\n\n* adding tests for toolkit examples\n\n* debug statements for failing test\n\n* removing debug statements, reordering\n\n* Let's be more tolerant\n\n* removing commented code\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* separating TransformerPooler as a new module (#4730)\n\n* separating TransformerPooler as a new module\n\n* adding size check\n\n* fix failing tests\n\n* Generalizing self attention (#4756)\n\n* generalizing SelfAttention\n\n* typecheck changes\n\n* adding shape information to docstring\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* Multitask data loading and scheduling (#4625)\n\n* Some initial work, still a bunch left to do\n\n* Adds a utility function that can shuffle iterables\n\n* remove shuffle\n\n* Getting close; saving state before fixing lint and adding tests\n\n* mypy and flake\n\n* put in some initial schedulers and samplers; just need to write tests\n\n* added some tests\n\n* changelog\n\n* add more-itertools to setup.py\n\n* finish docstring\n\n* some PR comments addressed\n\n* mypy\n\n* use homogeneous scheduler by default, not the non-homogeneous one\n\n* add option to not shuffle\n\n* normalize dataset proportions\n\n* Update allennlp/data/data_loaders/multitask_data_loader.py\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* improve independence of vision components (#4793)\n\n* improve independence of vision components\n\n* fix install\n\n* fix failing test\n\n* haha, actually fix\n\n* include torchvision exception too\n\n* fix torchvision install\n\n* remove vision push trigger\n\n* VQAv2 (#4639)\n\n* albert works, but bert-base-uncased still gives zero gradients\n\n* Note\n\n* Formatting\n\n* Adds Registrable base classes for image operations\n\n* Adds a real example of a image2image module\n\n* Run the new code (without implementation) in the nlvr2 reader\n\n* Solve some issue involving circular imports\n\n* add new modules for vilbert\n\n* add parameters for detectron image loader.\n\n* push current code on implementing proposal generator.\n\n* push current progress on proposal generator\n\n* Update FasterRCNNProposalGenerator & Merge Detectron2 config\n\n* Loading of weights should now work\n\n* black, flake, mypy\n\n* Run detectron pipeline pieces one at a time\n\nThis is unfinished and will not run this way.\n\n* Fix the data format for the backbone\n\n* Handle image sizes separately\n\n* remove drop and mask functionality from reader\n\n* make comment better\n\n* remove proposal_embedder, and finish proposal generator\n\n* working on grid embedder\n\n* added simple test for resnet backbone, which passes\n\n* Got proposal generator test passing\n\n* Change default number of detections per image: 100 => 36\n\n* Fix detectron config hierarchy: test_detectron_per_image\n\n* Make number of detections configurable & Add test\n\n* rename ProposalGenerator to RegionDetector\n\n* try to fix makefile\n\n* another attempt at makefile\n\n* quotes in the pip command...\n\n* added a simple test for the dataset reader, made it pass\n\n* add feature caching to the dataset reader\n\n* another try with the makefile\n\n* a better temporary fix for installing detectron\n\n* writing files before committing is good...\n\n* fix tests\n\n* fix (at least part of) the vilbert tests\n\n* ok, this makefile change should actually work\n\n* add torchvision, try to remove eager import of detectron code\n\n* flake\n\n* cleanup\n\n* more cleanup\n\n* mypy, flake\n\n* add back code I shouldn't have removed\n\n* black\n\n* test and flake fixes\n\n* fix region_detector for multiple images and add feature and coords padding\n\n* fix imports\n\n* restore null grid embedder\n\n* add back (todo) null region detector\n\n* Bring back import changes, to fix circular imports caused by NLVR2\nreader\n\n* region detector test passing\n\n* model test finally passing\n\n* update torchvision version\n\n* add vqav2 dataset\n\n* add gpu support for detectron feature extraction\n\n* add lmdbCache to cache feature into lmdb database\n\n* fix typo\n\n* update vqa jsonnet\n\n* fix url adding by cat\n\n* Fixes type annotation\n\n* Fixes borked error message\n\n* New feature cache\n\n* Formatting\n\n* Fix the tensor cache\n\n* Be explicit about our dependencies\n\n* Use the new tensor cache\n\n* Adds a test using the tensor cache\n\n* Run NLVR dataprep on GPU\n\n* Tqdm when finding images\n\n* Fixes padding in array field\n\n* Adjust max_length when truncating in PretrainedTransformerTokenizer\n\n* Fewer print statements\n\n* remove VQA from this branch and copy default vilbert parameters.\n\n* add VQAv2 dataset\n\n* Added dataset reader and model tests, which are now passing\n\n* Sanjay's vision features cache script (#4633)\n\n* Use LMDB cache in NLVR2 dataset reader; fix a few typos\n\n* Standalone script for caching image features\n\n* Removing reference to LMDB cache in NLVR2 dataset reader\n\n* Adding back asterisk in nlvr2 dataset reader\n\n* Fixing one variable name mistake\n\n* Decreasing batch size and making a few cuda-related changes\n\n* Loading images in batches to avoid GPU OOM error\n\n* Pedantic changes for consistency\n\n* Run the pre-processing with the models and not the data loading\n\n* Filter out paths of images already cached\n\n* Add image extensions other than png\n\n* Fixes import error\n\n* Makes the vision features script work alongside other scripts or training runs\n\nCo-authored-by: sanjays <sanjays@ip-10-0-0-157.us-west-2.compute.internal>\nCo-authored-by: sanjays <sanjays@ip-10-1-10-157.us-west-2.compute.internal>\nCo-authored-by: Sanjay Subramanian <sanjays@allennlp-server1.corp.ai2>\nCo-authored-by: Sanjay Subramanian <sanjays_ssubramanian@hotmail.com>\n\n* Adds missing imports\n\n* Makes TensorCache into a real MutableMapping\n\n* Formatting\n\n* Changelog\n\n* Fix typecheck\n\n* Makes the NLVR2 reader work with Pete's new code\n\n* Fix type annotation\n\n* Formatting\n\n* Backwards compatibility\n\n* Restore NLVR to former glory\n\n* Types and multi-process reading for VQAv2\n\n* Formatting\n\n* Fix tests\n\n* Fix broken config\n\n* Update grid embedder test\n\n* Fix vilbert_from_huggingface configuration\n\n* Don't run the vilbert_from_huggingface test anymore\n\n* Remove unused test fixtures\n\n* Fix the region detector test\n\n* Fix vilbert-from-huggingface and bring it back\n\n* Fuck the linter\n\n* Fix for VQA test\n\n* Why was this metric disabled?\n\n* Black and flake\n\n* Re-add VQA reader\n\n* Image featurizers now need to be called with sizes\n\n* Run the region detector test on GPU\n\n* Run more stuff on GPU\n\nThe CPU test runner doesn't have enough memory.\n\n* Depend on newer version of Detectron\n\n* Reinstall Detectron before running tests\n\n* Just force CUDA to be on, instead of reinstalling Detecton2\n\n* Fixes VQA2 DatasetReader\n\n* Fix documentation\n\n* Detectron needs CUDA_HOME to be set during install\n\nAt least this thing fails quickly.\n\n* Try a different way of wrangling the detectron installer\n\n* Try a different way of wrangling the detectron installer\n\n* Bring back amp\n\n* Refactored VQA reader\n\n* More training paths\n\n* Remove debug code\n\n* Don't check in debug code\n\n* Auto-detect GPU to use\n\n* Apply indexers later\n\n* Fix typo\n\n* Register the model\n\n* Fields live on CPU. Only batches get GPUs.\n\n* black\n\n* black, flake\n\n* mypy\n\n* more flake\n\n* More realistic training config\n\n* Adds a basic Predictor for VQAv2\n\n* Make vilbert output human-readable\n\n* Forgot to enumerate\n\n* Use the right namspace\n\n* Trying to make tests faster, and passing\n\n* add image prefix when loading coco image\n\n* fix vqav2 dataset reader and config file\n\n* use two regions, to make tests pass\n\n* black\n\n* Output probabilities in addition to logits\n\n* Make it possible to turn off the cache\n\n* Turn off the cache in the predictor\n\n* Fix the VQA predictor\n\n* change the experiment to the defualt vilbert hyperparams.\n\n* add default experiment_from_huggingface.json\n\n* fix typos in vqa reader\n\n* Proper probabilities\n\n* Formatting\n\n* Remove unused variable\n\n* Make mypy happy\n\n* Fixed loss function, metric, and got tests to pass\n\n* Updates the big training config\n\n* Put real settings into the vilbert_vqa config\n\n* Strings are lists in Python\n\n* Make mypy happy\n\n* Formatting\n\n* Unsatisfying mypy\n\n* Config changes to make this run\n\n* Fix dimensionality of embeddings\n\n* clean the code and add the image_num_heads and combine_num_heads\n\n* fix answer vocab and add save and load from pre-extracted vocab\n\n* fix loss and update save_answer_vocab script\n\n* Typo\n\n* Fixed fusion method\n\n* Tweaking the VQA config some more\n\n* Moved the from_huggingface config\n\n* 20 epochs\n\n* Set up the learning rate properly\n\n* Simplify\n\n* Hardcoded answer vocab\n\n* Don't be lazy\n\n* Steps per epoch cannot be None\n\n* Let's chase the right score\n\n* Fixing some parameter names\n\n* Fields are stored on CPUs\n\n* Bigger batch size, easier distributed training\n\n* Don't run the debug code by default\n\n* VQA with the Transformer Toolkit (#4729)\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* Attention scoring functions\n\n* merging output and self output\n\n* utility to replicate layers, further cleanup\n\n* adding sinusoidal positional encoding\n\n* adding activation layer\n\n* adding base class for generic loading of pretrained weights\n\n* further generalizing, adding tests\n\n* updates\n\n* adding bimodal encoder, kwargs in from_pretrained_module\n\n* vilbert using transformer toolkit\n\n* fixing test function\n\n* changing to torch.allclose\n\n* fixing attention score api\n\n* bug fix in bimodal output\n\n* changing to older attention modules\n\n* _construct_default_mapping returns mapping\n\n* adding kwargs to _get_input_arguments, adding examples\n\n* using cached_transformers\n\n* making transformer_encoder more general\n\n* added get_relevant_module, loading by name\n\n* fixing constructor name\n\n* undoing failure after merge\n\n* misc minor changes\n\n* Transformer toolkit (#4577)\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* Attention scoring functions\n\n* merging output and self output\n\n* utility to replicate layers, further cleanup\n\n* adding sinusoidal positional encoding\n\n* adding activation layer\n\n* adding base class for generic loading of pretrained weights\n\n* further generalizing, adding tests\n\n* updates\n\n* adding bimodal encoder, kwargs in from_pretrained_module\n\n* vilbert using transformer toolkit\n\n* fixing test function\n\n* changing to torch.allclose\n\n* fixing attention score api\n\n* bug fix in bimodal output\n\n* changing to older attention modules\n\n* _construct_default_mapping returns mapping\n\n* adding kwargs to _get_input_arguments, adding examples\n\n* using cached_transformers\n\n* making transformer_encoder more general\n\n* added get_relevant_module, loading by name\n\n* fixing constructor name\n\n* undoing failure after merge\n\n* misc minor changes\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* separate num_attention_heads for both modalities, default arguments\n\n* adding tests for toolkit examples\n\n* debug statements for failing test\n\n* removing debug statements, reordering\n\n* Typo\n\n* Some compatibility with the transformer toolkit\n\n* Reorganize the image inputs\n\n* More transformer toolkit compatibility\n\n* Debug settings\n\n* Let's be more tolerant\n\n* Fix how VilBERT runs\n\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\n\n* Make the region detector and region embedder lazy\n\n* Fix references to the model\n\n* Make various automated tests pass\n\n* Formatting\n\n* More logging\n\n* One more logging statement\n\n* Read answer vocab from vocab file instead of determining it automatically\n\n* Don't keep the files open so long\n\n* Use most of the validation set for training as well\n\n* Get ready to be lazy\n\n* Upgrade paths\n\n* Be lazy\n\n* Keep unanswerable questions only during test time\n\n* Fix the from_huggingface config\n\n* Fixes the VQA score\n\n* VQA specific metric\n\n* Fixes some tests\n\n* Tests pass!\n\n* Formatting\n\n* Use the correct directory\n\n* Use the region detector that's meant for testing\n\n* Read the test split properly\n\n* Be a little more verbose while discovering images\n\n* Modernize Vilbert VQA\n\n* Update NLVR, but it still doesn't run\n\n* Formatting\n\n* Remove NLVR\n\n* Fix the last test\n\n* Formatting\n\n* Conditionally export the VilbertVqaPredictor\n\n* ModuleNotFoundError is a type of ImportError\n\n* Fix test-install\n\n* Try the broken test with a fixed seed\n\n* Try a bunch of seeds\n\n* Smaller model to get bigger magnitudes\n\n* Now that the test works, we don't need to specify the seeds anymore\n\nCo-authored-by: Matt Gardner <mattg@allenai.org>\nCo-authored-by: jiasenlu <jiasenlu@gatech.edu>\nCo-authored-by: Jaemin Cho <heythisischo@gmail.com>\nCo-authored-by: jiasenlu <echosenm@gmail.com>\nCo-authored-by: sanjays <sanjays@ip-10-0-0-157.us-west-2.compute.internal>\nCo-authored-by: sanjays <sanjays@ip-10-1-10-157.us-west-2.compute.internal>\nCo-authored-by: Sanjay Subramanian <sanjays@allennlp-server1.corp.ai2>\nCo-authored-by: Sanjay Subramanian <sanjays_ssubramanian@hotmail.com>\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\nCo-authored-by: Evan Pete Walsh <epwalsh10@gmail.com>\n\n* SNLI_VE dataset reader (#4799)\n\n* adding VE reader\n\n* removing jsonlines\n\n* blackify\n\n* intial VE model\n\n* adding VisionReader for common vision components\n\n* fix test file\n\n* fix doc\n\n* temporarily removing VE model\n\n* bug fix\n\n* cleanup\n\n* removing unnecessary check\n\n* simplify\n\n* Visual entailment model code (#4822)\n\n* VE model code\n\n* adding VE model\n\n* misc minor updates\n\n* update changelog\n\n* Added GQA reader (#4832)\n\n* Adds reader for GQA dataset. Will download questions from https://cs.stanford.edu/people/dorarad/gqa/download.html.\n\n* Cleaned up GQA reader tests\n\n* Other VQA datasets (#4834)\n\n* Make the VQA reader work for the other datasets\n\n* Also find pngs\n\n* Really support pngs\n\n* Remove debug code\n\n* More logging\n\n* Unexpected formatting\n\n* Respect the device\n\n* This is how your replace things in named tuples.\n\n* Remove unused import\n\n* This is how you override a method properly.\n\n* This is how you set parameters in detectron.\n\n* Also set the device for the region detector\n\n* Training configs for all three datasets contained in VQA\n\n* Bigger batches\n\n* Bigger batches for image processing\n\n* Fix vilbert-from-huggingface config\n\n* Make the config switch modes for constructing vocab\n\n* More vocab, more docs, better way of deriving vocab\n\n* Modernize the from_huggingface config\n\n* More updates to the from_huggingface config\n\n* Better hyperparameters stolen from another project\n\n* Fix for inverted parameter\n\n* Formatting\n\n* Throw a meaningful error message when we don't have images\n\n* Add a warning that includes instructions for how to fix things\n\n* Remove unused script\n\n* Merge issue\n\n* adding multilabel option (#4843)\n\n* Generalizing transformer layers (#4776)\n\n* adding HF tests, docstrings for AttentionLayer, TransformerLayer, TransformerBlock\n\n* temp change to check if tests pass\n\n* undoing temp change\n\n* ci update\n\n* more ci updates\n\n* changing test run\n\n* update makefile\n\n* temp change\n\n* isolating failing case\n\n* further debugging\n\n* fail check\n\n* reverting to older CI\n\n* test with reduced batch size\n\n* cleanup\n\n* more cleanup\n\n* oops, fix\n\n* gqa reader fixes during vilbert training (#4851)\n\n* Refactored shared code\n\n* typecheck fix\n\n* rebase\n\n* Refactored shared code\n\n* typecheck fix\n\n* rebase\n\n* Cleaned up GQA reader tests\n\n* Modify instance format for vilbert-vqa model\n\n* update for vision branch bump\n\nCo-authored-by: Jackson Stokes <jacksons@Jacksons-MacBook-Pro.local>\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* Toolkit: Adding documentation and small changes for `BiModalAttention` (#4859)\n\n* adding documentation for bimodal attn, small fixes\n\n* changing the way mask is applied\n\n* using large value rather than inf\n\n* Update comment\n\nCo-authored-by: Dirk Groeneveld <groeneveld@gmail.com>\n\n* moving apply_mask to util\n\nCo-authored-by: Dirk Groeneveld <groeneveld@gmail.com>\n\n* Make tests work again (#4865)\n\n* New import paths\n\n* Duplicate entries\n\n* Dataset readers can't be lazy anymore\n\n* Switch to torchvision for vision components üëÄ, simplify and improve MultiProcessDataLoader (#4821)\n\n* implement TorchImageLoader\n\n* implement ResnetBackbone\n\n* add resize + normalize to image loader\n\n* finalize FasterRcnnRegionDetector\n\n* pin torchvision\n\n* fix VQAv2Reader\n\n* add box mask field\n\n* dataset reader fixes\n\n* fix model tests\n\n* doc fixes\n\n* add threshold parameters to FasterRcnnRegionDetector\n\n* address @dirkgr comments\n\n* mask fixes\n\n* shape comments\n\n* add some more comments\n\n* cache answers_by_question_id\n\n* implement LocalCacheResource\n\n* fix\n\n* add read-only option to cache\n\n* fix\n\n* simplify data loader\n\n* make featurizer and detector optional in readers\n\n* Cache in memory\n\n* back pressure is important I guess\n\n* merge\n\n* Updated configs\n\n* Fixes the way we apply masks\n\n* Use more of Jiasen's real settings\n\n* Upgrade the from_huggingface config\n\n* Switch back to the images on corpnet\n\n* Fix random seeds\n\n* Bigger model needs smaller batch size\n\n* Adds ability to selectively ignore one input\n\n* address some comments\n\n* format + lint\n\n* fixes\n\n* Bring back bert-base configs\n\n* fix error handling\n\n* fix test\n\n* fix typo\n\n* use lock when possible\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* doc fixes\n\n* Only cache, no featurizing (#4870)\n\n* implement TorchImageLoader\n\n* implement ResnetBackbone\n\n* add resize + normalize to image loader\n\n* finalize FasterRcnnRegionDetector\n\n* pin torchvision\n\n* fix VQAv2Reader\n\n* add box mask field\n\n* dataset reader fixes\n\n* fix model tests\n\n* doc fixes\n\n* add threshold parameters to FasterRcnnRegionDetector\n\n* address @dirkgr comments\n\n* mask fixes\n\n* shape comments\n\n* add some more comments\n\n* cache answers_by_question_id\n\n* implement LocalCacheResource\n\n* fix\n\n* add read-only option to cache\n\n* fix\n\n* simplify data loader\n\n* make featurizer and detector optional in readers\n\n* Cache in memory\n\n* back pressure is important I guess\n\n* merge\n\n* Updated configs\n\n* Fixes the way we apply masks\n\n* Use more of Jiasen's real settings\n\n* Upgrade the from_huggingface config\n\n* Switch back to the images on corpnet\n\n* Fix random seeds\n\n* Bigger model needs smaller batch size\n\n* Adds ability to selectively ignore one input\n\n* address some comments\n\n* format + lint\n\n* fixes\n\n* Bring back bert-base configs\n\n* fix error handling\n\n* fix test\n\n* Adds the ability to read from a feature cache, but not run any featurization\n\n* Update tests\n\n* Let's stick with \"feature_cache\"\n\nAs long as we're consistent ...\n\n* More epochs, more random\n\n* Use the new parameters\n\n* Fix initialization\n\n* Make tests work, add some documentation\n\n* Remove the read_from_cache parameter\n\n* Cleanup of training configs\n\n* Typecheck\n\n* Building docs right\n\n* Better settings for VQA\n\n* Leave the image_feature_dim at 1024\n\nCo-authored-by: epwalsh <epwalsh10@gmail.com>\n\n* Make images easier to find for Visual Entailment (#4878)\n\n* implement TorchImageLoader\n\n* implement ResnetBackbone\n\n* add resize + normalize to image loader\n\n* finalize FasterRcnnRegionDetector\n\n* pin torchvision\n\n* fix VQAv2Reader\n\n* add box mask field\n\n* dataset reader fixes\n\n* fix model tests\n\n* doc fixes\n\n* add threshold parameters to FasterRcnnRegionDetector\n\n* address @dirkgr comments\n\n* mask fixes\n\n* shape comments\n\n* add some more comments\n\n* cache answers_by_question_id\n\n* implement LocalCacheResource\n\n* fix\n\n* add read-only option to cache\n\n* fix\n\n* simplify data loader\n\n* make featurizer and detector optional in readers\n\n* Cache in memory\n\n* back pressure is important I guess\n\n* merge\n\n* Updated configs\n\n* Fixes the way we apply masks\n\n* Use more of Jiasen's real settings\n\n* Upgrade the from_huggingface config\n\n* Switch back to the images on corpnet\n\n* Fix random seeds\n\n* Bigger model needs smaller batch size\n\n* Adds ability to selectively ignore one input\n\n* address some comments\n\n* format + lint\n\n* fixes\n\n* Bring back bert-base configs\n\n* fix error handling\n\n* fix test\n\n* Adds the ability to read from a feature cache, but not run any featurization\n\n* Update tests\n\n* Let's stick with \"feature_cache\"\n\nAs long as we're consistent ...\n\n* More epochs, more random\n\n* Use the new parameters\n\n* Fix initialization\n\n* Make tests work, add some documentation\n\n* Remove the read_from_cache parameter\n\n* Cleanup of training configs\n\n* Typecheck\n\n* Building docs right\n\n* Better settings for VQA\n\n* Open cached paths when reading json lines\n\n* By default, autodetect GPUs when training\n\n* Switch to torchvision\n\n* Download training data from the web\n\n* This needs to stay at 1024 until we get the new featurization model\n\n* Have a more descriptive error message when images are missing\n\n* Update vilbert_ve_from_huggingface.jsonnet\n\nCo-authored-by: epwalsh <epwalsh10@gmail.com>\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\n\n* Adding f1 score (#4890)\n\n* adding f1 score\n\n* updated config\n\n* import MultiTaskDataLoader to data_loaders/__init__.py (#4885)\n\n* Make GQA work (#4884)\n\n* Refactored shared code\n\n* typecheck fix\n\n* rebase\n\n* Refactored shared code\n\n* typecheck fix\n\n* rebase\n\n* Cleaned up GQA reader tests\n\n* Modify instance format for vilbert-vqa model\n\n* update for vision branch bump\n\n* Adding training config for GQA\n\n* Unnamed variable\n\n* Various GQA fixes\n\n* Temporary extra configs needed to make vocab\n\n* Remove unused file\n\n* Optimize VQA score instead of F-Score\n\n* Use our newly created vocab\n\n* Remove temporary configs\n\n* Don't fail when we don't need to create a directory\n\n* Make a config that works on the servers as well\n\n* Update comment\n\n* A new command to count instances\n\n* Temporary config to count instances\n\n* Undo temporary changes\n\n* Put in the correct number of steps per epoch\n\n* Remove this number from the config because it's almost certainly wrong\n\n* Don't put Fields in Tuples\n\n* Formatting\n\n* More informative error message when batches are heterogeneous\n\n* Formatting\n\n* Not my type\n\n* Generate the fields properly when answers are missing\n\n* Properly discard instances with missing answers\n\n* Changelog\n\n* Update number of steps per epoch\n\n* Adds a config for balanced GQA\n\n* fix file_utils extract with directory\n\n* fix Batch._check_types\n\n* Fill in URL\n\nCo-authored-by: Jackson Stokes <jacksons@Jacksons-MacBook-Pro.local>\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\nCo-authored-by: Evan Pete Walsh <epwalsh10@gmail.com>\n\n* Toolkit: Cleaning up TransformerEmbeddings (#4900)\n\n* fixing issue of non-deterministic dropout\n\n* updating TransformerEmbeddings\n\n* ImageFeatureEmbeddings is now a subclass of Embeddings\n\n* allowing for no token type embeddings\n\n* fixing kwargs for loading pretrained module\n\n* Data loading cuda device (#4879)\n\n* add test with tensor fields\n\n* improve nn.util.move_to_device\n\n* ensure start_method is 'spawn' when using lazy and mem pin\n\n* add 'non_blocking' arg to 'move_to_device'\n\n* fix fake test tensor\n\n* fix sampler test\n\n* lint\n\n* fix 'move_to_device'\n\n* fix condition check\n\n* add device to data loader\n\n* clean up doc string\n\n* rename 'device' arg to 'cuda_device'\n\n* pinning is very slow, revert\n\n* DataLoaders load to CUDA device\n\n* fix evaluate test\n\n* rename 'multi_process_*' -> 'multiprocess' for consistency (#4906)\n\n* MultiProcessDataLoader takes PathLike data_path (#4908)\n\n* remove PyTorchDataLoader, add SimpleDataLoader for testing (#4907)\n\n* remove PyTorchDataLoader, add SimpleDataLoader for testing\n\n* fix test\n\n* comments\n\n* improve data loading docs (#4909)\n\n* improve data loading docs\n\n* document best practices, add 'get_batch_size' method to samplers\n\n* try fix annoying unrelated test\n\n* revert that\n\n* clarify handling of 'max_instances_in_memory'\n\n* fix imports in file_utils\n\n* rename 'master' -> 'primary' for distributed training (#4910)\n\n* improve worker error handling in MultiProcessDataLoader (#4912)\n\n* improve worker error handling\n\n* rename test file\n\n* Toolkit decoder (#4914)\n\n* adding cross_attention, renaming block -> stack\n\n* stack can be initialized with layer too\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* resolve _read type (#4916)\n\n* resolve _read type\n\n* fix sharded reader\n\n* fix data loader arg\n\n* Multitask example (#4898)\n\n* Make the VQA reader work for the other datasets\n\n* Also find pngs\n\n* Really support pngs\n\n* Remove debug code\n\n* More logging\n\n* Unexpected formatting\n\n* Respect the device\n\n* This is how your replace things in named tuples.\n\n* Remove unused import\n\n* This is how you override a method properly.\n\n* This is how you set parameters in detectron.\n\n* Also set the device for the region detector\n\n* Training configs for all three datasets contained in VQA\n\n* Bigger batches\n\n* Bigger batches for image processing\n\n* Fix vilbert-from-huggingface config\n\n* Make the config switch modes for constructing vocab\n\n* More vocab, more docs, better way of deriving vocab\n\n* Modernize the from_huggingface config\n\n* More updates to the from_huggingface config\n\n* Better hyperparameters stolen from another project\n\n* Fix for inverted parameter\n\n* Formatting\n\n* Throw a meaningful error message when we don't have images\n\n* Add a warning that includes instructions for how to fix things\n\n* Remove unused script\n\n* Merge issue\n\n* Adds named splits to the SNLI-VE reader\n\n* Make the multitask data loader discoverable\n\n* Formatting\n\n* More flexible inputs to the dataset readers\n\n* Prototype config for the multitask training job\n\n* json_lines_from_file() already calls cached_path()\n\n* Visual entailment should track accuracy\n\n* Switching to torch\n\n* Fixing VE image paths\n\n* Formatting\n\n* Experimentally use threaded_generator to read instances from readers simultaneously\n\n* Vilbert backbone\n\n* Fixed paths\n\n* Formatting\n\n* Adds heads\n\n* Revert \"Experimentally use threaded_generator to read instances from readers simultaneously\"\n\nThis reverts commit a633e67134cf82f103071eba8ee560825f258c7b.\n\n* Multitask trains now!\n\n* Remove useless parameter from GQA reader\n\n* Updated multitask config\n\n* Schedulers produce batches, not instances\n\n* Track multiple metrics\n\n* Make mypy happy\n\n* Formatting\n\n* Keep better track of which heads have been called\n\n* Fix the merge\n\n* We have more than strings for input\n\n* Remove unused imports\n\n* -1 is CPU\n\n* Go back to tracking instances per epoch so that the samplers can work\n\n* Better error message\n\n* A useful sampler to have\n\n* We haven't indexed until we've indexed\n\n* Makes tests pass\n\n* Formatting\n\n* Fine-tuning the metric tracker\n\n* Update model configs for my changes\n\n* Fixing model configs for Akshita's changes\n\n* Implement VisionTextModel in terms of VilbertBackbone\n\n* Formatting\n\n* Fix stale comment\n\n* Use the server paths by default, not Dirk's desktop\n\n* Fix tests\n\n* Formatting again\n\n* Removed data loader parameters that don't exist anymore\n\n* Clarified comment\n\nCo-authored-by: Evan Pete Walsh <epwalsh10@gmail.com>\n\n* Moves vision models to allennlp-models (#4918)\n\n* Moves vision models to allennlp-models\n\n* Also move test fixtures\n\n* Don't return so many instances if we're cutting them out later anyways\n\n* We actually need this image\n\n* Formatting\n\n* Fixing more paths\n\n* Prepare for release v2.0.0rc1\n\n* Make releasing work with the renamed master branch, and with the vision branch\n\n* Debugging the release process in the slowest way possible\n\n* Another attempt at fixing the release process\n\n* Generic Callbacks (#4917)\n\n* Better Callbacks\n\n* Reformatting\n\n* Fixes\n\n* Tests for updated TrainerCallback\n\n* Formatting and Type-Checking fixes\n\n* Consistent metric tracker (#4928)\n\n* Makes the metric tracker more consistent\n\n* Turns out we need best_epoch_metrics after all.\n\n* Backwards compatibility\n\n* Formatting\n\n* Remove old script\n\n* Changes CI since we won't have a `vision` branch anymore\n\n* fix up CHANGELOG\n\nCo-authored-by: Matt Gardner <mattg@allenai.org>\nCo-authored-by: epwalsh <epwalsh10@gmail.com>\nCo-authored-by: James Barry <james.barry26@mail.dcu.ie>\nCo-authored-by: jiasenlu <jiasenlu@gatech.edu>\nCo-authored-by: Jaemin Cho <heythisischo@gmail.com>\nCo-authored-by: jiasenlu <echosenm@gmail.com>\nCo-authored-by: sanjays <sanjays@ip-10-0-0-157.us-west-2.compute.internal>\nCo-authored-by: sanjays <sanjays@ip-10-1-10-157.us-west-2.compute.internal>\nCo-authored-by: Sanjay Subramanian <sanjays@allennlp-server1.corp.ai2>\nCo-authored-by: Sanjay Subramanian <sanjays_ssubramanian@hotmail.com>\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\nCo-authored-by: jvstokes <40584422+jvstokes@users.noreply.github.com>\nCo-authored-by: Jackson Stokes <jacksons@Jacksons-MacBook-Pro.local>\nCo-authored-by: Karen Hambardzumyan <mahnerak@gmail.com>\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestNnUtil(AllenNlpTestCase):",
            "assert parameters_inspection_dict == util.inspect_parameters(model)",
            "",
            "def test_move_to_device(self):",
            "-        # We're faking the tensor here so that we can test the calls to .cuda() without actually",
            "+        # We're faking the tensor here so that we can test the calls to .to() without actually",
            "# needing a GPU.",
            "class FakeTensor(torch.Tensor):",
            "def __init__(self):",
            "self._device = None",
            "",
            "-            def cuda(self, device):",
            "+            def to(self, device, **kwargs):",
            "self._device = device",
            "return self"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=cuda), value='to')",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=5260)",
            "Insert(target_node=ASTNode(type=parameters), node=('dictionary_splat_pattern', None), position=5, insert_id=5261)",
            "Insert(target_node=IN(type=dictionary_splat_pattern), node=('**', '**'), position=0, insert_id=5262)",
            "Insert(target_node=IN(type=dictionary_splat_pattern), node=('identifier', 'kwargs'), position=1, insert_id=5263)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 388,
        "neg_line": [
            "-# We're faking the tensor here so that we can test the calls to .cuda() without actually",
            "-def cuda(self, device):"
        ],
        "pos_line": [
            "+# We're faking the tensor here so that we can test the calls to .to() without actually",
            "+def to(self, device, **kwargs):"
        ],
        "core_change": "-# We're faking the tensor here so that we can test the calls to .cuda() without actually +# We're faking the tensor here so that we can test the calls to .to() without actually -def cuda(self, device): +def to(self, device, **kwargs):",
        "core_API": "inspect_parameters"
    },
    {
        "commit_hash": "6a6e1c7602df5dc112c99792681422a529d9a13e",
        "index": "5526836bc4..1763fcd658 100644",
        "commit_message": "Fixing out arg (#1770)\n\n* Updated out arg in functional ivy activations\n\n* Updated out arg docstring in activations\n\n* Updated out arg in functional ivy creation\n\n* Updated out arg in functional ivy dtype\n\n* Updated out arg in functional ivy elementwise\n\n* Updated out arg in functional ivy general\n\n* Updated out arg in functional ivy gradients\n\n* Updated out arg in functional ivy layers\n\n* Updated out arg in functional ivy linalg\n\n* Updated out arg in functional ivy manipulation\n\n* Updated out arg in functional ivy random\n\n* Updated out arg in functional ivy searching\n\n* Updated out arg in functional ivy set\n\n* Updated out arg in functional ivy statistical\n\n* Updated out arg in functional ivy norms\n\n* Updated @handle-out-arg decorator in compositional functions\n\n* Updated out arg in functional ivy general\n\n* Updated out arg in ivy functional layers, searching and set\n\n* lint fixes\n\n* lint fixes in dtype and general\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def solve(x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:",
            "",
            "",
            "def svd(",
            "-    x: torch.Tensor, full_matrices: bool = True, out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor, full_matrices: bool = True",
            ") -> Union[torch.Tensor, Tuple[torch.Tensor, ...]]:",
            "results = namedtuple(\"svd\", \"U S Vh\")",
            "",
            "-    U, D, VT = torch.linalg.svd(x, full_matrices=full_matrices, out=out)",
            "+    U, D, VT = torch.linalg.svd(x, full_matrices=full_matrices)",
            "ret = results(U, D, VT)",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=Optional))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 389,
        "neg_line": [
            "-x: torch.Tensor, full_matrices: bool = True, out: Optional[torch.Tensor] = None",
            "-U, D, VT = torch.linalg.svd(x, full_matrices=full_matrices, out=out)"
        ],
        "pos_line": [
            "+x: torch.Tensor, full_matrices: bool = True",
            "+U, D, VT = torch.linalg.svd(x, full_matrices=full_matrices)"
        ],
        "core_change": "-x: torch.Tensor, full_matrices: bool = True, out: Optional[torch.Tensor] = None +x: torch.Tensor, full_matrices: bool = True -U, D, VT = torch.linalg.svd(x, full_matrices=full_matrices, out=out) +U, D, VT = torch.linalg.svd(x, full_matrices=full_matrices)",
        "core_API": "svd"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "11d9c07d9..aaf96fcb1 100644",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def _calculate_expected_result(",
            "aggregation_op_only_probs = gumbel_dist.sample()",
            "else:",
            "# <float32>[batch_size, num_aggregation_labels - 1]",
            "-        aggregation_op_only_probs = torch.nn.functional.softmax(",
            "+        aggregation_op_only_probs = nn.functional.softmax(",
            "logits_aggregation[:, 1:] / config.aggregation_temperature, dim=-1",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=., text=.), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 390,
        "neg_line": [
            "-aggregation_op_only_probs = torch.nn.functional.softmax("
        ],
        "pos_line": [
            "+aggregation_op_only_probs = nn.functional.softmax("
        ],
        "core_change": "-aggregation_op_only_probs = torch.nn.functional.softmax( +aggregation_op_only_probs = nn.functional.softmax(",
        "core_API": "sample"
    },
    {
        "commit_hash": "c11270cb427606270f7eb1d88915b7150400fdd8",
        "index": "45949995..c85fbcd2 100644",
        "commit_message": "Fix critical bug for DQN variants and DPG agent\n\n",
        "file": "tensorforce.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorforceModel(Model):",
            "discounts = tf.math.pow(x=discount, y=exponent)",
            "if not self.predict_terminal_values:",
            "discounts = tf.where(",
            "-                    condition=tf.math.greater(x=_terminal, y=one),",
            "-                    x=discounts, y=tf.zeros_like(input=discounts)",
            "+                    condition=tf.math.equal(x=_terminal, y=one),",
            "+                    x=tf.zeros_like(input=discounts), y=discounts",
            ")",
            "",
            "-            reward += discounts * horizon_values",
            "+            reward = reward + discounts * horizon_values",
            "",
            "dependencies = [reward]",
            "if self.summaries == 'all' or 'reward' in self.summaries:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=2222374)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'reward'), position=0, insert_id=2222375)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2222376)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=2222377)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=reward), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2222378)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=2222379)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=6, insert_id=2222380)",
            "Update(target_node=ASTNode(type=identifier, text=y), value='x')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'y'), position=0, insert_id=2222381)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2222382)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'discounts'), position=2, insert_id=2222383)",
            "Update(target_node=ASTNode(type=identifier, text=greater), value='equal')",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=discounts))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=+=, text=+=))",
            "Delete(target_node=ASTNode(type=augmented_assignment))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 392,
        "neg_line": [
            "-condition=tf.math.greater(x=_terminal, y=one),",
            "-x=discounts, y=tf.zeros_like(input=discounts)",
            "-reward += discounts * horizon_values"
        ],
        "pos_line": [
            "+condition=tf.math.equal(x=_terminal, y=one),",
            "+x=tf.zeros_like(input=discounts), y=discounts",
            "+reward = reward + discounts * horizon_values"
        ],
        "core_change": "-condition=tf.math.greater(x=_terminal, y=one), -x=discounts, y=tf.zeros_like(input=discounts) +condition=tf.math.equal(x=_terminal, y=one), +x=tf.zeros_like(input=discounts), y=discounts -reward += discounts * horizon_values +reward = reward + discounts * horizon_values",
        "core_API": "pow"
    },
    {
        "commit_hash": "4963b88f64c4968061a04dc00d741168b506467c",
        "index": "3bebae4c9b..285cb5ea70 100644",
        "commit_message": "Fixed test discovery errors (#7136)\n\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "text fix",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def test_tensorflow_deserialize(",
            "",
            "",
            "@handle_frontend_test(",
            "+    fn_tree=\"tensorflow.keras.activations.get\",",
            "fn_name=st.sampled_from(get_callable_functions(\"keras.activations\")).filter(",
            "lambda x: not x[0].isupper()",
            "and x"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('keyword_argument', None), position=6, insert_id=2488251)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=7, insert_id=2488252)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'fn_tree'), position=0, insert_id=2488253)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2488254)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"tensorflow.keras.activations.get\"'), position=2, insert_id=2488255)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 5,
        "number": 393,
        "neg_line": [],
        "pos_line": [
            "+fn_tree=\"tensorflow.keras.activations.get\","
        ],
        "core_change": "+fn_tree=\"tensorflow.keras.activations.get\",",
        "core_API": "sampled_from"
    },
    {
        "commit_hash": "521d329b975de97ec0b52395f02bb32466b8dc35",
        "index": "a159ece2..fa609860 100755",
        "commit_message": "Fix CI issues related to cupy install (#2483)\n\n* remove any cupy install when setting up environments\n\n* revert previous changes to run on cu111 runners\n\n* fix for when no cupy is installed\n\n* remove cupy uninstall for workflows not using latest torch version\n\n* update to cu116 for inference tests\n\n* fix pip uninstall line\n\n* move python environment list to after DS install\n\n* remove cupy uninstall\n\n* re-add --forked\n\n* fix how we get cupy version (should be based on nvcc version)\n",
        "file": "DeepSpeed.txt.json",
        "label": "yes",
        "comments": "update API call for resource fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "if torch_available and torch.cuda.is_available():",
            "if rocm_major <= 4:",
            "cupy = f\"cupy-rocm-{rocm_major}-{rocm_minor}\"",
            "else:",
            "-        cupy = f\"cupy-cuda{torch.version.cuda.replace('.','')[:3]}\"",
            "+        cupy = f\"cupy-cuda{''.join(map(str,installed_cuda_version()))}\"",
            "if cupy:",
            "extras_require['1bit'].append(cupy)",
            "extras_require['1bit_mpi'].append(cupy)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"cupy-cuda{torch.version.cuda.replace('.','')[:3]}\"), value='f\"cupy-cuda{\\'\\'.join(map(str,installed_cuda_version()))}\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 394,
        "neg_line": [
            "-cupy = f\"cupy-cuda{torch.version.cuda.replace('.','')[:3]}\""
        ],
        "pos_line": [
            "+cupy = f\"cupy-cuda{''.join(map(str,installed_cuda_version()))}\""
        ],
        "core_change": "-cupy = f\"cupy-cuda{torch.version.cuda.replace('.','')[:3]}\" +cupy = f\"cupy-cuda{''.join(map(str,installed_cuda_version()))}\"",
        "core_API": "is_available"
    },
    {
        "commit_hash": "d41fbe5f5e5513baf64a95e5d0b28576c12c17df",
        "index": "38213f08..3b1944b1 100644",
        "commit_message": "Fixes test\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_cgcnn_conv():",
            "edge_index = torch.tensor([[0, 0, 0, 1, 2, 3], [1, 2, 3, 0, 0, 0]])",
            "num_nodes = edge_index.max().item() + 1",
            "x = torch.randn((num_nodes, node_dim))",
            "-    pseudo = torch.rand((edge_index.size(1), 3))",
            "+    pseudo = torch.rand((edge_index.size(1), edge_dim))",
            "",
            "conv = CGCNNConv(node_dim, edge_dim)",
            "assert conv.__repr__() == 'CGCNNConv(16, 16)'"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=('identifier', 'edge_dim'), position=3, insert_id=1040774)",
            "Delete(target_node=ASTNode(type=integer, text=3))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 395,
        "neg_line": [
            "-pseudo = torch.rand((edge_index.size(1), 3))"
        ],
        "pos_line": [
            "+pseudo = torch.rand((edge_index.size(1), edge_dim))"
        ],
        "core_change": "-pseudo = torch.rand((edge_index.size(1), 3)) +pseudo = torch.rand((edge_index.size(1), edge_dim))",
        "core_API": "tensor"
    },
    {
        "commit_hash": "49cfff2ebf791f831018bb3e851c76e572d02cd7",
        "index": "9a3cf4a4..fa41403a 100644",
        "commit_message": "Refactor/framework context to context (#2115)\n\n* refactor(model): rename framework_context to context\n\n* style: reformat\n\n* refactor: context format changes\n\n* test: fix tensorflow_hub test\n\n* fix: tensorflow_hub related tests\n\n* Apply suggestions from code review\n\n* fix: move detectron test sample image to local\n\n* fix: fix onnx rebase\n\n* fix: fix style\n\nCo-authored-by: Chaoyu <paranoyang@gmail.com>\n",
        "file": "BentoML.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def save(",
            "",
            "Examples::",
            "\"\"\"  # noqa",
            "-    context: t.Dict[str, t.Any] = {\"statsmodels\": statsmodels.__version__}",
            "+    context: t.Dict[str, t.Any] = {",
            "+        \"framework_name\": \"statsmodels\",",
            "+        \"pip_dependencies\": [f\"statsmodels=={_statsmodels_version}\"],",
            "+    }",
            "_model = Model.create(",
            "name,",
            "module=__name__,",
            "metadata=metadata,",
            "-        framework_context=context,",
            "+        context=context,",
            ")",
            "",
            "model.save(_model.path_of(f\"{SAVE_NAMESPACE}{PKL_EXT}\"))"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 396,
        "neg_line": [
            "-context: t.Dict[str, t.Any] = {\"statsmodels\": statsmodels.__version__}",
            "-framework_context=context,"
        ],
        "pos_line": [
            "+context: t.Dict[str, t.Any] = {",
            "+\"framework_name\": \"statsmodels\",",
            "+\"pip_dependencies\": [f\"statsmodels=={_statsmodels_version}\"],",
            "+}",
            "+context=context,"
        ],
        "core_change": "-context: t.Dict[str, t.Any] = {\"statsmodels\": statsmodels.__version__} +context: t.Dict[str, t.Any] = { +\"framework_name\": \"statsmodels\", +\"pip_dependencies\": [f\"statsmodels=={_statsmodels_version}\"], +} -framework_context=context, +context=context,",
        "core_API": "create"
    },
    {
        "commit_hash": "03a14462c9c4f9588f4b6897daa6f816f22ebcc0",
        "index": "e1ee1609..2f5d1585 100644",
        "commit_message": "Fixed bug after contrib merging\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def spatial_soft_argmax2d(",
            ">>> coords = kornia.spatial_soft_argmax2d(input, False)",
            "tensor([[[1.0000, 1.0000]]])",
            "\"\"\"",
            "-    input_soft: torch.Tensor = dsnt.spatial_softmax_2d(input, temperature)",
            "-    output: torch.Tensor = dsnt.spatial_softargmax_2d(input_soft,",
            "-                                                      normalized_coordinates)",
            "+    input_soft: torch.Tensor = spatial_softmax_2d(input, temperature)",
            "+    output: torch.Tensor = spatial_softargmax_2d(input_soft,",
            "+                                                 normalized_coordinates)",
            "return output"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=ERROR), position=8)",
            "Insert(target_node=ASTNode(type=ERROR), node=('attribute', None), position=4, insert_id=461301)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=461302)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=461303)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=461304)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=spatial_softmax_2d), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=spatial_softargmax_2d), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=dsnt))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=dsnt))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 397,
        "neg_line": [
            "-input_soft: torch.Tensor = dsnt.spatial_softmax_2d(input, temperature)",
            "-output: torch.Tensor = dsnt.spatial_softargmax_2d(input_soft,",
            "-normalized_coordinates)"
        ],
        "pos_line": [
            "+input_soft: torch.Tensor = spatial_softmax_2d(input, temperature)",
            "+output: torch.Tensor = spatial_softargmax_2d(input_soft,",
            "+normalized_coordinates)"
        ],
        "core_change": "-input_soft: torch.Tensor = dsnt.spatial_softmax_2d(input, temperature) -output: torch.Tensor = dsnt.spatial_softargmax_2d(input_soft, -normalized_coordinates) +input_soft: torch.Tensor = spatial_softmax_2d(input, temperature) +output: torch.Tensor = spatial_softargmax_2d(input_soft, +normalized_coordinates)",
        "core_API": "spatial_soft_argmax2d"
    },
    {
        "commit_hash": "5e6265be6eda62fa338b7110a26b822ef23ec116",
        "index": "e138d1b..a5b83cb 100644",
        "commit_message": "metal timing, fix speed test\n\n",
        "file": "tinygrad.txt.json",
        "label": "no",
        "comments": "test",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSpeed(unittest.TestCase):",
            "",
            "def test_sum(self):",
            "def f(a, b): return a.sum()",
            "+    helper_test_generic_square('sum', 2048, f, f, onearg=True)",
            "helper_test_generic_square('sum', 4096, f, f, onearg=True)",
            "",
            "def test_partial_sum(self):"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1160598)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1160599)",
            "Insert(target_node=IN(type=call), node=('identifier', 'helper_test_generic_square'), position=0, insert_id=1160600)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1160601)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1160602)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'sum'\"), position=1, insert_id=1160603)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1160604)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '2048'), position=3, insert_id=1160605)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=1160606)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'f'), position=5, insert_id=1160607)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=6, insert_id=1160608)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'f'), position=7, insert_id=1160609)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=8, insert_id=1160610)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=9, insert_id=1160611)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=10, insert_id=1160612)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'onearg'), position=0, insert_id=1160613)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1160614)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=1160615)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 398,
        "neg_line": [],
        "pos_line": [
            "+helper_test_generic_square('sum', 2048, f, f, onearg=True)"
        ],
        "core_change": "+helper_test_generic_square('sum', 2048, f, f, onearg=True)",
        "core_API": "sum"
    },
    {
        "commit_hash": "99edb367a6ecb21d57fe412eaabd46b4dc1bd2a0",
        "index": "5c47e1a11b..7223169337 100644",
        "commit_message": "small fixes to test_frontend_function.\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def test_frontend_function(",
            "ivy.set_backend(frontend)",
            "",
            "# check for unsupported dtypes in frontend framework",
            "-    function = getattr(ivy, fn_name)",
            "+    function = getattr(ivy.functional.frontends.__dict__[frontend], fn_name)",
            "for d in input_dtypes:",
            "if d in ivy.function_unsupported_dtypes(function, None):",
            "return"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=1, insert_id=1731872)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=1731873)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1731874)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'frontend'), position=2, insert_id=1731875)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1731876)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1731877)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1731878)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__dict__'), position=2, insert_id=1731879)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1731880)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1731881)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'frontends'), position=2, insert_id=1731882)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=ivy), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1731883)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1731884)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 400,
        "neg_line": [
            "-function = getattr(ivy, fn_name)"
        ],
        "pos_line": [
            "+function = getattr(ivy.functional.frontends.__dict__[frontend], fn_name)"
        ],
        "core_change": "-function = getattr(ivy, fn_name) +function = getattr(ivy.functional.frontends.__dict__[frontend], fn_name)",
        "core_API": "set_backend"
    },
    {
        "commit_hash": "848448405ac1fcb7a237bc2edb9be99aac635963",
        "index": "830e5804b5..eebab5304b 100644",
        "commit_message": "Fixed torch.Tensor.to to support device input as a positional argument\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_torch_instance_to(",
            "frontend,",
            "):",
            "input_dtype, x, method_num_positional_args, method_all_as_kwargs_np = args_kwargs",
            "+    method_flags.num_positional_args = method_num_positional_args",
            "helpers.test_frontend_method(",
            "init_input_dtypes=input_dtype,",
            "init_all_as_kwargs_np={"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=259757)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=259758)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=259759)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=259760)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'method_num_positional_args'), position=2, insert_id=259761)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'method_flags'), position=0, insert_id=259762)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=259763)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'num_positional_args'), position=2, insert_id=259764)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 8,
        "number": 401,
        "neg_line": [],
        "pos_line": [
            "+method_flags.num_positional_args = method_num_positional_args"
        ],
        "core_change": "+method_flags.num_positional_args = method_num_positional_args",
        "core_API": "test_frontend_method"
    },
    {
        "commit_hash": "1d83557fd2e3e6c117bb24cf370152f3575fe3c6",
        "index": "44cfbda0..bf41d9d6 100644",
        "commit_message": "Fix some typos (#759)\n\n* Fix some typos\n\n* add change log for PR #759\n\n* Update CHANGELOG.md\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def absolute_difference_error(output, target, is_mean=False, name=\"mean_squared_",
            "An optional name to attach to this function.",
            "",
            "\"\"\"",
            "-    # with tf.name_scope(\"mean_squared_error_loss\"):",
            "+    # with tf.name_scope(\"absolute_difference_error_loss\"):",
            "if output.get_shape().ndims == 2:  # [batch_size, n_feature]",
            "if is_mean:",
            "loss = tf.reduce_mean(tf.reduce_mean(tf.abs(output - target), 1), name=name)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n    # with tf.name_scope(\"mean_squared_error_loss\"), value='\"\"\"\\n    # with tf.name_scope(\"absolute_difference_error_loss\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 403,
        "neg_line": [
            "-# with tf.name_scope(\"mean_squared_error_loss\"):"
        ],
        "pos_line": [
            "+# with tf.name_scope(\"absolute_difference_error_loss\"):"
        ],
        "core_change": "-# with tf.name_scope(\"mean_squared_error_loss\"): +# with tf.name_scope(\"absolute_difference_error_loss\"):",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "e4e1a604..29e8489a 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "update param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UnigramRecall(Metric):",
            "A tensor of predictions of shape (batch_size, k, sequence_length).",
            "gold_labels : `torch.Tensor`, required.",
            "A tensor of integer class label of shape (batch_size, sequence_length).",
            "-        mask : `torch.Tensor`, optional (default = None).",
            "+        mask : `torch.BoolTensor`, optional (default = None).",
            "A masking tensor the same size as `gold_labels`.",
            "\"\"\"",
            "predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=`torch.Tensor`), value='`torch.BoolTensor`')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 407,
        "neg_line": [
            "-mask : `torch.Tensor`, optional (default = None)."
        ],
        "pos_line": [
            "+mask : `torch.BoolTensor`, optional (default = None)."
        ],
        "core_change": "-mask : `torch.Tensor`, optional (default = None). +mask : `torch.BoolTensor`, optional (default = None).",
        "core_API": "detach_tensors"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "df0f69a4..59420f3c 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"        # compute the gating function and one minus the gating function\\n\",",
            "\"        gate_intermediate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\\n\",",
            "\"        gate = self.sigmoid(self.lin_gate_hidden_to_z(gate_intermediate))\\n\",",
            "-    \"        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\\n\",",
            "+    \"        one_minus_gate = torch.ones(gate.size()).type_as(gate) - gate\\n\",",
            "\"        # compute the 'proposed mean'\\n\",",
            "\"        proposed_mean_intermediate = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\\n\",",
            "\"        proposed_mean = self.lin_proposed_mean_hidden_to_z(proposed_mean_intermediate)\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\\n\"), value='\"        one_minus_gate = torch.ones(gate.size()).type_as(gate) - gate\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 408,
        "neg_line": [
            "-\"        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\\n\","
        ],
        "pos_line": [
            "+\"        one_minus_gate = torch.ones(gate.size()).type_as(gate) - gate\\n\","
        ],
        "core_change": "-\"        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\\n\", +\"        one_minus_gate = torch.ones(gate.size()).type_as(gate) - gate\\n\",",
        "core_API": "relu"
    },
    {
        "commit_hash": "ca052849f34bb2fc78e6402cc7357f2748e92cae",
        "index": "5e04b349..76023240 100644",
        "commit_message": "fix formulation issue in rotation_matrix_to_quaternion\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rotation_matrix_to_quaternion(",
            "return torch.cat([qx, qy, qz, qw], dim=-1)",
            "",
            "def cond_3():",
            "-        sq = torch.sqrt(trace + 1.0 + m00 - m11 - m22) * 2.  # sq = 4 * qw.",
            "+        sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.  # sq = 4 * qw.",
            "qw = safe_zero_division(m10 - m01, sq)",
            "qx = safe_zero_division(m02 - m20, sq)",
            "qy = safe_zero_division(m12 - m21, sq)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('+', '+'), position=1, insert_id=465201)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('identifier', 'eps'), position=2, insert_id=465202)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=m22), position=2)",
            "Move(target_node=ASTNode(type=+, text=+), node=ASTNode(type=binary_operator), position=2)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('-', '-'), position=2, insert_id=465203)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=m11), position=3)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=m00), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=trace))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=-, text=-))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 410,
        "neg_line": [
            "-sq = torch.sqrt(trace + 1.0 + m00 - m11 - m22) * 2.  # sq = 4 * qw."
        ],
        "pos_line": [
            "+sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.  # sq = 4 * qw."
        ],
        "core_change": "-sq = torch.sqrt(trace + 1.0 + m00 - m11 - m22) * 2.  # sq = 4 * qw. +sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.  # sq = 4 * qw.",
        "core_API": "cat"
    },
    {
        "commit_hash": "586b4dea61ff69b7fdbf5872fbf96c93ffc1e161",
        "index": "e8088842..7556f6a5 100644",
        "commit_message": "Do not ignore undefined names in Python code (#1210)\n\n* Undefined name: import io for line 86\n\nDiscovered via #1209\n\n* Undefined name: import Tuple for lines 103, 145, 187\n\n* Undefined name: import Dict for line 35\n\n* Undefined name: import Dict for line 158\n\n* Undefined name: import RandomCrop for lines 830 and 841\n\n* Undefined name: create_pinhole() -> dgm.utils.create_pinhole()\n\n* Undefined name: import rtvec_to_pose for line 505\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Raising NameErrors inside with pytest.raises(Exception):\n\n* Undefined name: op_script = torch.jit.script(op)\n\n* Do not ignore undefined names in Python code\n\n* Do not ignore undefined names in Python code\n\n* Do not ignore undefined names in Python code\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* black --diff to help contributors\n\n* requirements-dev.txt: Add torchgeometry\n\n* Ignore missing rtvec_to_pose()\n\n* Revert changes to requirements-dev.txt\n\n* # noqa: F821 type: ignore\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "test",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestPosterize(BaseTester):",
            "@pytest.mark.jit",
            "def test_jit(self, device, dtype):",
            "op = torch.jit.script(kornia.enhance.adjust.posterize)",
            "+        op_script = torch.jit.script(op)",
            "inputs = torch.rand(2, 1, 3, 3).to(device=device, dtype=dtype)",
            "expected = op(input, 8)",
            "actual = op_script(input, 8)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=419506)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=419507)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'op_script'), position=0, insert_id=419508)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=419509)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=419510)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=419511)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=419512)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=419513)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=419514)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'script'), position=2, insert_id=419515)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=419516)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'op'), position=1, insert_id=419517)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=419518)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=419519)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=419520)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'jit'), position=2, insert_id=419521)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 411,
        "neg_line": [],
        "pos_line": [
            "+op_script = torch.jit.script(op)"
        ],
        "core_change": "+op_script = torch.jit.script(op)",
        "core_API": "script"
    },
    {
        "commit_hash": "20a9acca2636512522116601ae09c6be0408b486",
        "index": "e1aeb970..8b79ce55 100644",
        "commit_message": "Speed-up warp_affine and fix bugs in RandomAffine [WIP: to add tests] (#474)\n\n* speed-up warp_affine, rotate, random_crop\n\n* added basic speed test for warp_affine\n\n* fixed centerization for random shear and bug (radians instead of degrees)\n\n* add test versus torchvision\n\n* added convert_affinematrix_to_homography function\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "change API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def crop_by_boxes(tensor, src_box, dst_box,",
            "dst_trans_src = dst_trans_src.expand(tensor.shape[0], -1, -1)",
            "",
            "bbox = _infer_bounding_box(dst_box)",
            "-    patches: torch.Tensor = warp_perspective(",
            "-        tensor, dst_trans_src, (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))",
            "+    patches: torch.Tensor = warp_affine(",
            "+        tensor, dst_trans_src[:, :2, :], (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))",
            "",
            "# return in the original shape",
            "if is_unbatched:"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=warp_perspective), value='warp_affine')",
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=3, insert_id=443248)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=443249)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=dst_trans_src), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=443250)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=443251)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=,, text=,), position=3)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=4, insert_id=443252)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=5, insert_id=443253)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=6, insert_id=443254)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=7, insert_id=443255)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=443256)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=443257)",
            "Insert(target_node=IN(type=slice), node=('integer', '2'), position=1, insert_id=443258)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=443259)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 412,
        "neg_line": [
            "-patches: torch.Tensor = warp_perspective(",
            "-tensor, dst_trans_src, (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))"
        ],
        "pos_line": [
            "+patches: torch.Tensor = warp_affine(",
            "+tensor, dst_trans_src[:, :2, :], (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))"
        ],
        "core_change": "-patches: torch.Tensor = warp_perspective( -tensor, dst_trans_src, (int(bbox[0].int().data.item()), int(bbox[1].int().data.item()))) +patches: torch.Tensor = warp_affine( +tensor, dst_trans_src[:, :2, :], (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))",
        "core_API": "expand"
    },
    {
        "commit_hash": "7bb236c71fc70c97ee5657010c05cfd489bd8356",
        "index": "cacd44a..4c969d8 100644",
        "commit_message": "Fix typo\n\n",
        "file": "MMdnn.txt.json",
        "label": "no",
        "comments": "typo fix",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def KitModel(weight_file = None):",
            "if not dilations:",
            "dilations = [1] * len(IR_node.get_attr('kernel_shape'))",
            "",
            "-        self.add_body(1, \"{:<15} = layers.Conv2DTranpose(name='{}', {}, kernel_size=({}), strides=({}), dilation_rate=({}), padding='{}', use_bias={})({})\".format(",
            "+        self.add_body(1, \"{:<15} = layers.Conv2DTranspose(name='{}', {}, kernel_size=({}), strides=({}), dilation_rate=({}), padding='{}', use_bias={})({})\".format(",
            "IR_node.variable_name,",
            "IR_node.name,",
            "filters_str,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"{:<15} = layers.Conv2DTranpose(name='{}', {}, kernel_size=({}), strides=({}), dilation_rate=({}), padding='{}', use_bias={})({})\"), value='\"{:<15} = layers.Conv2DTranspose(name=\\'{}\\', {}, kernel_size=({}), strides=({}), dilation_rate=({}), padding=\\'{}\\', use_bias={})({})\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 413,
        "neg_line": [
            "-self.add_body(1, \"{:<15} = layers.Conv2DTranpose(name='{}', {}, kernel_size=({}), strides=({}), dilation_rate=({}), padding='{}', use_bias={})({})\".format("
        ],
        "pos_line": [
            "+self.add_body(1, \"{:<15} = layers.Conv2DTranspose(name='{}', {}, kernel_size=({}), strides=({}), dilation_rate=({}), padding='{}', use_bias={})({})\".format("
        ],
        "core_change": "-self.add_body(1, \"{:<15} = layers.Conv2DTranpose(name='{}', {}, kernel_size=({}), strides=({}), dilation_rate=({}), padding='{}', use_bias={})({})\".format( +self.add_body(1, \"{:<15} = layers.Conv2DTranspose(name='{}', {}, kernel_size=({}), strides=({}), dilation_rate=({}), padding='{}', use_bias={})({})\".format(",
        "core_API": "get_attr"
    },
    {
        "commit_hash": "8c23a747ceb505c8a31ed743faa0c22e0d7c88c3",
        "index": "4751b403..ff1e94d2 100644",
        "commit_message": "Fix bipartite message passing with `target_to_source` flow (#3907)\n\n* fix bipartite message passing with reverse flow\n\n* typo\n\n* update\n\n* fix test\n\n* update\n\n* update\n\n* update\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "not clear",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_dynamic_edge_conv_conv():",
            "jit = torch.jit.script(conv.jittable(t))",
            "assert jit((x1, x2)).tolist() == out21.tolist()",
            "assert jit((x1, x2), (batch1, batch2)).tolist() == out22.tolist()",
            "+",
            "+    torch.jit.script(conv.jittable())  # Test without explicit typing."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=990373)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=990374)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=990375)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=990376)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=990377)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=990378)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'script'), position=2, insert_id=990379)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=990380)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=990381)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=990382)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=990383)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=990384)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'jit'), position=2, insert_id=990385)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=990386)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=990387)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'conv'), position=0, insert_id=990388)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=990389)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'jittable'), position=2, insert_id=990390)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=990391)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=990392)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 414,
        "neg_line": [],
        "pos_line": [
            "+",
            "+torch.jit.script(conv.jittable())  # Test without explicit typing."
        ],
        "core_change": "+ +torch.jit.script(conv.jittable())  # Test without explicit typing.",
        "core_API": "script"
    },
    {
        "commit_hash": "2b68b2fe27ae6d6ad98499b8a81aba26f6d5c36a",
        "index": "ba5d55304b..7d6c67e46b 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def gradient(",
            "",
            "",
            "def xlogy(",
            "-    x: torch.tensor,",
            "-    y: torch.tensor,",
            "-    /,",
            "-    *,",
            "-    out: Optional[torch.tensor] = None",
            "+    x: torch.tensor, y: torch.tensor, /, *, out: Optional[torch.tensor] = None",
            ") -> torch.tensor:",
            "return torch.xlogy(x, y, out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 17,
        "number": 415,
        "neg_line": [
            "-x: torch.tensor,",
            "-y: torch.tensor,",
            "-/,",
            "-*,",
            "-out: Optional[torch.tensor] = None"
        ],
        "pos_line": [
            "+x: torch.tensor, y: torch.tensor, /, *, out: Optional[torch.tensor] = None"
        ],
        "core_change": "-x: torch.tensor, -y: torch.tensor, -/, -*, -out: Optional[torch.tensor] = None +x: torch.tensor, y: torch.tensor, /, *, out: Optional[torch.tensor] = None",
        "core_API": "xlogy"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "b3f49f1022..b8ad9f8011 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "change API doc",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def ddpg_actor_critic_loss(policy, model, _, train_batch):",
            "twin_q_t = model.get_twin_q_values(model_out_t,",
            "train_batch[SampleBatch.ACTIONS])",
            "# q_batchnorm_update_ops = list(",
            "-    #     set(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) - prev_update_ops)",
            "+    #     set(tf1.get_collection(tf.GraphKeys.UPDATE_OPS)) - prev_update_ops)",
            "",
            "# Target q-net(s) evaluation.",
            "q_tp1 = policy.target_model.get_q_values(target_model_out_tp1,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 416,
        "neg_line": [
            "-#     set(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) - prev_update_ops)"
        ],
        "pos_line": [
            "+#     set(tf1.get_collection(tf.GraphKeys.UPDATE_OPS)) - prev_update_ops)"
        ],
        "core_change": "-#     set(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) - prev_update_ops) +#     set(tf1.get_collection(tf.GraphKeys.UPDATE_OPS)) - prev_update_ops)",
        "core_API": "get_twin_q_values"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "7d0e98a4..d710b0de 100755",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "if get_current_tower_context().is_training:",
            "wd_w = tf.train.exponential_decay(2e-4, get_global_step_var(),",
            "80000, 0.7, True)",
            "-            wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')",
            "+            wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')",
            "costs.append(wd_cost)",
            "",
            "add_param_summary(('.*/W', ['histogram']))   # monitor W"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mul), value='multiply')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 417,
        "neg_line": [
            "-wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')"
        ],
        "pos_line": [
            "+wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')"
        ],
        "core_change": "-wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost') +wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')",
        "core_API": "exponential_decay"
    },
    {
        "commit_hash": "cc8b4156dfdae012a834109be74bd51d43edcb2b",
        "index": "75fa4e1..1dab52b 100644",
        "commit_message": "More speed improvements, doc for the model, fixed improper cycling in random_partial_utterances, new color scale for UMAP\n\n",
        "file": "Real-Time-Voice-Cloning.txt.json",
        "label": "yes",
        "comments": "remove API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if __name__ == '__main__':",
            "loss_values.clear()",
            "accuracies.clear()",
            "if step % 100 == 0:",
            "-            vis.draw_projections(embeds.detach().cpu().numpy(), utterances_per_speaker, step)",
            "+            vis.draw_projections(embeds.detach().numpy(), utterances_per_speaker, step)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cpu))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 418,
        "neg_line": [
            "-vis.draw_projections(embeds.detach().cpu().numpy(), utterances_per_speaker, step)"
        ],
        "pos_line": [
            "+vis.draw_projections(embeds.detach().numpy(), utterances_per_speaker, step)"
        ],
        "core_change": "-vis.draw_projections(embeds.detach().cpu().numpy(), utterances_per_speaker, step) +vis.draw_projections(embeds.detach().numpy(), utterances_per_speaker, step)",
        "core_API": "clear"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "5e4fe402..293df408 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "change API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScoreSdeVpScheduler(SchedulerMixin, ConfigMixin):",
            "x_mean = x + drift * dt",
            "",
            "# add noise",
            "-        noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device)",
            "+        noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)",
            "x = x_mean + diffusion * math.sqrt(-dt) * noise",
            "",
            "return x, x_mean"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=91905)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=91906)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=91907)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=9, insert_id=91908)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=91909)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=91910)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=91911)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=91912)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=91913)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=91914)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=91915)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=91916)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 419,
        "neg_line": [
            "-noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device)"
        ],
        "pos_line": [
            "+noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)"
        ],
        "core_change": "-noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device) +noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)",
        "core_API": "randn"
    },
    {
        "commit_hash": "34d20a813eb09857cc571f9754de34f8b6f7ec5c",
        "index": "572491ef..4444b84a 100755",
        "commit_message": "fix #1182\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update param for math fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_keras_model():",
            "M.add(KL.Conv2D(32, 3, padding='same', activation='relu'))",
            "M.add(KL.Flatten())",
            "M.add(KL.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-5)))",
            "-        M.add(KL.Dropout(0.5))",
            "+        M.add(KL.Dropout(rate=0.5))",
            "M.add(KL.Dense(10, activation=None, kernel_regularizer=keras.regularizers.l2(1e-5)))",
            "return M"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2275600)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rate'), position=0, insert_id=2275601)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2275602)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=float, text=0.5), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 422,
        "neg_line": [
            "-M.add(KL.Dropout(0.5))"
        ],
        "pos_line": [
            "+M.add(KL.Dropout(rate=0.5))"
        ],
        "core_change": "-M.add(KL.Dropout(0.5)) +M.add(KL.Dropout(rate=0.5))",
        "core_API": "add"
    },
    {
        "commit_hash": "5ea92e7ee2ad73fe891e0586ac9bf5b0c35f84e1",
        "index": "bfdddee..cb2ea0f 100755",
        "commit_message": "FIX: trainig fails if targets list is empty (#198)\n\n* FIX: trainig fails if targets list is empty\n\n* Update utils.py\n\n",
        "file": "yolov3.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def build_targets(model, targets):",
            "",
            "# Class",
            "tcls.append(c)",
            "-        if c.shape[0]:",
            "+        if nt:",
            "assert c.max() <= layer.nC, 'Target classes exceed model classes'",
            "",
            "return txy, twh, tcls, indices"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=c), value='nt')",
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=identifier, text=c), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=shape))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 423,
        "neg_line": [
            "-if c.shape[0]:"
        ],
        "pos_line": [
            "+if nt:"
        ],
        "core_change": "-if c.shape[0]: +if nt:",
        "core_API": "append"
    },
    {
        "commit_hash": "f3680fd80426d384f5b3764953e90d7c4b0968f8",
        "index": "6acfe889..2827fdf1 100644",
        "commit_message": "adding eval lm changes for model parallel (#1113)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding ÔøΩ\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1113\n\nReviewed By: myleott\n\nDifferential Revision: D20670665\n\nfbshipit-source-id: 8e2846637195b7200f1f60a8421d2fe5ffab789b\n\n",
        "file": "fairseq.txt.json",
        "label": "yes",
        "comments": "change API call for resource fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def main(parsed_args):",
            "",
            "def cli_main():",
            "parser = options.get_eval_lm_parser()",
            "+    add_distributed_training_args(parser)",
            "args = options.parse_args_and_arch(parser)",
            "-    main(args)",
            "+    distributed_utils.call_main(args, main)",
            "",
            "",
            "if __name__ == '__main__':"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1831350)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1831351)",
            "Insert(target_node=IN(type=call), node=('identifier', 'add_distributed_training_args'), position=0, insert_id=1831352)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1831353)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1831354)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1831355)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'parser'), position=1, insert_id=1831356)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1831357)",
            "Update(target_node=ASTNode(type=identifier, text=main), value='distributed_utils')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=main), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1831358)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'call_main'), position=2, insert_id=1831359)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1831360)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'main'), position=3, insert_id=1831361)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 424,
        "neg_line": [
            "-main(args)"
        ],
        "pos_line": [
            "+add_distributed_training_args(parser)",
            "+distributed_utils.call_main(args, main)"
        ],
        "core_change": "+add_distributed_training_args(parser) -main(args) +distributed_utils.call_main(args, main)",
        "core_API": "get_eval_lm_parser"
    },
    {
        "commit_hash": "8f6da3ba463d7f22ac48290c983d34b83d84a8d3",
        "index": "1f68c056..7d378240 100644",
        "commit_message": "degree device bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def degree(index, num_nodes=None, dtype=None, device=None):",
            "tensor([3., 1., 1.])",
            "\"\"\"",
            "num_nodes = maybe_num_nodes(index, num_nodes)",
            "-    out = torch.zeros((num_nodes), dtype=dtype, device=device)",
            "+    out = torch.zeros((num_nodes), dtype=dtype, device=index.device)",
            "return out.scatter_add_(0, index, out.new_ones((index.size(0))))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=1067685)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'index'), position=0, insert_id=1067686)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1067687)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=device), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 425,
        "neg_line": [
            "-out = torch.zeros((num_nodes), dtype=dtype, device=device)"
        ],
        "pos_line": [
            "+out = torch.zeros((num_nodes), dtype=dtype, device=index.device)"
        ],
        "core_change": "-out = torch.zeros((num_nodes), dtype=dtype, device=device) +out = torch.zeros((num_nodes), dtype=dtype, device=index.device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "4e58e987269247d79831d97d2e28364273b6fee5",
        "index": "b7eb26d09..eb1fecb98 100644",
        "commit_message": "fix ctc init condition\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "change condition check for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransducerTasks(torch.nn.Module):",
            "if ctc_loss:",
            "self.ctc_lin = torch.nn.Linear(encoder_dim, output_dim)",
            "",
            "-            if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\"):",
            "+            if LooseVersion(torch.__version__) > LooseVersion(\"1.0.1\"):",
            "self.ctc_loss = torch.nn.CTCLoss(",
            "blank=blank_id,",
            "reduction=\"sum\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('>', '>'), position=1, insert_id=139920)",
            "Update(target_node=ASTNode(type=string, text=\"1.7.0\"), value='\"1.0.1\"')",
            "Delete(target_node=ASTNode(type=<, text=<))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 426,
        "neg_line": [
            "-if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\"):"
        ],
        "pos_line": [
            "+if LooseVersion(torch.__version__) > LooseVersion(\"1.0.1\"):"
        ],
        "core_change": "-if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\"): +if LooseVersion(torch.__version__) > LooseVersion(\"1.0.1\"):",
        "core_API": "Linear"
    },
    {
        "commit_hash": "9329a5bed2aa05bbc1e3a51707335156638f9060",
        "index": "bb707312..b4526b21 100644",
        "commit_message": "[Feat] Initiate AutoAugment modules (#2181)\n\n* init\n\n* Added autoaugment\n\n* Added RandAugment\n\n* Added trivial augment\n\n* Added missing files\n\n* Updated docs\n\n* refactoring\n\n* Added missing files\n\n* Added shear and translate functions\n\n* update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fixed translate bug\n\n* Added color\n\n* Added autocontrast\n\n* Fix\n\n* Fix doctest\n\n* Fixed typing\n\n* Removed legacy generators\n\n* fix\n\n* fixed cutmix float64 bug\n\n* include more test\n\n* Sequential support first commit\n\n* Added missing files\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* first refactor\n\n* Make it compatible with AugmentationSequential\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Added more tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make ops module lazy loading\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make policy module lazy loading\n\n* Lazy loading auto module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make augmentation module lazy loading\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* final fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Apply suggestions from code review\n\nCo-authored-by: Jo√£o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n\n* Revert review commnets\n\n* Update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\nCo-authored-by: Jo√£o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomThinPlateSpline(AugmentationBase2D):",
            "",
            "def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:",
            "B, _, _, _ = shape",
            "-        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2",
            "+        src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2",
            "dst = src + self.dist.rsample(src.shape)",
            "return dict(src=src, dst=dst)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tensor), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 429,
        "neg_line": [
            "-src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2"
        ],
        "pos_line": [
            "+src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2"
        ],
        "core_change": "-src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2 +src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2",
        "core_API": "tensor"
    },
    {
        "commit_hash": "323c266cfe65f16f522092f1bc84998e04ed7f94",
        "index": "aeed2f4b..7734d6ef 100755",
        "commit_message": "[Bug Fixed] use torch.cuda.is_available() (#2661)\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "no",
        "comments": "typo fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class FP16_Optimizer(DeepSpeedOptimizer):",
            "self.deepspeed = deepspeed",
            "self.has_moe_layers = has_moe_layers",
            "self.using_pipeline = self.deepspeed.pipeline_parallelism",
            "-        if not torch.cuda.is_available:",
            "+        if not torch.cuda.is_available():",
            "raise SystemError(\"Cannot use fp16 without CUDA.\")",
            "self.optimizer = init_optimizer"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=not_operator), node=('call', None), position=1, insert_id=1816807)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1816808)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816809)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816810)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 431,
        "neg_line": [
            "-if not torch.cuda.is_available:"
        ],
        "pos_line": [
            "+if not torch.cuda.is_available():"
        ],
        "core_change": "-if not torch.cuda.is_available: +if not torch.cuda.is_available():",
        "core_API": "is_available"
    },
    {
        "commit_hash": "726aba089d12503249d824bbaf4070f47d0fe44d",
        "index": "3974335a..86e9b35c 100644",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PNDMScheduler(SchedulerMixin, ConfigMixin):",
            "::-1",
            "].copy()  # we copy to avoid having negative strides which are not supported by torch.from_numpy",
            "",
            "-        self.timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)",
            "+        timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)",
            "+        self.timesteps = torch.from_numpy(timesteps).to(device)",
            "",
            "self.ets = []",
            "self.counter = 0"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=104050)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=104051)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'timesteps'), position=0, insert_id=104052)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=104053)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=104054)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=104055)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=104056)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=104057)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=104058)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=104059)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=104060)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=104061)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=104062)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=104063)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=104064)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=104065)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=104066)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'from_numpy'), position=2, insert_id=104067)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=104068)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'timesteps'), position=1, insert_id=104069)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=104070)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 432,
        "neg_line": [
            "-self.timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)"
        ],
        "pos_line": [
            "+timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)",
            "+self.timesteps = torch.from_numpy(timesteps).to(device)"
        ],
        "core_change": "-self.timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64) +timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64) +self.timesteps = torch.from_numpy(timesteps).to(device)",
        "core_API": "concatenate"
    },
    {
        "commit_hash": "d649c4e9ebebe83d440b5adff792229dc42868ba",
        "index": "3077f392..11074c0f 100644",
        "commit_message": "fix homography regression example (#349)\n\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def HomographyRegressionApp():",
            "[-1, 1],  # top-right",
            "]]).to(dst_homo_src.device)",
            "# transform points",
            "-            pts_dst = dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)",
            "+            pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)",
            "",
            "def compute_factor(size):",
            "return 1.0 * size / 2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=dgm), value='torch')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 433,
        "neg_line": [
            "-pts_dst = dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)"
        ],
        "pos_line": [
            "+pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)"
        ],
        "core_change": "-pts_dst = dgm.transform_points(dgm.inverse(dst_homo_src), pts_src) +pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)",
        "core_API": "transform_points"
    },
    {
        "commit_hash": "007bcb4d022494885fb8f9d18e64e1eab87927ff",
        "index": "db788c3..2d6db4c 100644",
        "commit_message": "fix examples broken by external changes\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "if __name__ == \"__main__\":",
            "print(\"args:\", args)",
            "",
            "cluster = TFCluster.run(sc, main_fun, args, args.cluster_size, num_ps=0, tensorboard=args.tensorboard, input_mode=TFCluster.InputMode.TENSORFLOW, log_dir=args.model_dir, master_node='chief', eval_node=True)",
            "-  cluster.shutdown(grace_secs=120)",
            "+  cluster.shutdown(grace_secs=60)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=120), value='60')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 435,
        "neg_line": [
            "-cluster.shutdown(grace_secs=120)"
        ],
        "pos_line": [
            "+cluster.shutdown(grace_secs=60)"
        ],
        "core_change": "-cluster.shutdown(grace_secs=120) +cluster.shutdown(grace_secs=60)",
        "core_API": "run"
    },
    {
        "commit_hash": "c2e70ca76872daf5c975bf8256a210938eaf3599",
        "index": "f842a834..596d43c9 100644",
        "commit_message": "upgrade to pytorch 0.4.1 + make work with python 3.7 (but still 3.6 also) (#1543)\n\n* changes for pytorch 0.4.1\n\n* increase tolerance for srl test\n\n* update versions in setup.py\n\n* add script to check requirements.txt vs setup.py + fix setup.py\n\n* loosen bounds on pytorch version\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class Highway(torch.nn.Module):",
            "# above, too.",
            "nonlinear_part, gate = projected_input.chunk(2, dim=-1)",
            "nonlinear_part = self._activation(nonlinear_part)",
            "-            gate = torch.nn.functional.sigmoid(gate)",
            "+            gate = torch.sigmoid(gate)",
            "current_input = gate * linear_part + (1 - gate) * nonlinear_part",
            "return current_input"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=functional))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 436,
        "neg_line": [
            "-gate = torch.nn.functional.sigmoid(gate)"
        ],
        "pos_line": [
            "+gate = torch.sigmoid(gate)"
        ],
        "core_change": "-gate = torch.nn.functional.sigmoid(gate) +gate = torch.sigmoid(gate)",
        "core_API": "chunk"
    },
    {
        "commit_hash": "b80010033a33208baabbca4024515a27166747da",
        "index": "dff185be..4c467371 100644",
        "commit_message": "fix is_tensor\n\n",
        "file": "TensorLayer.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(object):",
            "\"It should be either Tensor or a list of Tensor.\"",
            ")",
            "for idx in range(len(check_argu)):",
            "-                        if not isinstance(check_argu[idx], tf_ops._TensorLike) or not tf_ops.is_dense_tensor_like(",
            "+                        if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like(",
            "check_argu[idx]):",
            "raise TypeError(",
            "\"The argument `%s` should be either Tensor or a list of Tensor \" % (check_order[co]) +"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=3, insert_id=2249321)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2249322)",
            "Move(target_node=IN(type=list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=2249323)",
            "Insert(target_node=IN(type=list), node=('attribute', None), position=3, insert_id=2249324)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=4, insert_id=2249325)",
            "Insert(target_node=IN(type=list), node=('attribute', None), position=5, insert_id=2249326)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=6, insert_id=2249327)",
            "Update(target_node=ASTNode(type=identifier, text=tf_ops), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=_TensorLike), value='Tensor')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2249328)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2249329)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'SparseTensor'), position=2, insert_id=2249330)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2249331)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2249332)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Variable'), position=2, insert_id=2249333)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 437,
        "neg_line": [
            "-if not isinstance(check_argu[idx], tf_ops._TensorLike) or not tf_ops.is_dense_tensor_like("
        ],
        "pos_line": [
            "+if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like("
        ],
        "core_change": "-if not isinstance(check_argu[idx], tf_ops._TensorLike) or not tf_ops.is_dense_tensor_like( +if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like(",
        "core_API": "is_dense_tensor_like"
    },
    {
        "commit_hash": "c07a02a4b7892edfee22cbe57d3cdd9e10ae7a4d",
        "index": "eca13d433..06a6f010d 100644",
        "commit_message": "Update vision docstring bool masked pos (#22237)\n\n* Add bool_masked_pos to forward docstrings\n\n* Add note about mask ratio - videomae\n\n* Fix up\n\n* Fix indenting\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFData2VecVisionModel(TFData2VecVisionPreTrainedModel):",
            "return_dict: Optional[bool] = None,",
            "training: bool = False,",
            ") -> Union[tuple, TFData2VecVisionModelOutputWithPooling]:",
            "+        r\"\"\"",
            "+        bool_masked_pos (`tf.Tensor` of shape `(batch_size, num_patches)`, *optional*):",
            "+            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).",
            "+        \"\"\"",
            "outputs = self.data2vec_vision(",
            "pixel_values=pixel_values,",
            "bool_masked_pos=bool_masked_pos,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=3, insert_id=2357101)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2357102)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=->, text=->), position=1)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2357103)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=outputs), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=subscript), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=type), position=2)",
            "Insert(target_node=ASTNode(type=type), node=('string', 'r\"\"\"\\n        bool_masked_pos (`tf.Tensor` of shape `(batch_size, num_patches)`, *optional*):\\n            Boolean masked positions. Indicates which patches are masked (1) and which aren\\'t (0).\\n        \"\"\"'), position=0, insert_id=2357104)"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 10,
        "number": 438,
        "neg_line": [],
        "pos_line": [
            "+r\"\"\"",
            "+bool_masked_pos (`tf.Tensor` of shape `(batch_size, num_patches)`, *optional*):",
            "+Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).",
            "+\"\"\""
        ],
        "core_change": "+r\"\"\" +bool_masked_pos (`tf.Tensor` of shape `(batch_size, num_patches)`, *optional*): +Boolean masked positions. Indicates which patches are masked (1) and which aren't (0). +\"\"\"",
        "core_API": "data2vec_vision"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "ba218ec7..96ec233f 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBasicTextFieldEmbedder(AllenNlpTestCase):",
            "})",
            "token_embedder = BasicTextFieldEmbedder.from_params(self.vocab, params)",
            "inputs = {",
            "-                'words': Variable(torch.rand(3, 4, 5, 6) * 20).long(),",
            "-                'characters': Variable(torch.rand(3, 4, 5, 6, 7) * 15).long(),",
            "+                'words': (torch.rand(3, 4, 5, 6) * 20).long(),",
            "+                'characters': (torch.rand(3, 4, 5, 6, 7) * 15).long(),",
            "}",
            "assert token_embedder(inputs, num_wrapping_dims=2).size() == (3, 4, 5, 6, 12)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=37133)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=attribute), node=('parenthesized_expression', None), position=0, insert_id=37134)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=37135)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=37136)",
            "Insert(target_node=ASTNode(type=attribute), node=('parenthesized_expression', None), position=0, insert_id=37137)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=), text=)), position=2)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 439,
        "neg_line": [
            "-'words': Variable(torch.rand(3, 4, 5, 6) * 20).long(),",
            "-'characters': Variable(torch.rand(3, 4, 5, 6, 7) * 15).long(),"
        ],
        "pos_line": [
            "+'words': (torch.rand(3, 4, 5, 6) * 20).long(),",
            "+'characters': (torch.rand(3, 4, 5, 6, 7) * 15).long(),"
        ],
        "core_change": "-'words': Variable(torch.rand(3, 4, 5, 6) * 20).long(), -'characters': Variable(torch.rand(3, 4, 5, 6, 7) * 15).long(), +'words': (torch.rand(3, 4, 5, 6) * 20).long(), +'characters': (torch.rand(3, 4, 5, 6, 7) * 15).long(),",
        "core_API": "from_params"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "7e802b28..be658ad9 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Metric(Registrable):",
            "return cls.by_name(metric_type)(**params.as_dict())  # type: ignore",
            "",
            "@staticmethod",
            "-    def unwrap_to_tensors(*tensors):",
            "+    def unwrap_to_tensors(*tensors: torch.Tensor):",
            "\"\"\"",
            "-        If you actually passed in Variables to a Metric instead of Tensors, there will be",
            "+        If you actually passed gradient-tracking Tensors to a Metric, there will be",
            "a huge memory leak, because it will prevent garbage collection for the computation",
            "graph. This method ensures that you're using tensors directly and that they are on",
            "the CPU.",
            "\"\"\"",
            "-        return (x.data.cpu() if isinstance(x, torch.autograd.Variable) else x for x in tensors)",
            "+        return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        If you actually passed in Variables to a Metric instead of Tensors, there will be\na huge memory leak, because it will prevent garbage collection for the computation\ngraph. This method ensures that you're using tensors directly and that they are on\nthe CPU.\n\"\"\"), value='\"\"\"\\n        If you actually passed gradient-tracking Tensors to a Metric, there will be\\na huge memory leak, because it will prevent garbage collection for the computation\\ngraph. This method ensures that you\\'re using tensors directly and that they are on\\nthe CPU.\\n\"\"\"')",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=1, insert_id=37336)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=list_splat_pattern), position=0)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=37337)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=37338)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=37339)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=37340)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=37341)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=37342)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=37343)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=37344)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=37345)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=37346)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=autograd), value='Tensor')",
            "Update(target_node=ASTNode(type=identifier, text=data), value='detach')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 440,
        "neg_line": [
            "-def unwrap_to_tensors(*tensors):",
            "-If you actually passed in Variables to a Metric instead of Tensors, there will be",
            "-return (x.data.cpu() if isinstance(x, torch.autograd.Variable) else x for x in tensors)"
        ],
        "pos_line": [
            "+def unwrap_to_tensors(*tensors: torch.Tensor):",
            "+If you actually passed gradient-tracking Tensors to a Metric, there will be",
            "+return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)"
        ],
        "core_change": "-def unwrap_to_tensors(*tensors): +def unwrap_to_tensors(*tensors: torch.Tensor): -If you actually passed in Variables to a Metric instead of Tensors, there will be +If you actually passed gradient-tracking Tensors to a Metric, there will be -return (x.data.cpu() if isinstance(x, torch.autograd.Variable) else x for x in tensors) +return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)",
        "core_API": "by_name"
    },
    {
        "commit_hash": "9a199b48c4e92fb4a7dc3b5131cbc3a3bef3889f",
        "index": "c4c9d41c..278cf358 100644",
        "commit_message": "1. Implementation of local volatility model.\n2. Bug fix in implied volatility calculation with non-trivial discount factors.\n\nPiperOrigin-RevId: 343091512\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _make_black_objective_and_vega_func(prices, forwards, strikes, expiries,",
            "implied_prices = tf.where(",
            "tf.broadcast_to(is_call_options, tf.shape(put_prices)),",
            "implied_prices, put_prices)",
            "-    vega = x * phi.prob(d1) * sqrt_t",
            "+    vega = x * phi.prob(d1) * sqrt_t / discount_factors",
            "return implied_prices - normalized_prices, vega",
            "",
            "return _black_objective_and_vega"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=binary_operator), position=0)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('/', '/'), position=1, insert_id=2340161)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('identifier', 'discount_factors'), position=2, insert_id=2340162)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 442,
        "neg_line": [
            "-vega = x * phi.prob(d1) * sqrt_t"
        ],
        "pos_line": [
            "+vega = x * phi.prob(d1) * sqrt_t / discount_factors"
        ],
        "core_change": "-vega = x * phi.prob(d1) * sqrt_t +vega = x * phi.prob(d1) * sqrt_t / discount_factors",
        "core_API": "where"
    },
    {
        "commit_hash": "5dc80a650090c700bd4fc158f46257170fe3e8cb",
        "index": "c799e652..7bc724ce 100644",
        "commit_message": "Adds a dataset that can be read and written lazily (#5344)\n\n* Adds a dataset that can be read and written lazily\n\nThis does not work yet. I'm still working on supporting classes.\n\n* This approach might work better.\n\n* Make ShuffledSequence take indices\n\n* Formatting\n\n* Adds failing test\n\n* Fix sparse sequence tests\n\n* Fixes the Sqlite format\n\n* Quality-of-life hack\n\n* Makes an internal string less alarming\n\n* Save the files to the right place\n\n* Formatting\n\n* Fix for SqliteDatasetFormat\n\n* Performance improvement for SqliteSparseSequence\n\n* Changelog\n\n* Global imports\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class IndexField(Field[torch.Tensor]):",
            "",
            "@overrides",
            "def get_padding_lengths(self) -> Dict[str, int]:",
            "-",
            "return {}",
            "",
            "@overrides",
            "def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:",
            "-",
            "-        tensor = torch.LongTensor([self.sequence_index])",
            "-        return tensor",
            "+        return torch.LongTensor([self.sequence_index])",
            "",
            "@overrides",
            "def empty_field(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('return_statement', None), position=0, insert_id=654)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=655)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 443,
        "neg_line": [
            "-",
            "-",
            "-tensor = torch.LongTensor([self.sequence_index])",
            "-return tensor"
        ],
        "pos_line": [
            "+return torch.LongTensor([self.sequence_index])"
        ],
        "core_change": "- - -tensor = torch.LongTensor([self.sequence_index]) -return tensor +return torch.LongTensor([self.sequence_index])",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "b5eb33b36c1efa78b0e109784812a01a1df4fe5a",
        "index": "58ac3da..f37f593 100644",
        "commit_message": "bugfix\n\nSummary: It seemed that even though the chamfer diff was rebased on top of the knn autograd diff, some of the final updates did not get applied. I'm really surprised that the sandcastle tests did not fail and prevent the diff from landing.\n\nReviewed By: gkioxari\n\nDifferential Revision: D21066156\n\nfbshipit-source-id: 5216efe95180c1b6082d0bac404fa1920cfb7b02\n\n",
        "file": "pytorch3d.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def chamfer_distance(",
            "",
            "if return_normals:",
            "# Gather the normals using the indices and keep only value for k=0",
            "-        x_normals_near = knn_gather(y_normals, x_idx, y_lengths)[..., 0, :]",
            "-        y_normals_near = knn_gather(x_normals, y_idx, x_lengths)[..., 0, :]",
            "+        x_normals_near = knn_gather(y_normals, x_nn.idx, y_lengths)[..., 0, :]",
            "+        y_normals_near = knn_gather(x_normals, y_nn.idx, x_lengths)[..., 0, :]",
            "",
            "cham_norm_x = 1 - torch.abs(",
            "F.cosine_similarity(x_normals, x_normals_near, dim=2, eps=1e-6)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1484263)",
            "Update(target_node=ASTNode(type=identifier, text=y_idx), value='y_nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=y_idx), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1484264)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'idx'), position=2, insert_id=1484265)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1484266)",
            "Update(target_node=ASTNode(type=identifier, text=x_idx), value='x_nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=x_idx), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1484267)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'idx'), position=2, insert_id=1484268)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 444,
        "neg_line": [
            "-x_normals_near = knn_gather(y_normals, x_idx, y_lengths)[..., 0, :]",
            "-y_normals_near = knn_gather(x_normals, y_idx, x_lengths)[..., 0, :]"
        ],
        "pos_line": [
            "+x_normals_near = knn_gather(y_normals, x_nn.idx, y_lengths)[..., 0, :]",
            "+y_normals_near = knn_gather(x_normals, y_nn.idx, x_lengths)[..., 0, :]"
        ],
        "core_change": "-x_normals_near = knn_gather(y_normals, x_idx, y_lengths)[..., 0, :] -y_normals_near = knn_gather(x_normals, y_idx, x_lengths)[..., 0, :] +x_normals_near = knn_gather(y_normals, x_nn.idx, y_lengths)[..., 0, :] +y_normals_near = knn_gather(x_normals, y_nn.idx, x_lengths)[..., 0, :]",
        "core_API": "abs"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "049f08fd..479f6c09 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SOSNet(nn.Module):",
            "nn.Conv2d(128, 128, kernel_size=8, bias=False),",
            "nn.BatchNorm2d(128, affine=False),",
            ")",
            "-        self.desc_norm = nn.Sequential(",
            "-            nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0)",
            "-        )",
            "+        self.desc_norm = nn.Sequential(nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0))",
            "# load pretrained model",
            "if pretrained:",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-                urls['lib'], map_location=lambda storage, loc: storage",
            "-            )",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=lambda storage, loc: storage)",
            "self.load_state_dict(pretrained_dict, strict=True)",
            "",
            "return"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 445,
        "neg_line": [
            "-self.desc_norm = nn.Sequential(",
            "-nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0)",
            "-)",
            "-pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-urls['lib'], map_location=lambda storage, loc: storage",
            "-)"
        ],
        "pos_line": [
            "+self.desc_norm = nn.Sequential(nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0))",
            "+pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=lambda storage, loc: storage)"
        ],
        "core_change": "-self.desc_norm = nn.Sequential( -nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0) -) +self.desc_norm = nn.Sequential(nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0)) -pretrained_dict = torch.hub.load_state_dict_from_url( -urls['lib'], map_location=lambda storage, loc: storage -) +pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=lambda storage, loc: storage)",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "7feeb5648a63b6135a8259dedc3b1e19185ee4c7",
        "index": "fb3a77855..c3e3f9b90 100644",
        "commit_message": "Unpack `dl_manager.iter_files` to allow parallization (#4625)\n\n* Unpack `dl_manager.iter_files` to allow parallization\n\n* Fix _generate_tables\n\n* Fix remaining tests\n",
        "file": "datasets.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Csv(datasets.ArrowBasedBuilder):",
            "if schema is not None",
            "else None",
            ")",
            "-        for file_idx, file in enumerate(files):",
            "+        for file_idx, file in enumerate(itertools.chain.from_iterable(files)):",
            "csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)",
            "try:",
            "for batch_idx, df in enumerate(csv_file_reader):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=1777834)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1777835)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=1777836)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1777837)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1777838)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1777839)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'from_iterable'), position=2, insert_id=1777840)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'itertools'), position=0, insert_id=1777841)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1777842)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'chain'), position=2, insert_id=1777843)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 446,
        "neg_line": [
            "-for file_idx, file in enumerate(files):"
        ],
        "pos_line": [
            "+for file_idx, file in enumerate(itertools.chain.from_iterable(files)):"
        ],
        "core_change": "-for file_idx, file in enumerate(files): +for file_idx, file in enumerate(itertools.chain.from_iterable(files)):",
        "core_API": "from_iterable"
    },
    {
        "commit_hash": "b5e2b183af5e40e33a4dc7659e697d137259d56e",
        "index": "3f0011151..8ad939d5f 100644",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ElectraForPreTraining(ElectraPreTrainedModel):",
            ">>> from transformers import ElectraTokenizer, ElectraForPreTraining",
            ">>> import torch",
            "",
            "-        >>> tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')",
            "-        >>> model = ElectraForPreTraining.from_pretrained('google/electra-small-discriminator')",
            "+        >>> tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")",
            "+        >>> model = ElectraForPreTraining.from_pretrained(\"google/electra-small-discriminator\")",
            "",
            "-        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1",
            "+        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(",
            "+        ...     0",
            "+        >>> )  # Batch size 1",
            ">>> logits = model(input_ids).logits",
            "```\"\"\"",
            "return_dict = return_dict if return_dict is not None else self.config.use_return_dict"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=10)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=1, insert_id=1208516)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=0, insert_id=1208517)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=1, insert_id=1208518)",
            "Update(target_node=ASTNode(type=string, text='google/electra-small-discriminator'), value='\"google/electra-small-discriminator\"')",
            "Update(target_node=ASTNode(type=string, text='google/electra-small-discriminator'), value='\"google/electra-small-discriminator\"')",
            "Insert(target_node=ASTNode(type=argument_list), node=('ellipsis', '...'), position=1, insert_id=1208519)",
            "Insert(target_node=ASTNode(type=argument_list), node=('ERROR', None), position=2, insert_id=1208520)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=integer, text=0), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=1, insert_id=1208521)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=2, insert_id=1208522)",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 447,
        "neg_line": [
            "->>> tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')",
            "->>> model = ElectraForPreTraining.from_pretrained('google/electra-small-discriminator')",
            "->>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1"
        ],
        "pos_line": [
            "+>>> tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")",
            "+>>> model = ElectraForPreTraining.from_pretrained(\"google/electra-small-discriminator\")",
            "+>>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(",
            "+...     0",
            "+>>> )  # Batch size 1"
        ],
        "core_change": "->>> tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator') ->>> model = ElectraForPreTraining.from_pretrained('google/electra-small-discriminator') +>>> tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\") +>>> model = ElectraForPreTraining.from_pretrained(\"google/electra-small-discriminator\") ->>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1 +>>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze( +...     0 +>>> )  # Batch size 1",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "19a11c9c5ae7d22384d92af82d7471eb04cf1e4a",
        "index": "335a5350..5db8dbb8 100644",
        "commit_message": "fix arma conv\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "add condition check for null fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ARMAConv(MessagePassing):",
            "if self.bias is not None:",
            "out += self.bias[0 if self.shared_weights else t]",
            "",
            "-            if t < self.num_layers - 1:",
            "+            if self.act is not None and t < self.num_layers - 1:",
            "out = self.act(out)",
            "",
            "return out.mean(dim=-3)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=1011990)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=1011991)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1011992)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1011993)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1011994)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1011995)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1011996)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1011997)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1011998)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'act'), position=2, insert_id=1011999)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 448,
        "neg_line": [
            "-if t < self.num_layers - 1:"
        ],
        "pos_line": [
            "+if self.act is not None and t < self.num_layers - 1:"
        ],
        "core_change": "-if t < self.num_layers - 1: +if self.act is not None and t < self.num_layers - 1:",
        "core_API": "act"
    },
    {
        "commit_hash": "54931159b647465e90765149320e6ea80f1173ee",
        "index": "0e742fe5..e8aa0dfb 100644",
        "commit_message": "fix typing in dependency parser model\n\n",
        "file": "flair.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DependencyParser(flair.nn.Model):",
            "sentence_tensor = self.word_dropout(sentence_tensor)",
            "",
            "if self.use_rnn:",
            "-            sentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False)",
            "+            sentence_sequence = pack_padded_sequence(sentence_tensor, torch.IntTensor(lengths), True, False)",
            "",
            "-            sentence_tensor, _ = self.lstm(sentence_tensor)",
            "-            sentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)",
            "+            sentence_sequence, _ = self.lstm(sentence_sequence)",
            "+            sentence_tensor, _ = pad_packed_sequence(sentence_sequence, True, total_length=seq_len)",
            "",
            "# apply MLPs for arc and relations to the BiLSTM output states",
            "arc_h = self.mlp_arc_h(sentence_tensor)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sentence_tensor), value='sentence_sequence')",
            "Insert(target_node=ASTNode(type=assignment), node=('pattern_list', None), position=0, insert_id=234703)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=pattern_list), position=0)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'sentence_sequence'), position=0, insert_id=234704)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=1, insert_id=234705)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', '_'), position=2, insert_id=234706)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=234707)",
            "Update(target_node=ASTNode(type=identifier, text=sentence_tensor), value='sentence_sequence')",
            "Update(target_node=ASTNode(type=identifier, text=sentence_tensor), value='sentence_sequence')",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=234708)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=234709)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=234710)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=234711)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'IntTensor'), position=2, insert_id=234712)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=234713)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=lengths), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=234714)",
            "Delete(target_node=ASTNode(type=identifier, text=sentence_tensor))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_))",
            "Delete(target_node=ASTNode(type=pattern_list))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 449,
        "neg_line": [
            "-sentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False)",
            "-sentence_tensor, _ = self.lstm(sentence_tensor)",
            "-sentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)"
        ],
        "pos_line": [
            "+sentence_sequence = pack_padded_sequence(sentence_tensor, torch.IntTensor(lengths), True, False)",
            "+sentence_sequence, _ = self.lstm(sentence_sequence)",
            "+sentence_tensor, _ = pad_packed_sequence(sentence_sequence, True, total_length=seq_len)"
        ],
        "core_change": "-sentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False) +sentence_sequence = pack_padded_sequence(sentence_tensor, torch.IntTensor(lengths), True, False) -sentence_tensor, _ = self.lstm(sentence_tensor) -sentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len) +sentence_sequence, _ = self.lstm(sentence_sequence) +sentence_tensor, _ = pad_packed_sequence(sentence_sequence, True, total_length=seq_len)",
        "core_API": "word_dropout"
    },
    {
        "commit_hash": "3abf200be51fb577682681a85814aceb2460f804",
        "index": "3e591b7..8b6405c 100644",
        "commit_message": "fix pickle\n\n",
        "file": "text-detection-ctpn.txt.json",
        "label": "no",
        "comments": "rename version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "if __name__ == '__main__':",
            "saver = tf.train.Saver()",
            "",
            "try:",
            "-        ckpt = tf.train.get_checkpoint_state(\"checkpoints/\")",
            "+        ckpt = tf.train.get_checkpoint_state(cfg.TEST.checkpoints_path)",
            "+        #ckpt=tf.train.get_checkpoint_state(\"output/ctpn_end2end/voc_2007_trainval/\")",
            "print('Restoring from {}...'.format(ckpt.model_checkpoint_path), end=' ')",
            "saver.restore(sess, ckpt.model_checkpoint_path)",
            "print('done')"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=2328636)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2328637)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2328638)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'checkpoints_path'), position=2, insert_id=2328639)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cfg'), position=0, insert_id=2328640)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2328641)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TEST'), position=2, insert_id=2328642)",
            "Delete(target_node=ASTNode(type=string, text=\"checkpoints/\"))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 450,
        "neg_line": [
            "-ckpt = tf.train.get_checkpoint_state(\"checkpoints/\")"
        ],
        "pos_line": [
            "+ckpt = tf.train.get_checkpoint_state(cfg.TEST.checkpoints_path)",
            "+#ckpt=tf.train.get_checkpoint_state(\"output/ctpn_end2end/voc_2007_trainval/\")"
        ],
        "core_change": "-ckpt = tf.train.get_checkpoint_state(\"checkpoints/\") +ckpt = tf.train.get_checkpoint_state(cfg.TEST.checkpoints_path) +#ckpt=tf.train.get_checkpoint_state(\"output/ctpn_end2end/voc_2007_trainval/\")",
        "core_API": "Saver"
    },
    {
        "commit_hash": "8c0639280f9bccd6072fa3b27777c98e1de9fe72",
        "index": "f8d5c0f91..51af1a099 100644",
        "commit_message": "Fix linspace error\n\n",
        "file": "PySyft.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def testtanh():",
            "",
            "Ptensor = PolynomialTensor()",
            "",
            "-    x = torch.linspace(-3, 3, steps=10)",
            "+    x = torch.tensor(np.linspace(-3, 3, 10))",
            "expected = torch.tensor(",
            "[",
            "-3.3883e02,"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=835077)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=835078)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=835079)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=835080)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=835081)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=835082)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=835083)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=835084)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='np')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=10), position=5)",
            "Delete(target_node=ASTNode(type=identifier, text=steps))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=float, text=3.3883e02))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 451,
        "neg_line": [
            "-x = torch.linspace(-3, 3, steps=10)",
            "-3.3883e02,"
        ],
        "pos_line": [
            "+x = torch.tensor(np.linspace(-3, 3, 10))"
        ],
        "core_change": "-x = torch.linspace(-3, 3, steps=10) +x = torch.tensor(np.linspace(-3, 3, 10)) -3.3883e02,",
        "core_API": "linspace"
    },
    {
        "commit_hash": "b86e42e0ac1b59f21f0eccf351d3346bbe3ed4eb",
        "index": "718d41ca3..49989ed40 100644",
        "commit_message": "[ci] fix 3 remaining slow GPU failures (#4584)\n\n\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BartTranslationTests(unittest.TestCase):",
            "with torch.no_grad():",
            "logits, *other_stuff = model(**self.net_input)",
            "",
            "-        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787])",
            "+        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787], device=torch_device)",
            "result_slice = logits[0][0][:3]",
            "self.assertTrue(torch.allclose(expected_slice, result_slice, atol=TOLERANCE))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1237872)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1237873)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1237874)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1237875)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_device'), position=2, insert_id=1237876)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 452,
        "neg_line": [
            "-expected_slice = torch.tensor([9.0078, 10.1113, 14.4787])"
        ],
        "pos_line": [
            "+expected_slice = torch.tensor([9.0078, 10.1113, 14.4787], device=torch_device)"
        ],
        "core_change": "-expected_slice = torch.tensor([9.0078, 10.1113, 14.4787]) +expected_slice = torch.tensor([9.0078, 10.1113, 14.4787], device=torch_device)",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "55f43d0bebeb2caba5e8194fad6e3f17520d1be7",
        "index": "305793fe1..712de05e0 100644",
        "commit_message": "Fix unit test errors in test_dc_crn_separator.py\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_dc_crn_separator_invalid_type():",
            "def test_dc_crn_separator_output():",
            "real = torch.rand(2, 10, 17)",
            "imag = torch.rand(2, 10, 17)",
            "-    x = ComplexTensor(real, imag) if is_torch_1_9_plus else torch.complex(real, imag)",
            "+    x = torch.complex(real, imag) if is_torch_1_9_plus else ComplexTensor(real, imag)",
            "x_lens = torch.tensor([10, 8], dtype=torch.long)",
            "",
            "for num_spk in range(1, 3):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=conditional_expression), position=3)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=conditional_expression), position=0)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 453,
        "neg_line": [
            "-x = ComplexTensor(real, imag) if is_torch_1_9_plus else torch.complex(real, imag)"
        ],
        "pos_line": [
            "+x = torch.complex(real, imag) if is_torch_1_9_plus else ComplexTensor(real, imag)"
        ],
        "core_change": "-x = ComplexTensor(real, imag) if is_torch_1_9_plus else torch.complex(real, imag) +x = torch.complex(real, imag) if is_torch_1_9_plus else ComplexTensor(real, imag)",
        "core_API": "rand"
    },
    {
        "commit_hash": "11e789532905beb702b492432dc72b136a37587e",
        "index": "56b794ce..2ef39c09 100644",
        "commit_message": "Fix Pylint issues\n\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelSavingTests(unittest.TestCase):",
            "model = T.nn.DataParallel(layer)",
            "",
            "# save the model",
            "-        best_loss = save_best_model(model, None, 0, 100, OUT_PATH, 10, 1)",
            "+        save_best_model(model, None, 0, 100, OUT_PATH, 10, 1)",
            "",
            "# load the model to CPU",
            "-        model_dict = torch.load(",
            "+        model_dict = T.load(",
            "MODEL_PATH, map_location=lambda storage, loc: storage)",
            "model.load_state_dict(model_dict['model'])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='T')",
            "Delete(target_node=ASTNode(type=identifier, text=best_loss))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 454,
        "neg_line": [
            "-best_loss = save_best_model(model, None, 0, 100, OUT_PATH, 10, 1)",
            "-model_dict = torch.load("
        ],
        "pos_line": [
            "+save_best_model(model, None, 0, 100, OUT_PATH, 10, 1)",
            "+model_dict = T.load("
        ],
        "core_change": "-best_loss = save_best_model(model, None, 0, 100, OUT_PATH, 10, 1) +save_best_model(model, None, 0, 100, OUT_PATH, 10, 1) -model_dict = torch.load( +model_dict = T.load(",
        "core_API": "DataParallel"
    },
    {
        "commit_hash": "af94838993d6d4883befd078d33da282278de06b",
        "index": "e81ad7a51..a313672e4 100644",
        "commit_message": "Fix mypy issues\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DatasetRequestAPI(RequestAPI):",
            "super().create(**kwargs)",
            "",
            "def create_grid_ui(self, path: str, **kwargs) -> Dict[str, str]:  # type: ignore",
            "-        response = self.node.conn.send_files(",
            "+        response = self.node.conn.send_files( # type: ignore",
            "\"/datasets\", path, form_name=\"metadata\", form_values=kwargs",
            ")  # type: ignore",
            "logging.info(response[RequestAPIFields.MESSAGE])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 456,
        "neg_line": [
            "-response = self.node.conn.send_files("
        ],
        "pos_line": [
            "+response = self.node.conn.send_files( # type: ignore"
        ],
        "core_change": "-response = self.node.conn.send_files( +response = self.node.conn.send_files( # type: ignore",
        "core_API": "send_files"
    },
    {
        "commit_hash": "edb78974b84b324df08ec198e6ff7f267ee23d33",
        "index": "9eecfd19..92a54cb5 100644",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def multilevel_roi_align(features, rcnn_boxes, resolution):",
            "all_rois = tf.concat(all_rois, axis=0)  # NCHW",
            "# Unshuffle to the original order, to match the original samples",
            "level_id_perm = tf.concat(level_ids, axis=0)  # A permutation of 1~N",
            "-    level_id_invert_perm = tf.invert_permutation(level_id_perm)",
            "+    level_id_invert_perm = tf.math.invert_permutation(level_id_perm)",
            "all_rois = tf.gather(all_rois, level_id_invert_perm, name=\"output\")",
            "return all_rois"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2273107)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2273108)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2273109)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 457,
        "neg_line": [
            "-level_id_invert_perm = tf.invert_permutation(level_id_perm)"
        ],
        "pos_line": [
            "+level_id_invert_perm = tf.math.invert_permutation(level_id_perm)"
        ],
        "core_change": "-level_id_invert_perm = tf.invert_permutation(level_id_perm) +level_id_invert_perm = tf.math.invert_permutation(level_id_perm)",
        "core_API": "concat"
    },
    {
        "commit_hash": "3fcd3b577297367bd60fd5b1ba6c17ac093c4bdb",
        "index": "bbe00ed2..a5ebd049 100644",
        "commit_message": "fix bug in temperature & async\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def SoftMax(x, use_temperature=False, temperature_init=1.0):",
            ":param x: a 2D tensor",
            "\"\"\"",
            "if use_temperature:",
            "-        t = tf.get_variable('temp', [1],",
            "+        t = tf.get_variable('invtemp', [],",
            "initializer=tf.constant_initializer(1.0 / float(temperature_init)))",
            "x = x * t",
            "return tf.nn.softmax(x, name='output')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='temp'), value=\"'invtemp'\")",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 458,
        "neg_line": [
            "-t = tf.get_variable('temp', [1],"
        ],
        "pos_line": [
            "+t = tf.get_variable('invtemp', [],"
        ],
        "core_change": "-t = tf.get_variable('temp', [1], +t = tf.get_variable('invtemp', [],",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "c6bf7558b2121c3f2f280a0d15ee4354994e0c86",
        "index": "290e304c..dde26943 100644",
        "commit_message": "Potential deconv model saving fix? (#4999)\n\nAdding this cast to a tuple seems to fix this issue.\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _preprocess_deconv_output_shape(x, shape, dim_ordering):",
            "shape = (shape[0], shape[2], shape[3], shape[1])",
            "",
            "if shape[0] is None:",
            "-        shape = (tf.shape(x)[0], ) + shape[1:]",
            "+        shape = (tf.shape(x)[0], ) + tuple(shape[1:])",
            "return shape"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2114724)",
            "Insert(target_node=IN(type=call), node=('identifier', 'tuple'), position=0, insert_id=2114725)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2114726)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2114727)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2114728)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 459,
        "neg_line": [
            "-shape = (tf.shape(x)[0], ) + shape[1:]"
        ],
        "pos_line": [
            "+shape = (tf.shape(x)[0], ) + tuple(shape[1:])"
        ],
        "core_change": "-shape = (tf.shape(x)[0], ) + shape[1:] +shape = (tf.shape(x)[0], ) + tuple(shape[1:])",
        "core_API": "shape"
    },
    {
        "commit_hash": "e1a5cc338ba9fba27b0ca1fb54c9951c5146a86f",
        "index": "f86d6a47e..5ccd6c198 100644",
        "commit_message": "Fix doctests for `DeiT` and `TFGroupViT` (#19466)\n\n* Fix some doctests\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGroupViTModel(TFGroupViTPreTrainedModel):",
            "",
            ">>> outputs = model(**inputs)",
            ">>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score",
            "-        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities",
            "+        >>> probs = tf.math.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities",
            "```\"\"\"",
            "",
            "outputs = self.groupvit("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2359456)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=logits_per_image), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2359457)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2359458)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2359459)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2359460)",
            "Update(target_node=ASTNode(type=identifier, text=dim), value='axis')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 460,
        "neg_line": [
            "->>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
        ],
        "pos_line": [
            "+>>> probs = tf.math.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities"
        ],
        "core_change": "->>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities +>>> probs = tf.math.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities",
        "core_API": "softmax"
    },
    {
        "commit_hash": "619f984c362a2e5fd8f04ae24ea0eb8ce9d9e57a",
        "index": "4c0e89d10..d97eb07c5 100644",
        "commit_message": "Option to provide seed to random generators to ensure reproducibility (#1572)\n\n* Option to provide seed to random generators to ensure reproducibility\n\nI added small function in utilities which imports torch, numpy, python\nrandom and sets seed for all of the libraries to ensure reproducibility\nof results.\n\n* Apply recommendations from core contributors on seeding\n\n1. Moved the seeding code to another file\n2. Make deterministic as a parameter for trainer class\n3. Add assertions for seeding numpy\n4. Added warnings\n5. torch.manual_seed should be enough for seeding torch\n\n* Revert \"Apply recommendations from core contributors on seeding\"\n\nThis reverts commit a213c8e6882eec8a9e7408b9418926d2db7c5461.\n\n* Revert \"Revert \"Apply recommendations from core contributors on seeding\"\"\n\nThis reverts commit 59b2da53c62878de7aab0aa3feb3115e105eea06.\n\n* Change in test, for correct seeding\n\n* Allow seed equal to 0\n\n* Allow seed to be uint32.max\n\n* Added deterministic to benchmarks\n\n* Cuda manual seed as in benchmark seeding\n\n* Seeding should be done before model initialization\n\n* cuda manual_seed is not necessary\n\n* Fixing seed test_cpu_lbfgs\n\nOn some seeds seems like lbfgs doesn't converge.\nSo I fixed the seed during testing.\n\n* rebasing issue with old reproducibility.py\n\n* Improved documentation and ability to seed before initializing Train\nclass\n\n* Change in docs\n\n* Removed seed from trainer, update for documentation\n\n* Typo in the docs\n\n* Added seed_everything to _all_\n\n* Fixing old changes\n\n* Model initialization should be earlier then Trainer\n\n* Update pytorch_lightning/trainer/__init__.py\n\nFrom Example to testcode\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Fixing according to the contributors suggestions\n\n* Moving horovod deterministic to Trainer class\n\n* deterministic flag affects horovod docs update\n\n* Improved static typing\n\n* Added deterministic to test runners of horovod\n\nIt is failing on some versions, not very predictable\n\n* static seeds for horovod tests\n\n* Change for reset_seed function in tests\n\n* Seeding horovod using reset_seed from tutils\n\n* Update pytorch_lightning/trainer/__init__.py\n\n* chlog\n\n* Update trainer.py\n\n* change \"testcode\" to \"Example\" in trainer init documentation\n\n* Update pytorch_lightning/trainer/seed.py, first line in comment\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "custom API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def lightning_loop(MODEL, num_runs=10, num_epochs=10):",
            "",
            "# set seed",
            "seed = i",
            "-        _set_seed(seed)",
            "+        seed_everything(seed)",
            "",
            "-        # init model parts",
            "model = MODEL()",
            "+        # init model parts",
            "trainer = Trainer(",
            "max_epochs=num_epochs,",
            "progress_bar_refresh_rate=0,",
            "weights_summary=None,",
            "gpus=1,",
            "early_stop_callback=False,",
            "-            checkpoint_callback=False",
            "+            checkpoint_callback=False,",
            "+            deterministic=True,",
            ")",
            "trainer.fit(model)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_set_seed), value='seed_everything')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=12, insert_id=581480)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=13, insert_id=581481)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=14, insert_id=581482)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'deterministic'), position=0, insert_id=581483)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=581484)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=581485)"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 7,
        "number": 461,
        "neg_line": [
            "-_set_seed(seed)",
            "-# init model parts",
            "-checkpoint_callback=False"
        ],
        "pos_line": [
            "+seed_everything(seed)",
            "+# init model parts",
            "+checkpoint_callback=False,",
            "+deterministic=True,"
        ],
        "core_change": "-_set_seed(seed) +seed_everything(seed) -# init model parts +# init model parts -checkpoint_callback=False +checkpoint_callback=False, +deterministic=True,",
        "core_API": "fit"
    },
    {
        "commit_hash": "ba2bfd786830b3fba86d59af18a4416966637ef0",
        "index": "c8206ef6..9ee766bc 100644",
        "commit_message": "rgcn bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "change condition check for null fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RGCNConv(MessagePassing):",
            "return out if edge_norm is None else out * edge_norm.view(-1, 1)",
            "",
            "def update(self, aggr_out, x):",
            "-        if x.dtype == torch.long:",
            "+        if x is None:",
            "out = aggr_out + self.root",
            "else:",
            "out = aggr_out + torch.matmul(x, self.root)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=identifier, text=x), position=0)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('is', 'is'), position=1, insert_id=1057330)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('none', 'None'), position=2, insert_id=1057331)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=long))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 462,
        "neg_line": [
            "-if x.dtype == torch.long:"
        ],
        "pos_line": [
            "+if x is None:"
        ],
        "core_change": "-if x.dtype == torch.long: +if x is None:",
        "core_API": "view"
    },
    {
        "commit_hash": "8d051d7bf2742e758f53cfb83d50ccc47f926d14",
        "index": "60e7cf3a..eef5111d 100755",
        "commit_message": "Saver/summaries/distributed handling updated to using MonitoredSession, hooks, etc; custom save problems fixed; added saver/summary/distributed_spec config entries; added batched_observe config entry to address performance problem; modified episode/timestep counter handling in runner; various other and related fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Baseline(object):",
            "self.all_variables[name] = variable",
            "if kwargs.get('trainable', True) and not name.startswith('optimization'):",
            "self.variables[name] = variable",
            "-                    if 'variables' in self.summary_labels:",
            "-                        summary = tf.summary.histogram(name=name, values=variable)",
            "-                        self.summaries.append(summary)",
            "+                        if 'variables' in self.summary_labels:",
            "+                            summary = tf.summary.histogram(name=name, values=variable)",
            "+                            self.summaries.append(summary)",
            "return variable",
            "",
            "self.predict = tf.make_template("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 467,
        "neg_line": [
            "-if 'variables' in self.summary_labels:",
            "-summary = tf.summary.histogram(name=name, values=variable)",
            "-self.summaries.append(summary)"
        ],
        "pos_line": [
            "+if 'variables' in self.summary_labels:",
            "+summary = tf.summary.histogram(name=name, values=variable)",
            "+self.summaries.append(summary)"
        ],
        "core_change": "-if 'variables' in self.summary_labels: -summary = tf.summary.histogram(name=name, values=variable) -self.summaries.append(summary) +if 'variables' in self.summary_labels: +summary = tf.summary.histogram(name=name, values=variable) +self.summaries.append(summary)",
        "core_API": "get"
    },
    {
        "commit_hash": "6d839c8290b67dabfad9c1ffdc7e428c6030a499",
        "index": "7b10a597..6a3c01de 100644",
        "commit_message": "Convert docstrings to active tense (#1275)\n\n* Convert docstrings to active tense\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def load_depth(file_name):",
            "",
            "",
            "def load_camera_data(file_name):",
            "-    \"\"\"Loads the camera data using the sintel SDK and converts to torch.Tensor.\"\"\"",
            "+    \"\"\"Load the camera data using the sintel SDK and converts to torch.Tensor.\"\"\"",
            "if not os.path.isfile(file_name):",
            "raise AssertionError(f\"Invalid file {file_name}\")",
            "import sintel_io"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Loads the camera data using the sintel SDK and converts to torch.Tensor.\"\"\"), value='\"\"\"Load the camera data using the sintel SDK and converts to torch.Tensor.\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 469,
        "neg_line": [
            "-\"\"\"Loads the camera data using the sintel SDK and converts to torch.Tensor.\"\"\""
        ],
        "pos_line": [
            "+\"\"\"Load the camera data using the sintel SDK and converts to torch.Tensor.\"\"\""
        ],
        "core_change": "-\"\"\"Loads the camera data using the sintel SDK and converts to torch.Tensor.\"\"\" +\"\"\"Load the camera data using the sintel SDK and converts to torch.Tensor.\"\"\"",
        "core_API": "isfile"
    },
    {
        "commit_hash": "f162fd00152cf65dd7d99c3de6be90f3ce3f6563",
        "index": "a99f6030..c0345628 100644",
        "commit_message": "Merge PyG master (#48)\n\n* avoid the 'inf'\n\n* Create GATv2Conv\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* More doc\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* Update README.md\n\n* Update README.md\n\n* Create test_gatv2_conv.py\n\n* Update test_gatv2_conv.py\n\n* fixed gatv2 test\n\n* Fixed types\n\n* Update gatv2_conv.py\n\n* fix types\n\n* remove script folder\n\n* update test CI\n\n* fixed comments\n\n* lint + type\n\n* lint\n\n* Update test_gatv2_conv.py\n\n* Update test_gatv2_conv.py\n\n* fixed gatv2 test+ types\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* pytorch 1.9.0 support\n\n* typo\n\n* The dataset was introduced in the MUSAE paper\n\nDear Matthias,\n\nThese datasets were introduced in the Multi-scale Attributed Node Embedding paper.\n\nhttps://arxiv.org/abs/1909.13021\n\nBenedek\n\n* Github Dataset\n\n* Github Dataset\n\n* Github Dataset\n\n* Revert \"Merge branch 'master' into master\"\n\nThis reverts commit ef38f142465f736692c7c251a315ada287d7f104, reversing\nchanges made to d86de00a98173653a6158fc40238d34d0fb57cc1.\n\n* clean up\n\n* fix doc\n\n* fix gnn explainer\n\n* remove OGB-LSC\n\nCo-authored-by: Ethanzjp <13810907+Ethanzjp@users.noreply.github.com>\nCo-authored-by: shakedbr <shakedbr@campus.technion.ac.il>\nCo-authored-by: Uri Alon <urialon1@gmail.com>\nCo-authored-by: Shaked Brody <shakedbr@gmail.com>\nCo-authored-by: Benedek Rozemberczki <benedek.rozemberczki@gmail.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DenseGCNConv(torch.nn.Module):",
            "idx = torch.arange(N, dtype=torch.long, device=adj.device)",
            "adj[:, idx, idx] = 1 if not self.improved else 2",
            "",
            "-        out = self.lin(x)",
            "+        out = torch.matmul(x, self.weight)",
            "deg_inv_sqrt = adj.sum(dim=-1).clamp(min=1).pow(-0.5)",
            "",
            "adj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1002584)",
            "Update(target_node=ASTNode(type=identifier, text=lin), value='matmul')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1002585)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1002586)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1002587)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weight'), position=2, insert_id=1002588)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 470,
        "neg_line": [
            "-out = self.lin(x)"
        ],
        "pos_line": [
            "+out = torch.matmul(x, self.weight)"
        ],
        "core_change": "-out = self.lin(x) +out = torch.matmul(x, self.weight)",
        "core_API": "arange"
    },
    {
        "commit_hash": "afe5d42d8d1d80af911ed980c2936bfe887078f6",
        "index": "7b86a99fd..412959128 100644",
        "commit_message": "Black preview (#17217)\n\n* Black preview\n\n* Fixup too!\n\n* Fix check copies\n\n* Use the same version as the CI\n\n* Bump black\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFConvNextModelTest(TFModelTesterMixin, unittest.TestCase):",
            "else:",
            "self.assertTrue(",
            "all(tf.equal(tuple_object, dict_object)),",
            "-                        msg=f\"Tuple and dict output are not equal. Difference: {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\",",
            "+                        msg=(",
            "+                            \"Tuple and dict output are not equal. Difference:\"",
            "+                            f\" {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\"",
            "+                        ),",
            ")",
            "",
            "recursive_check(tuple_output, dict_output)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('parenthesized_expression', None), position=2, insert_id=2364292)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=2364293)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('concatenated_string', None), position=1, insert_id=2364294)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=2364295)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\"Tuple and dict output are not equal. Difference:\"'), position=0, insert_id=2364296)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', 'f\" {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\"'), position=1, insert_id=2364297)",
            "Delete(target_node=ASTNode(type=string, text=f\"Tuple and dict output are not equal. Difference: {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\"))"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 471,
        "neg_line": [
            "-msg=f\"Tuple and dict output are not equal. Difference: {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\","
        ],
        "pos_line": [
            "+msg=(",
            "+\"Tuple and dict output are not equal. Difference:\"",
            "+f\" {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\"",
            "+),"
        ],
        "core_change": "-msg=f\"Tuple and dict output are not equal. Difference: {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\", +msg=( +\"Tuple and dict output are not equal. Difference:\" +f\" {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\" +),",
        "core_API": "assertTrue"
    },
    {
        "commit_hash": "1e859f75070ea2c145850bddc61a0de82f69db61",
        "index": "746629f9..acc35f46 100644",
        "commit_message": "Fix half of the tests that are failing in v1.\n\nMost of them are failing since the actual code are expected to run only in v2 (eg need eager/resource variable, or certain fix we added is only applied to the v2 code path).\n\nPiperOrigin-RevId: 394765626\n\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "add condition check for state fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GradientsTest(tf.test.TestCase):",
            "",
            "",
            "if __name__ == \"__main__\":",
            "-  tf.test.main()",
            "+  if tf.__internal__.tf2.enabled():",
            "+    tf.test.main()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=2080166)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2080167)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=2080168)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2080169)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2080170)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2080171)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2080172)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2080173)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'enabled'), position=2, insert_id=2080174)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2080175)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2080176)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2080177)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2080178)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf2'), position=2, insert_id=2080179)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2080180)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2080181)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__internal__'), position=2, insert_id=2080182)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 472,
        "neg_line": [
            "-tf.test.main()"
        ],
        "pos_line": [
            "+if tf.__internal__.tf2.enabled():",
            "+tf.test.main()"
        ],
        "core_change": "-tf.test.main() +if tf.__internal__.tf2.enabled(): +tf.test.main()",
        "core_API": "main"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "ed0d8aef..ab640e2b 100755",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        acc = tf.to_float(tf.nn.in_top_k(logits, label, 1))",
            "+        acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)",
            "",
            "acc = tf.reduce_mean(acc, name='accuracy')",
            "summary.add_moving_summary(acc)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278966)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278967)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278968)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278969)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278970)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 473,
        "neg_line": [
            "-acc = tf.to_float(tf.nn.in_top_k(logits, label, 1))"
        ],
        "pos_line": [
            "+acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)"
        ],
        "core_change": "-acc = tf.to_float(tf.nn.in_top_k(logits, label, 1)) +acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)",
        "core_API": "sparse_softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "8eb4ae91b0f798719436f03cdf56e5b522508323",
        "index": "4e180a9..efd7288 100644",
        "commit_message": "fix moe override defaults\n\n",
        "file": "gpt-neo.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def block(params, scope, layer_num, bias, sequence_dim, memory_length_dim, varia",
            "if use_moe:",
            "moe_params = mtf.transformer.moe.HParams()",
            "mtf.transformer.moe.set_default_moe_hparams(moe_params)",
            "+",
            "+                # override defaults",
            "for k, v in params[\"moe_params\"].items():",
            "moe_params.add_hparam(k, v)",
            "-                mtf.transformer.moe.set_default_moe_hparams(moe_params)",
            "+",
            "moe_train = params[\"mode\"] == \"train\"",
            "",
            "m, aux_loss = mtf.transformer.moe.transformer_moe_layer_v1(res_x, x.shape[-1], moe_params,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=mtf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=transformer))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=moe))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=set_default_moe_hparams))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=moe_params))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 475,
        "neg_line": [
            "-mtf.transformer.moe.set_default_moe_hparams(moe_params)"
        ],
        "pos_line": [
            "+",
            "+# override defaults",
            "+"
        ],
        "core_change": "+ +# override defaults -mtf.transformer.moe.set_default_moe_hparams(moe_params) +",
        "core_API": "HParams"
    },
    {
        "commit_hash": "651e48e1e55afb76b44b0de4b9048a359b3d14f6",
        "index": "8edfc8eab..abdce6868 100644",
        "commit_message": "Fix tests of mixed precision now that experimental is deprecated (#17300)\n\n* Fix tests of mixed precision now that experimental is deprecated\n\n* Fix mixed precision in training_args_tf.py too\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFCoreModelTesterMixin:",
            "",
            "self.assertIsNotNone(outputs)",
            "",
            "-        tf.keras.mixed_precision.experimental.set_policy(\"float32\")",
            "+        tf.keras.mixed_precision.set_global_policy(\"float32\")",
            "",
            "@slow",
            "def test_train_pipeline_custom_model(self):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=experimental), value='set_global_policy')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=set_policy))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 476,
        "neg_line": [
            "-tf.keras.mixed_precision.experimental.set_policy(\"float32\")"
        ],
        "pos_line": [
            "+tf.keras.mixed_precision.set_global_policy(\"float32\")"
        ],
        "core_change": "-tf.keras.mixed_precision.experimental.set_policy(\"float32\") +tf.keras.mixed_precision.set_global_policy(\"float32\")",
        "core_API": "assertIsNotNone"
    },
    {
        "commit_hash": "f351f8073b446f91c1e7dac4496f4de00ea789c8",
        "index": "20eb3699..7aceb007 100644",
        "commit_message": "Fix various failing tests in v1. Most of them are failing because of the slight different behavior between v1 and v2. Some of them are only targeting to work with v2 behavior.\n\nPiperOrigin-RevId: 395026999\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class KerasCallbacksTest(keras_parameterized.TestCase):",
            "1, activation='sigmoid'),))",
            "model.compile(",
            "optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])",
            "-    expected_log = r'(.*- loss:.*- accuracy:.*epoch)+'",
            "+    expected_log = r'(.*- loss:.*- acc.*:.*epoch)+'",
            "with self.captureWritesToStream(sys.stdout) as printed:",
            "model.fit(data, labels, verbose=2, epochs=20)",
            "self.assertRegex(printed.contents(), expected_log)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=r'(.*- loss:.*- accuracy:.*epoch)+'), value=\"r'(.*- loss:.*- acc.*:.*epoch)+'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 477,
        "neg_line": [
            "-expected_log = r'(.*- loss:.*- accuracy:.*epoch)+'"
        ],
        "pos_line": [
            "+expected_log = r'(.*- loss:.*- acc.*:.*epoch)+'"
        ],
        "core_change": "-expected_log = r'(.*- loss:.*- accuracy:.*epoch)+' +expected_log = r'(.*- loss:.*- acc.*:.*epoch)+'",
        "core_API": "compile"
    },
    {
        "commit_hash": "7d92b309465b58005450b1cc6459ef6e119c8453",
        "index": "a5d89d5a..12a29a72 100644",
        "commit_message": "Fix tests\n\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "test",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Wavernn(BaseVocoder):",
            "f\"test_{idx}/prediction\": plot_spectrogram(x_hat.T),",
            "}",
            ")",
            "-            audios.update({f\"test_{idx}/audio\", y_hat})",
            "+            audios.update({f\"test_{idx}/audio\": y_hat})",
            "return figures, audios",
            "",
            "@staticmethod"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('dictionary', None), position=1, insert_id=1257580)",
            "Move(target_node=IN(type=dictionary), node=ASTNode(type={, text={), position=0)",
            "Insert(target_node=IN(type=dictionary), node=('pair', None), position=1, insert_id=1257581)",
            "Move(target_node=IN(type=dictionary), node=ASTNode(type=}, text=}), position=2)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=string, text=f\"test_{idx}/audio\"), position=0)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=1257582)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=identifier, text=y_hat), position=2)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=set))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 478,
        "neg_line": [
            "-audios.update({f\"test_{idx}/audio\", y_hat})"
        ],
        "pos_line": [
            "+audios.update({f\"test_{idx}/audio\": y_hat})"
        ],
        "core_change": "-audios.update({f\"test_{idx}/audio\", y_hat}) +audios.update({f\"test_{idx}/audio\": y_hat})",
        "core_API": "update"
    },
    {
        "commit_hash": "f04257fdbcb6ecb5a9bef75f4c2a8d2e8b5a6209",
        "index": "b31ac1bd6..b7de8be6e 100644",
        "commit_message": "Add test to ensure models can take int64 inputs (#17210)\n\n* Add test to ensure models can take int64 inputs\n\n* is_integer is an attribute, not a method\n\n* Fix test when some inputs aren't tensors\n\n* Add casts to blenderbot and blenderbot-small\n\n* Add casts to the other failing models\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFMBartForConditionalGeneration(TFMBartPreTrainedModel, TFCausalLanguageMo",
            "if labels is not None:",
            "labels = tf.where(",
            "labels == self.config.pad_token_id,",
            "-                tf.fill(shape_list(labels), -100),",
            "+                tf.cast(tf.fill(shape_list(labels), -100), labels.dtype),",
            "labels,",
            ")",
            "use_cache = False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2364364)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2364365)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2364366)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2364367)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2364368)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2364369)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2364370)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2364371)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2364372)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'labels'), position=0, insert_id=2364373)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2364374)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2364375)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 479,
        "neg_line": [
            "-tf.fill(shape_list(labels), -100),"
        ],
        "pos_line": [
            "+tf.cast(tf.fill(shape_list(labels), -100), labels.dtype),"
        ],
        "core_change": "-tf.fill(shape_list(labels), -100), +tf.cast(tf.fill(shape_list(labels), -100), labels.dtype),",
        "core_API": "where"
    },
    {
        "commit_hash": "3f94170a1048bbcff77b222a708470e482fdaff8",
        "index": "839c06420..88bfaa63c 100644",
        "commit_message": "[WIP] Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleC‚Ä¶ (#5614)\n\n* Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleChoice} models and tests\n\n* AutoModels\n\n\nTiny tweaks\n\n* Style\n\n* Final changes before merge\n\n* Re-order for simpler review\n\n* Final fixes\n\n* Addressing @sgugger's comments\n\n* Test MultipleChoice\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFModelTesterMixin:",
            "def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):",
            "if model_class in TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING.values():",
            "inputs_dict = {",
            "-                k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices, 1))",
            "-                if isinstance(v, tf.Tensor) and v.ndim != 0",
            "+                k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices) + (1,) * (v.ndim - 1))",
            "+                if isinstance(v, tf.Tensor) and v.ndim > 0",
            "else v",
            "for k, v in inputs_dict.items()",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=3, insert_id=2380949)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('>', '>'), position=1, insert_id=2380950)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=tuple), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2380951)",
            "Insert(target_node=IN(type=binary_operator), node=('binary_operator', None), position=2, insert_id=2380952)",
            "Insert(target_node=IN(type=binary_operator), node=('tuple', None), position=0, insert_id=2380953)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=2380954)",
            "Insert(target_node=IN(type=binary_operator), node=('parenthesized_expression', None), position=2, insert_id=2380955)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2380956)",
            "Insert(target_node=IN(type=tuple), node=('integer', '1'), position=1, insert_id=2380957)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=2380958)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=2380959)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=2380960)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=2380961)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=0, insert_id=2380962)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=2380963)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=integer, text=1), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v'), position=0, insert_id=2380964)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2380965)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ndim'), position=2, insert_id=2380966)",
            "Delete(target_node=ASTNode(type=!=, text=!=))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 480,
        "neg_line": [
            "-k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices, 1))",
            "-if isinstance(v, tf.Tensor) and v.ndim != 0"
        ],
        "pos_line": [
            "+k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices) + (1,) * (v.ndim - 1))",
            "+if isinstance(v, tf.Tensor) and v.ndim > 0"
        ],
        "core_change": "-k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices, 1)) -if isinstance(v, tf.Tensor) and v.ndim != 0 +k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices) + (1,) * (v.ndim - 1)) +if isinstance(v, tf.Tensor) and v.ndim > 0",
        "core_API": "values"
    },
    {
        "commit_hash": "6b1ff250842f52136d5159bb67a26b50ba01485d",
        "index": "f3c31b0c0..f5bbde903 100644",
        "commit_message": "fix n_gpu count when no_cuda flag is activated (#3077)\n\n* fix n_gpu count when no_cuda flag is activated\n\n* someone was left behind\n\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def main():",
            "# Setup CUDA, GPU & distributed training",
            "if args.local_rank == -1 or args.no_cuda:",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")",
            "-        args.n_gpu = torch.cuda.device_count()",
            "+        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()",
            "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs",
            "torch.cuda.set_device(args.local_rank)",
            "device = torch.device(\"cuda\", args.local_rank)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('conditional_expression', None), position=2, insert_id=1878565)",
            "Insert(target_node=IN(type=conditional_expression), node=('integer', '0'), position=0, insert_id=1878566)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1878567)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=2, insert_id=1878568)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1878569)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=4)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=1878570)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1878571)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_cuda'), position=2, insert_id=1878572)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 481,
        "neg_line": [
            "-args.n_gpu = torch.cuda.device_count()"
        ],
        "pos_line": [
            "+args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()"
        ],
        "core_change": "-args.n_gpu = torch.cuda.device_count() +args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()",
        "core_API": "device"
    },
    {
        "commit_hash": "7feeb5648a63b6135a8259dedc3b1e19185ee4c7",
        "index": "271ba0557..6ad9a6f49 100644",
        "commit_message": "Unpack `dl_manager.iter_files` to allow parallization (#4625)\n\n* Unpack `dl_manager.iter_files` to allow parallization\n\n* Fix _generate_tables\n\n* Fix remaining tests\n",
        "file": "datasets.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Pandas(datasets.ArrowBasedBuilder):",
            "return pa_table",
            "",
            "def _generate_tables(self, files):",
            "-        for i, file in enumerate(files):",
            "+        for i, file in enumerate(itertools.chain.from_iterable(files)):",
            "with open(file, \"rb\") as f:",
            "pa_table = pa.Table.from_pandas(pd.read_pickle(f))",
            "yield i, self._cast_table(pa_table)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=1777906)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1777907)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=1777908)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1777909)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1777910)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1777911)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'from_iterable'), position=2, insert_id=1777912)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'itertools'), position=0, insert_id=1777913)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1777914)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'chain'), position=2, insert_id=1777915)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 482,
        "neg_line": [
            "-for i, file in enumerate(files):"
        ],
        "pos_line": [
            "+for i, file in enumerate(itertools.chain.from_iterable(files)):"
        ],
        "core_change": "-for i, file in enumerate(files): +for i, file in enumerate(itertools.chain.from_iterable(files)):",
        "core_API": "from_iterable"
    },
    {
        "commit_hash": "7db18ce0c28c0b7c93e9a61af55433f2d88d4213",
        "index": "b79042a4..2684ef6c 100644",
        "commit_message": "fix mypy errors\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DiceLoss(nn.Module):",
            "cardinality = torch.sum(input_soft + target_one_hot, dims)",
            "",
            "dice_score = 2. * intersection / (cardinality + self.eps)",
            "-        return torch.mean(1. - dice_score)",
            "+        return torch.mean(torch.tensor(1.) - dice_score)",
            "",
            "",
            "######################"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=470427)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=470428)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=470429)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=470430)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=470431)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=470432)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=470433)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=float, text=1.), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=470434)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 483,
        "neg_line": [
            "-return torch.mean(1. - dice_score)"
        ],
        "pos_line": [
            "+return torch.mean(torch.tensor(1.) - dice_score)"
        ],
        "core_change": "-return torch.mean(1. - dice_score) +return torch.mean(torch.tensor(1.) - dice_score)",
        "core_API": "sum"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "ac430764..c426d541 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from allennlp.training.metrics import ConllCorefScores",
            "class ConllCorefScoresTest(AllenNlpTestCase):",
            "def test_get_predicted_clusters(self):",
            "top_spans = torch.Tensor([[0, 1], [4, 6], [8, 9]]).long()",
            "-        antecedent_indices = torch.Tensor([[-1, -1, -1],",
            "-                                           [0, -1, -1],",
            "-                                           [0, 1, -1]]).long()",
            "+        antecedent_indices = torch.Tensor([[-1, -1, -1], [0, -1, -1], [0, 1, -1]]).long()",
            "predicted_antecedents = torch.Tensor([-1, -1, 1]).long()",
            "-        clusters, mention_to_cluster = ConllCorefScores.get_predicted_clusters(top_spans,",
            "-                                                                               antecedent_indices,",
            "-                                                                               predicted_antecedents)",
            "+        clusters, mention_to_cluster = ConllCorefScores.get_predicted_clusters(",
            "+            top_spans, antecedent_indices, predicted_antecedents",
            "+        )",
            "assert len(clusters) == 1",
            "assert set(clusters[0]) == {(4, 6), (8, 9)}",
            "assert mention_to_cluster == {(4, 6): clusters[0], (8, 9): clusters[0]}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 486,
        "neg_line": [
            "-antecedent_indices = torch.Tensor([[-1, -1, -1],",
            "-[0, -1, -1],",
            "-[0, 1, -1]]).long()",
            "-clusters, mention_to_cluster = ConllCorefScores.get_predicted_clusters(top_spans,",
            "-antecedent_indices,",
            "-predicted_antecedents)"
        ],
        "pos_line": [
            "+antecedent_indices = torch.Tensor([[-1, -1, -1], [0, -1, -1], [0, 1, -1]]).long()",
            "+clusters, mention_to_cluster = ConllCorefScores.get_predicted_clusters(",
            "+top_spans, antecedent_indices, predicted_antecedents",
            "+)"
        ],
        "core_change": "-antecedent_indices = torch.Tensor([[-1, -1, -1], -[0, -1, -1], -[0, 1, -1]]).long() +antecedent_indices = torch.Tensor([[-1, -1, -1], [0, -1, -1], [0, 1, -1]]).long() -clusters, mention_to_cluster = ConllCorefScores.get_predicted_clusters(top_spans, -antecedent_indices, -predicted_antecedents) +clusters, mention_to_cluster = ConllCorefScores.get_predicted_clusters( +top_spans, antecedent_indices, predicted_antecedents +)",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "63bb6a795a66855a5108c69620615f4c477a052c",
        "index": "5e4300b..63783d2 100644",
        "commit_message": "Fix TF Hub compat.as_str PY3 compatibility issues.\n\nPiperOrigin-RevId: 209449340\n\n",
        "file": "hub.txt.json",
        "label": "no",
        "comments": "customize API",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def find_state_op_colocation_error(graph, reported_tags=None):",
            "for op in state_op_map.values():",
            "for colocation_group in op.colocation_groups():",
            "if not (colocation_group.startswith(tf.compat.as_bytes(\"loc:@\")) and",
            "-              tf.compat.as_str(colocation_group[5:]) in state_op_map):",
            "+              tf.compat.as_str_any(colocation_group[5:]) in state_op_map):",
            "tags_prefix = (\"\" if reported_tags is None else",
            "\"in the graph for tags %s, \" % reported_tags)",
            "return ("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=as_str), value='as_str_any')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 487,
        "neg_line": [
            "-tf.compat.as_str(colocation_group[5:]) in state_op_map):"
        ],
        "pos_line": [
            "+tf.compat.as_str_any(colocation_group[5:]) in state_op_map):"
        ],
        "core_change": "-tf.compat.as_str(colocation_group[5:]) in state_op_map): +tf.compat.as_str_any(colocation_group[5:]) in state_op_map):",
        "core_API": "values"
    },
    {
        "commit_hash": "fe58929ad612fe0027cf745b4e198cdf65d0dbe9",
        "index": "6be0deb1f..e4a356a25 100644",
        "commit_message": "Adds timeout argument to training_args to avoid socket timeouts in DDP (#18562)\n\n* chore(training_args): Adds support for timeout argument.\n\n* fix(training_args): Passes make style through changes.\n\n* fix(training_args): Removes wrong docstring sentence.\n\n* fix(training_args): Fixes timeout not being JSON serializable.\n\n* fix(training_args_sm): Also updates timeout to timeout_delta.\n\n* fix(training_args): Fixes PR according to suggestions.\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SageMakerTrainingArguments(TrainingArguments):",
            "# Here, we'll use torch.distributed.",
            "# Initializes the distributed backend which will take care of synchronizing nodes/GPUs",
            "if not torch.distributed.is_initialized():",
            "-                torch.distributed.init_process_group(backend=\"nccl\")",
            "+                torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)",
            "device = torch.device(\"cuda\", self.local_rank)",
            "self._n_gpu = 1"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1192522)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1192523)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'timeout'), position=0, insert_id=1192524)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1192525)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1192526)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1192527)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1192528)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ddp_timeout_delta'), position=2, insert_id=1192529)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 488,
        "neg_line": [
            "-torch.distributed.init_process_group(backend=\"nccl\")"
        ],
        "pos_line": [
            "+torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)"
        ],
        "core_change": "-torch.distributed.init_process_group(backend=\"nccl\") +torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)",
        "core_API": "is_initialized"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "fd40275c..9995b1c4 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "yes",
        "comments": "change param for null fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main(_):",
            "# net = tl.layers.ReshapeLayer(net,",
            "#       shape=[-1, int(net.outputs._shape[-1])], name='reshape')",
            "net = tl.layers.DropoutLayer(net, keep=keep_prob, is_fix=True, is_train=is_training, name='drop3')",
            "-            net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=tf.identity, name='output')",
            "+            net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=None, name='output')",
            "return net, lstm1, lstm2",
            "",
            "# Inference for Training"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('none', 'None'), position=2, insert_id=2262280)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 489,
        "neg_line": [
            "-net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=tf.identity, name='output')"
        ],
        "pos_line": [
            "+net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=None, name='output')"
        ],
        "core_change": "-net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=tf.identity, name='output') +net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=None, name='output')",
        "core_API": "ReshapeLayer"
    },
    {
        "commit_hash": "66f8d43518bc38c326cde38425f1f090ef968681",
        "index": "4adbe6e3..868c7c71 100644",
        "commit_message": "Layer API Refactoring (#692)\n\n* Layer API Refactoring\n\n* Conv Layers refactored\n\n* dense layers refactored\n\n* Error fix in ElementwiseLayer\n\n* Various cleaning\n\n* docstring cleaning\n\n* Documentation print clean\n\n* Changelog updated\n\n* @zsdonghao corrections added\n\n* Import Bug Fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "change API raise error",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class SlimNetsLayer(Layer):",
            "slim_variables = tf.get_collection(TF_GRAPHKEYS_VARIABLES, scope=self.name)",
            "",
            "if slim_variables == []:",
            "-            logging.error(",
            "-                \"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file, see tutorial_inceptionV3_tfslim.py for more details\"",
            "-                % self.name",
            "+            raise RuntimeError(",
            "+                \"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file.\\n\"",
            "+                \"see tutorial_inceptionV3_tfslim.py for more details\" % self.name",
            ")",
            "",
            "slim_layers = []",
            "",
            "for v in end_points.values():",
            "-            # tf.contrib.layers.summaries.summarize_activation(v)",
            "slim_layers.append(v)",
            "",
            "self._add_layers(slim_layers)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('raise_statement', None), position=0, insert_id=2634006)",
            "Insert(target_node=IN(type=raise_statement), node=('raise', 'raise'), position=0, insert_id=2634007)",
            "Move(target_node=IN(type=raise_statement), node=ASTNode(type=call), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=logging), value='RuntimeError')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=logging), position=0)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('concatenated_string', None), position=0, insert_id=2634008)",
            "Update(target_node=ASTNode(type=string, text=\"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file, see tutorial_inceptionV3_tfslim.py for more details\"), value='\"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file.\\\\n\"')",
            "Move(target_node=IN(type=concatenated_string), node=ASTNode(type=string, text=\"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file, see tutorial_inceptionV3_tfslim.py for more details\"), position=0)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\"see tutorial_inceptionV3_tfslim.py for more details\"'), position=1, insert_id=2634009)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=error))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 13,
        "number": 490,
        "neg_line": [
            "-logging.error(",
            "-\"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file, see tutorial_inceptionV3_tfslim.py for more details\"",
            "-% self.name",
            "-# tf.contrib.layers.summaries.summarize_activation(v)"
        ],
        "pos_line": [
            "+raise RuntimeError(",
            "+\"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file.\\n\"",
            "+\"see tutorial_inceptionV3_tfslim.py for more details\" % self.name"
        ],
        "core_change": "-logging.error( -\"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file, see tutorial_inceptionV3_tfslim.py for more details\" -% self.name +raise RuntimeError( +\"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file.\\n\" +\"see tutorial_inceptionV3_tfslim.py for more details\" % self.name -# tf.contrib.layers.summaries.summarize_activation(v)",
        "core_API": "get_collection"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "0fa6d924..63ff5766 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "update param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DecoderLayer(nn.Module):",
            "self.sublayer = nn_util.clone(SublayerConnection(size, dropout), 3)",
            "",
            "def forward(",
            "-        self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor",
            "+        self,",
            "+        x: torch.Tensor,",
            "+        memory: torch.Tensor,",
            "+        src_mask: torch.BoolTensor,",
            "+        tgt_mask: torch.BoolTensor,",
            ") -> torch.Tensor:",
            "# Follow Figure 1 (right) for connections.",
            "x = self.sublayer[0](x, lambda y: self.self_attn(y, y, y, tgt_mask))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=10, insert_id=19575)",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='BoolTensor')",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='BoolTensor')"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 494,
        "neg_line": [
            "-self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor"
        ],
        "pos_line": [
            "+self,",
            "+x: torch.Tensor,",
            "+memory: torch.Tensor,",
            "+src_mask: torch.BoolTensor,",
            "+tgt_mask: torch.BoolTensor,"
        ],
        "core_change": "-self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor +self, +x: torch.Tensor, +memory: torch.Tensor, +src_mask: torch.BoolTensor, +tgt_mask: torch.BoolTensor,",
        "core_API": "clone"
    },
    {
        "commit_hash": "954012557ab2488bebda4101d81357ae9a6d02ce",
        "index": "041f4cb2..9a1f70dc 100644",
        "commit_message": "Tree decoding fix (#1606)\n\n* initial fix\n\n* correct approach\n\n* fix and test\n\n* fix predictor test\n\n* fix pylint\n\n* use unique edge weights\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "no fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BiaffineDependencyParser(Model):",
            "head_tags.append(head_tag)",
            "return torch.from_numpy(numpy.stack(heads)), torch.from_numpy(numpy.stack(head_tags))",
            "",
            "-",
            "def _get_head_tags(self,",
            "head_tag_representation: torch.Tensor,",
            "child_tag_representation: torch.Tensor,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 495,
        "neg_line": [
            "-"
        ],
        "pos_line": [],
        "core_change": "-",
        "core_API": "append"
    },
    {
        "commit_hash": "aa750becf439224df59f80eb57aef5737cf11337",
        "index": "9daeb97e..8b134695 100644",
        "commit_message": "fix Average metric (#4624)\n\n* fix Average metric\n\n* try spawn\n\n* try again\n\n* oops\n\n* clean up, fix evalb too\n\n* use different start method for GPU vs CPU tests\n\n* add comment\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EvalbBracketingScorer(Metric):",
            "shutil.rmtree(tempdir)",
            "",
            "if is_distributed():",
            "-            # Setting the device to CPU since this metric is not expected to run on GPUs.",
            "-            device = torch.device(\"cpu\")",
            "+            device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")",
            "correct_predicted_brackets = torch.tensor(_correct_predicted_brackets).to(device)",
            "predicted_brackets = torch.tensor(_predicted_brackets).to(device)",
            "gold_brackets = torch.tensor(_gold_brackets).to(device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=7210)",
            "Insert(target_node=ASTNode(type=argument_list), node=('conditional_expression', None), position=1, insert_id=7211)",
            "Insert(target_node=IN(type=conditional_expression), node=('string', '\"cuda\"'), position=0, insert_id=7212)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=7213)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=7214)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=7215)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=string, text=\"cpu\"), position=4)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=7216)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=7217)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"nccl\"'), position=2, insert_id=7218)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=7219)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=7220)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dist'), position=0, insert_id=7221)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=7222)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_backend'), position=2, insert_id=7223)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=7224)"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 497,
        "neg_line": [
            "-# Setting the device to CPU since this metric is not expected to run on GPUs.",
            "-device = torch.device(\"cpu\")"
        ],
        "pos_line": [
            "+device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")"
        ],
        "core_change": "-# Setting the device to CPU since this metric is not expected to run on GPUs. -device = torch.device(\"cpu\") +device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")",
        "core_API": "rmtree"
    },
    {
        "commit_hash": "2a6fcc6cab0a0371698bc9fa375c67e633817c86",
        "index": "58a313e7d6..6120b4ae09 100644",
        "commit_message": "formatting fixes for Array API submodule in Torch backend.\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def ones(shape: Union[int, Tuple[int, ...]],",
            "dtype: Optional[torch.dtype] = None,",
            "device: Optional[Union[torch.device, str]] = None) \\",
            "-> torch.Tensor:",
            "-    dtype_val: torch.dtype = ivy.dtype_from_str(dtype)",
            "-    dev = ivy.default_device(device)",
            "-    return torch.ones(shape, dtype=dtype_val, device=ivy.dev_from_str(dev))",
            "+    dtype_val: torch.dtype = dtype_from_str(dtype)",
            "+    dev = default_device(device)",
            "+    return torch.ones(shape, dtype=dtype_val, device=dev_from_str(dev))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=1)",
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=return_statement), position=2)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'dtype_val'), position=0, insert_id=378602)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=default_device), position=0)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=dtype_from_str), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=dev_from_str), position=0)",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype_val))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=identifier, text=ivy))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=ivy))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=ivy))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 498,
        "neg_line": [
            "-> torch.Tensor:",
            "-dtype_val: torch.dtype = ivy.dtype_from_str(dtype)",
            "-dev = ivy.default_device(device)",
            "-return torch.ones(shape, dtype=dtype_val, device=ivy.dev_from_str(dev))"
        ],
        "pos_line": [
            "+dtype_val: torch.dtype = dtype_from_str(dtype)",
            "+dev = default_device(device)",
            "+return torch.ones(shape, dtype=dtype_val, device=dev_from_str(dev))"
        ],
        "core_change": "-> torch.Tensor: -dtype_val: torch.dtype = ivy.dtype_from_str(dtype) -dev = ivy.default_device(device) -return torch.ones(shape, dtype=dtype_val, device=ivy.dev_from_str(dev)) +dtype_val: torch.dtype = dtype_from_str(dtype) +dev = default_device(device) +return torch.ones(shape, dtype=dtype_val, device=dev_from_str(dev))",
        "core_API": "dtype_from_str"
    },
    {
        "commit_hash": "24092b14643b890c174df29f8ae74f5daa64ecac",
        "index": "5012aacf4..953dd1d34 100644",
        "commit_message": "Fix typo of variable names for key and query projection layer (#17155)\n\nself.pos_proj and self.pos_q_proj should be changed to self.pos_key_proj and self.pos_query_proj as same as PyTorch implements.\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFDebertaV2DisentangledSelfAttention(tf.keras.layers.Layer):",
            "",
            "if not self.share_att_key:",
            "if \"c2p\" in self.pos_att_type:",
            "-                    self.pos_proj = tf.keras.layers.Dense(",
            "+                    self.pos_key_proj = tf.keras.layers.Dense(",
            "self.all_head_size,",
            "kernel_initializer=get_initializer(config.initializer_range),",
            "name=\"pos_proj\",",
            "use_bias=True,",
            ")",
            "if \"p2c\" in self.pos_att_type:",
            "-                    self.pos_q_proj = tf.keras.layers.Dense(",
            "+                    self.pos_query_proj = tf.keras.layers.Dense(",
            "self.all_head_size,",
            "kernel_initializer=get_initializer(config.initializer_range),",
            "name=\"pos_q_proj\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=pos_q_proj), value='pos_query_proj')",
            "Update(target_node=ASTNode(type=identifier, text=pos_proj), value='pos_key_proj')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 499,
        "neg_line": [
            "-self.pos_proj = tf.keras.layers.Dense(",
            "-self.pos_q_proj = tf.keras.layers.Dense("
        ],
        "pos_line": [
            "+self.pos_key_proj = tf.keras.layers.Dense(",
            "+self.pos_query_proj = tf.keras.layers.Dense("
        ],
        "core_change": "-self.pos_proj = tf.keras.layers.Dense( +self.pos_key_proj = tf.keras.layers.Dense( -self.pos_q_proj = tf.keras.layers.Dense( +self.pos_query_proj = tf.keras.layers.Dense(",
        "core_API": "Dense"
    },
    {
        "commit_hash": "0dfd6b624081fc4e1c72fc74ae0cd2de199c334c",
        "index": "14c015b7..eceeef8b 100644",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding ÔøΩ\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "file": "fairseq.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def quantize_model_(",
            "print(num_assignments)",
            "print(num_extra)",
            "assignments_bins = torch.arange(counts)",
            "-            assignments_rand = torch.randint(0, counts-1, (num_extra, ))",
            "+            assignments_rand = torch.randint(0, counts - 1, (num_extra,))",
            "assignments = torch.cat((assignments_bins, assignments_rand), 0)",
            "# assignments = assignments.type(torch.IntTensor)",
            "assignments.cuda()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 500,
        "neg_line": [
            "-assignments_rand = torch.randint(0, counts-1, (num_extra, ))"
        ],
        "pos_line": [
            "+assignments_rand = torch.randint(0, counts - 1, (num_extra,))"
        ],
        "core_change": "-assignments_rand = torch.randint(0, counts-1, (num_extra, )) +assignments_rand = torch.randint(0, counts - 1, (num_extra,))",
        "core_API": "arange"
    },
    {
        "commit_hash": "02673760229ad7310ba0f50b09225fceba0be91b",
        "index": "48ce08e8f..74d314366 100644",
        "commit_message": "Bug fix for ctc mode:   espnet/nets/pytorch_backend/e2e_asr.py\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add condition check for null fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class E2E(torch.nn.Module):",
            "# Neither CPUTensor nor float/int value can be used",
            "# because NCCL communicates between GPU devices.",
            "device = next(self.parameters()).device",
            "-        acc = torch.tensor([acc], device=device)",
            "+",
            "+        acc = torch.tensor([acc], device=device) if acc is not None else None",
            "cer = torch.tensor([cer], device=device)",
            "wer = torch.tensor([wer], device=device)",
            "return self.loss, loss_ctc, loss_att, acc, cer, wer"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('conditional_expression', None), position=2, insert_id=176870)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=176871)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=176872)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=176873)",
            "Insert(target_node=IN(type=conditional_expression), node=('none', 'None'), position=4, insert_id=176874)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'acc'), position=0, insert_id=176875)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=176876)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=176877)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=176878)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 501,
        "neg_line": [
            "-acc = torch.tensor([acc], device=device)"
        ],
        "pos_line": [
            "+",
            "+acc = torch.tensor([acc], device=device) if acc is not None else None"
        ],
        "core_change": "-acc = torch.tensor([acc], device=device) + +acc = torch.tensor([acc], device=device) if acc is not None else None",
        "core_API": "parameters"
    },
    {
        "commit_hash": "2a4e336b927f4423173b17eb641a599342425c09",
        "index": "b880b7f8..595c8785 100644",
        "commit_message": "bug fixes, under construction\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "remove API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DeepQNetwork(ValueFunction):",
            "\"\"\"",
            "",
            "# Compute estimated future value",
            "-        float_terminals = tf.to_float(batch['terminals'])",
            "+        float_terminals = batch['terminals'].astype(float)",
            "q_targets = batch['rewards'] + (1. - float_terminals) \\",
            "* self.gamma * self.get_target_values(batch['next_states'])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n\n# Compute estimated future value\n        float_terminals = tf.to_float(batch['terminals'])\nq_targets = batch['rewards'] + (1. - float_terminals) \\\n), value='\"\"\"\\n\\n# Compute estimated future value\\n        float_terminals = batch[\\'terminals\\'].astype(float)\\nq_targets = batch[\\'rewards\\'] + (1. - float_terminals) \\\\\\n')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 502,
        "neg_line": [
            "-float_terminals = tf.to_float(batch['terminals'])"
        ],
        "pos_line": [
            "+float_terminals = batch['terminals'].astype(float)"
        ],
        "core_change": "-float_terminals = tf.to_float(batch['terminals']) +float_terminals = batch['terminals'].astype(float)",
        "core_API": "to_float"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "66747d8b..1b7d9fd0 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BoxBlur(nn.Module):",
            "torch.Size([2, 4, 5, 7])",
            "\"\"\"",
            "",
            "-    def __init__(self, kernel_size: Tuple[int, int],",
            "-                 border_type: str = 'reflect',",
            "-                 normalized: bool = True) -> None:",
            "+    def __init__(self, kernel_size: Tuple[int, int], border_type: str = 'reflect', normalized: bool = True) -> None:",
            "super(BoxBlur, self).__init__()",
            "self.kernel_size: Tuple[int, int] = kernel_size",
            "self.border_type: str = border_type"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 503,
        "neg_line": [
            "-def __init__(self, kernel_size: Tuple[int, int],",
            "-border_type: str = 'reflect',",
            "-normalized: bool = True) -> None:"
        ],
        "pos_line": [
            "+def __init__(self, kernel_size: Tuple[int, int], border_type: str = 'reflect', normalized: bool = True) -> None:"
        ],
        "core_change": "-def __init__(self, kernel_size: Tuple[int, int], -border_type: str = 'reflect', -normalized: bool = True) -> None: +def __init__(self, kernel_size: Tuple[int, int], border_type: str = 'reflect', normalized: bool = True) -> None:",
        "core_API": "Size"
    },
    {
        "commit_hash": "8a95aa1af8b4da59048a3fdb86b3cc772c78b434",
        "index": "da4126fe9..28b5f8e78 100644",
        "commit_message": "Update dataset_infos for udhr dataset (#4362)\n\n* Update dataset_infos for UDHN/udhr dataset\n\n* Update languages list, Add versioning\n\n* Reset version; remove stage1+2 languages; add licensing and repo information\n\n* bump dummy version\n\n* Fix language tags\n\n* Add citation to dataset card\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "add version fix",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "_TXT_DOWNLOAD_URL = \"https://unicode.org/udhr/assemblies/udhr_txt.zip\"",
            "class UDHN(datasets.GeneratorBasedBuilder):",
            "\"\"\"Universal Declaration of Human Rights\"\"\"",
            "",
            "+    VERSION = datasets.Version(\"1.0.0\")",
            "+",
            "def _info(self):",
            "return datasets.DatasetInfo(",
            "description=_DESCRIPTION,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1778039)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1778040)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'VERSION'), position=0, insert_id=1778041)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1778042)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1778043)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1778044)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1778045)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=0, insert_id=1778046)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1778047)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Version'), position=2, insert_id=1778048)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1778049)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"1.0.0\"'), position=1, insert_id=1778050)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1778051)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 505,
        "neg_line": [],
        "pos_line": [
            "+VERSION = datasets.Version(\"1.0.0\")",
            "+"
        ],
        "core_change": "+VERSION = datasets.Version(\"1.0.0\") +",
        "core_API": "Version"
    },
    {
        "commit_hash": "3ccdfaa39cd771ef6b0e767637f5d8191dc97270",
        "index": "536d52d..8990a7b 100644",
        "commit_message": "Bug fix\n\n",
        "file": "apex.txt.json",
        "label": "yes",
        "comments": "remove condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistributedFusedAdam(torch.optim.Optimizer):",
            "grp = torch.distributed.new_group(ranks=ranks)",
            "if torch.distributed.get_rank() in ranks:",
            "self._rs_pg.append(grp)",
            "-            if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks:",
            "-                self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "-                torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)",
            "+            if self._compute_L2_grad_norm:",
            "+                l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "+                if torch.distributed.get_rank() in ranks:",
            "+                    self._l2_grad_norm_pg = l2_grad_norm_pg",
            "+                    torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)",
            "self._rs_st = [torch.cuda.Stream() for _ in range(self._num_rs_pg)]",
            "for rs_pg in self._rs_pg:",
            "torch.distributed.all_reduce(self._overflow_buf,group=rs_pg)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=ASTNode(type=if_statement), node=(':', ':'), position=2, insert_id=52210)",
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=1, insert_id=52211)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=52212)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=:, text=:), position=2)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=52213)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'l2_grad_norm_pg'), position=0, insert_id=52214)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=52215)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=1)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=52216)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=52217)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'l2_grad_norm_pg'), position=2, insert_id=52218)",
            "Delete(target_node=ASTNode(type=and, text=and))",
            "Delete(target_node=ASTNode(type=boolean_operator))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 508,
        "neg_line": [
            "-if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks:",
            "-self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "-torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)"
        ],
        "pos_line": [
            "+if self._compute_L2_grad_norm:",
            "+l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "+if torch.distributed.get_rank() in ranks:",
            "+self._l2_grad_norm_pg = l2_grad_norm_pg",
            "+torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)"
        ],
        "core_change": "-if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks: -self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks) -torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg) +if self._compute_L2_grad_norm: +l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks) +if torch.distributed.get_rank() in ranks: +self._l2_grad_norm_pg = l2_grad_norm_pg +torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)",
        "core_API": "new_group"
    },
    {
        "commit_hash": "d6947d6b5c2e2d60c0009cbefd39b51d88bee2c4",
        "index": "b9eea34..0000000",
        "commit_message": "[MRG] Simple version Image Pipeline Implemented (#650)\n\n* legacy\n\n* new files\n\n* Update demo.py\n\n* test tunner bug fixed\n\n* classes and funtions in the demo created\n\n* network implemented not tested\n\n* travis to ignore legacy tests\n\n* test connectedHyperparameter\n\n* test fixed\n\n* some basic blocks implemented\n\n* still debuging\n\n* basic test for hypergraph passed\n\n* merge layer implemented\n\n* hyper_graph fully tested\n\n* more args added to auto model\n\n* test fixed\n\n* local changes\n\n* test auto_model\n\n* auto_model fit signature changed for validation data\n\n* super classes extending object\n\n* change n_ to num_\n\n* refactored automodel to extend hypermodel and removed tuner from signature\n\n* rename HyperNode to Node. TextInput added\n\n* rename HyperGraph to GraphAutoModel extending AutoModel\n\n* refactor hyperhead, removed tensor heads\n\n* removed unnecessary blocks, rename build_output to build\n\n* changed some functions and attributes to private\n\n* remove trails from AutoModel public API\n\n* test cases changed accordingly\n\n* import modules instead of objects\n\n* tuner deleted from AutoModel contructor\n\n* change trails to num_trials\n\n* use the same quote sign\n\n* revised auto pipeline docs\n\n* removed compile from AutoModel\n\n* loss and metrics moved to hyper heads\n\n* do not flatten by default in hyperheads\n\n* inputs and outputs down to GraphAutoModel\n\n* changed AutoModel\n\n* name_scope changed to tf 2.0 and moved to hyperparameters and hypermodel\n\n* remove HierarchicalHyperParameters\n\n* renaming the tests\n\n* auto_pipeline\n\n* image module tested\n\n* image regressor tested\n\n* removed some requirements\n\n* update tf to 2.0 beta\n\n* Refactor (#646)\n\n* super classes extending object\n\n* change n_ to num_\n\n* refactored automodel to extend hypermodel and removed tuner from signature\n\n* rename HyperNode to Node. TextInput added\n\n* rename HyperGraph to GraphAutoModel extending AutoModel\n\n* refactor hyperhead, removed tensor heads\n\n* removed unnecessary blocks, rename build_output to build\n\n* changed some functions and attributes to private\n\n* remove trails from AutoModel public API\n\n* test cases changed accordingly\n\n* import modules instead of objects\n\n* tuner deleted from AutoModel contructor\n\n* change trails to num_trials\n\n* use the same quote sign\n\n* revised auto pipeline docs\n\n* removed compile from AutoModel\n\n* loss and metrics moved to hyper heads\n\n* do not flatten by default in hyperheads\n\n* inputs and outputs down to GraphAutoModel\n\n* changed AutoModel\n\n* name_scope changed to tf 2.0 and moved to hyperparameters and hypermodel\n\n* remove HierarchicalHyperParameters\n\n* renaming some variables and make private some members\n\n* demo update\n\n* remove legacy\n\n* test fixed\n\n* changed setup.py\n\n* Update hyper_block.py\n\nImprove the DenseBlock: Add some other layers'category to it.\n\n* Update hyper_block.py\n\n* pep8 style fix\n\n* changed docstrings to use markdown\n\n* renaming some of the variables and classes\n\n* extracted check new and old search space to tuner base class\n\n* fixing some pylint issues\n\n* change the normalization to use mean and stddev only\n\n* dependency\n\n* dependency changed to keras-tuner\n\n* pep8 formatting\n\n",
        "file": "autokeras.txt.json",
        "label": "no",
        "comments": "remove old code",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "-import tensorflow as tf",
            "-from autokeras.hyperparameters import HyperParameters",
            "-",
            "-",
            "-def test_hierarchical_hyperparameters():",
            "-    hp = HyperParameters()",
            "-    with tf.name_scope('abc'):",
            "-        hp.Choice('num_layers', [1, 2, 3], default=1)",
            "-    assert 'abc/num_layers' in hp.values"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 509,
        "neg_line": [
            "-import tensorflow as tf",
            "-from autokeras.hyperparameters import HyperParameters",
            "-",
            "-",
            "-def test_hierarchical_hyperparameters():",
            "-hp = HyperParameters()",
            "-with tf.name_scope('abc'):",
            "-hp.Choice('num_layers', [1, 2, 3], default=1)",
            "-assert 'abc/num_layers' in hp.values"
        ],
        "pos_line": [],
        "core_change": "-import tensorflow as tf -from autokeras.hyperparameters import HyperParameters - - -def test_hierarchical_hyperparameters(): -hp = HyperParameters() -with tf.name_scope('abc'): -hp.Choice('num_layers', [1, 2, 3], default=1) -assert 'abc/num_layers' in hp.values",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "d18c392a666b4d242413705848e7500cc7280046",
        "index": "9d1aaa75..f8db05c4 100644",
        "commit_message": "Remove assert statement from non-test files (#1206)\n\n* Remove assert statement from non-test files\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Autofix issues in 4 files\n\nResolved issues in the following files via DeepSource Autofix:\n1. kornia/augmentation/container/image.py\n2. kornia/augmentation/utils/helpers.py\n3. kornia/augmentation/utils/param_validation.py\n4. kornia/enhance/adjust.py\n\n* fixl linting issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Autofix issues in 1 file\n\nResolved issues in kornia/augmentation/container/augment.py via DeepSource Autofix\n\nCo-authored-by: deepsource-autofix[bot] <62050782+deepsource-autofix[bot]@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add condition check for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def cartesian_product_of_parameters(**possible_parameters):",
            "",
            "",
            "def default_with_one_parameter_changed(*, default={}, **possible_parameters):",
            "-    assert isinstance(default, dict), f\"default should be a dict not a {type(default)}\"",
            "+    if not isinstance(default, dict):",
            "+        raise AssertionError(f\"default should be a dict not a {type(default)}\")",
            "",
            "for parameter_name, possible_values in possible_parameters.items():",
            "for v in possible_values:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=419701)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=419702)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=419703)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=419704)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=419705)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=419706)",
            "Move(target_node=IN(type=not_operator), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=block), node=('raise_statement', None), position=0, insert_id=419707)",
            "Insert(target_node=IN(type=raise_statement), node=('raise', 'raise'), position=0, insert_id=419708)",
            "Insert(target_node=IN(type=raise_statement), node=('call', None), position=1, insert_id=419709)",
            "Insert(target_node=IN(type=call), node=('identifier', 'AssertionError'), position=0, insert_id=419710)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=419711)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=419712)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=string, text=f\"default should be a dict not a {type(default)}\"), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=419713)",
            "Delete(target_node=ASTNode(type=assert, text=assert))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=assert_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 510,
        "neg_line": [
            "-assert isinstance(default, dict), f\"default should be a dict not a {type(default)}\""
        ],
        "pos_line": [
            "+if not isinstance(default, dict):",
            "+raise AssertionError(f\"default should be a dict not a {type(default)}\")"
        ],
        "core_change": "-assert isinstance(default, dict), f\"default should be a dict not a {type(default)}\" +if not isinstance(default, dict): +raise AssertionError(f\"default should be a dict not a {type(default)}\")",
        "core_API": "items"
    },
    {
        "commit_hash": "dcacbc09ea21a4f8d3a97b38ac81a123929ebb16",
        "index": "07e75bb7..3b87a6d6 100644",
        "commit_message": "center transform, py 2.7 fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "rename var",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def read_ply_data(path):",
            "pos = ([torch.tensor(data['vertex'][axis]) for axis in ['x', 'y', 'z']])",
            "pos = torch.stack(pos, dim=-1)",
            "",
            "-    face = data['face']['vertex_indices']",
            "-    face = [torch.tensor(f, dtype=torch.long) for f in face]",
            "-    face = torch.stack(face, dim=-1)",
            "+    faces = data['face']['vertex_indices']",
            "+    faces = [torch.tensor(face, dtype=torch.long) for face in faces]",
            "+    face = torch.stack(faces, dim=-1)",
            "",
            "edge_index = face_to_edge_index(face, num_nodes=pos.size(0))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=face), value='faces')",
            "Update(target_node=ASTNode(type=identifier, text=face), value='faces')",
            "Update(target_node=ASTNode(type=identifier, text=f), value='face')",
            "Update(target_node=ASTNode(type=identifier, text=face), value='faces')",
            "Update(target_node=ASTNode(type=identifier, text=face), value='faces')",
            "Update(target_node=ASTNode(type=identifier, text=f), value='face')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 6,
        "number": 511,
        "neg_line": [
            "-face = data['face']['vertex_indices']",
            "-face = [torch.tensor(f, dtype=torch.long) for f in face]",
            "-face = torch.stack(face, dim=-1)"
        ],
        "pos_line": [
            "+faces = data['face']['vertex_indices']",
            "+faces = [torch.tensor(face, dtype=torch.long) for face in faces]",
            "+face = torch.stack(faces, dim=-1)"
        ],
        "core_change": "-face = data['face']['vertex_indices'] -face = [torch.tensor(f, dtype=torch.long) for f in face] -face = torch.stack(face, dim=-1) +faces = data['face']['vertex_indices'] +faces = [torch.tensor(face, dtype=torch.long) for face in faces] +face = torch.stack(faces, dim=-1)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "17f62e96c445ea597682e73c7598ae5b3601d7e6",
        "index": "2350ab40ce..5a128a294d 100644",
        "commit_message": "fix `utility` submodule (#1709)\n\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def any(",
            "axis = tuple(range(num_dims))",
            "elif isinstance(axis, list):",
            "axis = tuple(axis)",
            "-    ret = tf.reduce_any(tf.cast(x, tf.bool), axis=axis, keepdims=keepdims)",
            "-    return ret",
            "+    return tf.reduce_any(tf.cast(x, tf.bool), axis=axis, keepdims=keepdims)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=2, insert_id=2006657)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2006658)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 514,
        "neg_line": [
            "-ret = tf.reduce_any(tf.cast(x, tf.bool), axis=axis, keepdims=keepdims)",
            "-return ret"
        ],
        "pos_line": [
            "+return tf.reduce_any(tf.cast(x, tf.bool), axis=axis, keepdims=keepdims)"
        ],
        "core_change": "-ret = tf.reduce_any(tf.cast(x, tf.bool), axis=axis, keepdims=keepdims) -return ret +return tf.reduce_any(tf.cast(x, tf.bool), axis=axis, keepdims=keepdims)",
        "core_API": "reduce_any"
    },
    {
        "commit_hash": "c66466133acd986514a5a91b29d85e72c55409f8",
        "index": "9dc22e11a..005f7215f 100755",
        "commit_message": "Fix `get_embedding` dtype at init. time (#19473)\n\n* cast positions dtype in XGLMModel\n\n* Get the correct dtype at init time\n\n* Get the correct dtype at init time\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Speech2Text2SinusoidalPositionalEmbedding(nn.Module):",
            "emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)",
            "if padding_idx is not None:",
            "emb[padding_idx, :] = 0",
            "-        return emb",
            "+        return emb.to(torch.get_default_dtype())",
            "",
            "@torch.no_grad()",
            "def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=1188909)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1188910)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1188911)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=emb), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1188912)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1188913)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1188914)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1188915)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1188916)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1188917)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1188918)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1188919)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1188920)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_default_dtype'), position=2, insert_id=1188921)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1188922)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1188923)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 516,
        "neg_line": [
            "-return emb"
        ],
        "pos_line": [
            "+return emb.to(torch.get_default_dtype())"
        ],
        "core_change": "-return emb +return emb.to(torch.get_default_dtype())",
        "core_API": "cat"
    },
    {
        "commit_hash": "ac65fcfb64df510764f8c4e55a63c7c7129856dd",
        "index": "43efe0ad..a0079a01 100644",
        "commit_message": "Reinforcement learning fix opt (#999)\n\n* change readme\n\n* Add files via upload\n\n* fix opt and make format\n\n* readme\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DDPG(object):",
            "with tf.GradientTape() as tape:",
            "a = self.actor(bs)",
            "q = self.critic([bs, a])",
            "-            a_loss = -tf.reduce_mean(q)  # maximize the q",
            "+            a_loss = - tf.reduce_mean(q)  # maximize the q",
            "a_grads = tape.gradient(a_loss, self.actor.trainable_weights)",
            "self.actor_opt.apply_gradients(zip(a_grads, self.actor.trainable_weights))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 517,
        "neg_line": [
            "-a_loss = -tf.reduce_mean(q)  # maximize the q"
        ],
        "pos_line": [
            "+a_loss = - tf.reduce_mean(q)  # maximize the q"
        ],
        "core_change": "-a_loss = -tf.reduce_mean(q)  # maximize the q +a_loss = - tf.reduce_mean(q)  # maximize the q",
        "core_API": "GradientTape"
    },
    {
        "commit_hash": "813744e5f3af32a81cf31427940d1a2d3abdf578",
        "index": "7f440673..33505c81 100644",
        "commit_message": "MPS schedulers: don't use float64 (#1169)\n\n* Schedulers: don't use float64 on mps\n\n* Test set_timesteps() on device (float schedulers).\n\n* SD pipeline: use device in set_timesteps.\n\n* SD in-painting pipeline: use device in set_timesteps.\n\n* Tests: fix mps crashes.\n\n* Skip test_load_pipeline_from_git on mps.\n\nNot compatible with float16.\n\n* Use device.type instead of str in Euler schedulers.\n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):",
            "",
            "prev_sample = sample + derivative * dt",
            "",
            "-        device = model_output.device if torch.is_tensor(model_output) else \"cpu\"",
            "-        if str(device) == \"mps\":",
            "+        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")",
            "+        if device.type == \"mps\":",
            "# randn does not work reproducibly on mps",
            "noise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to(",
            "device"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=0, insert_id=99938)",
            "Insert(target_node=ASTNode(type=conditional_expression), node=('call', None), position=4, insert_id=99939)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=device), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=99940)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=99941)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=99942)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=99943)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=99944)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=99945)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=99946)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=99947)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=string, text=\"cpu\"), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=99948)",
            "Delete(target_node=ASTNode(type=identifier, text=str))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 518,
        "neg_line": [
            "-device = model_output.device if torch.is_tensor(model_output) else \"cpu\"",
            "-if str(device) == \"mps\":"
        ],
        "pos_line": [
            "+device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")",
            "+if device.type == \"mps\":"
        ],
        "core_change": "-device = model_output.device if torch.is_tensor(model_output) else \"cpu\" -if str(device) == \"mps\": +device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\") +if device.type == \"mps\":",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "84cf269e409563052fe87320ca3cc88d31e15152",
        "index": "78c412bd..0ee86b8b 100644",
        "commit_message": "Fix AutoRegNN with default CUDA tensor (#1308)\n\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AutoRegressiveNN(nn.Module):",
            "",
            "if permutation is None:",
            "# By default set a random permutation of variables, which is important for performance with multiple steps",
            "-            self.permutation = torch.randperm(input_dim)",
            "+            self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device)",
            "else:",
            "# The permutation is chosen by the user",
            "self.permutation = permutation.type(dtype=torch.int64)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=737465)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=737466)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=737467)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=737468)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=737469)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=737470)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=737471)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=737472)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=737473)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=737474)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=737475)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=737476)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=737477)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=737478)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=737479)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=737480)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=737481)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'cpu'\"), position=2, insert_id=737482)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=737483)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=737484)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=737485)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=737486)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=737487)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 519,
        "neg_line": [
            "-self.permutation = torch.randperm(input_dim)"
        ],
        "pos_line": [
            "+self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device)"
        ],
        "core_change": "-self.permutation = torch.randperm(input_dim) +self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device)",
        "core_API": "randperm"
    },
    {
        "commit_hash": "d6947d6b5c2e2d60c0009cbefd39b51d88bee2c4",
        "index": "2b1461e..e0e7cc8 100644",
        "commit_message": "[MRG] Simple version Image Pipeline Implemented (#650)\n\n* legacy\n\n* new files\n\n* Update demo.py\n\n* test tunner bug fixed\n\n* classes and funtions in the demo created\n\n* network implemented not tested\n\n* travis to ignore legacy tests\n\n* test connectedHyperparameter\n\n* test fixed\n\n* some basic blocks implemented\n\n* still debuging\n\n* basic test for hypergraph passed\n\n* merge layer implemented\n\n* hyper_graph fully tested\n\n* more args added to auto model\n\n* test fixed\n\n* local changes\n\n* test auto_model\n\n* auto_model fit signature changed for validation data\n\n* super classes extending object\n\n* change n_ to num_\n\n* refactored automodel to extend hypermodel and removed tuner from signature\n\n* rename HyperNode to Node. TextInput added\n\n* rename HyperGraph to GraphAutoModel extending AutoModel\n\n* refactor hyperhead, removed tensor heads\n\n* removed unnecessary blocks, rename build_output to build\n\n* changed some functions and attributes to private\n\n* remove trails from AutoModel public API\n\n* test cases changed accordingly\n\n* import modules instead of objects\n\n* tuner deleted from AutoModel contructor\n\n* change trails to num_trials\n\n* use the same quote sign\n\n* revised auto pipeline docs\n\n* removed compile from AutoModel\n\n* loss and metrics moved to hyper heads\n\n* do not flatten by default in hyperheads\n\n* inputs and outputs down to GraphAutoModel\n\n* changed AutoModel\n\n* name_scope changed to tf 2.0 and moved to hyperparameters and hypermodel\n\n* remove HierarchicalHyperParameters\n\n* renaming the tests\n\n* auto_pipeline\n\n* image module tested\n\n* image regressor tested\n\n* removed some requirements\n\n* update tf to 2.0 beta\n\n* Refactor (#646)\n\n* super classes extending object\n\n* change n_ to num_\n\n* refactored automodel to extend hypermodel and removed tuner from signature\n\n* rename HyperNode to Node. TextInput added\n\n* rename HyperGraph to GraphAutoModel extending AutoModel\n\n* refactor hyperhead, removed tensor heads\n\n* removed unnecessary blocks, rename build_output to build\n\n* changed some functions and attributes to private\n\n* remove trails from AutoModel public API\n\n* test cases changed accordingly\n\n* import modules instead of objects\n\n* tuner deleted from AutoModel contructor\n\n* change trails to num_trials\n\n* use the same quote sign\n\n* revised auto pipeline docs\n\n* removed compile from AutoModel\n\n* loss and metrics moved to hyper heads\n\n* do not flatten by default in hyperheads\n\n* inputs and outputs down to GraphAutoModel\n\n* changed AutoModel\n\n* name_scope changed to tf 2.0 and moved to hyperparameters and hypermodel\n\n* remove HierarchicalHyperParameters\n\n* renaming some variables and make private some members\n\n* demo update\n\n* remove legacy\n\n* test fixed\n\n* changed setup.py\n\n* Update hyper_block.py\n\nImprove the DenseBlock: Add some other layers'category to it.\n\n* Update hyper_block.py\n\n* pep8 style fix\n\n* changed docstrings to use markdown\n\n* renaming some of the variables and classes\n\n* extracted check new and old search space to tuner base class\n\n* fixing some pylint issues\n\n* change the normalization to use mean and stddev only\n\n* dependency\n\n* dependency changed to keras-tuner\n\n* pep8 formatting\n\n",
        "file": "autokeras.txt.json",
        "label": "no",
        "comments": "not clear",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class Input(Node):",
            "",
            "",
            "class ImageInput(Node):",
            "+",
            "def __init__(self, **kwargs):",
            "super().__init__(**kwargs)",
            "",
            "def build(self, hp):",
            "-        pass",
            "+        return tf.keras.Input(shape=self.shape)",
            "",
            "",
            "class TextInput(Node):",
            "+",
            "def __init__(self, **kwargs):",
            "super().__init__(**kwargs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2476033)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=2476034)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2476035)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=2476036)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2476037)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2476038)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2476039)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2476040)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Input'), position=2, insert_id=2476041)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2476042)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2476043)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2476044)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2476045)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2476046)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'keras'), position=2, insert_id=2476047)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'shape'), position=0, insert_id=2476048)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2476049)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2476050)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2476051)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2476052)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=2476053)",
            "Delete(target_node=ASTNode(type=pass, text=pass))",
            "Delete(target_node=ASTNode(type=pass_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 521,
        "neg_line": [
            "-pass"
        ],
        "pos_line": [
            "+",
            "+return tf.keras.Input(shape=self.shape)",
            "+"
        ],
        "core_change": "+ -pass +return tf.keras.Input(shape=self.shape) +",
        "core_API": "Input"
    },
    {
        "commit_hash": "b3911f89a30a12ed34e993b090a748d4a8f886bb",
        "index": "fea2b3e5..aa087bb0 100644",
        "commit_message": "make fix copies\n\n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "change param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionInpaintPipeline(DiffusionPipeline):",
            "else:",
            "raise ImportError(\"Please install accelerate via `pip install accelerate`\")",
            "",
            "-        device = torch.device(\"cuda\")",
            "+        device = torch.device(f\"cuda:{gpu_id}\")",
            "",
            "for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:",
            "if cpu_offloaded_model is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"cuda\"), value='f\"cuda:{gpu_id}\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 522,
        "neg_line": [
            "-device = torch.device(\"cuda\")"
        ],
        "pos_line": [
            "+device = torch.device(f\"cuda:{gpu_id}\")"
        ],
        "core_change": "-device = torch.device(\"cuda\") +device = torch.device(f\"cuda:{gpu_id}\")",
        "core_API": "device"
    },
    {
        "commit_hash": "091f5e28df3974109145a903d276ff48c2634d66",
        "index": "d9eee280..9bb343f8 100644",
        "commit_message": "Add onnx convert script (#2291)\n\n* add onnx convert script\n\n* upload pytorch2onnx.py\n\n* restore config\n\n* set use_torchvion on-the-fly, update doc\n\n* update doc\n\n* support pass input\n\n* fixed passes\n",
        "file": "mmdetection.txt.json",
        "label": "yes",
        "comments": "change API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TwoStageDetector(BaseDetector, RPNTestMixin, BBoxTestMixin,",
            "if self.with_rpn:",
            "rpn_outs = self.rpn_head(x)",
            "outs = outs + (rpn_outs, )",
            "-        proposals = torch.randn(1000, 4).cuda()",
            "+        proposals = torch.randn(1000, 4).to(device=img.device)",
            "# bbox head",
            "rois = bbox2roi([proposals])",
            "if self.with_bbox:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=cuda), value='to')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=640483)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=640484)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=640485)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=640486)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'img'), position=0, insert_id=640487)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640488)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=640489)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 524,
        "neg_line": [
            "-proposals = torch.randn(1000, 4).cuda()"
        ],
        "pos_line": [
            "+proposals = torch.randn(1000, 4).to(device=img.device)"
        ],
        "core_change": "-proposals = torch.randn(1000, 4).cuda() +proposals = torch.randn(1000, 4).to(device=img.device)",
        "core_API": "rpn_head"
    },
    {
        "commit_hash": "bed94cbf553c843142fd0e738dd98b8823a09f4a",
        "index": "db2e115b..48542f4e 100644",
        "commit_message": "fixed shape errors, now numerical instability in categorical trpo update\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "add API call for shape fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PGModel(Model):",
            "actions = np.concatenate([path['actions'] for path in batch])",
            "batch_advantage = np.concatenate([path[\"advantage\"] for path in batch])",
            "batch_advantage = zero_mean_unit_variance(batch_advantage)",
            "+        batch_advantage = np.expand_dims(batch_advantage, axis=1)",
            "states = np.concatenate([path['states'] for path in batch])",
            "",
            "return action_log_stds, action_means, actions, batch_advantage, states"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2246922)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2246923)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'batch_advantage'), position=0, insert_id=2246924)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2246925)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2246926)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2246927)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2246928)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=2246929)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2246930)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'expand_dims'), position=2, insert_id=2246931)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2246932)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'batch_advantage'), position=1, insert_id=2246933)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2246934)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2246935)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2246936)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2246937)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2246938)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=2246939)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 526,
        "neg_line": [],
        "pos_line": [
            "+batch_advantage = np.expand_dims(batch_advantage, axis=1)"
        ],
        "core_change": "+batch_advantage = np.expand_dims(batch_advantage, axis=1)",
        "core_API": "concatenate"
    },
    {
        "commit_hash": "de67c1546d065c86d5bd5cecf022925bb01d0ec7",
        "index": "0175dc08..6ee14922 100644",
        "commit_message": "Add Cuda tests for pyro.distributions; fix errors (#297)\n\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Categorical(Distribution):",
            "elif one_hot:",
            "boolean_mask = x",
            "else:",
            "-                boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1)",
            "+                boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)",
            "# apply log function to masked probability tensor",
            "return torch.log(ps.masked_select(boolean_mask.byte()).contiguous().view(*batch_pdf_size))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='torch_zeros_like')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=size), value='data')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 529,
        "neg_line": [
            "-boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1)"
        ],
        "pos_line": [
            "+boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)"
        ],
        "core_change": "-boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1) +boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "765bafb8e4ed11073f196a3372d7990428e270b0",
        "index": "6ae62cb1c..8cbde5b2c 100644",
        "commit_message": "Fix CI: test_inference_for_pretraining in ViTMAEModelTest (#16591)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for resource fixfor resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ViTMAEModelIntegrationTest(unittest.TestCase):",
            "",
            "# forward pass",
            "with torch.no_grad():",
            "-            outputs = model(**inputs, noise=torch.from_numpy(noise))",
            "+            outputs = model(**inputs, noise=torch.from_numpy(noise).to(device=torch_device))",
            "",
            "# verify the logits",
            "expected_shape = torch.Size((1, 196, 768))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1202151)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1202152)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1202153)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1202154)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1202155)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1202156)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1202157)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1202158)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1202159)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_device'), position=2, insert_id=1202160)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 530,
        "neg_line": [
            "-outputs = model(**inputs, noise=torch.from_numpy(noise))"
        ],
        "pos_line": [
            "+outputs = model(**inputs, noise=torch.from_numpy(noise).to(device=torch_device))"
        ],
        "core_change": "-outputs = model(**inputs, noise=torch.from_numpy(noise)) +outputs = model(**inputs, noise=torch.from_numpy(noise).to(device=torch_device))",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "500bf6df9bf04775cfdc7ac8bdfcd9dec44ebb5a",
        "index": "bd8f0c677..f4761f8b6 100644",
        "commit_message": "fix comment\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "\"source\": [",
            "\"## Computation\\n\",",
            "\"\\n\",",
            "-    \"**Note copmut\"",
            "+    \"**Note computation, tfe.serving.QueueServer etc. will move into model.share()**\"",
            "]",
            "},",
            "{"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"**Note copmut\"), value='\"**Note computation, tfe.serving.QueueServer etc. will move into model.share()**\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 531,
        "neg_line": [
            "-\"**Note copmut\""
        ],
        "pos_line": [
            "+\"**Note computation, tfe.serving.QueueServer etc. will move into model.share()**\""
        ],
        "core_change": "-\"**Note copmut\" +\"**Note computation, tfe.serving.QueueServer etc. will move into model.share()**\"",
        "core_API": "share"
    },
    {
        "commit_hash": "8dd401af7bc97644dac71ddd56c2ea558dd9e4a0",
        "index": "5705140e..953f2b38 100644",
        "commit_message": "Update nlp.py\n\nFix some bugs like https://github.com/tensorflow/tensorflow/issues/5118.\n",
        "file": "TensorLayer.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def initialize_vocabulary(vocabulary_path):",
            "rev_vocab = []",
            "with gfile.GFile(vocabulary_path, mode=\"rb\") as f:",
            "rev_vocab.extend(f.readlines())",
            "-    rev_vocab = [line.strip() for line in rev_vocab]",
            "+    rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]",
            "vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])",
            "return vocab, rev_vocab",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2269162)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2269163)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2269164)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2269165)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_bytes'), position=2, insert_id=2269166)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2269167)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2269168)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2269169)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2269170)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2269171)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 532,
        "neg_line": [
            "-rev_vocab = [line.strip() for line in rev_vocab]"
        ],
        "pos_line": [
            "+rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]"
        ],
        "core_change": "-rev_vocab = [line.strip() for line in rev_vocab] +rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]",
        "core_API": "GFile"
    },
    {
        "commit_hash": "f8798c1c13bece96a0fd92d9b7e7bd69576fa34d",
        "index": "9b6f9735..22f26b56 100644",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CenterCrop(GeometricAugmentationBase2D):",
            "padding_mode=\"zeros\",",
            ")",
            "",
            "-    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, Tensor]:",
            "+    def generate_parameters(self, batch_shape: Tuple[int, ...]) -> Dict[str, Tensor]:",
            "return rg.center_crop_generator(batch_shape[0], batch_shape[-2], batch_shape[-1], self.size, self.device)",
            "",
            "def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('type', None), position=0, insert_id=387766)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=387767)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Tuple'), position=0, insert_id=387768)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=387769)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'int'), position=2, insert_id=387770)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=387771)",
            "Insert(target_node=IN(type=subscript), node=('ellipsis', '...'), position=4, insert_id=387772)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=387773)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Size))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 533,
        "neg_line": [
            "-def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, Tensor]:"
        ],
        "pos_line": [
            "+def generate_parameters(self, batch_shape: Tuple[int, ...]) -> Dict[str, Tensor]:"
        ],
        "core_change": "-def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, Tensor]: +def generate_parameters(self, batch_shape: Tuple[int, ...]) -> Dict[str, Tensor]:",
        "core_API": "center_crop_generator"
    },
    {
        "commit_hash": "aac0f9efc6243c5674029ff2b7a4ee2fb7d234f8",
        "index": "26fd6a248..7f87cc7aa 100644",
        "commit_message": "Fix complex support\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add condition check for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ConformerSeparator(AbsSeparator):",
            "\"\"\"",
            "",
            "# if complex spectrum,",
            "-        if isinstance(input, ComplexTensor):",
            "+        if isinstance(input, ComplexTensor) or (",
            "+            is_torch_1_8_plus and torch.is_complex(input)",
            "+        ):",
            "feature = abs(input)",
            "else:",
            "feature = input"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=138241)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=138242)",
            "Insert(target_node=IN(type=boolean_operator), node=('parenthesized_expression', None), position=2, insert_id=138243)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=138244)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=138245)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=138246)",
            "Insert(target_node=IN(type=boolean_operator), node=('identifier', 'is_torch_1_8_plus'), position=0, insert_id=138247)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=138248)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=2, insert_id=138249)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=138250)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=138251)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=138252)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=138253)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_complex'), position=2, insert_id=138254)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=138255)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'input'), position=1, insert_id=138256)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=138257)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 534,
        "neg_line": [
            "-if isinstance(input, ComplexTensor):"
        ],
        "pos_line": [
            "+if isinstance(input, ComplexTensor) or (",
            "+is_torch_1_8_plus and torch.is_complex(input)",
            "+):"
        ],
        "core_change": "-if isinstance(input, ComplexTensor): +if isinstance(input, ComplexTensor) or ( +is_torch_1_8_plus and torch.is_complex(input) +):",
        "core_API": "is_complex"
    },
    {
        "commit_hash": "1d4823c0ec446e93d00df8ca654db4b45b63b3d4",
        "index": "8d3846aaba..cc800c9f3b 100644",
        "commit_message": "[RLlib] Add testing framework_iterator. (#7852)\n\n* Add testing framework_iterator.\n\n* LINT.\n\n* WIP.\n\n* Fix and LINT.\n\n* LINT fix.\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "remove condition",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def model_import_test(algo, config, env):",
            "agent.import_model(import_file=import_file)",
            "check(current_weight(agent), weight_after_import)",
            "",
            "-        if eager_mode_ctx:",
            "-            eager_mode_ctx.__exit__(None, None, None)",
            "-",
            "",
            "class TestModelImport(unittest.TestCase):",
            "def setUp(self):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=eager_mode_ctx))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=eager_mode_ctx))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__exit__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 535,
        "neg_line": [
            "-if eager_mode_ctx:",
            "-eager_mode_ctx.__exit__(None, None, None)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-if eager_mode_ctx: -eager_mode_ctx.__exit__(None, None, None) -",
        "core_API": "import_model"
    },
    {
        "commit_hash": "519d1e742011258742af664bb2a6db7a805add38",
        "index": "1850d661..59393f92 100644",
        "commit_message": "bug fix : cnn tensorflow backend error (#3558)\n\n\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def batch_flatten(x):",
            "'''Turn a n-D tensor into a 2D tensor where",
            "the first dimension is conserved.",
            "'''",
            "-    x = tf.reshape(x, [-1, prod(shape(x)[1:])])",
            "+    x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))",
            "return x"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=2116059)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2116060)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2116061)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2116062)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2116063)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2116064)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pack'), position=2, insert_id=2116065)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2116066)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=list), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 537,
        "neg_line": [
            "-x = tf.reshape(x, [-1, prod(shape(x)[1:])])"
        ],
        "pos_line": [
            "+x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))"
        ],
        "core_change": "-x = tf.reshape(x, [-1, prod(shape(x)[1:])]) +x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))",
        "core_API": "reshape"
    },
    {
        "commit_hash": "bc797fd37613f18ddf0fd5122776b4cdcc4922ae",
        "index": "86bd7da10..8b39c74cd 100644",
        "commit_message": "[App] Fix multi-node pytorch example CI (#15753)\n\n\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def distributed_train(local_rank: int, main_address: str, main_port: int, num_no",
            "# 2. PREPARE DISTRIBUTED MODEL",
            "model = torch.nn.Linear(32, 2)",
            "device = torch.device(f\"cuda:{local_rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")",
            "-    model = DistributedDataParallel(model, device_ids=[local_rank]).to(device)",
            "+    model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None).to(device)",
            "",
            "# 3. SETUP LOSS AND OPTIMIZER",
            "criterion = torch.nn.MSELoss()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('conditional_expression', None), position=2, insert_id=495313)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=list), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=495314)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=2, insert_id=495315)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=495316)",
            "Insert(target_node=IN(type=conditional_expression), node=('none', 'None'), position=4, insert_id=495317)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=495318)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=495319)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=495320)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=495321)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_available'), position=2, insert_id=495322)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=495323)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=495324)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=495325)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=495326)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=495327)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 538,
        "neg_line": [
            "-model = DistributedDataParallel(model, device_ids=[local_rank]).to(device)"
        ],
        "pos_line": [
            "+model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None).to(device)"
        ],
        "core_change": "-model = DistributedDataParallel(model, device_ids=[local_rank]).to(device) +model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None).to(device)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "d86c7fb53933f5b7558dd1ce202836d164becca5",
        "index": "7655eb096..a2005c4d7 100644",
        "commit_message": "Refac module factory + avoid etag requests for hub datasets (#2986)\n\n* refac module factory + avoid etag requests for hub datasets\n\n* fix tests\n\n* typing\n\n* fixes\n\n* prepare timeout\n\n* fix offline simulator with hugginggace_hub\n\n* add module factory tests (1/N)\n\n* add module factory test (2/N)\n\n* add data files tests (1/N)\n\n* add data fiels tests (2/N)\n\n* add data files tests (3/N)\n\n* style\n\n* docstrings\n\n* don't update counts when running tests\n\n* nump huggingface_hub\n\n* add timeouts for offline mode\n\n* minor\n\n* minor bis\n\n* install ruamel-yaml properly in the CI for windows\n\n* fix windows test\n\n* style\n\n* fix comet intensive calls patcher\n\n* warning message when loading from the master branch\n\n* style\n\n* albert's comments\n\n* remove unnecessary check\n\n* don't use master if HF_SCRIPTS_VERSION is specified\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class COMET(datasets.Metric):",
            "",
            "def _download_and_prepare(self, dl_manager):",
            "if self.config_name == \"default\":",
            "-            self.scorer = download_model(\"wmt-large-da-estimator-1719\")",
            "+            self.scorer = comet.models.download_model(\"wmt-large-da-estimator-1719\")",
            "else:",
            "-            self.scorer = download_model(self.config_name)",
            "+            self.scorer = comet.models.download_model(self.config_name)",
            "",
            "def _compute(self, sources, predictions, references, cuda=True, show_progress=False):",
            "data = {\"src\": sources, \"mt\": predictions, \"ref\": references}"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('attribute', None), position=0, insert_id=2653431)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2653432)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2653433)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'scorer'), position=2, insert_id=2653434)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2653435)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2653436)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2653437)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=download_model), position=2)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2653438)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'comet'), position=0, insert_id=2653439)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2653440)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'models'), position=2, insert_id=2653441)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2653442)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2653443)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=download_model), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'comet'), position=0, insert_id=2653444)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2653445)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'models'), position=2, insert_id=2653446)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=scorer))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 543,
        "neg_line": [
            "-self.scorer = download_model(\"wmt-large-da-estimator-1719\")",
            "-self.scorer = download_model(self.config_name)"
        ],
        "pos_line": [
            "+self.scorer = comet.models.download_model(\"wmt-large-da-estimator-1719\")",
            "+self.scorer = comet.models.download_model(self.config_name)"
        ],
        "core_change": "-self.scorer = download_model(\"wmt-large-da-estimator-1719\") +self.scorer = comet.models.download_model(\"wmt-large-da-estimator-1719\") -self.scorer = download_model(self.config_name) +self.scorer = comet.models.download_model(self.config_name)",
        "core_API": "download_model"
    },
    {
        "commit_hash": "e9928defa40758fa41c4493fda4fdd5be3b3c751",
        "index": "7e46b06d07..c780b828e8 100644",
        "commit_message": "fixed a problem of quantile of torch backend that it rejects integer input\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def quantile(",
            "temp, q, dim=-1, keepdim=keepdims, interpolation=interpolation, out=out",
            ")",
            "return torch.quantile(",
            "-        a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out",
            "+        temp, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=a), value='temp')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 544,
        "neg_line": [
            "-a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out"
        ],
        "pos_line": [
            "+temp, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out"
        ],
        "core_change": "-a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out +temp, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out",
        "core_API": "quantile"
    },
    {
        "commit_hash": "039aca02737f4a240b73dd1742fecf45987225be",
        "index": "c8f1bd1..215826b 100644",
        "commit_message": "fix a size mismatch bug in faster_rcnn\n\n",
        "file": "faster-rcnn.pytorch.txt.json",
        "label": "yes",
        "comments": "change param for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if __name__ == '__main__':",
            "",
            "# dataset = roiLoader(roidb, imdb.num_classes)",
            "dataset = roibatchLoader(roidb, imdb.num_classes)",
            "-  dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.IMS_PER_BATCH,",
            "+  dataloader = torch.utils.data.DataLoader(dataset, batch_size=8,",
            "shuffle=False, num_workers=5)",
            "",
            "# initilize the tensor holder here."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('integer', '8'), position=2, insert_id=229423)",
            "Delete(target_node=ASTNode(type=identifier, text=cfg))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TRAIN))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=IMS_PER_BATCH))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 545,
        "neg_line": [
            "-dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.IMS_PER_BATCH,"
        ],
        "pos_line": [
            "+dataloader = torch.utils.data.DataLoader(dataset, batch_size=8,"
        ],
        "core_change": "-dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.IMS_PER_BATCH, +dataloader = torch.utils.data.DataLoader(dataset, batch_size=8,",
        "core_API": "DataLoader"
    },
    {
        "commit_hash": "e3d15a2d5ca3a9de89fc4c10f93ffa87d85ac032",
        "index": "8ce203b50c..fca84476a9 100644",
        "commit_message": "test validity fix\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "name change",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def test_confusion_matrix(",
            "@handle_cmd_line_args",
            "@given(",
            "dtype_and_x=helpers.dtype_and_values(",
            "-        available_dtypes=tuple(ivy_tf.valid_numeric_dtypes)",
            "+        available_dtypes=tuple(ivy_tf.valid_float_dtypes)",
            "),",
            "x=helpers.array_values(shape=(3,), dtype=ivy.int32),",
            "as_variable=st.booleans(),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=valid_numeric_dtypes), value='valid_float_dtypes')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 547,
        "neg_line": [
            "-available_dtypes=tuple(ivy_tf.valid_numeric_dtypes)"
        ],
        "pos_line": [
            "+available_dtypes=tuple(ivy_tf.valid_float_dtypes)"
        ],
        "core_change": "-available_dtypes=tuple(ivy_tf.valid_numeric_dtypes) +available_dtypes=tuple(ivy_tf.valid_float_dtypes)",
        "core_API": "dtype_and_values"
    },
    {
        "commit_hash": "288682796f4a18760f1276dd2d184f297fdf5182",
        "index": "578a5f0..7a7afbf 100644",
        "commit_message": "Update benchmark script to add precision arg. Fix some downstream (DeiT) compat issues with latest changes. Bump version to 0.4.7\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "yes",
        "comments": "change condition check for null fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class VisionTransformer(nn.Module):",
            "",
            "def forward(self, x):",
            "x = self.forward_features(x)",
            "-        if isinstance(x, tuple):",
            "-            x, x_dist = self.head(x[0]), self.head_dist(x[1])",
            "+        if self.head_dist is not None:",
            "+            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple",
            "if self.training and not torch.jit.is_scripting():",
            "# during inference, return the average of both classifier predictions",
            "return x, x_dist"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1479188)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1479189)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1479190)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1479191)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1479192)",
            "Update(target_node=ASTNode(type=identifier, text=isinstance), value='self')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=isinstance), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1479193)",
            "Update(target_node=ASTNode(type=identifier, text=x), value='head_dist')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=x), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=tuple))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 548,
        "neg_line": [
            "-if isinstance(x, tuple):",
            "-x, x_dist = self.head(x[0]), self.head_dist(x[1])"
        ],
        "pos_line": [
            "+if self.head_dist is not None:",
            "+x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple"
        ],
        "core_change": "-if isinstance(x, tuple): -x, x_dist = self.head(x[0]), self.head_dist(x[1]) +if self.head_dist is not None: +x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple",
        "core_API": "forward_features"
    },
    {
        "commit_hash": "a860d5ee4790ef8207558e2e4b806544cb61bdab",
        "index": "193d9a8..6fcdd97 100644",
        "commit_message": "Wrap dataset generation function to disable autograph to fix issues with invalid tensor shapes (#2069)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n",
        "file": "horovod.txt.json",
        "label": "yes",
        "comments": "add condition check for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFKerasUtil(object):",
            "",
            "dataset = dataset.batch(batch_size).map(prep_data_tf_keras)",
            "return dataset",
            "-        return fn",
            "+        return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn",
            "",
            "@staticmethod",
            "def get_horovod():"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('conditional_expression', None), position=1, insert_id=1946077)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=0, insert_id=1946078)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1946079)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', '_HAS_AUTOGRAPH'), position=2, insert_id=1946080)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1946081)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'fn'), position=4, insert_id=1946082)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1946083)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1946084)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1946085)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1946086)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'do_not_convert'), position=2, insert_id=1946087)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1946088)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=fn), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1946089)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1946090)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1946091)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'experimental'), position=2, insert_id=1946092)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1946093)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1946094)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'autograph'), position=2, insert_id=1946095)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 549,
        "neg_line": [
            "-return fn"
        ],
        "pos_line": [
            "+return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn"
        ],
        "core_change": "-return fn +return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn",
        "core_API": "batch"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "49b32f13..b37e3e69 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "yes",
        "comments": "change param for null fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.device('/cpu:0'):",
            "net = FlattenLayer(net, name='flatten')",
            "net = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')",
            "net = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')",
            "-            net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output')",
            "+            net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')",
            "y = net.outputs",
            "",
            "ce = tl.cost.cross_entropy(y, y_, name='cost')"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('none', 'None'), position=2, insert_id=2262259)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 554,
        "neg_line": [
            "-net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output')"
        ],
        "pos_line": [
            "+net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')"
        ],
        "core_change": "-net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output') +net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')",
        "core_API": "device"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "16554c9279..34ebba45d0 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GaussianNoise(Exploration):",
            "true_fn=lambda: stochastic_actions,",
            "false_fn=lambda: deterministic_actions)",
            "# Logp=always zero.",
            "-        logp = tf.zeros(shape=(batch_size, ), dtype=tf.float32)",
            "+        logp = tf.zeros(shape=(batch_size,), dtype=tf.float32)",
            "",
            "# Increment `last_timestep` by 1 (or set to `timestep`).",
            "-        assign_op = \\",
            "-            tf.assign_add(self.last_timestep, 1) if timestep is None else \\",
            "-            tf.assign(self.last_timestep, timestep)",
            "-        with tf.control_dependencies([assign_op]):",
            "+        assign_op = (",
            "+            tf1.assign_add(self.last_timestep, 1) if timestep is None else",
            "+            tf1.assign(self.last_timestep, timestep))",
            "+        with tf1.control_dependencies([assign_op]):",
            "return action, logp",
            "",
            "def _get_torch_exploration_action(self, action_dist, explore, timestep):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=2145764)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=2145765)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=conditional_expression), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=2145766)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 7,
        "number": 555,
        "neg_line": [
            "-logp = tf.zeros(shape=(batch_size, ), dtype=tf.float32)",
            "-assign_op = \\",
            "-tf.assign_add(self.last_timestep, 1) if timestep is None else \\",
            "-tf.assign(self.last_timestep, timestep)",
            "-with tf.control_dependencies([assign_op]):"
        ],
        "pos_line": [
            "+logp = tf.zeros(shape=(batch_size,), dtype=tf.float32)",
            "+assign_op = (",
            "+tf1.assign_add(self.last_timestep, 1) if timestep is None else",
            "+tf1.assign(self.last_timestep, timestep))",
            "+with tf1.control_dependencies([assign_op]):"
        ],
        "core_change": "-logp = tf.zeros(shape=(batch_size, ), dtype=tf.float32) +logp = tf.zeros(shape=(batch_size,), dtype=tf.float32) -assign_op = \\ -tf.assign_add(self.last_timestep, 1) if timestep is None else \\ -tf.assign(self.last_timestep, timestep) -with tf.control_dependencies([assign_op]): +assign_op = ( +tf1.assign_add(self.last_timestep, 1) if timestep is None else +tf1.assign(self.last_timestep, timestep)) +with tf1.control_dependencies([assign_op]):",
        "core_API": "zeros"
    },
    {
        "commit_hash": "245b072a82f2e87a9032a1b87136a9930c008afc",
        "index": "66149d2..c7128d0 100644",
        "commit_message": "Fixed bad use of ConvTranspose2D\n",
        "file": "Pytorch-UNet.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class up(nn.Module):",
            "if bilinear:",
            "self.up = nn.UpsamplingBilinear2d(scale_factor=2)",
            "else:",
            "-            self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)",
            "+            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)",
            "",
            "self.conv = double_conv(in_ch, out_ch)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1482831)",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=4, insert_id=1482832)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=in_ch), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('//', '//'), position=1, insert_id=1482833)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=1482834)",
            "Update(target_node=ASTNode(type=identifier, text=out_ch), value='in_ch')",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=out_ch), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('//', '//'), position=1, insert_id=1482835)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=1482836)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 557,
        "neg_line": [
            "-self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)"
        ],
        "pos_line": [
            "+self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)"
        ],
        "core_change": "-self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2) +self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)",
        "core_API": "UpsamplingBilinear2d"
    },
    {
        "commit_hash": "19187d38f936e213e1c2e35b8d2a86f16afff8cd",
        "index": "3a853be0e..f003e0d3d 100644",
        "commit_message": "[Metrics] Detach bugfix (#4313)\n\n* detach on buffer\n\n* doc update\n\n* remove file\n\n* changelog\n\n* suggestions\n\n* Update docs/source/metrics.rst\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* fix for 4266\n\n* Update docs/source/metrics.rst\n\nCo-authored-by: Adrian W√§lchli <aedu.waelchli@gmail.com>\n\n* Update CHANGELOG.md\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\nCo-authored-by: chaton <thomas@grid.ai>\nCo-authored-by: Adrian W√§lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Ananya Harsh Jha <ananya@pytorchlightning.ai>\nCo-authored-by: Roger Shieh <sh.rog@protonmail.ch>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Metric(nn.Module, ABC):",
            "Automatically calls ``update()``. Returns the metric value over inputs if ``compute_on_step`` is True.",
            "\"\"\"",
            "# add current step",
            "-        self.update(*args, **kwargs)",
            "+        with torch.no_grad():",
            "+            self.update(*args, **kwargs)",
            "self._forward_cache = None",
            "",
            "if self.compute_on_step:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=2, insert_id=558824)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=558825)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=558826)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=558827)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=558828)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=558829)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=558830)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=558831)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=558832)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=558833)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=558834)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_grad'), position=2, insert_id=558835)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=558836)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=558837)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 559,
        "neg_line": [
            "-self.update(*args, **kwargs)"
        ],
        "pos_line": [
            "+with torch.no_grad():",
            "+self.update(*args, **kwargs)"
        ],
        "core_change": "-self.update(*args, **kwargs) +with torch.no_grad(): +self.update(*args, **kwargs)",
        "core_API": "update"
    },
    {
        "commit_hash": "4c932387a4d67189ec862e060dedc81198dea5d5",
        "index": "495393f..d0c1d7a 100644",
        "commit_message": "fixing inference to use volatile variables\n\n",
        "file": "examples.txt.json",
        "label": "yes",
        "comments": "change API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "temperature = max(args.temperature, 1e-3)",
            "with open(args.outf, 'w') as outf:",
            "for i in range(args.nwords):",
            "",
            "-        output, hidden = model(Variable(input, requires_grad=False), hidden)",
            "-        gen = torch.multinomial(output[0].data.cpu().div(temperature).exp(), 1)[0][0] # FIXME: no multinomial on GPU?",
            "+        output, hidden = model(Variable(input, volatile=True), hidden)",
            "+        gen = torch.multinomial(output[0].data.div(temperature).exp().cpu(), 1)[0][0] # FIXME: multinomial is only for CPU",
            "input.fill_(gen)",
            "word = corpus.dic.idx2word[gen]",
            "outf.write(word)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=requires_grad), value='volatile')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('true', 'True'), position=2, insert_id=191996)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=exp), value='cpu')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=exp), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=div), value='exp')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=cpu), value='div')",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 560,
        "neg_line": [
            "-output, hidden = model(Variable(input, requires_grad=False), hidden)",
            "-gen = torch.multinomial(output[0].data.cpu().div(temperature).exp(), 1)[0][0] # FIXME: no multinomial on GPU?"
        ],
        "pos_line": [
            "+output, hidden = model(Variable(input, volatile=True), hidden)",
            "+gen = torch.multinomial(output[0].data.div(temperature).exp().cpu(), 1)[0][0] # FIXME: multinomial is only for CPU"
        ],
        "core_change": "-output, hidden = model(Variable(input, requires_grad=False), hidden) -gen = torch.multinomial(output[0].data.cpu().div(temperature).exp(), 1)[0][0] # FIXME: no multinomial on GPU? +output, hidden = model(Variable(input, volatile=True), hidden) +gen = torch.multinomial(output[0].data.div(temperature).exp().cpu(), 1)[0][0] # FIXME: multinomial is only for CPU",
        "core_API": "multinomial"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "2ec5debbf..6e5b10fa4 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFFunnelForMultipleChoice(TFFunnelPreTrainedModel, TFMultipleChoiceLoss):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2357964)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2357965)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2357966)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2357967)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2357968)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2357969)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2357970)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2357971)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2357972)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2357973)",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 561,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-\"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+\"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), -\"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"), +\"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "0c0ef06780b121b2b50b271852bde88fc998936e",
        "index": "ee4bb7d6..d1574dd6 100644",
        "commit_message": "Prosody-aware Generative Spoken Language Modelling (#3063)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding ÔøΩ\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/3063\n\nReviewed By: eugene-kharitonov\n\nDifferential Revision: D34323605\n\nPulled By: wnhsu\n\nfbshipit-source-id: 9dc779a6c399cda710863596e0880b9277ff2919\n\n",
        "file": "fairseq.txt.json",
        "label": "no",
        "comments": "not clear",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class CodeGenerator(Generator):",
            "x = torch.cat([x, spkr], dim=1)",
            "",
            "for k, feat in kwargs.items():",
            "-            if k in [\"spkr\", \"code\", \"dur_prediction\"]:",
            "+            if k in [\"spkr\", \"code\", \"f0\", \"dur_prediction\"]:",
            "continue",
            "",
            "feat = self._upsample(feat, x.shape[-1])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('string', '\"f0\"'), position=5, insert_id=1355479)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=6, insert_id=1355480)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 563,
        "neg_line": [
            "-if k in [\"spkr\", \"code\", \"dur_prediction\"]:"
        ],
        "pos_line": [
            "+if k in [\"spkr\", \"code\", \"f0\", \"dur_prediction\"]:"
        ],
        "core_change": "-if k in [\"spkr\", \"code\", \"dur_prediction\"]: +if k in [\"spkr\", \"code\", \"f0\", \"dur_prediction\"]:",
        "core_API": "cat"
    },
    {
        "commit_hash": "fa92b2e87dbad23aedfedf7ad3bb00d4d53ef6ea",
        "index": "97e2a7f3c..bc1fbf13a 100644",
        "commit_message": "fixed todo\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TorchHook:",
            "setattr(torch_module, func, new_func)",
            "",
            "torch_modules = syft.torch.torch_modules",
            "-        # torch_modules = {\"torch.nn.functional\": self.torch.nn.functional,",
            "-                         # \"torch\": self.torch}",
            "-        # TODO Replace with syft.torch.torch_modules when hooking 'torch' will not break msgpack",
            "",
            "for module_name, torch_module in torch_modules.items():",
            "for func in dir(torch_module):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 564,
        "neg_line": [
            "-# torch_modules = {\"torch.nn.functional\": self.torch.nn.functional,",
            "-# \"torch\": self.torch}",
            "-# TODO Replace with syft.torch.torch_modules when hooking 'torch' will not break msgpack"
        ],
        "pos_line": [],
        "core_change": "-# torch_modules = {\"torch.nn.functional\": self.torch.nn.functional, -# \"torch\": self.torch} -# TODO Replace with syft.torch.torch_modules when hooking 'torch' will not break msgpack",
        "core_API": "items"
    },
    {
        "commit_hash": "ee950b503eeed5aca3747a4bcf2a40f624b743a0",
        "index": "95b164c8..79f6b61c 100644",
        "commit_message": "fixed trpo\n\n",
        "file": "tensorforce.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PiecewiseConstant(Parameter):",
            "elif self.unit == 'episodes':",
            "step = Module.retrieve_tensor(name='episode')",
            "",
            "-        # step = tf.Print(step, (step,))",
            "-",
            "parameter = tf.train.piecewise_constant(",
            "x=step, boundaries=self.boundaries, values=self.values",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 565,
        "neg_line": [
            "-# step = tf.Print(step, (step,))",
            "-"
        ],
        "pos_line": [],
        "core_change": "-# step = tf.Print(step, (step,)) -",
        "core_API": "retrieve_tensor"
    },
    {
        "commit_hash": "4eb2be0971c94fa3220810909eb0db86c4a13947",
        "index": "eeda83b2..dac5c233 100644",
        "commit_message": "Fix horovod training; import pyarrow without torch\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "change condition check for null fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ModelSaver(Callback):",
            "self.var_collections = var_collections",
            "if checkpoint_dir is None:",
            "checkpoint_dir = logger.get_logger_dir()",
            "-        assert checkpoint_dir is not None",
            "-        if not tf.gfile.IsDirectory(checkpoint_dir):",
            "-            tf.gfile.MakeDirs(checkpoint_dir)",
            "+        if checkpoint_dir is not None:",
            "+            if not tf.gfile.IsDirectory(checkpoint_dir):",
            "+                tf.gfile.MakeDirs(checkpoint_dir)",
            "self.checkpoint_dir = checkpoint_dir",
            "",
            "def _setup_graph(self):",
            "+        assert self.checkpoint_dir is not None, \\",
            "+            \"ModelSaver() doesn't have a valid checkpoint directory.\"",
            "vars = []",
            "for key in self.var_collections:",
            "vars.extend(tf.get_collection(key))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=2290231)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2290232)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2290233)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2290234)",
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=4, insert_id=2290235)",
            "Move(target_node=IN(type=block), node=ASTNode(type=if_statement), position=0)",
            "Insert(target_node=IN(type=block), node=('assert_statement', None), position=0, insert_id=2290236)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=2290237)",
            "Insert(target_node=IN(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=2290238)",
            "Insert(target_node=IN(type=assert_statement), node=(',', ','), position=2, insert_id=2290239)",
            "Insert(target_node=IN(type=assert_statement), node=('string', '\"ModelSaver() doesn\\'t have a valid checkpoint directory.\"'), position=3, insert_id=2290240)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=2290241)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=2290242)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=2290243)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=2290244)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2290245)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2290246)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'checkpoint_dir'), position=2, insert_id=2290247)",
            "Delete(target_node=ASTNode(type=assert, text=assert))",
            "Delete(target_node=ASTNode(type=assert_statement))",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 566,
        "neg_line": [
            "-assert checkpoint_dir is not None",
            "-if not tf.gfile.IsDirectory(checkpoint_dir):",
            "-tf.gfile.MakeDirs(checkpoint_dir)"
        ],
        "pos_line": [
            "+if checkpoint_dir is not None:",
            "+if not tf.gfile.IsDirectory(checkpoint_dir):",
            "+tf.gfile.MakeDirs(checkpoint_dir)",
            "+assert self.checkpoint_dir is not None, \\",
            "+\"ModelSaver() doesn't have a valid checkpoint directory.\""
        ],
        "core_change": "-assert checkpoint_dir is not None -if not tf.gfile.IsDirectory(checkpoint_dir): -tf.gfile.MakeDirs(checkpoint_dir) +if checkpoint_dir is not None: +if not tf.gfile.IsDirectory(checkpoint_dir): +tf.gfile.MakeDirs(checkpoint_dir) +assert self.checkpoint_dir is not None, \\ +\"ModelSaver() doesn't have a valid checkpoint directory.\"",
        "core_API": "get_logger_dir"
    },
    {
        "commit_hash": "0f584faa6b3520efa378f78de4e12a34f039d8f9",
        "index": "b946d23c7..e7cc7c612 100644",
        "commit_message": "PyTorch 1.7 Stable support (#3821)\n\n* prepare for 1.7 support [ci skip]\n\n* tpu [ci skip]\n\n* test run 1.7\n\n* all 1.7, needs to fix tests\n\n* couple with torchvision\n\n* windows try\n\n* remove windows\n\n* 1.7 is here\n\n* on purpose fail [ci skip]\n\n* return [ci skip]\n\n* 1.7 docker\n\n* back to normal [ci skip]\n\n* change to some_val [ci skip]\n\n* add seed [ci skip]\n\n* 4 places [ci skip]\n\n* fail on purpose [ci skip]\n\n* verbose=True [ci skip]\n\n* use filename to track\n\n* use filename to track\n\n* monitor epoch + changelog\n\n* Update tests/checkpointing/test_model_checkpoint.py\n\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def setup_ddp(rank, world_size):",
            "os.environ[\"MASTER_ADDR\"] = 'localhost'",
            "os.environ['MASTER_PORT'] = '8088'",
            "",
            "-    if torch.distributed.is_available():",
            "+    if torch.distributed.is_available() and sys.platform not in ['win32', 'cygwin']:",
            "torch.distributed.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=558840)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=558841)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=558842)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=558843)",
            "Insert(target_node=IN(type=comparison_operator), node=('not in', 'not'), position=1, insert_id=558844)",
            "Insert(target_node=IN(type=comparison_operator), node=('not in', 'in'), position=2, insert_id=558845)",
            "Insert(target_node=IN(type=comparison_operator), node=('list', None), position=3, insert_id=558846)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sys'), position=0, insert_id=558847)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=558848)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'platform'), position=2, insert_id=558849)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=558850)",
            "Insert(target_node=IN(type=list), node=('string', \"'win32'\"), position=1, insert_id=558851)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=558852)",
            "Insert(target_node=IN(type=list), node=('string', \"'cygwin'\"), position=3, insert_id=558853)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=558854)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 569,
        "neg_line": [
            "-if torch.distributed.is_available():"
        ],
        "pos_line": [
            "+if torch.distributed.is_available() and sys.platform not in ['win32', 'cygwin']:"
        ],
        "core_change": "-if torch.distributed.is_available(): +if torch.distributed.is_available() and sys.platform not in ['win32', 'cygwin']:",
        "core_API": "is_available"
    },
    {
        "commit_hash": "87a0ead264ce5fc97997071acb9fe2286d6c425c",
        "index": "a16babd5..a73455b0 100644",
        "commit_message": "fix few jit and cuda errors in color (#767)\n\n\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def luv_to_rgb(image: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:",
            "g: torch.Tensor = torch.where(gs > 0.0031308, 1.055 * torch.pow(gs, 1 / 2.4) - 0.055, 12.92 * gs)",
            "b: torch.Tensor = torch.where(bs > 0.0031308, 1.055 * torch.pow(bs, 1 / 2.4) - 0.055, 12.92 * bs)",
            "",
            "-    rgb_im: torch.Tensor = torch.stack((r, g, b), dim=-3)",
            "+    rgb_im: torch.Tensor = torch.stack([r, g, b], dim=-3)",
            "",
            "return rgb_im"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=1, insert_id=434211)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=434212)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=r), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=g), position=3)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=b), position=5)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=6, insert_id=434213)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 570,
        "neg_line": [
            "-rgb_im: torch.Tensor = torch.stack((r, g, b), dim=-3)"
        ],
        "pos_line": [
            "+rgb_im: torch.Tensor = torch.stack([r, g, b], dim=-3)"
        ],
        "core_change": "-rgb_im: torch.Tensor = torch.stack((r, g, b), dim=-3) +rgb_im: torch.Tensor = torch.stack([r, g, b], dim=-3)",
        "core_API": "where"
    },
    {
        "commit_hash": "5f94900361ed53bf55caee66ad6d7eac5230b573",
        "index": "7fd7a571a..3fc2b54d9 100644",
        "commit_message": "[Feat] Cleanup ModelCheckpoint / EarlyStopping by moving logic to LoggerConnector (#5218)\n\n* [bug-fix] Metric reduction with Logging (#5150)\n\n* add test\n\n* resolve bug\n\n* udpate test\n\n* wrongly copy / paste\n\n* update test\n\n* resolve a second bug\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\n\n* iupdate\n\n* resolve bugs\n\n* add test back\n\n* correct flake8\n\n* resolve flake8\n\n* update on comments\n\n* update tests\n\n* add a test\n\n* add test\n\n* update to Callable\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "change condition check for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelCheckpoint(Callback):",
            "self.best_k_models.pop(del_filepath)",
            "",
            "# do not save nan, replace with +/- inf",
            "-        if torch.isnan(current):",
            "+        if isinstance(current, torch.Tensor) and torch.isnan(current):",
            "current = torch.tensor(float('inf' if self.mode == \"min\" else '-inf'))",
            "",
            "filepath = self._get_metric_interpolated_filepath_name(ckpt_name_metrics, epoch, step, del_filepath)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=550427)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=550428)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=550429)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=550430)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=550431)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=550432)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'current'), position=1, insert_id=550433)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=550434)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=550435)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=550436)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=550437)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=550438)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=550439)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 571,
        "neg_line": [
            "-if torch.isnan(current):"
        ],
        "pos_line": [
            "+if isinstance(current, torch.Tensor) and torch.isnan(current):"
        ],
        "core_change": "-if torch.isnan(current): +if isinstance(current, torch.Tensor) and torch.isnan(current):",
        "core_API": "pop"
    },
    {
        "commit_hash": "9c683ef01e19c4dc1216dcd1ae3c8e7c44d7b2b9",
        "index": "6a441c9fe..e4e5449b1 100644",
        "commit_message": "Add t5 to pipeline(task='summarization') (#3413)\n\n* solve conflicts\n\n* move warnings below\n\n* incorporate changes\n\n* add pad_to_max_length to pipelines\n\n* add bug fix for T5 beam search\n\n* add prefix patterns\n\n* make style\n\n* fix conflicts\n\n* adapt pipelines for task specific parameters\n\n* improve docstring\n\n* remove unused patterns\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFPreTrainedModel(tf.keras.Model, TFModelUtilsMixin):",
            "# set eos token prob to zero if min_length is not reached",
            "if eos_token_id is not None and cur_len < min_length:",
            "# create eos_token_id boolean mask",
            "+                num_batch_hypotheses = batch_size * num_beams",
            "+",
            "is_token_logit_eos_token = tf.convert_to_tensor(",
            "[True if token is eos_token_id else False for token in range(vocab_size)], dtype=tf.bool",
            ")",
            "-                eos_token_indices_mask = tf.broadcast_to(is_token_logit_eos_token, [batch_size, vocab_size])",
            "+                eos_token_indices_mask = tf.broadcast_to(is_token_logit_eos_token, [num_batch_hypotheses, vocab_size])",
            "",
            "scores = set_tensor_by_indices_to_value(scores, eos_token_indices_mask, -float(\"inf\"))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=2382480)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2382481)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2382482)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'num_batch_hypotheses'), position=0, insert_id=2382483)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2382484)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=2382485)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'batch_size'), position=0, insert_id=2382486)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=2382487)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'num_beams'), position=2, insert_id=2382488)",
            "Update(target_node=ASTNode(type=identifier, text=batch_size), value='num_batch_hypotheses')",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 573,
        "neg_line": [
            "-eos_token_indices_mask = tf.broadcast_to(is_token_logit_eos_token, [batch_size, vocab_size])"
        ],
        "pos_line": [
            "+num_batch_hypotheses = batch_size * num_beams",
            "+",
            "+eos_token_indices_mask = tf.broadcast_to(is_token_logit_eos_token, [num_batch_hypotheses, vocab_size])"
        ],
        "core_change": "+num_batch_hypotheses = batch_size * num_beams + -eos_token_indices_mask = tf.broadcast_to(is_token_logit_eos_token, [batch_size, vocab_size]) +eos_token_indices_mask = tf.broadcast_to(is_token_logit_eos_token, [num_batch_hypotheses, vocab_size])",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "ede75e52d17a85a58f96df125295bc59b966863e",
        "index": "9321740..555b118 100644",
        "commit_message": "bug fix for distributed strategy (#1285)\n\n\n",
        "file": "autokeras.txt.json",
        "label": "yes",
        "comments": "remove API call for resource fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Graph(kerastuner.HyperModel, serializable.Serializable):",
            "",
            "def build(self, hp):",
            "\"\"\"Build the HyperModel into a Keras Model.\"\"\"",
            "-        tf.keras.backend.clear_session()",
            "self._register_hps(hp)",
            "self.compile()",
            "real_nodes = {}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=keras))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=backend))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=clear_session))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 574,
        "neg_line": [
            "-tf.keras.backend.clear_session()"
        ],
        "pos_line": [],
        "core_change": "-tf.keras.backend.clear_session()",
        "core_API": "clear_session"
    },
    {
        "commit_hash": "e24d9433b15969ee0bd75ccbb7f3f6b88eca4a41",
        "index": "f1c90d9..2dfccca 100644",
        "commit_message": "Add support for HuggingFace's TensorFlow models (#127)\n\n* added support for for HuggingFace's TensorFlow models\n\n* added notebook for HuggingFace's tensorflow bert model\n\n* change nebullvm name in logs\n\n* Add optimized model details + warning if static shape is used for HF models (#1)\n\n* add optimized model type info\n\n* fix tvm issue\n\n* edit dockerfile and add image auto building\n\n* add docker installation on azure pipeline\n\n* fix bug in neural compressor output shape\n\n* add support for openvino with python 3.10\n\n* add build docker image to azure pipelines\n\n* revert docker build from az pipelines and edit format of the optimization results\n\n* revert docker build from az pipelines\n\n* added tabulate to setup.py and general fixes\n",
        "file": "nebullvm.txt.json",
        "label": "yes",
        "comments": "change param for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TensorflowONNXTensorRTInferenceLearner(",
            "else None",
            ")",
            "out_arrays = self._predict_array(cuda_input_arrays, input_shapes)",
            "-        return tuple(tf.convert_to_tensor(array) for array in out_arrays)",
            "+        return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)",
            "",
            "",
            "class NumpyONNXTensorRTInferenceLearner("
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=1, insert_id=653088)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=array), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=653089)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=653090)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=653091)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 575,
        "neg_line": [
            "-return tuple(tf.convert_to_tensor(array) for array in out_arrays)"
        ],
        "pos_line": [
            "+return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)"
        ],
        "core_change": "-return tuple(tf.convert_to_tensor(array) for array in out_arrays) +return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)",
        "core_API": "_predict_array"
    },
    {
        "commit_hash": "a5c6af3fc4bab3a1dee96860022bf4bee6780c10",
        "index": "9c2cfcdb..63521e70 100644",
        "commit_message": "Fixed typos in AdditiveAttention description.\n\nPiperOrigin-RevId: 381049091\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AdditiveAttention(BaseDenseAttention):",
            "shape `[batch_size, Tv, dim]` and `key` tensor of shape",
            "`[batch_size, Tv, dim]`. The calculation follows the steps:",
            "",
            "-  1. Reshape `query` and `value` into shapes `[batch_size, Tq, 1, dim]`",
            "+  1. Reshape `query` and `key` into shapes `[batch_size, Tq, 1, dim]`",
            "and `[batch_size, 1, Tv, dim]` respectively.",
            "2. Calculate scores with shape `[batch_size, Tq, Tv]` as a non-linear",
            "-     sum: `scores = tf.reduce_sum(tf.tanh(query + value), axis=-1)`",
            "+     sum: `scores = tf.reduce_sum(tf.tanh(query + key), axis=-1)`",
            "3. Use scores to calculate a distribution with shape",
            "`[batch_size, Tq, Tv]`: `distribution = tf.nn.softmax(scores)`.",
            "4. Use `distribution` to create a linear combination of `value` with"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=1, insert_id=2086327)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'shape'), position=0, insert_id=2086328)",
            "Move(target_node=ASTNode(type=with_statement), node=ASTNode(type=ERROR), position=1)",
            "Update(target_node=ASTNode(type=string, text=`scores = tf.reduce_sum(tf.tanh(query + value), axis=-1)`), value='`scores = tf.reduce_sum(tf.tanh(query + key), axis=-1)`')",
            "Move(target_node=ASTNode(type=boolean_operator), node=ASTNode(type=ERROR), position=2)",
            "Move(target_node=ASTNode(type=boolean_operator), node=ASTNode(type=string, text=`[batch_size, Tq, 1, dim]`), position=3)",
            "Insert(target_node=ASTNode(type=ERROR), node=('string', '`key`'), position=0, insert_id=2086329)",
            "Delete(target_node=ASTNode(type=string, text=`value`))",
            "Delete(target_node=ASTNode(type=concatenated_string))",
            "Delete(target_node=ASTNode(type=identifier, text=shape))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 576,
        "neg_line": [
            "-1. Reshape `query` and `value` into shapes `[batch_size, Tq, 1, dim]`",
            "-sum: `scores = tf.reduce_sum(tf.tanh(query + value), axis=-1)`"
        ],
        "pos_line": [
            "+1. Reshape `query` and `key` into shapes `[batch_size, Tq, 1, dim]`",
            "+sum: `scores = tf.reduce_sum(tf.tanh(query + key), axis=-1)`"
        ],
        "core_change": "-1. Reshape `query` and `value` into shapes `[batch_size, Tq, 1, dim]` +1. Reshape `query` and `key` into shapes `[batch_size, Tq, 1, dim]` -sum: `scores = tf.reduce_sum(tf.tanh(query + value), axis=-1)` +sum: `scores = tf.reduce_sum(tf.tanh(query + key), axis=-1)`",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "bc1280332dc4932c35f6d7892bf47c5625a10b79",
        "index": "dffeaee..0e42b18 100644",
        "commit_message": "Some small fixes\n",
        "file": "facenet.txt.json",
        "label": "no",
        "comments": "not clear",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main(args):",
            "update_gradient_vars.append(var)",
            "else:",
            "restore_vars = tf.all_variables()",
            "+            update_gradient_vars = tf.all_variables()",
            "",
            "# Build a Graph that trains the model with one batch of examples and updates the model parameters",
            "train_op = facenet.train(total_loss, global_step, args.optimizer,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1929850)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1929851)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'update_gradient_vars'), position=0, insert_id=1929852)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1929853)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1929854)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1929855)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1929856)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1929857)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1929858)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'all_variables'), position=2, insert_id=1929859)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1929860)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1929861)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 577,
        "neg_line": [],
        "pos_line": [
            "+update_gradient_vars = tf.all_variables()"
        ],
        "core_change": "+update_gradient_vars = tf.all_variables()",
        "core_API": "append"
    },
    {
        "commit_hash": "98d40fed3a4515077163adab9dfd8fb2fccf1267",
        "index": "a3402f9a4..8a5f247e6 100644",
        "commit_message": "Cleanup the usage of `layer_norm_eps` in some models (#21336)\n\n* fix\n\n* fix\n\n* make style\n\n* For CLIP\n\n* For OwlViT\n\n* For XCLIP\n\n* For CLIPSeg\n\n* For GroupViT\n\n* fix docstrings\n\n* fix docstrings\n\n* For AltCLIP\n\n* For ChineseCLIP\n\n* For Blip\n\n* For GiT\n\n* make style\n\n* update\n\n* update\n\n* update\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for math fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class GroupViTVisionTransformer(nn.Module):",
            "",
            "self.embeddings = GroupViTVisionEmbeddings(config)",
            "self.encoder = GroupViTVisionEncoder(config)",
            "-        self.layernorm = nn.LayerNorm(embed_dim)",
            "+        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "",
            "@add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)",
            "@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1532863)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1532864)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=1532865)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1532866)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1532867)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=1532868)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1532869)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'layer_norm_eps'), position=2, insert_id=1532870)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 578,
        "neg_line": [
            "-self.layernorm = nn.LayerNorm(embed_dim)"
        ],
        "pos_line": [
            "+self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)"
        ],
        "core_change": "-self.layernorm = nn.LayerNorm(embed_dim) +self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
        "core_API": "LayerNorm"
    },
    {
        "commit_hash": "c317aaa3de289fc0a69e6b1001ffc586bb4d9c9a",
        "index": "2214f863..bcbf0758 100644",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def configuration():",
            "",
            "",
            "class TestImageClassifierTrainer:",
            "-",
            "def test_fit(self, model, dataloader, criterion, optimizer, scheduler, configuration):",
            "-        trainer = ImageClassifierTrainer(",
            "-            model, dataloader, dataloader, criterion, optimizer, scheduler, configuration,",
            "-        )",
            "+        trainer = ImageClassifierTrainer(model, dataloader, dataloader, criterion, optimizer, scheduler, configuration)",
            "trainer.fit()",
            "",
            "def test_exception(self, model, dataloader, criterion, optimizer, scheduler, configuration):",
            "with pytest.raises(ValueError):",
            "ImageClassifierTrainer(",
            "-                model, dataloader, dataloader, criterion, optimizer, scheduler, configuration,",
            "-                callbacks={'frodo': None},",
            "+                model, dataloader, dataloader, criterion, optimizer, scheduler, configuration, callbacks={'frodo': None}",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 5,
        "AST_diff_line": 2,
        "number": 579,
        "neg_line": [
            "-",
            "-trainer = ImageClassifierTrainer(",
            "-model, dataloader, dataloader, criterion, optimizer, scheduler, configuration,",
            "-)",
            "-model, dataloader, dataloader, criterion, optimizer, scheduler, configuration,",
            "-callbacks={'frodo': None},"
        ],
        "pos_line": [
            "+trainer = ImageClassifierTrainer(model, dataloader, dataloader, criterion, optimizer, scheduler, configuration)",
            "+model, dataloader, dataloader, criterion, optimizer, scheduler, configuration, callbacks={'frodo': None}"
        ],
        "core_change": "- -trainer = ImageClassifierTrainer( -model, dataloader, dataloader, criterion, optimizer, scheduler, configuration, -) +trainer = ImageClassifierTrainer(model, dataloader, dataloader, criterion, optimizer, scheduler, configuration) -model, dataloader, dataloader, criterion, optimizer, scheduler, configuration, -callbacks={'frodo': None}, +model, dataloader, dataloader, criterion, optimizer, scheduler, configuration, callbacks={'frodo': None}",
        "core_API": "fit"
    },
    {
        "commit_hash": "89d8ecff09b426ddc89eb5b432825f8f4c218051",
        "index": "0128419b..4541af18 100644",
        "commit_message": "small fixes\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train_hypernetwork(hypernetwork_name, learn_rate, batch_size, gradient_step,",
            "shared.state.textinfo = f\"\"\"",
            "<p>",
            "Loss: {loss_step:.7f}<br/>",
            "-Step: {hypernetwork.step}<br/>",
            "+Step: {steps_done}<br/>",
            "Last prompt: {html.escape(batch.cond_text[0])}<br/>",
            "Last saved hypernetwork: {html.escape(last_saved_file)}<br/>",
            "Last saved image: {html.escape(last_saved_image)}<br/>"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"\"\"\n<p>\nLoss: {loss_step:.7f}<br/>\nStep: {hypernetwork.step}<br/>\nLast prompt: {html.escape(batch.cond_text[0])}<br/>\nLast saved hypernetwork: {html.escape(last_saved_file)}<br/>\nLast saved image: {html.escape(last_saved_image)}), value='f\"\"\"\\n<p>\\nLoss: {loss_step:.7f}<br/>\\nStep: {steps_done}<br/>\\nLast prompt: {html.escape(batch.cond_text[0])}<br/>\\nLast saved hypernetwork: {html.escape(last_saved_file)}<br/>\\nLast saved image: {html.escape(last_saved_image)}')"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 580,
        "neg_line": [
            "-Step: {hypernetwork.step}<br/>"
        ],
        "pos_line": [
            "+Step: {steps_done}<br/>"
        ],
        "core_change": "-Step: {hypernetwork.step}<br/> +Step: {steps_done}<br/>",
        "core_API": "escape"
    },
    {
        "commit_hash": "b6ad9dbc50597911b0c656c7ea670bccf0eac037",
        "index": "6ab4977..b6d4b1b 100644",
        "commit_message": "Fix hooks test\n\n",
        "file": "seq2seq.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TestTrainSampleHook(tf.test.TestCase):",
            "pred_dict = {}",
            "pred_dict[\"predicted_tokens\"] = tf.constant([[\"Hello\", \"World\", \"Á¨ëw\"]])",
            "pred_dict[\"labels.target_tokens\"] = tf.constant([[\"Hello\", \"World\", \"Á¨ëw\"]])",
            "-    pred_dict[\"labels.target_len\"] = tf.constant([2]),",
            "+    pred_dict[\"labels.target_len\"] = tf.constant(2),",
            "graph_utils.add_dict_to_collection(pred_dict, \"predictions\")",
            "",
            "def tearDown(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=2), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 585,
        "neg_line": [
            "-pred_dict[\"labels.target_len\"] = tf.constant([2]),"
        ],
        "pos_line": [
            "+pred_dict[\"labels.target_len\"] = tf.constant(2),"
        ],
        "core_change": "-pred_dict[\"labels.target_len\"] = tf.constant([2]), +pred_dict[\"labels.target_len\"] = tf.constant(2),",
        "core_API": "constant"
    },
    {
        "commit_hash": "ce02e1801adcca38fc602b2eabbef28b86395c1b",
        "index": "ac6737ed60..a732662e2d 100644",
        "commit_message": "fixing float_power promotion issue in backends and numpy frontend (#10332)\n\nFixed the promotion of the function but as the torch backend only is directed to return float64 and complex128 will add dtype option to the ivy function to govern the type of return we want from the function this will permit to produce proper results in the frontends\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def xlogy(",
            "return torch.xlogy(x, y, out=out)",
            "",
            "",
            "-def real(",
            "-    x: Union[torch.Tensor], /, *, out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def real(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "return torch.real(x)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('ERROR', None), position=1, insert_id=268720)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=2, insert_id=268721)",
            "Move(target_node=ASTNode(type=parameters), node=ASTNode(type=,, text=,), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=return), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=torch), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=., text=.), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=xlogy), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=tuple_pattern), position=4)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=def), position=5)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=real), position=6)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=(, text=(), position=7)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=identifier, text=x), position=0)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=:, text=:), position=1)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=268722)",
            "Move(target_node=IN(type=type), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=Union))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 586,
        "neg_line": [
            "-def real(",
            "-x: Union[torch.Tensor], /, *, out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def real(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "core_change": "-def real( -x: Union[torch.Tensor], /, *, out: Optional[torch.Tensor] = None -) -> torch.Tensor: +def real(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
        "core_API": "xlogy"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "207c30ba..c7d0b932 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "change API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VonMises(TorchDistribution):",
            "\"\"\"",
            "shape = self._extended_shape(sample_shape)",
            "x = torch.empty(shape, dtype=self.loc.dtype, device=self.loc.device)",
            "-        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).byte()",
            "+        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).bool()",
            "while not done.all():",
            "u = torch.rand((3,) + shape, dtype=self.loc.dtype, device=self.loc.device)",
            "u1, u2, u3 = u.unbind()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=byte), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 587,
        "neg_line": [
            "-done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).byte()"
        ],
        "pos_line": [
            "+done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).bool()"
        ],
        "core_change": "-done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).byte() +done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).bool()",
        "core_API": "_extended_shape"
    },
    {
        "commit_hash": "f6c748bce55f5b74eb0a0e864ab11c7db7f4db39",
        "index": "0255a111..a419fb6c 100644",
        "commit_message": "bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "change API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GridTest(TestCase):",
            "assert_equal(adj.to_dense().numpy(), expected_adj)",
            "",
            "def test_grid_with_connectivity_8(self):",
            "-        adj = grid(torch.Size([3, 2]), connectivity=8)",
            "+        adj = grid_3x3(torch.Size([3, 2]), connectivity=8)",
            "",
            "expected_adj = [",
            "[0, 1, 1, 2, 0, 0],"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=grid), value='grid_3x3')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 588,
        "neg_line": [
            "-adj = grid(torch.Size([3, 2]), connectivity=8)"
        ],
        "pos_line": [
            "+adj = grid_3x3(torch.Size([3, 2]), connectivity=8)"
        ],
        "core_change": "-adj = grid(torch.Size([3, 2]), connectivity=8) +adj = grid_3x3(torch.Size([3, 2]), connectivity=8)",
        "core_API": "to_dense"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "f29290c6..cfe3efeb 100755",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "self.last_state = tf.identity(last_state, 'last_state')",
            "",
            "# seqlen x (Bxrnnsize)",
            "-        output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize",
            "+        output = tf.reshape(tf.concat_v2(outputs, 1), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize",
            "logits = FullyConnected('fc', output, param.vocab_size, nl=tf.identity)",
            "self.prob = tf.nn.softmax(logits / param.softmax_temprature)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=integer, text=1), node=ASTNode(type=argument_list), position=3)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 591,
        "neg_line": [
            "-output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize"
        ],
        "pos_line": [
            "+output = tf.reshape(tf.concat_v2(outputs, 1), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize"
        ],
        "core_change": "-output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize +output = tf.reshape(tf.concat_v2(outputs, 1), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize",
        "core_API": "identity"
    },
    {
        "commit_hash": "e01ed1855d292f6fbfe128bda8701eabd151ad73",
        "index": "1d912c35..33818ea6 100644",
        "commit_message": "Logging fix (#661)\n\n* `tf.logging` replaced by: `tl.logging`\n\n* Changelog updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def prefetch_input_data(",
            "for pattern in file_pattern.split(\",\"):",
            "data_files.extend(tf.gfile.Glob(pattern))",
            "if not data_files:",
            "-        tf.logging.fatal(\"Found no input files matching %s\", file_pattern)",
            "+        tl.logging.fatal(\"Found no input files matching %s\", file_pattern)",
            "else:",
            "-        tf.logging.info(\"Prefetching values from %d files matching %s\", len(data_files), file_pattern)",
            "+        tl.logging.info(\"Prefetching values from %d files matching %s\", len(data_files), file_pattern)",
            "",
            "if is_training:",
            "print(\"   is_training == True : RandomShuffleQueue\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tl')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tl')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 594,
        "neg_line": [
            "-tf.logging.fatal(\"Found no input files matching %s\", file_pattern)",
            "-tf.logging.info(\"Prefetching values from %d files matching %s\", len(data_files), file_pattern)"
        ],
        "pos_line": [
            "+tl.logging.fatal(\"Found no input files matching %s\", file_pattern)",
            "+tl.logging.info(\"Prefetching values from %d files matching %s\", len(data_files), file_pattern)"
        ],
        "core_change": "-tf.logging.fatal(\"Found no input files matching %s\", file_pattern) +tl.logging.fatal(\"Found no input files matching %s\", file_pattern) -tf.logging.info(\"Prefetching values from %d files matching %s\", len(data_files), file_pattern) +tl.logging.info(\"Prefetching values from %d files matching %s\", len(data_files), file_pattern)",
        "core_API": "split"
    },
    {
        "commit_hash": "35fe3213ef5b1c566c4d73a9d90b0c2aace04f88",
        "index": "01c613bd..14899818 100644",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def fit_line(points: Tensor, weights: Optional[Tensor] = None) -> ParametrizedLi",
            "A = A.transpose(-2, -1) @ A",
            "",
            "# NOTE: not optimal for 2d points, but for now works for other dimensions",
            "-    _, _, V = torch.linalg.svd(A)",
            "+    _, _, V = _torch_svd_cast(A)",
            "+    V = V.transpose(-2, -1)",
            "",
            "# the first left eigenvector is the direction on the fited line",
            "direction = V[..., 0, :]  # BxD"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=398990)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=398991)",
            "Insert(target_node=ASTNode(type=attribute), node=('ERROR', None), position=1, insert_id=398992)",
            "Update(target_node=ASTNode(type=identifier, text=svd), value='transpose')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=398993)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-2'), position=1, insert_id=398994)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=398995)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-1'), position=3, insert_id=398996)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=398997)",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=A), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=ERROR), position=1)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=2)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'V'), position=0, insert_id=398998)",
            "Insert(target_node=IN(type=ERROR), node=('=', '='), position=1, insert_id=398999)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'V'), position=2, insert_id=399000)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_svd_cast')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=linalg))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 595,
        "neg_line": [
            "-_, _, V = torch.linalg.svd(A)"
        ],
        "pos_line": [
            "+_, _, V = _torch_svd_cast(A)",
            "+V = V.transpose(-2, -1)"
        ],
        "core_change": "-_, _, V = torch.linalg.svd(A) +_, _, V = _torch_svd_cast(A) +V = V.transpose(-2, -1)",
        "core_API": "transpose"
    },
    {
        "commit_hash": "49386e8da49f0c63c467c28456c995ad24cc99fb",
        "index": "03f4f289..4e1320eb 100644",
        "commit_message": "Bug fix when target is a SparseTensor. (#4200)\n\n* Bug fix when target is a SparseTensor.\nCheck for sparsity when creating target placeholder.\nRemove shape argument when creating sparse placeholder.\n\n* Fixed ndim behavior for sparse tensor\n\n* Fix sparse variable instantiation.\n\n* Bug fix\n\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "change API call for shape fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def ndim(x):",
            "'''Returns the number of axes in a tensor, as an integer.",
            "'''",
            "if is_sparse(x):",
            "-        return int(x.shape.get_shape()[0])",
            "+        return x._dims",
            "",
            "dims = x.get_shape()._dims",
            "if dims is not None:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=attribute), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=shape), value='_dims')",
            "Delete(target_node=ASTNode(type=identifier, text=int))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_shape))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 596,
        "neg_line": [
            "-return int(x.shape.get_shape()[0])"
        ],
        "pos_line": [
            "+return x._dims"
        ],
        "core_change": "-return int(x.shape.get_shape()[0]) +return x._dims",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "57d02eec2e369d659c73b758586306a30e687cd8",
        "index": "13e947dc5f..d81e600ab4 100644",
        "commit_message": "[Ray Dataset] fix the type infer of `pd.dataframe` (when dtype is `object`.)  (#25563)\n\nthis is a temp fix of #25556. When the dtype from the pandas dataframe gives object, we set the dtype to be None and make use of the auto-inferring of the type in the conversion.\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add condition check for null fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def convert_pandas_to_tf_tensor(",
            "# them. If the columns contain different types (for example, `float32`s",
            "# and `int32`s), then `tf.concat` raises an error.",
            "dtype: np.dtype = np.find_common_type(df.dtypes, [])",
            "+",
            "+            # if the columns are `ray.data.extensions.tensor_extension.TensorArray`,",
            "+            # the dtype will be `object`. In this case, we need to set the dtype to",
            "+            # none, and use the automatic type casting of `tf.convert_to_tensor`.",
            "+            if isinstance(dtype, object):",
            "+                dtype = None",
            "+",
            "except TypeError:",
            "# `find_common_type` fails if a series has `TensorDtype`. In this case,",
            "# don't cast any of the series and continue."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=dtype), position=3)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=:, text=:), position=4)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=type), position=5)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type==, text==), position=6)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=call), position=7)",
            "Insert(target_node=ASTNode(type=ERROR), node=('if', 'if'), position=8, insert_id=2136290)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=9, insert_id=2136291)",
            "Insert(target_node=IN(type=ERROR), node=('call', None), position=0, insert_id=2136292)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=1, insert_id=2136293)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'dtype'), position=2, insert_id=2136294)",
            "Insert(target_node=IN(type=ERROR), node=('=', '='), position=3, insert_id=2136295)",
            "Insert(target_node=IN(type=ERROR), node=('none', 'None'), position=4, insert_id=2136296)",
            "Insert(target_node=IN(type=ERROR), node=('except', 'except'), position=5, insert_id=2136297)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=2136298)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2136299)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2136300)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dtype'), position=1, insert_id=2136301)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2136302)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'object'), position=3, insert_id=2136303)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2136304)",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=identifier, text=except))"
        ],
        "plus_line": 5,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 599,
        "neg_line": [],
        "pos_line": [
            "+",
            "+# if the columns are `ray.data.extensions.tensor_extension.TensorArray`,",
            "+# the dtype will be `object`. In this case, we need to set the dtype to",
            "+# none, and use the automatic type casting of `tf.convert_to_tensor`.",
            "+if isinstance(dtype, object):",
            "+dtype = None",
            "+"
        ],
        "core_change": "+ +# if the columns are `ray.data.extensions.tensor_extension.TensorArray`, +# the dtype will be `object`. In this case, we need to set the dtype to +# none, and use the automatic type casting of `tf.convert_to_tensor`. +if isinstance(dtype, object): +dtype = None +",
        "core_API": "find_common_type"
    },
    {
        "commit_hash": "13b2782b2cb090624781d31f245fbedb3d5a12e9",
        "index": "71e13e329..8ec73cdd9 100644",
        "commit_message": "fix: add missing symbolink of data_prep and reset GroupNorm initialization\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def initialize(model: torch.nn.Module, init: str):",
            "",
            "# reset some modules with default init",
            "for m in model.modules():",
            "-            if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm)):",
            "+            if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm, torch.nn.GroupNorm)):",
            "m.reset_parameters()",
            "if hasattr(m, \"espnet_initialization_fn\"):",
            "m.espnet_initialization_fn()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=(',', ','), position=4, insert_id=133339)",
            "Insert(target_node=ASTNode(type=tuple), node=('attribute', None), position=5, insert_id=133340)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=133341)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=133342)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'GroupNorm'), position=2, insert_id=133343)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=133344)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=133345)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=133346)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 600,
        "neg_line": [
            "-if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm)):"
        ],
        "pos_line": [
            "+if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm, torch.nn.GroupNorm)):"
        ],
        "core_change": "-if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm)): +if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm, torch.nn.GroupNorm)):",
        "core_API": "modules"
    },
    {
        "commit_hash": "4b99af0a44eabee91ec58f2ec4acd76d50fa8a55",
        "index": "3f0694ff..95999f99 100755",
        "commit_message": "Change rnn-cell to fix #103 (#104)\n\n* Change rnn-cell to fix #103\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "input_list = tf.unstack(input_feature, axis=1)  # seqlen x (Bxrnnsize)",
            "",
            "# seqlen is 1 in inference. don't need loop_function",
            "-        outputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')",
            "+        outputs, last_state = tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')",
            "self.last_state = tf.identity(last_state, 'last_state')",
            "",
            "# seqlen x (Bxrnnsize)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2307594)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2307595)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'static_rnn'), position=2, insert_id=2307596)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=rnn), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=nn), value='contrib')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 601,
        "neg_line": [
            "-outputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')"
        ],
        "pos_line": [
            "+outputs, last_state = tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')"
        ],
        "core_change": "-outputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm') +outputs, last_state = tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')",
        "core_API": "unstack"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "3670d56c..e8377e0f 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NoamLR(LearningRateScheduler):",
            "else:",
            "self.last_epoch = batch_num_total",
            "for param_group, learning_rate in zip(self.optimizer.param_groups, self.get_values()):",
            "-            param_group['lr'] = learning_rate",
            "+            param_group[\"lr\"] = learning_rate",
            "",
            "def get_values(self):",
            "step = max(self.last_epoch, 1)",
            "-        scale = self.factor * (self.model_size ** (-0.5) *",
            "-                               min(step ** (-0.5), step * self.warmup_steps ** (-1.5)))",
            "+        scale = self.factor * (",
            "+            self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup_steps ** (-1.5))",
            "+        )",
            "",
            "return [scale for _ in range(len(self.base_values))]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='lr'), value='\"lr\"')"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 605,
        "neg_line": [
            "-param_group['lr'] = learning_rate",
            "-scale = self.factor * (self.model_size ** (-0.5) *",
            "-min(step ** (-0.5), step * self.warmup_steps ** (-1.5)))"
        ],
        "pos_line": [
            "+param_group[\"lr\"] = learning_rate",
            "+scale = self.factor * (",
            "+self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup_steps ** (-1.5))",
            "+)"
        ],
        "core_change": "-param_group['lr'] = learning_rate +param_group[\"lr\"] = learning_rate -scale = self.factor * (self.model_size ** (-0.5) * -min(step ** (-0.5), step * self.warmup_steps ** (-1.5))) +scale = self.factor * ( +self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup_steps ** (-1.5)) +)",
        "core_API": "get_values"
    },
    {
        "commit_hash": "236f4359669813572a869cb07d83f9e180bb8552",
        "index": "7deed470d..426908c8c 100644",
        "commit_message": "bugfix VirtualWorker\n\n",
        "file": "PySyft.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def tests_worker_convenience_methods():",
            "\"\"\"",
            "",
            "me = sy.torch.hook.local_worker",
            "-    bob = VirtualWorker()",
            "-    alice = VirtualWorker()",
            "+    bob = VirtualWorker(sy.torch.hook)",
            "+    alice = VirtualWorker(sy.torch.hook)",
            "obj = torch.Tensor([100, 100])",
            "",
            "# Send data to alice"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=841309)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=841310)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=841311)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=841312)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hook'), position=2, insert_id=841313)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=841314)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=841315)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hook'), position=2, insert_id=841316)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sy'), position=0, insert_id=841317)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=841318)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=2, insert_id=841319)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sy'), position=0, insert_id=841320)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=841321)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=2, insert_id=841322)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 606,
        "neg_line": [
            "-bob = VirtualWorker()",
            "-alice = VirtualWorker()"
        ],
        "pos_line": [
            "+bob = VirtualWorker(sy.torch.hook)",
            "+alice = VirtualWorker(sy.torch.hook)"
        ],
        "core_change": "-bob = VirtualWorker() -alice = VirtualWorker() +bob = VirtualWorker(sy.torch.hook) +alice = VirtualWorker(sy.torch.hook)",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "d747fa38c703708c8092623bab6b910ff93fc0fb",
        "index": "e2e6ccc66..525d5cac0 100644",
        "commit_message": "fix multi-singer duration predictor\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "refactor",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DurationPredictor(torch.nn.Module):",
            "self.norm_2 = LayerNorm(filter_channels, dim=1)",
            "self.proj = torch.nn.Conv1d(filter_channels, 1, 1)",
            "",
            "-        if gin_channels != 0:",
            "-            self.cond = torch.nn.Conv1d(gin_channels, channels, 1)",
            "+        if global_channels > 0:",
            "+            self.cond = torch.nn.Conv1d(global_channels, channels, 1)",
            "",
            "def forward(self, x, x_mask, beat_lab, g=None):",
            "x = torch.detach(x)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=gin_channels), value='global_channels')",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('>', '>'), position=1, insert_id=118874)",
            "Update(target_node=ASTNode(type=identifier, text=gin_channels), value='global_channels')",
            "Delete(target_node=ASTNode(type=!=, text=!=))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 609,
        "neg_line": [
            "-if gin_channels != 0:",
            "-self.cond = torch.nn.Conv1d(gin_channels, channels, 1)"
        ],
        "pos_line": [
            "+if global_channels > 0:",
            "+self.cond = torch.nn.Conv1d(global_channels, channels, 1)"
        ],
        "core_change": "-if gin_channels != 0: -self.cond = torch.nn.Conv1d(gin_channels, channels, 1) +if global_channels > 0: +self.cond = torch.nn.Conv1d(global_channels, channels, 1)",
        "core_API": "Conv1d"
    },
    {
        "commit_hash": "93dbc1ce18613747fa981918d277f5bb843f752b",
        "index": "c98f798766..6af4bc955b 100644",
        "commit_message": "fix dtype, device in `sum`, `prod`, `to_dev` (#1358)\n\n* fix dtype, device in `sum`, `prod`, `to_dev`\n\n* make `device` have `None` as default\n\n* add `dtype = ivy.as_native_dtype(dtype)` and make `copy` positional\n\n* `to_dev` conform to array API\n\n* `astype` fixes to signature\n\n* black\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def prod(",
            "elif x.dtype == torch.bfloat16:",
            "dtype = torch.float16",
            "",
            "+    dtype = ivy.as_native_dtype(dtype)",
            "+",
            "if axis is None:",
            "axis = x.dim() - 1",
            "elif type(axis) == tuple:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('call', None), position=8, insert_id=351816)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=351817)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=351818)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=351819)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=351820)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_native_dtype'), position=3, insert_id=351821)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=351822)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dtype'), position=1, insert_id=351823)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=351824)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'dtype'), position=0, insert_id=351825)",
            "Insert(target_node=IN(type=ERROR), node=('=', '='), position=1, insert_id=351826)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'ivy'), position=2, insert_id=351827)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 610,
        "neg_line": [],
        "pos_line": [
            "+dtype = ivy.as_native_dtype(dtype)",
            "+"
        ],
        "core_change": "+dtype = ivy.as_native_dtype(dtype) +",
        "core_API": "as_native_dtype"
    },
    {
        "commit_hash": "72ad58d893fd74ac98bf21269c766a02ccb44bf7",
        "index": "98fc70ae..d5836a9f 100644",
        "commit_message": "change the bitwise for masking and small fixes\n\n",
        "file": "TTS.txt.json",
        "label": "yes",
        "comments": "remove API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Attention(nn.Module):",
            "query, processed_inputs)",
            "# apply masking",
            "if mask is not None:",
            "-            attention.data.masked_fill_(torch.bitwise_not(mask), self._mask_value)",
            "+            attention.data.masked_fill_(~mask, self._mask_value)",
            "# apply windowing - only in eval mode",
            "if not self.training and self.windowing:",
            "attention = self.apply_windowing(attention, inputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('unary_operator', None), position=1, insert_id=1272430)",
            "Insert(target_node=IN(type=unary_operator), node=('~', '~'), position=0, insert_id=1272431)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=identifier, text=mask), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=bitwise_not))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 611,
        "neg_line": [
            "-attention.data.masked_fill_(torch.bitwise_not(mask), self._mask_value)"
        ],
        "pos_line": [
            "+attention.data.masked_fill_(~mask, self._mask_value)"
        ],
        "core_change": "-attention.data.masked_fill_(torch.bitwise_not(mask), self._mask_value) +attention.data.masked_fill_(~mask, self._mask_value)",
        "core_API": "masked_fill_"
    },
    {
        "commit_hash": "2303f9ced81e8f9e1c8ec62aaa03a78e9e5efaa3",
        "index": "e40bb7180..b6bff43fd 100644",
        "commit_message": "Fix(Early Stopping): move best score to device (#7959)\n\n\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EarlyStopping(Callback):",
            "f\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.divergence_threshold}.\"",
            "\" Signaling Trainer to stop.\"",
            ")",
            "-        elif self.monitor_op(current - self.min_delta, self.best_score):",
            "+        elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):",
            "should_stop = False",
            "reason = self._improvement_message(current)",
            "self.best_score = current"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=532058)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=532059)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=532060)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=532061)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=532062)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=532063)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=532064)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=532065)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=532066)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=532067)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=532068)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'trainer'), position=0, insert_id=532069)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=532070)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'lightning_module'), position=2, insert_id=532071)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 613,
        "neg_line": [
            "-elif self.monitor_op(current - self.min_delta, self.best_score):"
        ],
        "pos_line": [
            "+elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):"
        ],
        "core_change": "-elif self.monitor_op(current - self.min_delta, self.best_score): +elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):",
        "core_API": "monitor_op"
    },
    {
        "commit_hash": "78bb26286e2a8486bf8b1e4e68c4ce0de9ad002a",
        "index": "b3fb557b8a..18f141d095 100644",
        "commit_message": "Replaced discontinued rnn_cell.BasicLSTMCell with rnn_cell.LSTMCell (#4703)\n\n* Fixed bug in Dirichlet (#4440)\n\n* Replaced deprecated rnn_cell.BasicLSTMCell with rnn_cell.LSTMCell\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LSTM(Model):",
            "last_layer = add_time_dimension(features, self.seq_lens)",
            "",
            "# Setup the LSTM cell",
            "-        lstm = rnn.BasicLSTMCell(cell_size, state_is_tuple=True)",
            "+        lstm = tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True)",
            "self.state_init = [",
            "np.zeros(lstm.state_size.c, np.float32),",
            "np.zeros(lstm.state_size.h, np.float32)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2150070)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2150071)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'LSTMCell'), position=2, insert_id=2150072)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2150073)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2150074)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'rnn_cell'), position=2, insert_id=2150075)",
            "Update(target_node=ASTNode(type=identifier, text=rnn), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=rnn), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=BasicLSTMCell), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=BasicLSTMCell), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 614,
        "neg_line": [
            "-lstm = rnn.BasicLSTMCell(cell_size, state_is_tuple=True)"
        ],
        "pos_line": [
            "+lstm = tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True)"
        ],
        "core_change": "-lstm = rnn.BasicLSTMCell(cell_size, state_is_tuple=True) +lstm = tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True)",
        "core_API": "BasicLSTMCell"
    },
    {
        "commit_hash": "28d0048218ad7bce69510b16024510afba0daed2",
        "index": "4047958d4..fa2a8c6eb 100755",
        "commit_message": "Fx support for multiple model architectures (#17393)\n\n* Support for Bart and LayoutLM, and partial support for XLNet\n\n* Support for mbart\n\n* A lot of new models supported\n\n* Support for other models\n\n* LayoutLM fix\n\n* Use strings instead of classes\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class XGLMModel(XGLMPreTrainedModel):",
            "",
            "hidden_states = inputs_embeds + positions",
            "",
            "-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)",
            "+        hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)",
            "",
            "# decoder layers",
            "all_hidden_states = () if output_hidden_states else None"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=1199029)",
            "Insert(target_node=IN(type=call), node=('identifier', 'float'), position=0, insert_id=1199030)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1199031)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1199032)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1199033)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 615,
        "neg_line": [
            "-hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)"
        ],
        "pos_line": [
            "+hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)"
        ],
        "core_change": "-hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training) +hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)",
        "core_API": "dropout"
    },
    {
        "commit_hash": "2c6bc0f13ba2ba609ac141022b4b56b677d74943",
        "index": "4eddc271..f8901e28 100644",
        "commit_message": "small fix\n\n",
        "file": "diffusers.txt.json",
        "label": "no",
        "comments": "print",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VersatileDiffusionImageVariationPipelineIntegrationTests(unittest.TestCase",
            "image_slice = image[0, 253:256, 253:256, -1]",
            "",
            "assert image.shape == (1, 512, 512, 3)",
            "+        print(torch.from_numpy(image_slice.flatten()))",
            "expected_slice = np.array([0.0113, 0.2241, 0.4024, 0.0839, 0.0871, 0.2725, 0.2581, 0.0, 0.1096])",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=97486)",
            "Move(target_node=IN(type=call), node=ASTNode(type=tuple), position=0)",
            "Insert(target_node=IN(type=call), node=('ERROR', None), position=1, insert_id=97487)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=2, insert_id=97488)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'print'), position=0, insert_id=97489)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=97490)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=97491)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=97492)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=97493)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=97494)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=97495)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=97496)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'from_numpy'), position=2, insert_id=97497)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=97498)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=97499)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=97500)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=97501)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=97502)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'image_slice'), position=0, insert_id=97503)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=97504)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flatten'), position=2, insert_id=97505)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=97506)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=97507)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 23,
        "number": 616,
        "neg_line": [],
        "pos_line": [
            "+print(torch.from_numpy(image_slice.flatten()))"
        ],
        "core_change": "+print(torch.from_numpy(image_slice.flatten()))",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "cc8aac66..1722ac1a 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CategoricalAccuracy(Metric):",
            "correct.unsqueeze_(-1)",
            "",
            "if mask is not None:",
            "-            correct *= mask.view(-1, 1).float()",
            "+            correct *= mask.view(-1, 1)",
            "self.total_count += mask.sum()",
            "else:",
            "self.total_count += gold_labels.numel()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 618,
        "neg_line": [
            "-correct *= mask.view(-1, 1).float()"
        ],
        "pos_line": [
            "+correct *= mask.view(-1, 1)"
        ],
        "core_change": "-correct *= mask.view(-1, 1).float() +correct *= mask.view(-1, 1)",
        "core_API": "unsqueeze_"
    },
    {
        "commit_hash": "679060f96f83a4815fac5e43231c5c319d9f477b",
        "index": "71b921a..c4015b2 100644",
        "commit_message": "fix unconditional after simplification of text mask\n\n",
        "file": "imagen-pytorch.txt.json",
        "label": "yes",
        "comments": "add condition check for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Imagen(nn.Module):",
            "text_embeds, text_masks = t5_encode_text(texts, name = self.text_encoder_name, return_attn_mask = True)",
            "text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))",
            "",
            "-        text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))",
            "+        if not self.unconditional:",
            "+            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))",
            "",
            "assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'",
            "assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=253559)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=253560)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=253561)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=253562)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=253563)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=253564)",
            "Insert(target_node=IN(type=not_operator), node=('attribute', None), position=1, insert_id=253565)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=253566)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=253567)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'unconditional'), position=2, insert_id=253568)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 619,
        "neg_line": [
            "-text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))"
        ],
        "pos_line": [
            "+if not self.unconditional:",
            "+text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))"
        ],
        "core_change": "-text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1)) +if not self.unconditional: +text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))",
        "core_API": "to"
    },
    {
        "commit_hash": "ebd0bc17929b36b851fc825bbfd5fbd41f6d8111",
        "index": "4309ec96..582cd28f 100644",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "change API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def conditional(",
            "if f_scale_tril is not None:",
            "pack = torch.cat((pack, f_scale_tril_2D), dim=1)",
            "",
            "-        Lffinv_pack = pack.triangular_solve(Lff, upper=False)[0]",
            "+        Lffinv_pack = torch.linalg.solve_triangular(Lff, pack, upper=False)",
            "# unpack",
            "v_2D = Lffinv_pack[:, : f_loc_2D.size(1)]",
            "W = Lffinv_pack[:, f_loc_2D.size(1) : f_loc_2D.size(1) + M].t()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=671980)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=671981)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'solve_triangular'), position=2, insert_id=671982)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'pack'), position=3, insert_id=671983)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=671984)",
            "Update(target_node=ASTNode(type=identifier, text=triangular_solve), value='linalg')",
            "Update(target_node=ASTNode(type=identifier, text=pack), value='torch')",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 621,
        "neg_line": [
            "-Lffinv_pack = pack.triangular_solve(Lff, upper=False)[0]"
        ],
        "pos_line": [
            "+Lffinv_pack = torch.linalg.solve_triangular(Lff, pack, upper=False)"
        ],
        "core_change": "-Lffinv_pack = pack.triangular_solve(Lff, upper=False)[0] +Lffinv_pack = torch.linalg.solve_triangular(Lff, pack, upper=False)",
        "core_API": "cat"
    },
    {
        "commit_hash": "592ca5055c38cac37aee86bd62fef977f1900f62",
        "index": "50c86244..79b1cec6 100644",
        "commit_message": "Test fixtures (#4241)\n\n* fix test\n\n* typo\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* skip tests\n\n* run cron only in master repo\n\n* fix test\n\n* update\n\n* Add test\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "remove try catch test",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_hgt_loader_on_cora():",
            "out2 = hetero_model(hetero_batch.x_dict, hetero_batch.edge_index_dict,",
            "hetero_batch.edge_weight_dict)['paper'][:batch_size]",
            "assert torch.allclose(out1, out2, atol=1e-6)",
            "-",
            "-    try:",
            "-        shutil.rmtree(root)",
            "-    except PermissionError:",
            "-        pass"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=try, text=try))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=shutil))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=rmtree))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=root))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=except, text=except))",
            "Delete(target_node=ASTNode(type=identifier, text=PermissionError))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=pass, text=pass))",
            "Delete(target_node=ASTNode(type=pass_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=except_clause))",
            "Delete(target_node=ASTNode(type=try_statement))"
        ],
        "plus_line": 0,
        "minus_line": 4,
        "AST_diff_line": 21,
        "number": 622,
        "neg_line": [
            "-",
            "-try:",
            "-shutil.rmtree(root)",
            "-except PermissionError:",
            "-pass"
        ],
        "pos_line": [],
        "core_change": "- -try: -shutil.rmtree(root) -except PermissionError: -pass",
        "core_API": "allclose"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "a5add8b0..e185735e 100755",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "wrong = prediction_incorrect(logits, label, 5, name='wrong-top5')",
            "add_moving_summary(tf.reduce_mean(wrong, name='train-error-top5'))",
            "",
            "-        wd_cost = tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')",
            "+        wd_cost = regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss')",
            "add_moving_summary(loss, wd_cost)",
            "self.cost = tf.add_n([loss, wd_cost], name='cost')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='regularize_cost')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tf), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', \"'.*/W'\"), position=1, insert_id=2307906)",
            "Update(target_node=ASTNode(type=identifier, text=regularize_cost), value='l2_regularizer')",
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '1e-4'), position=1, insert_id=2307907)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mul))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=float, text=1e-4))",
            "Delete(target_node=ASTNode(type=string, text='.*/W'))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=l2_loss))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 623,
        "neg_line": [
            "-wd_cost = tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')"
        ],
        "pos_line": [
            "+wd_cost = regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss')"
        ],
        "core_change": "-wd_cost = tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss') +wd_cost = regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss')",
        "core_API": "reduce_mean"
    },
    {
        "commit_hash": "7ea781c9a030fb2d49574d8d324e562e58dd49fb",
        "index": "5a480b113..cbddaecdb 100644",
        "commit_message": "CI fix3\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StochasticDurationPredictor(torch.nn.Module):",
            "z, logdet = flow(z, x_mask, g=x, inverse=inverse)",
            "logdet_tot = logdet_tot + logdet",
            "nll = (",
            "-                torch.sum(0.5 * (math.log(2 * math.pi) + (z**2)) * x_mask, [1, 2])",
            "+                torch.sum(0.5 * (math.log(2 * math.pi) + (z ** 2)) * x_mask, [1, 2])",
            "- logdet_tot",
            ")",
            "return nll + logq  # (B,)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=logdet_tot))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 625,
        "neg_line": [
            "-torch.sum(0.5 * (math.log(2 * math.pi) + (z**2)) * x_mask, [1, 2])",
            "-logdet_tot"
        ],
        "pos_line": [
            "+torch.sum(0.5 * (math.log(2 * math.pi) + (z ** 2)) * x_mask, [1, 2])"
        ],
        "core_change": "-torch.sum(0.5 * (math.log(2 * math.pi) + (z**2)) * x_mask, [1, 2]) +torch.sum(0.5 * (math.log(2 * math.pi) + (z ** 2)) * x_mask, [1, 2]) -logdet_tot",
        "core_API": "sum"
    },
    {
        "commit_hash": "8eee4fa9e133fe873a7993ba746d32ca2b687551",
        "index": "aad870ce4..94b3eb145 100644",
        "commit_message": "Add metrics usage examples and tests (#1820)\n\n* add metrics usage examples and tests\n\n* update template\n\n* remove instruction sentence\n\n* add metrics dependencies to the tests requirements\n\n* try fix pip install timeout\n\n* try again\n\n* try again\n\n* try again\n\n* try again by moving unbabel-comet\n\n* try again by ignoring fixed deps of comet\n\n* fix some comet deps\n\n* move deps\n\n* try again\n\n* download wordnet for meteor\n\n* style\n\n* don't test comet on windows\n\n* style\n\n* remove comet comment\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "annotation",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "CHECKPOINT_URLS = {",
            "}",
            "",
            "",
            "+@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)",
            "class BLEURT(datasets.Metric):",
            "def _info(self):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('decorated_definition', None), position=1, insert_id=1787487)",
            "Insert(target_node=IN(type=decorated_definition), node=('decorator', None), position=0, insert_id=1787488)",
            "Move(target_node=IN(type=decorated_definition), node=ASTNode(type=class_definition), position=1)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=1787489)",
            "Insert(target_node=IN(type=decorator), node=('call', None), position=1, insert_id=1787490)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1787491)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1787492)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1787493)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1787494)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'add_start_docstrings'), position=2, insert_id=1787495)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1787496)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', '_DESCRIPTION'), position=1, insert_id=1787497)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1787498)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', '_KWARGS_DESCRIPTION'), position=3, insert_id=1787499)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1787500)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1787501)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1787502)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'file_utils'), position=2, insert_id=1787503)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=0, insert_id=1787504)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1787505)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'utils'), position=2, insert_id=1787506)"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 626,
        "neg_line": [],
        "pos_line": [
            "+@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)"
        ],
        "core_change": "+@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)",
        "core_API": "add_start_docstrings"
    },
    {
        "commit_hash": "170e189e6a3da1da213644a6052fd4f6365ef270",
        "index": "de612fc..6ed88cb 100755",
        "commit_message": "Fix nccl package location on newer TF versions.\n\n",
        "file": "stylegan.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Optimizer:",
            "g = [dev_grads[dev][var_idx][0] for dev in devices]",
            "",
            "if np.prod(grad_shape):  # nccl does not support zero-sized tensors",
            "-                            g = tf.contrib.nccl.all_sum(g)",
            "+                            g = nccl_ops.all_sum(g)",
            "",
            "for dev, gg in zip(devices, g):",
            "dev_grads[dev][var_idx] = (gg, dev_grads[dev][var_idx][1])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='nccl_ops')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=contrib))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=nccl))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 627,
        "neg_line": [
            "-g = tf.contrib.nccl.all_sum(g)"
        ],
        "pos_line": [
            "+g = nccl_ops.all_sum(g)"
        ],
        "core_change": "-g = tf.contrib.nccl.all_sum(g) +g = nccl_ops.all_sum(g)",
        "core_API": "prod"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "77224c699..86905ac1b 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "doc",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .modeling_utils import PoolerAnswerClass, PoolerEndLogits, PoolerStartLogit",
            "logger = logging.getLogger(__name__)",
            "",
            "XLNET_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"xlnet-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin\",",
            "-    \"xlnet-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-pytorch_model.bin\",",
            "+    \"xlnet-base-cased\": \"https://cdn.huggingface.co/xlnet-base-cased-pytorch_model.bin\",",
            "+    \"xlnet-large-cased\": \"https://cdn.huggingface.co/xlnet-large-cased-pytorch_model.bin\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=dictionary), position=2)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689805)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689806)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/xlnet-base-cased-pytorch_model.bin\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/xlnet-large-cased-pytorch_model.bin\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 631,
        "neg_line": [
            "-\"xlnet-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin\",",
            "-\"xlnet-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-pytorch_model.bin\","
        ],
        "pos_line": [
            "+\"xlnet-base-cased\": \"https://cdn.huggingface.co/xlnet-base-cased-pytorch_model.bin\",",
            "+\"xlnet-large-cased\": \"https://cdn.huggingface.co/xlnet-large-cased-pytorch_model.bin\","
        ],
        "core_change": "-\"xlnet-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin\", -\"xlnet-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-pytorch_model.bin\", +\"xlnet-base-cased\": \"https://cdn.huggingface.co/xlnet-base-cased-pytorch_model.bin\", +\"xlnet-large-cased\": \"https://cdn.huggingface.co/xlnet-large-cased-pytorch_model.bin\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "f8fa6062993c71b4b53f8c7d7ae16ebbba709101",
        "index": "244a62966..8f1b22028 100644",
        "commit_message": "fix\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(torch.nn.Module, ScorerInterface):",
            "",
            "if self.labeldist is not None:",
            "if self.vlabeldist is None:",
            "-                self.vlabeldist = to_device(hs_pad, torch.from_numpy(self.labeldist))",
            "+                self.vlabeldist = to_device(hs_pad[0], torch.from_numpy(self.labeldist))",
            "loss_reg = -torch.sum(",
            "(F.log_softmax(y_all, dim=1) * self.vlabeldist).view(-1), dim=0",
            ") / len(ys_in)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=1, insert_id=147935)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=hs_pad), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=147936)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=147937)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=147938)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 633,
        "neg_line": [
            "-self.vlabeldist = to_device(hs_pad, torch.from_numpy(self.labeldist))"
        ],
        "pos_line": [
            "+self.vlabeldist = to_device(hs_pad[0], torch.from_numpy(self.labeldist))"
        ],
        "core_change": "-self.vlabeldist = to_device(hs_pad, torch.from_numpy(self.labeldist)) +self.vlabeldist = to_device(hs_pad[0], torch.from_numpy(self.labeldist))",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "3d63fe18c5016baedaf0170ba339f2412e4cc4f2",
        "index": "7b6c231..4a1d20e 100644",
        "commit_message": "Fix Transformer init_weights\n\nWeights of decoders are initialized twice. I think we need to call the zeros_() to initialize bias.\n",
        "file": "examples.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TransformerModel(nn.Module):",
            "def init_weights(self):",
            "initrange = 0.1",
            "nn.init.uniform_(self.encoder.weight, -initrange, initrange)",
            "-        nn.init.zeros_(self.decoder.weight)",
            "+        nn.init.zeros_(self.decoder.bias)",
            "nn.init.uniform_(self.decoder.weight, -initrange, initrange)",
            "",
            "def forward(self, src, has_mask=True):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=weight), value='bias')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 635,
        "neg_line": [
            "-nn.init.zeros_(self.decoder.weight)"
        ],
        "pos_line": [
            "+nn.init.zeros_(self.decoder.bias)"
        ],
        "core_change": "-nn.init.zeros_(self.decoder.weight) +nn.init.zeros_(self.decoder.bias)",
        "core_API": "uniform_"
    },
    {
        "commit_hash": "8cbe7d6af84f682102c2aaa7161c2d3118c59d98",
        "index": "0d9e6f869..c707a1cd2 100644",
        "commit_message": "FIX errors in loading eval Dataset in `run_squad_pytorch`\n\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "",
            "model.eval()",
            "all_results = []",
            "-        for input_ids, input_mask, segment_ids, label_ids, example_index in eval_dataloader:",
            "+        #for input_ids, input_mask, segment_ids, label_ids, example_index in eval_dataloader:",
            "+        for input_ids, input_mask, segment_ids, example_index in eval_dataloader:",
            "if len(all_results) % 1000 == 0:",
            "logger.info(\"Processing example: %d\" % (len(all_results)))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=pattern_list), position=2)",
            "Insert(target_node=ASTNode(type=pattern_list), node=(',', ','), position=1, insert_id=1252185)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=label_ids))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 636,
        "neg_line": [
            "-for input_ids, input_mask, segment_ids, label_ids, example_index in eval_dataloader:"
        ],
        "pos_line": [
            "+#for input_ids, input_mask, segment_ids, label_ids, example_index in eval_dataloader:",
            "+for input_ids, input_mask, segment_ids, example_index in eval_dataloader:"
        ],
        "core_change": "-for input_ids, input_mask, segment_ids, label_ids, example_index in eval_dataloader: +#for input_ids, input_mask, segment_ids, label_ids, example_index in eval_dataloader: +for input_ids, input_mask, segment_ids, example_index in eval_dataloader:",
        "core_API": "eval"
    },
    {
        "commit_hash": "35837814d2a9962b0be1cb3bbeed20df1f0187c1",
        "index": "dedfa7cc3..5d1589557 100644",
        "commit_message": "CrypTen Message Handler (#3676)\n\n* Fix plans framework\n\n* CrypTen Message Handler\n\n* Test fixup\n\n* Black fixup\n\n* Remove unused/undefined functions\n\n* Move message registration to worker level\n\n* Lint\n\n* Use global msgpack\n\n* factorize worker_id to rank translation\n\n* fix sequential model with last version of crypten\n\nCo-authored-by: youben11 <ayouben9@gmail.com>\n",
        "file": "PySyft.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "import syft",
            "def model():",
            "l_in, l_h, l_out = 32, 16, 2",
            "model = crypten.nn.Sequential(",
            "-        [crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)]",
            "+        crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)",
            ")",
            "return model"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=3)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=5)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 637,
        "neg_line": [
            "-[crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)]"
        ],
        "pos_line": [
            "+crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)"
        ],
        "core_change": "-[crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)] +crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "db7297865f34928cef6197104cb8effe38364ab6",
        "index": "b7610865dd..4427e6f35c 100644",
        "commit_message": "Added functionality for retrieving variables from control dependencies (#220)\n\n* Added test for retriving variables from an optimizer\n\n* Added comments to test\n\n* Addressed comments\n\n* Fixed travis bug\n\n* Added fix to circular controls\n\n* Added set for explored operations and duplicate prefix stripping\n\n* Removed embeded ipython\n\n* Removed prefix, use seperate graph for each network\n\n* Removed redundant imports\n\n* Addressed comments and added separate graph to initializer\n\n* fix typos\n\n* get rid of prefix in documentation\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LinearModel(object):",
            "return self.sess.run(self.cross_entropy_grads, feed_dict={self.x: xs, self.y_: ys})",
            "",
            "def net_initialization():",
            "-  return LinearModel([784,10])",
            "+  with tf.Graph().as_default():",
            "+    return LinearModel([784,10])",
            "",
            "# By default, when an environment variable is used by a remote function, the",
            "# initialization code will be rerun at the end of the remote task to ensure"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('with_statement', None), position=0, insert_id=2156819)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=2156820)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=2156821)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=2156822)",
            "Move(target_node=IN(type=with_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=2156823)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=2156824)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2156825)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2156826)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=2156827)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2156828)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_default'), position=2, insert_id=2156829)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2156830)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2156831)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2156832)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2156833)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2156834)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2156835)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Graph'), position=2, insert_id=2156836)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2156837)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2156838)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 639,
        "neg_line": [
            "-return LinearModel([784,10])"
        ],
        "pos_line": [
            "+with tf.Graph().as_default():",
            "+return LinearModel([784,10])"
        ],
        "core_change": "-return LinearModel([784,10]) +with tf.Graph().as_default(): +return LinearModel([784,10])",
        "core_API": "run"
    },
    {
        "commit_hash": "651c25feb66ba0a4c715ca671744a20f0a1355b0",
        "index": "c43520410..a547144c8 100644",
        "commit_message": "Fix for incorrect usage of detach(), cpu(), to() (#6216)\n\n* Fix for incorrect detach/cpu calls (#6214)\n\n* Fix incorrect use of detach(), to(), and cpu(), #6214\n\n* Fix incorrect use of detach() and cpu(), #6214\n\n* update pr\n\n* add typing\n\n* chlog\n\n* more...\n\n* revert on module\n\n* update on comments\n\n* revert changes on model\n\nCo-authored-by: tchaton <thomas@grid.ai>\nCo-authored-by: Jirka Borovec <jirka.borovec@seznam.cz>\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "refacotr",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EpochResultStore:",
            "# attach capture batch_size",
            "Result.attach_batch_size(self._batch_size, hook_result)",
            "",
            "-            hook_result.detach()",
            "+            hook_result = hook_result.detach()",
            "if self.trainer.move_metrics_to_cpu:",
            "-                hook_result.cpu()",
            "+                hook_result = hook_result.cpu()",
            "elif self.trainer._distrib_type == DistributedType.DP:",
            "-                hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))",
            "+                hook_result = hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))",
            "",
            "self._internals[fx_name].append(hook_result, info)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=545111)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'hook_result'), position=0, insert_id=545112)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=545113)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=545114)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'hook_result'), position=0, insert_id=545115)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=545116)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=545117)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'hook_result'), position=0, insert_id=545118)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=545119)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 12,
        "number": 640,
        "neg_line": [
            "-hook_result.detach()",
            "-hook_result.cpu()",
            "-hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))"
        ],
        "pos_line": [
            "+hook_result = hook_result.detach()",
            "+hook_result = hook_result.cpu()",
            "+hook_result = hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))"
        ],
        "core_change": "-hook_result.detach() +hook_result = hook_result.detach() -hook_result.cpu() +hook_result = hook_result.cpu() -hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu)) +hook_result = hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))",
        "core_API": "attach_batch_size"
    },
    {
        "commit_hash": "b68076d927775a1d7dd434ce40c15239dc2ab8ac",
        "index": "b80ee829..0091beca 100644",
        "commit_message": "Fix issue #2026\n\n",
        "file": "d2l-en.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LinearRegression(d2l.Module):",
            "def __init__(self, lr):",
            "super().__init__()",
            "self.save_hyperparameters()",
            "-        self.net = tf.keras.layers.Dense(1)",
            "+        initializer = tf.initializers.RandomNormal(stddev=0.01)",
            "+        self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)",
            "",
            "def forward(self, X):",
            "\"\"\"The linear regression model."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1917291)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1917292)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'initializer'), position=0, insert_id=1917293)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1917294)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1917295)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1917296)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1917297)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1917298)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1917299)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'RandomNormal'), position=2, insert_id=1917300)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1917301)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1917302)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1917303)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1917304)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1917305)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1917306)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1917307)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'initializers'), position=2, insert_id=1917308)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'stddev'), position=0, insert_id=1917309)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1917310)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '0.01'), position=2, insert_id=1917311)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'kernel_initializer'), position=0, insert_id=1917312)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1917313)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'initializer'), position=2, insert_id=1917314)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 641,
        "neg_line": [
            "-self.net = tf.keras.layers.Dense(1)"
        ],
        "pos_line": [
            "+initializer = tf.initializers.RandomNormal(stddev=0.01)",
            "+self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)"
        ],
        "core_change": "-self.net = tf.keras.layers.Dense(1) +initializer = tf.initializers.RandomNormal(stddev=0.01) +self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)",
        "core_API": "save_hyperparameters"
    },
    {
        "commit_hash": "510c8506512b8e9873ffbab0abdc1a668b022022",
        "index": "e63ea8beb..f3d6f03e7 100644",
        "commit_message": "[RLlib] SAC add discrete action support. (#7320)\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Cleanup/LINT.\n\n* Add `deterministic` to generic Trainer config (NOTE: this is still ignored by most Agents).\n\n* Add `error` option to deprecation_warning().\n\n* WIP.\n\n* Bug fix: Get exploration-info for tf framework.\nBug fix: Properly deprecate some DQN config keys.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Split PerWorkerEpsilonGreedy out of EpsilonGreedy.\nDocstrings.\n\n* Fix bug in sampler.py in case Policy has self.exploration = None\n\n* Update rllib/agents/dqn/dqn.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Update rllib/agents/trainer.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Change requests.\n\n* LINT\n\n* In tune/utils/util.py::deep_update() Only keep deep_updat'ing if both original and value are dicts. If value is not a dict, set\n\n* Completely obsolete syn_replay_optimizer.py's parameters schedule_max_timesteps AND beta_annealing_fraction (replaced with prioritized_replay_beta_annealing_timesteps).\n\n* Update rllib/evaluation/worker_set.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Review fixes.\n\n* Fix default value for DQN's exploration spec.\n\n* LINT\n\n* Fix recursion bug (wrong parent c'tor).\n\n* Do not pass timestep to get_exploration_info.\n\n* Update tf_policy.py\n\n* Fix some remaining issues with test cases and remove more deprecated DQN/APEX exploration configs.\n\n* Bug fix tf-action-dist\n\n* DDPG incompatibility bug fix with new DQN exploration handling (which is imported by DDPG).\n\n* Switch off exploration when getting action probs from off-policy-estimator's policy.\n\n* LINT\n\n* Fix test_checkpoint_restore.py.\n\n* Deprecate all SAC exploration (unused) configs.\n\n* Properly use `model.last_output()` everywhere. Instead of `model._last_output`.\n\n* WIP.\n\n* Take out set_epsilon from multi-agent-env test (not needed, decays anyway).\n\n* WIP.\n\n* Trigger re-test (flaky checkpoint-restore test).\n\n* WIP.\n\n* WIP.\n\n* Add test case for deterministic action sampling in PPO.\n\n* bug fix.\n\n* Added deterministic test cases for different Agents.\n\n* Fix problem with TupleActions in dynamic-tf-policy.\n\n* Separate supported_spaces tests so they can be run separately for easier debugging.\n\n* LINT.\n\n* Fix autoregressive_action_dist.py test case.\n\n* Re-test.\n\n* Fix.\n\n* Remove duplicate py_test rule from bazel.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* SAC fix.\n\n* SAC fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* FIX 2 examples tests.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Renamed test file.\n\n* WIP.\n\n* Add unittest.main.\n\n* Make action_dist_class mandatory.\n\n* fix\n\n* FIX.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix explorations test case (contextlib cannot find its own nullcontext??).\n\n* Force torch to be installed for QMIX.\n\n* LINT.\n\n* Fix determine_tests_to_run.py.\n\n* Fix determine_tests_to_run.py.\n\n* WIP\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Rename some stuff.\n\n* Rename some stuff.\n\n* WIP.\n\n* update.\n\n* WIP.\n\n* Gumbel Softmax Dist.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP\n\n* WIP.\n\n* WIP.\n\n* Hypertune.\n\n* Hypertune.\n\n* Hypertune.\n\n* Lock-in.\n\n* Cleanup.\n\n* LINT.\n\n* Fix.\n\n* Update rllib/policy/eager_tf_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/agents/sac/sac_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/agents/sac/sac_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/models/tf/tf_action_dist.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/models/tf/tf_action_dist.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Fix items from review comments.\n\n* Add dm_tree to RLlib dependencies.\n\n* Add dm_tree to RLlib dependencies.\n\n* Fix DQN test cases ((Torch)Categorical).\n\n* Fix wrong pip install.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\nCo-authored-by: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add param for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchCategorical(TorchDistributionWrapper):",
            "\"\"\"Wrapper class for PyTorch Categorical distribution.\"\"\"",
            "",
            "@override(ActionDistribution)",
            "-    def __init__(self, inputs, model):",
            "-        super().__init__(inputs, model)",
            "-        self.dist = torch.distributions.categorical.Categorical(logits=inputs)",
            "+    def __init__(self, inputs, model=None, temperature=1.0):",
            "+        assert temperature > 0.0, \"Categorical `temperature` must be > 0.0!\"",
            "+        super().__init__(inputs / temperature, model)",
            "+        self.dist = torch.distributions.categorical.Categorical(",
            "+            logits=self.inputs)",
            "",
            "@override(ActionDistribution)",
            "def deterministic_sample(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=5, insert_id=1126070)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=1126071)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=7, insert_id=1126072)",
            "Insert(target_node=ASTNode(type=block), node=('assert_statement', None), position=0, insert_id=1126073)",
            "Move(target_node=IN(type=default_parameter), node=ASTNode(type=identifier, text=model), position=0)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=1126074)",
            "Insert(target_node=IN(type=default_parameter), node=('none', 'None'), position=2, insert_id=1126075)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'temperature'), position=0, insert_id=1126076)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=1126077)",
            "Insert(target_node=IN(type=default_parameter), node=('float', '1.0'), position=2, insert_id=1126078)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=1126079)",
            "Insert(target_node=IN(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=1126080)",
            "Insert(target_node=IN(type=assert_statement), node=(',', ','), position=2, insert_id=1126081)",
            "Insert(target_node=IN(type=assert_statement), node=('string', '\"Categorical `temperature` must be > 0.0!\"'), position=3, insert_id=1126082)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'temperature'), position=0, insert_id=1126083)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=1126084)",
            "Insert(target_node=IN(type=comparison_operator), node=('float', '0.0'), position=2, insert_id=1126085)",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1126086)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=inputs), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=1126087)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'temperature'), position=2, insert_id=1126088)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=1126089)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1126090)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1126091)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=inputs), position=2)"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 642,
        "neg_line": [
            "-def __init__(self, inputs, model):",
            "-super().__init__(inputs, model)",
            "-self.dist = torch.distributions.categorical.Categorical(logits=inputs)"
        ],
        "pos_line": [
            "+def __init__(self, inputs, model=None, temperature=1.0):",
            "+assert temperature > 0.0, \"Categorical `temperature` must be > 0.0!\"",
            "+super().__init__(inputs / temperature, model)",
            "+self.dist = torch.distributions.categorical.Categorical(",
            "+logits=self.inputs)"
        ],
        "core_change": "-def __init__(self, inputs, model): -super().__init__(inputs, model) -self.dist = torch.distributions.categorical.Categorical(logits=inputs) +def __init__(self, inputs, model=None, temperature=1.0): +assert temperature > 0.0, \"Categorical `temperature` must be > 0.0!\" +super().__init__(inputs / temperature, model) +self.dist = torch.distributions.categorical.Categorical( +logits=self.inputs)",
        "core_API": "Categorical"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "6c867633..a5696978 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestPretrainedTransformerEmbedder(AllenNlpTestCase):",
            "def test_xlnet_token_type_ids(self):",
            "token_embedder = PretrainedTransformerEmbedder(\"xlnet-base-cased\")",
            "token_ids = torch.LongTensor([[1, 2, 3], [2, 3, 4]])",
            "-        mask = torch.ones_like(token_ids)",
            "+        mask = torch.ones_like(token_ids).bool()",
            "type_ids = torch.zeros_like(token_ids)",
            "type_ids[1, 1] = 1",
            "token_embedder(token_ids, mask, type_ids)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=19966)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=19967)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19968)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=19969)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=19970)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=19971)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 643,
        "neg_line": [
            "-mask = torch.ones_like(token_ids)"
        ],
        "pos_line": [
            "+mask = torch.ones_like(token_ids).bool()"
        ],
        "core_change": "-mask = torch.ones_like(token_ids) +mask = torch.ones_like(token_ids).bool()",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "ede44494409aa8310100b14169cbb5e2605101c8",
        "index": "754e2090..e381e5ea 100644",
        "commit_message": "Fix bug in block layer\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "change param for resource fix",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Block(Layer):",
            "layer_counter[layer_type] += 1",
            "",
            "# layer_name = self.name + '-' + layer_name",
            "-            self.layers[n] = self.submodule(",
            "+            layer = self.submodule(",
            "name=layer_name, module=layer_spec, modules=tensorforce.core.layer_modules,",
            "input_spec=self._input_spec",
            ")",
            "-            self._input_spec = self.layers[n].output_spec()",
            "-",
            "+            self.layers.append(layer)",
            "+            self._input_spec = layer.output_spec()",
            "",
            "return self.layers[0].input_spec.copy()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2624169)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2624170)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'layer'), position=0, insert_id=2624171)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2624172)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2624173)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2624174)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'append'), position=2, insert_id=2624175)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2624176)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'layer'), position=1, insert_id=2624177)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2624178)",
            "Update(target_node=ASTNode(type=identifier, text=self), value='layer')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=n))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=identifier, text=layers))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=n))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 645,
        "neg_line": [
            "-self.layers[n] = self.submodule(",
            "-self._input_spec = self.layers[n].output_spec()",
            "-"
        ],
        "pos_line": [
            "+layer = self.submodule(",
            "+self.layers.append(layer)",
            "+self._input_spec = layer.output_spec()"
        ],
        "core_change": "-self.layers[n] = self.submodule( +layer = self.submodule( -self._input_spec = self.layers[n].output_spec() - +self.layers.append(layer) +self._input_spec = layer.output_spec()",
        "core_API": "submodule"
    },
    {
        "commit_hash": "eef1990a5e6c41ecb6943ff5529316ad5ededb2a",
        "index": "0a58542d..0027343a 100644",
        "commit_message": "Fix Approx NN on devices other than CUDA\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def model():",
            "",
            "if sd_vae_approx_model is None:",
            "sd_vae_approx_model = VAEApprox()",
            "-        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))",
            "+        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))",
            "sd_vae_approx_model.eval()",
            "sd_vae_approx_model.to(devices.device, devices.dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1133408)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1133409)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=1133410)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1133411)",
            "Insert(target_node=IN(type=keyword_argument), node=('conditional_expression', None), position=2, insert_id=1133412)",
            "Insert(target_node=IN(type=conditional_expression), node=('string', \"'cpu'\"), position=0, insert_id=1133413)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1133414)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=1133415)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1133416)",
            "Insert(target_node=IN(type=conditional_expression), node=('none', 'None'), position=4, insert_id=1133417)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1133418)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=1133419)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', \"'cuda'\"), position=2, insert_id=1133420)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1133421)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1133422)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=1133423)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'devices'), position=0, insert_id=1133424)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1133425)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1133426)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 648,
        "neg_line": [
            "-sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))"
        ],
        "pos_line": [
            "+sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))"
        ],
        "core_change": "-sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"))) +sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))",
        "core_API": "load_state_dict"
    },
    {
        "commit_hash": "e0b856c105d3c35df8d53f737762f88ae68ee1e3",
        "index": "70332331f..c7255c6e4 100644",
        "commit_message": "[Metrics] Confusion matrix class interface (#4348)\n\n* docs + precision + recall + f_beta + refactor\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* rebase\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* fixes\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* added missing file\n\n* docs\n\n* docs\n\n* extra import\n\n* add confusion matrix\n\n* add to docs\n\n* add test\n\n* pep8 + isort\n\n* update tests\n\n* move util function\n\n* unify functional and class\n\n* add to init\n\n* remove old implementation\n\n* update tests\n\n* pep8\n\n* add duplicate\n\n* fix doctest\n\n* Update pytorch_lightning/metrics/classification/confusion_matrix.py\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* changelog\n\n* bullet point args\n\n* bullet docs\n\n* bullet docs\n\nCo-authored-by: ananyahjha93 <ananya@pytorchlightning.ai>\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: chaton <thomas@grid.ai>\nCo-authored-by: Roger Shieh <55400948+s-rog@users.noreply.github.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "customized API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Accuracy(Metric):",
            "preds: Predictions from model",
            "target: Ground truth values",
            "\"\"\"",
            "-        preds, target = self._input_format(preds, target)",
            "+        preds, target = _input_format_classification(preds, target, self.threshold)",
            "assert preds.shape == target.shape",
            "",
            "self.correct += torch.sum(preds == target)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', '_input_format_classification'), position=0, insert_id=558865)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=558866)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=5)",
            "Update(target_node=ASTNode(type=identifier, text=_input_format), value='threshold')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 650,
        "neg_line": [
            "-preds, target = self._input_format(preds, target)"
        ],
        "pos_line": [
            "+preds, target = _input_format_classification(preds, target, self.threshold)"
        ],
        "core_change": "-preds, target = self._input_format(preds, target) +preds, target = _input_format_classification(preds, target, self.threshold)",
        "core_API": "_input_format"
    },
    {
        "commit_hash": "18a09cb2a331fadca01d3215eca1f3173a482224",
        "index": "47b3be3e5..3148d5455 100644",
        "commit_message": "add pytest fixture as workaround for set_grad_enabled\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "remove state fix check",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_tacotron2_trainable_and_decodable(model_dict, loss_dict):",
            "assert att_ws.shape[0] == bs",
            "assert att_ws.shape[1] == max(olens)",
            "assert att_ws.shape[2] == max(ilens)",
            "-    if not torch_is_old:",
            "-        torch.set_grad_enabled(True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=identifier, text=torch_is_old))",
            "Delete(target_node=ASTNode(type=not_operator))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=set_grad_enabled))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 651,
        "neg_line": [
            "-if not torch_is_old:",
            "-torch.set_grad_enabled(True)"
        ],
        "pos_line": [],
        "core_change": "-if not torch_is_old: -torch.set_grad_enabled(True)",
        "core_API": "set_grad_enabled"
    },
    {
        "commit_hash": "f39b2ca08869b9030a30dd734d1e8a40bc0eb35e",
        "index": "a60dec24..147d64e3 100644",
        "commit_message": "fixed typos in processing.py\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_random_tensors(shape, seeds, subseeds=None, subseed_strength=0.0, see",
            "",
            "# if we have multiple seeds, this means we are working with batch size>1; this then",
            "# enables the generation of additional tensors with noise that the sampler will use during its processing.",
            "-    # Using those pre-genrated tensors instead of siimple torch.randn allows a batch with seeds [100, 101] to",
            "+    # Using those pre-generated tensors instead of simple torch.randn allows a batch with seeds [100, 101] to",
            "# produce the same images as with two batches [100], [101].",
            "if p is not None and p.sampler is not None and len(seeds) > 1 and opts.enable_batch_seeds:",
            "sampler_noises = [[] for _ in range(p.sampler.number_of_needed_noises(p))]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 653,
        "neg_line": [
            "-# Using those pre-genrated tensors instead of siimple torch.randn allows a batch with seeds [100, 101] to"
        ],
        "pos_line": [
            "+# Using those pre-generated tensors instead of simple torch.randn allows a batch with seeds [100, 101] to"
        ],
        "core_change": "-# Using those pre-genrated tensors instead of siimple torch.randn allows a batch with seeds [100, 101] to +# Using those pre-generated tensors instead of simple torch.randn allows a batch with seeds [100, 101] to",
        "core_API": "number_of_needed_noises"
    },
    {
        "commit_hash": "00c3a254a9a5915720863b8e281852be64519879",
        "index": "3325d9de..4fcbab59 100755",
        "commit_message": "Bug fix for norm calculation in absence of model parallel group (#551)\n\nIn the absence of a model parallel group, model_parallel_allreduce should not do any reduction. This commit fixes the bug which was doing a model parallel allreduce across world group when model parallel group is None\n",
        "file": "DeepSpeed.txt.json",
        "label": "yes",
        "comments": "remove API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FP16_DeepSpeedZeroOptimizer(object):",
            "\"\"\" Perform all reduce within model parallel group, if any.",
            "\"\"\"",
            "if self.model_parallel_group is None:",
            "-            torch.distributed.all_reduce(tensor=tensor, op=op)",
            "+            pass",
            "else:",
            "torch.distributed.all_reduce(tensor=tensor,",
            "op=op,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=84370)",
            "Insert(target_node=IN(type=block), node=('pass_statement', None), position=0, insert_id=84371)",
            "Insert(target_node=IN(type=pass_statement), node=('pass', 'pass'), position=0, insert_id=84372)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=distributed))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=all_reduce))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=op))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=op))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 654,
        "neg_line": [
            "-torch.distributed.all_reduce(tensor=tensor, op=op)"
        ],
        "pos_line": [
            "+pass"
        ],
        "core_change": "-torch.distributed.all_reduce(tensor=tensor, op=op) +pass",
        "core_API": "all_reduce"
    },
    {
        "commit_hash": "2d2ed2cc180475833d4315dde09fc5e5a1ccc9b5",
        "index": "94a200177..b63486ec5 100755",
        "commit_message": "[T5] Fix speed degradation bug t5 (#10496)\n\n* fix speed degradation bug t5\n\n* fix for all models\n\n* fix code quality\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change condition check for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class {{cookiecutter.camelcase_modelname}}EncoderLayer(nn.Module):",
            "hidden_states = residual + hidden_states",
            "hidden_states = self.final_layer_norm(hidden_states)",
            "",
            "-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "+        if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):",
            "clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=1221104)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1221105)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('parenthesized_expression', None), position=2, insert_id=1221106)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1221107)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1221108)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=2, insert_id=1221109)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1221110)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=boolean_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1221111)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hidden_states'), position=0, insert_id=1221112)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1221113)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1221114)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1221115)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1221116)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float16'), position=2, insert_id=1221117)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 655,
        "neg_line": [
            "-if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():"
        ],
        "pos_line": [
            "+if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):"
        ],
        "core_change": "-if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any(): +if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):",
        "core_API": "final_layer_norm"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "70da2a48c..ec055e647 100644",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change API call for refactor fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LabelSmoother:",
            "",
            "def __call__(self, model_output, labels):",
            "logits = model_output[\"logits\"] if isinstance(model_output, dict) else model_output[0]",
            "-        log_probs = -torch.nn.functional.log_softmax(logits, dim=-1)",
            "+        log_probs = -nn.functional.log_softmax(logits, dim=-1)",
            "if labels.dim() == log_probs.dim() - 1:",
            "labels = labels.unsqueeze(-1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 656,
        "neg_line": [
            "-log_probs = -torch.nn.functional.log_softmax(logits, dim=-1)"
        ],
        "pos_line": [
            "+log_probs = -nn.functional.log_softmax(logits, dim=-1)"
        ],
        "core_change": "-log_probs = -torch.nn.functional.log_softmax(logits, dim=-1) +log_probs = -nn.functional.log_softmax(logits, dim=-1)",
        "core_API": "log_softmax"
    },
    {
        "commit_hash": "3ae1de67aa8c771056facad561615b5e27b09b8f",
        "index": "06c90959..cbabfe7d 100644",
        "commit_message": "fix mypy errors\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NonMaximaSuppression2d(nn.Module):",
            "def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore",
            "assert len(x.shape) == 4, x.shape",
            "# find local maximum values",
            "-        x_max: torch.Tensor = self.max_pool2d(x)",
            "+        x_max: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] = \\",
            "+            self.max_pool2d(x)",
            "",
            "# create mask for maximums in the original map",
            "x_mask: torch.Tensor = torch.where("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=type), node=('subscript', None), position=0, insert_id=465032)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Union'), position=0, insert_id=465033)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=465034)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=465035)",
            "Insert(target_node=IN(type=subscript), node=('subscript', None), position=4, insert_id=465036)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=465037)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Tuple'), position=0, insert_id=465038)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=465039)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=465040)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=465041)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=4, insert_id=465042)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=465043)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=465044)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=465045)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=465046)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=465047)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=465048)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=465049)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 657,
        "neg_line": [
            "-x_max: torch.Tensor = self.max_pool2d(x)"
        ],
        "pos_line": [
            "+x_max: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] = \\",
            "+self.max_pool2d(x)"
        ],
        "core_change": "-x_max: torch.Tensor = self.max_pool2d(x) +x_max: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] = \\ +self.max_pool2d(x)",
        "core_API": "max_pool2d"
    },
    {
        "commit_hash": "6d4f83cae02129b7f49acf022561711cd937e8b8",
        "index": "7b9e1eae..99e22549 100644",
        "commit_message": "Drop JIT support for `core.check`, `Boxes`, and others (#2219)\n\n* Drop JIT support for `core.check` API\n\n- Consequently for this, we drop support of JIT on the following items: (in of dynamo)\n  - enhance\n    - AdjustSigmoid\n    - AdjustLog\n    - AddWeighted\n  - geometry\n    - UndistortPoints\n    - bbox and Boxes - follow up on #2218\n    - EuclideanDistance\n    - TransformPoints\n    - HomographyWarper\n    - WarpPerspective\n    - UpscaleDouble\n  - losses\n\n* Update typing with pyupgrade\n* drop all jit related from bbox and boxes\n\nfrom #2218\n* fix/skip failing dynamo tests\n* fix loss hd\n* fix typing\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestAdjustLog(BaseTester):",
            "f = kornia.enhance.AdjustLog()",
            "self.assert_close(f(data), expected)",
            "",
            "-    @pytest.mark.jit",
            "-    def test_jit(self, device, dtype):",
            "+    def test_dynamo(self, device, dtype, torch_optimizer):",
            "B, C, H, W = 2, 3, 4, 4",
            "img = torch.ones(B, C, H, W, device=device, dtype=dtype)",
            "op = kornia.enhance.adjust_log",
            "-        op_jit = torch.jit.script(op)",
            "-        self.assert_close(op(img), op_jit(img))",
            "+        op_optimized = torch_optimizer(op)",
            "+        self.assert_close(op(img), op_optimized(img))",
            "",
            "@pytest.mark.grad",
            "def test_gradcheck(self, device, dtype):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=function_definition), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=test_jit), value='test_dynamo')",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=388508)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'torch_optimizer'), position=7, insert_id=388509)",
            "Update(target_node=ASTNode(type=identifier, text=op_jit), value='op_optimized')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='torch_optimizer')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=op_jit), value='op_optimized')",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=pytest))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mark))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=jit))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type=decorated_definition))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=jit))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=script))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 24,
        "number": 658,
        "neg_line": [
            "-@pytest.mark.jit",
            "-def test_jit(self, device, dtype):",
            "-op_jit = torch.jit.script(op)",
            "-self.assert_close(op(img), op_jit(img))"
        ],
        "pos_line": [
            "+def test_dynamo(self, device, dtype, torch_optimizer):",
            "+op_optimized = torch_optimizer(op)",
            "+self.assert_close(op(img), op_optimized(img))"
        ],
        "core_change": "-@pytest.mark.jit -def test_jit(self, device, dtype): +def test_dynamo(self, device, dtype, torch_optimizer): -op_jit = torch.jit.script(op) -self.assert_close(op(img), op_jit(img)) +op_optimized = torch_optimizer(op) +self.assert_close(op(img), op_optimized(img))",
        "core_API": "AdjustLog"
    },
    {
        "commit_hash": "5b57be292c7d193419d7c8436441aff907068d5e",
        "index": "df0e53c4..8a9ac2d1 100644",
        "commit_message": "Adding normalization bias verification (#4990)\n\n* adding batchnorm verification\n\n* Adding trainer callback\n\n* updating changelog\n\n* renaming class\n\n* detailed message for sanity check\n\n* run sanity checks by default\n\n* fix normalization bias issue in image embeddings\n\n* update docstring\n\n* fix test\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "add param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ImageFeatureEmbeddings(Embeddings):",
            "",
            "def __init__(self, feature_size: int, embedding_size: int, dropout: float = 0.0):",
            "image_embeddings = torch.nn.Linear(feature_size, embedding_size)",
            "-        location_embeddings = torch.nn.Linear(4, embedding_size)",
            "+        location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)",
            "embeddings = torch.nn.ModuleDict(",
            "{\"image_embeddings\": image_embeddings, \"location_embeddings\": location_embeddings}",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=4576)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=4577)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'bias'), position=0, insert_id=4578)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=4579)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=4580)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 659,
        "neg_line": [
            "-location_embeddings = torch.nn.Linear(4, embedding_size)"
        ],
        "pos_line": [
            "+location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)"
        ],
        "core_change": "-location_embeddings = torch.nn.Linear(4, embedding_size) +location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "2f001bafa274bcdb2968f6961817b9d3c8cdd480",
        "index": "3d93685..8f873c8 100644",
        "commit_message": "fix linknet sum operation\n\n",
        "file": "segmentation_models.pytorch.txt.json",
        "label": "no",
        "comments": "format not clear deep",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DecoderBlock(nn.Module):",
            "x, skip = x",
            "x = self.block(x)",
            "if skip is not None:",
            "-            x += skip",
            "+            x = x + skip",
            "return x"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=1511241)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=x), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1511242)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=1511243)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'x'), position=0, insert_id=1511244)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1511245)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=skip), position=2)",
            "Delete(target_node=ASTNode(type=+=, text=+=))",
            "Delete(target_node=ASTNode(type=augmented_assignment))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 660,
        "neg_line": [
            "-x += skip"
        ],
        "pos_line": [
            "+x = x + skip"
        ],
        "core_change": "-x += skip +x = x + skip",
        "core_API": "block"
    },
    {
        "commit_hash": "be39dbdf37d57776f19e3b64a3a2bffc10d2ee93",
        "index": "5e121899..35b0270d 100755",
        "commit_message": "fix deprecation about dropout; fix Keras compatibility in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            ".apply(fg)",
            ".BatchNorm('bn5').apply(activate)",
            "# 5",
            "-                      .tf.nn.dropout(0.5 if is_training else 1.0)",
            "+                      .Dropout(rate=0.5 if is_training else 0.0)",
            ".Conv2D('conv6', 512, 5, padding='VALID')",
            ".apply(fg).BatchNorm('bn6')",
            ".apply(nonlin)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='Dropout')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2279017)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rate'), position=0, insert_id=2279018)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2279019)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=conditional_expression), position=2)",
            "Update(target_node=ASTNode(type=float, text=1.0), value='0.0')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=dropout))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 661,
        "neg_line": [
            "-.tf.nn.dropout(0.5 if is_training else 1.0)"
        ],
        "pos_line": [
            "+.Dropout(rate=0.5 if is_training else 0.0)"
        ],
        "core_change": "-.tf.nn.dropout(0.5 if is_training else 1.0) +.Dropout(rate=0.5 if is_training else 0.0)",
        "core_API": "dropout"
    },
    {
        "commit_hash": "0dfd6b624081fc4e1c72fc74ae0cd2de199c334c",
        "index": "a970876b..dbbbfb9f 100644",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding ÔøΩ\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "file": "fairseq.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LocationAttention(nn.Module):",
            "self.proj_enc = nn.Linear(encoder_dim, attn_dim)",
            "self.proj_dec = nn.Linear(decoder_dim, attn_dim, bias=False)",
            "self.proj_attn = nn.Linear(conv_dim, attn_dim, bias=False)",
            "-        self.conv = nn.Conv1d(attn_state_kernel_size, conv_dim,",
            "-                              2 * conv_kernel_size + 1,",
            "-                              padding=conv_kernel_size, bias=False)",
            "+        self.conv = nn.Conv1d(",
            "+            attn_state_kernel_size,",
            "+            conv_dim,",
            "+            2 * conv_kernel_size + 1,",
            "+            padding=conv_kernel_size,",
            "+            bias=False,",
            "+        )",
            "self.proj_out = nn.Sequential(nn.Tanh(), nn.Linear(attn_dim, 1))",
            "",
            "self.proj_enc_out = None  # cache"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=10, insert_id=1355517)"
        ],
        "plus_line": 7,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 662,
        "neg_line": [
            "-self.conv = nn.Conv1d(attn_state_kernel_size, conv_dim,",
            "-2 * conv_kernel_size + 1,",
            "-padding=conv_kernel_size, bias=False)"
        ],
        "pos_line": [
            "+self.conv = nn.Conv1d(",
            "+attn_state_kernel_size,",
            "+conv_dim,",
            "+2 * conv_kernel_size + 1,",
            "+padding=conv_kernel_size,",
            "+bias=False,",
            "+)"
        ],
        "core_change": "-self.conv = nn.Conv1d(attn_state_kernel_size, conv_dim, -2 * conv_kernel_size + 1, -padding=conv_kernel_size, bias=False) +self.conv = nn.Conv1d( +attn_state_kernel_size, +conv_dim, +2 * conv_kernel_size + 1, +padding=conv_kernel_size, +bias=False, +)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "c0b2040e38c5135c65dd2b5e9b24f00ff3f31548",
        "index": "2d7d7cf..aa177ed 100644",
        "commit_message": "Fixed module resolution for tf.keras optimizers and added unit tests (#1935)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n",
        "file": "horovod.txt.json",
        "label": "no",
        "comments": "no API fount",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def load_model(filepath, custom_optimizers=None, custom_objects=None, compressio",
            "\"\"\"",
            "def wrap_optimizer(cls):",
            "return lambda **kwargs: DistributedOptimizer(cls(**kwargs), compression=compression)",
            "-    return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects)",
            "+    return _impl.load_model(keras, wrap_optimizer, _OPTIMIZER_MODULES, filepath, custom_optimizers, custom_objects)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=identifier, text=filepath), node=ASTNode(type=argument_list), position=6)",
            "Move(target_node=ASTNode(type=identifier, text=custom_optimizers), node=ASTNode(type=argument_list), position=9)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', '_OPTIMIZER_MODULES'), position=5, insert_id=1946199)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=9, insert_id=1946200)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 663,
        "neg_line": [
            "-return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects)"
        ],
        "pos_line": [
            "+return _impl.load_model(keras, wrap_optimizer, _OPTIMIZER_MODULES, filepath, custom_optimizers, custom_objects)"
        ],
        "core_change": "-return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects) +return _impl.load_model(keras, wrap_optimizer, _OPTIMIZER_MODULES, filepath, custom_optimizers, custom_objects)",
        "core_API": "load_model"
    },
    {
        "commit_hash": "dfb99d4009a300e3f766b2484e2d47518b2917b0",
        "index": "070fc95c9f..2345478906 100644",
        "commit_message": "Fixing some linear algebra tests (#7418)\n\n* minor fix for cholesky test for jax\n\n* eigh:UPLO update inst. + test fix(to-do bug fix)\n\n* minor fix to matrix_power test\n\n* added container and static methods for eigh\n\n* updated positional args + out arg for qr\n\n* updated test_qr\n\n* fixed svd test\n\n* fixed tensordot pos args issue\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "custom method",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def pinv(",
            "",
            "",
            "@with_unsupported_dtypes({\"2.9.1 and below\": (\"float16\", \"bfloat16\")}, backend_version)",
            "-def qr(x: Union[tf.Tensor, tf.Variable], /, *, mode: str = \"reduced\") -> NamedTuple:",
            "+def qr(",
            "+    x: Union[tf.Tensor, tf.Variable],",
            "+    /,",
            "+    *,",
            "+    mode: str = \"reduced\",",
            "+    out: Optional[tf.Tensor] = None,",
            "+) -> NamedTuple:",
            "res = namedtuple(\"qr\", [\"Q\", \"R\"])",
            "if mode == \"reduced\":",
            "q, r = tf.linalg.qr(x, full_matrices=False)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=1972924)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=9, insert_id=1972925)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=10, insert_id=1972926)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('identifier', 'out'), position=0, insert_id=1972927)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=1972928)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=1972929)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=1972930)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('none', 'None'), position=4, insert_id=1972931)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=1972932)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Optional'), position=0, insert_id=1972933)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1972934)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=1972935)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1972936)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1972937)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1972938)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1972939)"
        ],
        "plus_line": 5,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 664,
        "neg_line": [
            "-def qr(x: Union[tf.Tensor, tf.Variable], /, *, mode: str = \"reduced\") -> NamedTuple:"
        ],
        "pos_line": [
            "+def qr(",
            "+x: Union[tf.Tensor, tf.Variable],",
            "+/,",
            "+*,",
            "+mode: str = \"reduced\",",
            "+out: Optional[tf.Tensor] = None,",
            "+) -> NamedTuple:"
        ],
        "core_change": "-def qr(x: Union[tf.Tensor, tf.Variable], /, *, mode: str = \"reduced\") -> NamedTuple: +def qr( +x: Union[tf.Tensor, tf.Variable], +/, +*, +mode: str = \"reduced\", +out: Optional[tf.Tensor] = None, +) -> NamedTuple:",
        "core_API": "qr"
    },
    {
        "commit_hash": "23efadd9a394d9d69e10e8539a7332665b118df0",
        "index": "0c65293c..7e95a5ad 100644",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BLEU(Metric):",
            "return math.exp(1.0 - self._reference_lengths / self._prediction_lengths)",
            "",
            "def _get_valid_tokens_mask(self, tensor: torch.LongTensor) -> torch.ByteTensor:",
            "-        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.uint8)",
            "+        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.bool)",
            "for index in self._exclude_indices:",
            "valid_tokens_mask = valid_tokens_mask & (tensor != index)",
            "return valid_tokens_mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 665,
        "neg_line": [
            "-valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.uint8)"
        ],
        "pos_line": [
            "+valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.bool)"
        ],
        "core_change": "-valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.uint8) +valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.bool)",
        "core_API": "exp"
    },
    {
        "commit_hash": "b38ed4be71d4be096f324c8c666e01e8af5f6c8b",
        "index": "964471ebc..24297e98b 100644",
        "commit_message": "[raysgd] Fix More Docs (#7565)\n\n\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "MOCK_MODULES = [",
            "\"torch.nn\",",
            "\"torch.nn.parallel\",",
            "\"torch.utils.data\",",
            "+    \"torch.utils.data.distributed\"",
            "]",
            "for mod_name in MOCK_MODULES:",
            "sys.modules[mod_name] = mock.Mock()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=list), position=5)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=1125921)",
            "Insert(target_node=ASTNode(type=list), node=('string', '\"torch.utils.data.distributed\"'), position=4, insert_id=1125922)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 667,
        "neg_line": [],
        "pos_line": [
            "+\"torch.utils.data.distributed\""
        ],
        "core_change": "+\"torch.utils.data.distributed\"",
        "core_API": "Mock"
    },
    {
        "commit_hash": "8e76796fd0535d8fbc9934e7f6604a1abcfb119e",
        "index": "6298bf20de..cf0069deae 100644",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ModelCatalogTest(unittest.TestCase):",
            "dist_cls, param_shape = ModelCatalog.get_action_dist(",
            "action_space, model_config)",
            "self.assertEqual(param_shape, (3, ))",
            "-        dist_input = tf1.placeholder(tf.float32, (None,) + param_shape)",
            "+        dist_input = tf1.placeholder(tf.float32, (None, ) + param_shape)",
            "model.model_config = model_config",
            "dist = dist_cls(dist_input, model=model)",
            "self.assertEqual(dist.sample().shape[1:], dist_input.shape[1:])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 668,
        "neg_line": [
            "-dist_input = tf1.placeholder(tf.float32, (None,) + param_shape)"
        ],
        "pos_line": [
            "+dist_input = tf1.placeholder(tf.float32, (None, ) + param_shape)"
        ],
        "core_change": "-dist_input = tf1.placeholder(tf.float32, (None,) + param_shape) +dist_input = tf1.placeholder(tf.float32, (None, ) + param_shape)",
        "core_API": "get_action_dist"
    },
    {
        "commit_hash": "ec27890e824a63964fadc9a64e60fd1998e41bc1",
        "index": "3c4194c..54adb76 100644",
        "commit_message": "Hotfix optimizer\n\n",
        "file": "PyTorch-YOLOv3.txt.json",
        "label": "no",
        "comments": "change optimizor",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "if __name__ == \"__main__\":",
            "collate_fn=dataset.collate_fn,",
            ")",
            "",
            "-    optimizer = torch.optim.SGD(model.parameters(), lr=0.0000)",
            "+    optimizer = torch.optim.Adam(model.parameters())",
            "",
            "metrics = [",
            "\"grid_size\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=SGD), value='Adam')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=lr))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=float, text=0.0000))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 669,
        "neg_line": [
            "-optimizer = torch.optim.SGD(model.parameters(), lr=0.0000)"
        ],
        "pos_line": [
            "+optimizer = torch.optim.Adam(model.parameters())"
        ],
        "core_change": "-optimizer = torch.optim.SGD(model.parameters(), lr=0.0000) +optimizer = torch.optim.Adam(model.parameters())",
        "core_API": "SGD"
    },
    {
        "commit_hash": "2ec84ebdca59278eaf15e8ddf32476d9d6d8b904",
        "index": "75ffbf0..4a93fcd 100644",
        "commit_message": "Fix LARC with mixed precision (#793)\n\nThe LARC optimizer wraps an underlying optimizer and then needs to be passed\nto amp.initialize for mixed precision. There were 3 different crashes happening\nin this situation, fix all of them and add a unit test.\n\nI don't know if the 'LARC' in sys.modules check ever worked. In my setup, the\nentry in sys.modules is 'apex.parallel.LARC'. Checking if the variable is\ndefined seems more reliable though.\n",
        "file": "apex.txt.json",
        "label": "no",
        "comments": "add method",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "class LARC(object):",
            "def __setstate__(self, state):",
            "self.optim.__setstate__(state)",
            "",
            "+    @property",
            "+    def state(self):",
            "+        return self.optim.state",
            "+",
            "def __repr__(self):",
            "return self.optim.__repr__()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('decorated_definition', None), position=3, insert_id=1585031)",
            "Insert(target_node=IN(type=decorated_definition), node=('decorator', None), position=0, insert_id=1585032)",
            "Insert(target_node=IN(type=decorated_definition), node=('function_definition', None), position=1, insert_id=1585033)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=1585034)",
            "Insert(target_node=IN(type=decorator), node=('identifier', 'property'), position=1, insert_id=1585035)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=1585036)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'state'), position=1, insert_id=1585037)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=1585038)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=1585039)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=1585040)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=1585041)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'self'), position=1, insert_id=1585042)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=1585043)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=1585044)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1585045)",
            "Insert(target_node=IN(type=return_statement), node=('attribute', None), position=1, insert_id=1585046)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1585047)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1585048)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'state'), position=2, insert_id=1585049)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1585050)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1585051)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'optim'), position=2, insert_id=1585052)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 670,
        "neg_line": [],
        "pos_line": [
            "+@property",
            "+def state(self):",
            "+return self.optim.state",
            "+"
        ],
        "core_change": "+@property +def state(self): +return self.optim.state +",
        "core_API": "__setstate__"
    },
    {
        "commit_hash": "7b09325a85d76435ced550bd5f7b757e8028e944",
        "index": "9c690cce71..de41ed34e0 100644",
        "commit_message": "Type promotion fixes (#2516)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def searchsorted(",
            "v: Union[tf.Tensor, tf.Variable],",
            "side=\"left\",",
            "sorter=None,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "return tf.searchsorted(x1, v, side=side)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=2002833)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 671,
        "neg_line": [
            "-out: Optional[Union[tf.Tensor, tf.Variable]] = None"
        ],
        "pos_line": [
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,"
        ],
        "core_change": "-out: Optional[Union[tf.Tensor, tf.Variable]] = None +out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
        "core_API": "searchsorted"
    },
    {
        "commit_hash": "575611f4f5e2209c7923dba977a1eebc207bd2e2",
        "index": "b0f901d3..662fcecd 100644",
        "commit_message": "Add proper error messages in `__check_input__()` (#5042)\n\n* Add proper error messages in `__check_input__()`\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Format update to pass linting error\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "asset check doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MessagePassing(torch.nn.Module):",
            "the_size: List[Optional[int]] = [None, None]",
            "",
            "if isinstance(edge_index, Tensor):",
            "-            assert edge_index.dtype == torch.long",
            "-            assert edge_index.dim() == 2",
            "-            assert edge_index.size(0) == 2",
            "+            assert edge_index.dtype == torch.long, \\",
            "+                \"edge_index.dtype is not of torch.long\"",
            "+            assert edge_index.dim() == 2, \\",
            "+                \"edge_index.dim() is not equal to 2\"",
            "+            assert edge_index.size(0) == 2, \\",
            "+                \"edge_index.size(0) is not equal to 2\"",
            "if size is not None:",
            "the_size[0] = size[0]",
            "the_size[1] = size[1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assert_statement), node=(',', ','), position=2, insert_id=971792)",
            "Insert(target_node=ASTNode(type=assert_statement), node=('string', '\"edge_index.dtype is not of torch.long\"'), position=3, insert_id=971793)",
            "Insert(target_node=ASTNode(type=assert_statement), node=(',', ','), position=2, insert_id=971794)",
            "Insert(target_node=ASTNode(type=assert_statement), node=('string', '\"edge_index.dim() is not equal to 2\"'), position=3, insert_id=971795)",
            "Insert(target_node=ASTNode(type=assert_statement), node=(',', ','), position=2, insert_id=971796)",
            "Insert(target_node=ASTNode(type=assert_statement), node=('string', '\"edge_index.size(0) is not equal to 2\"'), position=3, insert_id=971797)"
        ],
        "plus_line": 6,
        "minus_line": 3,
        "AST_diff_line": 6,
        "number": 672,
        "neg_line": [
            "-assert edge_index.dtype == torch.long",
            "-assert edge_index.dim() == 2",
            "-assert edge_index.size(0) == 2"
        ],
        "pos_line": [
            "+assert edge_index.dtype == torch.long, \\",
            "+\"edge_index.dtype is not of torch.long\"",
            "+assert edge_index.dim() == 2, \\",
            "+\"edge_index.dim() is not equal to 2\"",
            "+assert edge_index.size(0) == 2, \\",
            "+\"edge_index.size(0) is not equal to 2\""
        ],
        "core_change": "-assert edge_index.dtype == torch.long -assert edge_index.dim() == 2 -assert edge_index.size(0) == 2 +assert edge_index.dtype == torch.long, \\ +\"edge_index.dtype is not of torch.long\" +assert edge_index.dim() == 2, \\ +\"edge_index.dim() is not equal to 2\" +assert edge_index.size(0) == 2, \\ +\"edge_index.size(0) is not equal to 2\"",
        "core_API": "dim"
    },
    {
        "commit_hash": "2ee9f9b69e67426aaed690f652f9cdd8b524b99d",
        "index": "d7bf7715a..da353e8ca 100644",
        "commit_message": "Fix computation of attention_probs when head_mask is provided. (#9853)\n\n* Fix computation of attention_probs when head_mask is provided.\n\nSigned-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>\n\n* Apply changes to the template\n\nCo-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFBertSelfAttention(tf.keras.layers.Layer):",
            "",
            "# Mask heads if we want to",
            "if head_mask is not None:",
            "-            attention_scores = tf.multiply(attention_scores, head_mask)",
            "+            attention_probs = tf.multiply(attention_probs, head_mask)",
            "",
            "attention_output = tf.einsum(\"acbe,aecd->abcd\", attention_probs, value_layer)",
            "outputs = (attention_output, attention_probs) if output_attentions else (attention_output,)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=attention_scores), value='attention_probs')",
            "Update(target_node=ASTNode(type=identifier, text=attention_scores), value='attention_probs')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 673,
        "neg_line": [
            "-attention_scores = tf.multiply(attention_scores, head_mask)"
        ],
        "pos_line": [
            "+attention_probs = tf.multiply(attention_probs, head_mask)"
        ],
        "core_change": "-attention_scores = tf.multiply(attention_scores, head_mask) +attention_probs = tf.multiply(attention_probs, head_mask)",
        "core_API": "multiply"
    },
    {
        "commit_hash": "dfe7e571bb66b080b598aa10d03e6518be0453cf",
        "index": "c6772e017..d9e210f98 100644",
        "commit_message": "fixed\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add param for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DecoderLayer(nn.Module):",
            "if self.normalize_before:",
            "x = self.norm2(x)",
            "if self.concate_after:",
            "-            x_concat = torch.cat(x, self.src_attn(x, memory, memory, memory_mask))",
            "+            x_concat = torch.cat((x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)",
            "x = residual + self.concate_linear2(x_concat)",
            "else:",
            "x = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=173626)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=173627)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=173628)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=173629)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=173630)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=identifier, text=x), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=call), position=3)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=173631)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=173632)",
            "Insert(target_node=IN(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=173633)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 674,
        "neg_line": [
            "-x_concat = torch.cat(x, self.src_attn(x, memory, memory, memory_mask))"
        ],
        "pos_line": [
            "+x_concat = torch.cat((x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)"
        ],
        "core_change": "-x_concat = torch.cat(x, self.src_attn(x, memory, memory, memory_mask)) +x_concat = torch.cat((x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)",
        "core_API": "norm2"
    },
    {
        "commit_hash": "3e6167c51df23b7629d7830e81e8cf4ea52032fc",
        "index": "dc389fe1b..f7df31d15 100644",
        "commit_message": "Fixed format in some files\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FrequencyDomainDPCL(FrequencyDomainLoss):",
            ")",
            "",
            "V2 = torch.matmul(torch.transpose(inf, 2, 1), inf).pow(2).sum(dim=(1, 2))",
            "-        Y2 = torch.matmul(torch.transpose(re, 2, 1).float(), re.float()).pow(2).sum(dim=(1, 2))",
            "+        Y2 = (",
            "+            torch.matmul(torch.transpose(re, 2, 1).float(), re.float())",
            "+            .pow(2)",
            "+            .sum(dim=(1, 2))",
            "+        )",
            "VY = torch.matmul(torch.transpose(inf, 2, 1), re.float()).pow(2).sum(dim=(1, 2))",
            "",
            "return V2 + Y2 - 2 * VY"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=127922)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=127923)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=127924)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 675,
        "neg_line": [
            "-Y2 = torch.matmul(torch.transpose(re, 2, 1).float(), re.float()).pow(2).sum(dim=(1, 2))"
        ],
        "pos_line": [
            "+Y2 = (",
            "+torch.matmul(torch.transpose(re, 2, 1).float(), re.float())",
            "+.pow(2)",
            "+.sum(dim=(1, 2))",
            "+)"
        ],
        "core_change": "-Y2 = torch.matmul(torch.transpose(re, 2, 1).float(), re.float()).pow(2).sum(dim=(1, 2)) +Y2 = ( +torch.matmul(torch.transpose(re, 2, 1).float(), re.float()) +.pow(2) +.sum(dim=(1, 2)) +)",
        "core_API": "matmul"
    },
    {
        "commit_hash": "8caf81bc40032a8e884ec7fe42ea68a8b806f06f",
        "index": "dee8588b..274c6a14 100644",
        "commit_message": "Explicitly set max sequence length for the roberta encoder, fix output shape computation, and add unit test. (#2861)\n\n\n",
        "file": "ludwig.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RoBERTaEncoder(Encoder):",
            "@property",
            "def output_shape(self) -> torch.Size:",
            "if self.reduce_output is None:",
            "-            return torch.Size([self.max_sequence_length, self.transformer.module.config.hidden_size])",
            "+            return torch.Size([self.max_sequence_length - 2, self.transformer.module.config.hidden_size])",
            "return torch.Size([self.transformer.module.config.hidden_size])",
            "",
            "@property"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('binary_operator', None), position=1, insert_id=596814)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=596815)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=596816)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 676,
        "neg_line": [
            "-return torch.Size([self.max_sequence_length, self.transformer.module.config.hidden_size])"
        ],
        "pos_line": [
            "+return torch.Size([self.max_sequence_length - 2, self.transformer.module.config.hidden_size])"
        ],
        "core_change": "-return torch.Size([self.max_sequence_length, self.transformer.module.config.hidden_size]) +return torch.Size([self.max_sequence_length - 2, self.transformer.module.config.hidden_size])",
        "core_API": "Size"
    },
    {
        "commit_hash": "fb6e1a5ac63b40c5a24edc8dcfee2a8ba43e6ddb",
        "index": "7f201df905..ada71782b9 100644",
        "commit_message": "fix type issue of `torch.less` in `clip` (#3852)\n\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "change API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def clip(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"",
            "+    assert torch.all(",
            "+        torch.less(torch.tensor(x_min), x_max)",
            "+    ), \"Min value must be less than max.\"",
            "if hasattr(x_min, \"dtype\"):",
            "promoted_type = torch.promote_types(x_min.dtype, x_max.dtype)",
            "promoted_type = torch.promote_types(promoted_type, x.dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=335490)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=5, insert_id=335491)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=335492)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=335493)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=335494)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=335495)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=335496)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=335497)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=x_min), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 677,
        "neg_line": [
            "-assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\""
        ],
        "pos_line": [
            "+assert torch.all(",
            "+torch.less(torch.tensor(x_min), x_max)",
            "+), \"Min value must be less than max.\""
        ],
        "core_change": "-assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\" +assert torch.all( +torch.less(torch.tensor(x_min), x_max) +), \"Min value must be less than max.\"",
        "core_API": "all"
    },
    {
        "commit_hash": "6f289dc97aaa1ade5f658ecdd16cc7a842505444",
        "index": "c1d41beb0..47f0f30e9 100644",
        "commit_message": "Fix the TF Trainer gradient accumulation and the TF NER example (#6713)\n\n* Align TF NER example over the PT one\n\n* Fix Dataset call\n\n* Fix gradient accumulation training\n\n* Apply style\n\n* Address Sylvain's comments\n\n* Address Sylvain's comments\n\n* Apply style\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "remove API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTokenClassificationLoss:",
            ")",
            "# make sure only labels that are not equal to -100",
            "# are taken into account as loss",
            "-        if tf.math.reduce_any(labels == -1).numpy() is True:",
            "+        if tf.math.reduce_any(labels == -1):",
            "warnings.warn(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")",
            "active_loss = tf.reshape(labels, (-1,)) != -1",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=numpy))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=is, text=is))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 678,
        "neg_line": [
            "-if tf.math.reduce_any(labels == -1).numpy() is True:"
        ],
        "pos_line": [
            "+if tf.math.reduce_any(labels == -1):"
        ],
        "core_change": "-if tf.math.reduce_any(labels == -1).numpy() is True: +if tf.math.reduce_any(labels == -1):",
        "core_API": "reduce_any"
    },
    {
        "commit_hash": "5ef59abd1fb2cde1615d316ecc5185ee7b9ccfc7",
        "index": "03d9635e..fc27f8fe 100644",
        "commit_message": "Fix seed so that data is properly shuffled between epochs\n\n",
        "file": "fairseq.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_perplexity(loss):",
            "def train(args, epoch, batch_offset, trainer, dataset, max_positions, num_gpus):",
            "\"\"\"Train the model for one epoch.\"\"\"",
            "",
            "-    torch.manual_seed(args.seed + epoch)",
            "-    trainer.set_seed(args.seed + epoch)",
            "+    seed = args.seed + epoch",
            "+    torch.manual_seed(seed)",
            "+    trainer.set_seed(seed)",
            "",
            "itr = dataset.dataloader(",
            "args.train_subset, num_workers=args.workers, max_tokens=args.max_tokens,",
            "-        seed=args.seed, epoch=epoch, max_positions=max_positions,",
            "+        seed=seed, epoch=epoch, max_positions=max_positions,",
            "sample_without_replacement=args.sample_without_replacement,",
            "skip_invalid_size_inputs_valid_test=args.skip_invalid_size_inputs_valid_test,",
            "sort_by_source_size=(epoch <= args.curriculum))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=225961)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=225962)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'seed'), position=0, insert_id=225963)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=225964)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'seed'), position=1, insert_id=225965)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=seed), position=1)",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=identifier, text=seed), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=args))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=identifier, text=epoch))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=identifier, text=args))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 680,
        "neg_line": [
            "-torch.manual_seed(args.seed + epoch)",
            "-trainer.set_seed(args.seed + epoch)",
            "-seed=args.seed, epoch=epoch, max_positions=max_positions,"
        ],
        "pos_line": [
            "+seed = args.seed + epoch",
            "+torch.manual_seed(seed)",
            "+trainer.set_seed(seed)",
            "+seed=seed, epoch=epoch, max_positions=max_positions,"
        ],
        "core_change": "-torch.manual_seed(args.seed + epoch) -trainer.set_seed(args.seed + epoch) +seed = args.seed + epoch +torch.manual_seed(seed) +trainer.set_seed(seed) -seed=args.seed, epoch=epoch, max_positions=max_positions, +seed=seed, epoch=epoch, max_positions=max_positions,",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "f301afafd..f3bdd2cd8 100644",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LayoutLMv3Model(LayoutLMv3PreTrainedModel):",
            "position_ids = position_ids.expand_as(input_ids)",
            "final_position_ids = position_ids",
            "",
            "-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, None, device)",
            "+        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(",
            "+            attention_mask, None, device, dtype=embedding_output.dtype",
            "+        )",
            "",
            "# Prepare head mask if needed",
            "# 1.0 in head_mask indicate we keep the head"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=3)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1196328)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=7, insert_id=1196329)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=8, insert_id=1196330)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1196331)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1196332)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1196333)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'embedding_output'), position=0, insert_id=1196334)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196335)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1196336)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 681,
        "neg_line": [
            "-extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, None, device)"
        ],
        "pos_line": [
            "+extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(",
            "+attention_mask, None, device, dtype=embedding_output.dtype",
            "+)"
        ],
        "core_change": "-extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, None, device) +extended_attention_mask: torch.Tensor = self.get_extended_attention_mask( +attention_mask, None, device, dtype=embedding_output.dtype +)",
        "core_API": "expand_as"
    },
    {
        "commit_hash": "fe1da3c70661a5f95e20aeb7307c7d1716f39f70",
        "index": "114d8ee36c..4d0d42ff49 100644",
        "commit_message": "lint fixes\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def cumsum(x: torch.Tensor, axis: int = 0, out: Optional[torch.Tensor] = None):",
            "",
            "",
            "def cumprod(",
            "-    x: torch.Tensor, axis: int = 0, exclusive: Optional[bool] = False, out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor,",
            "+    axis: int = 0,",
            "+    exclusive: Optional[bool] = False,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if exclusive:",
            "x = torch.transpose(x, axis, -1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=350676)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 682,
        "neg_line": [
            "-x: torch.Tensor, axis: int = 0, exclusive: Optional[bool] = False, out: Optional[torch.Tensor] = None"
        ],
        "pos_line": [
            "+x: torch.Tensor,",
            "+axis: int = 0,",
            "+exclusive: Optional[bool] = False,",
            "+out: Optional[torch.Tensor] = None,"
        ],
        "core_change": "-x: torch.Tensor, axis: int = 0, exclusive: Optional[bool] = False, out: Optional[torch.Tensor] = None +x: torch.Tensor, +axis: int = 0, +exclusive: Optional[bool] = False, +out: Optional[torch.Tensor] = None,",
        "core_API": "transpose"
    },
    {
        "commit_hash": "b345c74d4d372e74b87512734b2b64f8f57df6a1",
        "index": "aa8626a5..273688ae 100755",
        "commit_message": "Make sure all pipelines can run with batched input (#1669)\n\n* [SD] Make sure batched input works correctly\n\n* uP\n\n* uP\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* fix mask stuff\n\n* up\n\n* uP\n\n* more up\n\n* up\n\n* uP\n\n* up\n\n* finish\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "change condition check for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionKDiffusionPipeline(DiffusionPipeline):",
            "return_tensors=\"pt\",",
            ")",
            "text_input_ids = text_inputs.input_ids",
            "-        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids",
            "+        untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids",
            "",
            "-        if not torch.equal(text_input_ids, untruncated_ids):",
            "+        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):",
            "removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])",
            "logger.warning(",
            "\"The following part of your input was truncated because CLIP can only handle sequences up to\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=94179)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=94180)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=94181)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=not_operator), position=2)",
            "Insert(target_node=IN(type=comparison_operator), node=('subscript', None), position=0, insert_id=94182)",
            "Insert(target_node=IN(type=comparison_operator), node=('>=', '>='), position=1, insert_id=94183)",
            "Insert(target_node=IN(type=comparison_operator), node=('subscript', None), position=2, insert_id=94184)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=94185)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=94186)",
            "Insert(target_node=IN(type=subscript), node=('unary_operator', '-1'), position=2, insert_id=94187)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=94188)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=94189)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=94190)",
            "Insert(target_node=IN(type=subscript), node=('unary_operator', '-1'), position=2, insert_id=94191)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=94192)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'untruncated_ids'), position=0, insert_id=94193)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=94194)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=94195)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'text_input_ids'), position=0, insert_id=94196)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=94197)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=94198)",
            "Update(target_node=ASTNode(type=string, text=\"max_length\"), value='\"longest\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 683,
        "neg_line": [
            "-untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids",
            "-if not torch.equal(text_input_ids, untruncated_ids):"
        ],
        "pos_line": [
            "+untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids",
            "+if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):"
        ],
        "core_change": "-untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids +untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids -if not torch.equal(text_input_ids, untruncated_ids): +if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):",
        "core_API": "tokenizer"
    },
    {
        "commit_hash": "ca10981597e6058c5391edad3fcb22f12e4aa6ee",
        "index": "51ea02b36..a087f4b1f 100644",
        "commit_message": "Fix zero padding:   espnet/nets/pytorch_backend/frontends/mask_estimator.py\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "format fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MaskEstimator(torch.nn.Module):",
            "# xs: (B, C, T, D) -> mask:(B, C, T, F)",
            "mask = linear(xs)",
            "",
            "+            mask = torch.sigmoid(mask)",
            "# Zero padding",
            "mask.masked_fill(make_pad_mask(ilens, mask, length_dim=2), 0)",
            "",
            "-            mask = torch.sigmoid(mask)",
            "-",
            "# (B, C, T, F) -> (B, F, C, T)",
            "mask = mask.permute(0, 3, 1, 2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 684,
        "neg_line": [
            "-mask = torch.sigmoid(mask)",
            "-"
        ],
        "pos_line": [
            "+mask = torch.sigmoid(mask)"
        ],
        "core_change": "+mask = torch.sigmoid(mask) -mask = torch.sigmoid(mask) -",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "ad96d3c4fb2e9e192c1424f8ee65c46b77cc401e",
        "index": "fdc73ed2bf..bdec2a8bfc 100644",
        "commit_message": "[tune] Fix TB Memory Leak (#5629)\n\n\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "change API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def to_tf_values(result, path):",
            "",
            "class TFLogger(Logger):",
            "def _init(self):",
            "-        logger.info(",
            "-            \"Initializing TFLogger instead of TF2Logger. We recommend \"",
            "-            \"migrating to TF2.0. This class will be removed in the future.\")",
            "-        self._file_writer = tf.summary.FileWriter(self.logdir)",
            "+        logger.info(\"Initializing TFLogger instead of TF2Logger.\")",
            "+        self._file_writer = tf.compat.v1.summary.FileWriter(self.logdir)",
            "",
            "def on_result(self, result):",
            "tmp = result.copy()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"Initializing TFLogger instead of TF2Logger. We recommend \"), value='\"Initializing TFLogger instead of TF2Logger.\"')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=string, text=\"Initializing TFLogger instead of TF2Logger. We recommend \"), position=1)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2454243)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2454244)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2454245)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2454246)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2454247)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2454248)",
            "Delete(target_node=ASTNode(type=string, text=\"migrating to TF2.0. This class will be removed in the future.\"))",
            "Delete(target_node=ASTNode(type=concatenated_string))"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 12,
        "number": 685,
        "neg_line": [
            "-logger.info(",
            "-\"Initializing TFLogger instead of TF2Logger. We recommend \"",
            "-\"migrating to TF2.0. This class will be removed in the future.\")",
            "-self._file_writer = tf.summary.FileWriter(self.logdir)"
        ],
        "pos_line": [
            "+logger.info(\"Initializing TFLogger instead of TF2Logger.\")",
            "+self._file_writer = tf.compat.v1.summary.FileWriter(self.logdir)"
        ],
        "core_change": "-logger.info( -\"Initializing TFLogger instead of TF2Logger. We recommend \" -\"migrating to TF2.0. This class will be removed in the future.\") -self._file_writer = tf.summary.FileWriter(self.logdir) +logger.info(\"Initializing TFLogger instead of TF2Logger.\") +self._file_writer = tf.compat.v1.summary.FileWriter(self.logdir)",
        "core_API": "info"
    },
    {
        "commit_hash": "074241c9d5fc1551b77308e43b6d9ff175414344",
        "index": "5026ff8db..42ea91958 100644",
        "commit_message": "Add tensorboard support (pytorch, tf2+) (#124)\n\n* Add base of tensorboard support (pytorch, tf2+)\n\n* fix formatting\n\n* send protobuf message\n\n* add internal tbwatcher stub\n\n* Add tbdir watcher threads\n\n* Save files that are out of wandb files dir\n\n* add consumer, need logger\n\n* Connected up history, still debuggin tho\n\n* move tensorflow to framework dir\n\n* fix missing metrics with keras\n\n* consolidate summary in internal process\n\n* comment out keras summary test that is no longer valid\n\n* add directory watcher shutdown delay and flush\n\n* fixes from PR comments\n\n* fix indents\n\n* more PR feedback fixes and circleci timeout bump\n\n* fix histogram logging (on some versions of tf/tb)\n\n* chicken out and dont fail users who we cant support yet\n\n* disable console on windows for now\n",
        "file": "wandb.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def test_hook():",
            "tf_summary.scalar(\"c1\", c1)",
            "summary_op = tf_summary.merge_all()",
            "",
            "-        hook = wandb_tensorflow.WandbHook(summary_op, history=history, steps_per_log=1)",
            "+        hook = wandb.tensorflow.WandbHook(summary_op, history=history, steps_per_log=1)",
            "with MonitoredTrainingSession(hooks=[hook]) as sess:",
            "summary, acc = sess.run([summary_op, c1])",
            "history.add({})  # Flush the previous row.",
            "",
            "-    assert wandb_tensorflow.tf_summary_to_dict(summary) == {\"c1\": 42.0}",
            "+    assert wandb.tensorboard.tf_summary_to_dict(summary) == {\"c1\": 42.0}",
            "assert summaries_logged[0][\"c1\"] == 42.0"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2461389)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2461390)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2461391)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2461392)",
            "Update(target_node=ASTNode(type=identifier, text=wandb_tensorflow), value='wandb')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=wandb_tensorflow), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensorflow'), position=2, insert_id=2461393)",
            "Update(target_node=ASTNode(type=identifier, text=wandb_tensorflow), value='wandb')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=wandb_tensorflow), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensorboard'), position=2, insert_id=2461394)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 686,
        "neg_line": [
            "-hook = wandb_tensorflow.WandbHook(summary_op, history=history, steps_per_log=1)",
            "-assert wandb_tensorflow.tf_summary_to_dict(summary) == {\"c1\": 42.0}"
        ],
        "pos_line": [
            "+hook = wandb.tensorflow.WandbHook(summary_op, history=history, steps_per_log=1)",
            "+assert wandb.tensorboard.tf_summary_to_dict(summary) == {\"c1\": 42.0}"
        ],
        "core_change": "-hook = wandb_tensorflow.WandbHook(summary_op, history=history, steps_per_log=1) +hook = wandb.tensorflow.WandbHook(summary_op, history=history, steps_per_log=1) -assert wandb_tensorflow.tf_summary_to_dict(summary) == {\"c1\": 42.0} +assert wandb.tensorboard.tf_summary_to_dict(summary) == {\"c1\": 42.0}",
        "core_API": "scalar"
    },
    {
        "commit_hash": "351753aae55894591dafa81814eaa82a59687f09",
        "index": "6a226d2374..5af38ed9e9 100644",
        "commit_message": "[rllib] Remove dependency on TensorFlow (#4764)\n\n* remove hard tf dep\n\n* add test\n\n* comment fix\n\n* fix test\n\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "refactor fix",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class QNetwork(object):",
            "distributions and \\sigma are trainable variables which are expected to",
            "vanish along the training procedure",
            "\"\"\"",
            "+        import tensorflow.contrib.layers as layers",
            "+",
            "in_size = int(action_in.shape[1])",
            "",
            "epsilon_in = tf.random_normal(shape=[in_size])"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('import_statement', None), position=5, insert_id=2454422)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=6, insert_id=2454423)",
            "Insert(target_node=IN(type=import_statement), node=('import', 'import'), position=0, insert_id=2454424)",
            "Insert(target_node=IN(type=import_statement), node=('aliased_import', None), position=1, insert_id=2454425)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=in_size), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=aliased_import), node=('dotted_name', None), position=0, insert_id=2454426)",
            "Insert(target_node=IN(type=aliased_import), node=('as', 'as'), position=1, insert_id=2454427)",
            "Insert(target_node=IN(type=aliased_import), node=('identifier', 'layers'), position=2, insert_id=2454428)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorflow'), position=0, insert_id=2454429)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=1, insert_id=2454430)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'contrib'), position=2, insert_id=2454431)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=3, insert_id=2454432)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'layers'), position=4, insert_id=2454433)",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 688,
        "neg_line": [],
        "pos_line": [
            "+import tensorflow.contrib.layers as layers",
            "+"
        ],
        "core_change": "+import tensorflow.contrib.layers as layers +",
        "core_API": "random_normal"
    },
    {
        "commit_hash": "6422ee88a9a3bdfd93ec98d2def1e9f2b02eacab",
        "index": "118006eb..ebaf7aaa 100644",
        "commit_message": "Replace Distribution+TorchDistribution with a thin Distribution mixin (#769)\n\n* Sketch Distribution class as mixin\n\n* Remove TorchDistribution class\n\n* Simplify TransformedDistribution\n\n* Update torch wrappers for most distributions\n\n* Fix docs\n\n* Use dist.Reshape() to set extra_event_dims\n\n* Fix bugs in Reshape distribution\n\n* Fix rejector tests\n\n* Update _Subsample distribution\n\n* Use .reshape() method for extra_event_dims and sample_shape\n\n* Refactor Distribution -> TorchDistribution class hierarchy\n\n* Update docs\n\n* Fix json error in air.ipynb\n\n* Fix bugs in air.ipynb and abstract_infer.py\n\n* Fix distributions docs\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _Subsample(Distribution):",
            "self.subsample_size = subsample_size",
            "self.use_cuda = torch.Tensor.is_cuda if use_cuda is None else use_cuda",
            "",
            "-    def sample(self, sample_shape=None):",
            "+    def sample(self, sample_shape=torch.Size()):",
            "\"\"\"",
            ":returns: a random subsample of `range(size)`",
            ":rtype: torch.autograd.Variable of torch.LongTensor",
            "\"\"\"",
            "+        if sample_shape:",
            "+            raise NotImplementedError",
            "subsample_size = self.subsample_size",
            "if subsample_size is None or subsample_size > self.size:",
            "subsample_size = self.size"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=5, insert_id=754475)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=754476)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', 'sample_shape'), position=1, insert_id=754477)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=754478)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=754479)",
            "Insert(target_node=IN(type=block), node=('raise_statement', None), position=0, insert_id=754480)",
            "Insert(target_node=ASTNode(type=default_parameter), node=('call', None), position=2, insert_id=754481)",
            "Insert(target_node=IN(type=raise_statement), node=('raise', 'raise'), position=0, insert_id=754482)",
            "Insert(target_node=IN(type=raise_statement), node=('identifier', 'NotImplementedError'), position=1, insert_id=754483)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=754484)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=754485)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=754486)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=754487)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Size'), position=2, insert_id=754488)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=754489)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=754490)",
            "Delete(target_node=ASTNode(type=none, text=None))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 690,
        "neg_line": [
            "-def sample(self, sample_shape=None):"
        ],
        "pos_line": [
            "+def sample(self, sample_shape=torch.Size()):",
            "+if sample_shape:",
            "+raise NotImplementedError"
        ],
        "core_change": "-def sample(self, sample_shape=None): +def sample(self, sample_shape=torch.Size()): +if sample_shape: +raise NotImplementedError",
        "core_API": "Size"
    },
    {
        "commit_hash": "f68b4f7839834dd18c4753fd92f0e8280eb7b6a1",
        "index": "779b4f7125..8e366f7743 100644",
        "commit_message": "remove excess examples & fix docstrings\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def vander(",
            "increasing: Optional[bool] = False,",
            "out: Optional[torch.tensor] = None,",
            ") -> torch.tensor:",
            "-    return torch.vander(",
            "-        x, N=N, increasing=increasing",
            "-    )",
            "+    return torch.vander(x, N=N, increasing=increasing)",
            "",
            "",
            "vander.support_native_out = False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 691,
        "neg_line": [
            "-return torch.vander(",
            "-x, N=N, increasing=increasing",
            "-)"
        ],
        "pos_line": [
            "+return torch.vander(x, N=N, increasing=increasing)"
        ],
        "core_change": "-return torch.vander( -x, N=N, increasing=increasing -) +return torch.vander(x, N=N, increasing=increasing)",
        "core_API": "vander"
    },
    {
        "commit_hash": "31ed933cb21eb7580b9699b7084e60138356af8d",
        "index": "0db73d8db2..996a1f28c7 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def imag(",
            "input: Union[tf.Tensor, tf.Variable],",
            "/,",
            "*,",
            "-    out: Optional[Union[tf.Tensor,tf.Variable]] = None,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "return tf.math.imag(input, name=None)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 692,
        "neg_line": [
            "-out: Optional[Union[tf.Tensor,tf.Variable]] = None,"
        ],
        "pos_line": [
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,"
        ],
        "core_change": "-out: Optional[Union[tf.Tensor,tf.Variable]] = None, +out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
        "core_API": "imag"
    },
    {
        "commit_hash": "18674e2f037953001f007e4cd5ab176048093e88",
        "index": "c2d9345..db8e5b8 100644",
        "commit_message": "experimental.py Apple MPS device fix (#8121)\n\n* experimental.py Apple MPS fix\n\nMay resolve https://github.com/ultralytics/yolov5/issues/8102\n\n* Update experimental.py\n\n* Update experimental.py\n",
        "file": "yolov5.txt.json",
        "label": "yes",
        "comments": "change param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Ensemble(nn.ModuleList):",
            "",
            "",
            "def attempt_load(weights, device=None, inplace=True, fuse=True):",
            "+    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "from models.yolo import Detect, Model",
            "",
            "-    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "-        ckpt = torch.load(attempt_download(w), map_location=device)",
            "-        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model",
            "+        ckpt = torch.load(attempt_download(w), map_location='cpu')  # load",
            "+        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model",
            "model.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode",
            "",
            "# Compatibility updates"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1294050)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1294051)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1294052)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=1294053)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', \"'cpu'\"), position=2, insert_id=1294054)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1294055)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='to')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1294056)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=1294057)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1294058)",
            "Delete(target_node=ASTNode(type=identifier, text=device))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 693,
        "neg_line": [
            "-# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "-ckpt = torch.load(attempt_download(w), map_location=device)",
            "-ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model"
        ],
        "pos_line": [
            "+# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "+ckpt = torch.load(attempt_download(w), map_location='cpu')  # load",
            "+ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model"
        ],
        "core_change": "+# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a -# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a -ckpt = torch.load(attempt_download(w), map_location=device) -ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model +ckpt = torch.load(attempt_download(w), map_location='cpu')  # load +ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model",
        "core_API": "load"
    },
    {
        "commit_hash": "600548d5ba3e9c4faed23768c32ec9bb7b4fa054",
        "index": "e4f185a3c..c654387e5 100644",
        "commit_message": "fix the calcluation of the output dim after 6 subsampmling\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Conv2dSubsampling6(torch.nn.Module):",
            "torch.nn.ReLU(),",
            ")",
            "self.out = torch.nn.Sequential(",
            "-            torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 3), odim),",
            "+            torch.nn.Linear(odim * (((idim - 1) // 2 - 2) // 3), odim),",
            "PositionalEncoding(odim, dropout_rate),",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=1), value='2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 694,
        "neg_line": [
            "-torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 3), odim),"
        ],
        "pos_line": [
            "+torch.nn.Linear(odim * (((idim - 1) // 2 - 2) // 3), odim),"
        ],
        "core_change": "-torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 3), odim), +torch.nn.Linear(odim * (((idim - 1) // 2 - 2) // 3), odim),",
        "core_API": "ReLU"
    },
    {
        "commit_hash": "27f0a29d6c97909247e2db88912fc374ccd4af94",
        "index": "5cecd956..63c24149 100644",
        "commit_message": "addressed pr comments: added docs, fixed typo, added batch_dim test\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "remove print",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NestedMapDataTest(TestCase):",
            "tr = poutine.trace(self.model)(self.means, self.stds)",
            "for name in tr.keys():",
            "if tr[name][\"type\"] == \"sample\":",
            "-                print(name, tr[name][\"scale\"])",
            "self.assertTrue(tr[name][\"scale\"] == 4.0 * 2.0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=767137)",
            "Delete(target_node=ASTNode(type=identifier, text=print))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=tr))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text=\"scale\"))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 698,
        "neg_line": [
            "-print(name, tr[name][\"scale\"])"
        ],
        "pos_line": [],
        "core_change": "-print(name, tr[name][\"scale\"])",
        "core_API": "trace"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "0759f895..dddbc019 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "remove constraint not clear reason",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def model(x, is_train, reuse):",
            ")",
            "n = tl.layers.FlattenLayer(n, name='flatten2')",
            "n = tl.layers.DenseLayer(n, n_units=1024, act=tf.nn.relu, name='out1')",
            "-        n = tl.layers.DenseLayer(n, n_units=10, act=tf.identity, name='out2')",
            "+        n = tl.layers.DenseLayer(n, n_units=10, name='out2')",
            "return n, s"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=act))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 699,
        "neg_line": [
            "-n = tl.layers.DenseLayer(n, n_units=10, act=tf.identity, name='out2')"
        ],
        "pos_line": [
            "+n = tl.layers.DenseLayer(n, n_units=10, name='out2')"
        ],
        "core_change": "-n = tl.layers.DenseLayer(n, n_units=10, act=tf.identity, name='out2') +n = tl.layers.DenseLayer(n, n_units=10, name='out2')",
        "core_API": "FlattenLayer"
    },
    {
        "commit_hash": "26497d119911d0ab481c68cc2517ae121ce1db9a",
        "index": "3dae24b28..46360d1dd 100644",
        "commit_message": "fix tests\n\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def ids_tensor(shape, vocab_size, rng=None, name=None, dtype=tf.int32):",
            "for _ in range(total_dims):",
            "values.append(rng.randint(0, vocab_size - 1))",
            "",
            "-    return tf.constant(values, shape=shape, dtype=dtype)",
            "+    output = tf.constant(values,",
            "+                         shape=shape,",
            "+                         dtype=dtype if dtype is not None else tf.int32)",
            "+",
            "+    return output",
            "",
            "",
            "class TFModelUtilsTest(unittest.TestCase):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2386330)",
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=4, insert_id=2386331)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2386332)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2386333)",
            "Insert(target_node=IN(type=return_statement), node=('identifier', 'output'), position=1, insert_id=2386334)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'output'), position=0, insert_id=2386335)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2386336)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('conditional_expression', None), position=2, insert_id=2386337)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=identifier, text=dtype), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=2386338)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=2386339)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=2386340)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=4, insert_id=2386341)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'dtype'), position=0, insert_id=2386342)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=2386343)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=2386344)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=2386345)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2386346)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2386347)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2386348)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 700,
        "neg_line": [
            "-return tf.constant(values, shape=shape, dtype=dtype)"
        ],
        "pos_line": [
            "+output = tf.constant(values,",
            "+shape=shape,",
            "+dtype=dtype if dtype is not None else tf.int32)",
            "+",
            "+return output"
        ],
        "core_change": "-return tf.constant(values, shape=shape, dtype=dtype) +output = tf.constant(values, +shape=shape, +dtype=dtype if dtype is not None else tf.int32) + +return output",
        "core_API": "append"
    },
    {
        "commit_hash": "324c84e8d9038dc9c707112dda7b29f5e5e36dd6",
        "index": "4952f4dda..acdec76dd 100644",
        "commit_message": "[Audio datasets] Adapting all audio datasets (#3081)\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* Update src/datasets/utils/resources/readme_structure.yaml\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* correct\n\n* correct 2\n\n* Update datasets/covost2/README.md\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* Fix typo\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "add attribute parama",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class LibrispeechASR(datasets.GeneratorBasedBuilder):",
            "\"speaker_id\": speaker_id,",
            "\"chapter_id\": chapter_id,",
            "\"file\": os.path.join(transcript_dir_path, audio_file),",
            "+                        \"audio\": os.path.join(transcript_dir_path, audio_file),",
            "\"text\": transcript,",
            "}",
            "key += 1"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1782424)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1782425)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1782426)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1782427)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1782428)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1782429)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1782430)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'join'), position=2, insert_id=1782431)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1782432)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'transcript_dir_path'), position=1, insert_id=1782433)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1782434)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'audio_file'), position=3, insert_id=1782435)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1782436)",
            "Insert(target_node=IN(type=attribute), node=('string', '\"audio\"'), position=0, insert_id=1782437)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=1782438)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=1782439)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'path'), position=3, insert_id=1782440)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=0, insert_id=1782441)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'os'), position=1, insert_id=1782442)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 702,
        "neg_line": [],
        "pos_line": [
            "+\"audio\": os.path.join(transcript_dir_path, audio_file),"
        ],
        "core_change": "+\"audio\": os.path.join(transcript_dir_path, audio_file),",
        "core_API": "join"
    },
    {
        "commit_hash": "26764145a83226d93a537e87cb62f4106e2b868d",
        "index": "079931e345..7e645eb914 100644",
        "commit_message": "Fixed positional and keyword arguments and added missing out argument to backend implementations (#9660)\n\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def broadcast_arrays(*arrays: torch.Tensor) -> List[torch.Tensor]:",
            "{\"1.11.0 and below\": (\"uint8\", \"uint16\", \"uint32\", \"uint64\")}, backend_version",
            ")",
            "def broadcast_to(",
            "-    x: torch.Tensor, shape: Union[ivy.NativeShape, Sequence[int]]",
            "+    x: torch.Tensor,",
            "+    /,",
            "+    shape: Union[ivy.NativeShape, Sequence[int]],",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if x.ndim > len(shape):",
            "return torch.broadcast_to(x.reshape(-1), shape)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('positional_separator', None), position=3, insert_id=274137)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=274138)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=274139)",
            "Insert(target_node=ASTNode(type=parameters), node=('keyword_separator', None), position=7, insert_id=274140)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=274141)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=9, insert_id=274142)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=10, insert_id=274143)",
            "Insert(target_node=IN(type=positional_separator), node=('/', '/'), position=0, insert_id=274144)",
            "Insert(target_node=IN(type=keyword_separator), node=('*', '*'), position=0, insert_id=274145)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('identifier', 'out'), position=0, insert_id=274146)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=274147)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=274148)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=274149)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('none', 'None'), position=4, insert_id=274150)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=274151)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Optional'), position=0, insert_id=274152)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=274153)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=274154)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=274155)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=274156)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=274157)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=274158)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 705,
        "neg_line": [
            "-x: torch.Tensor, shape: Union[ivy.NativeShape, Sequence[int]]"
        ],
        "pos_line": [
            "+x: torch.Tensor,",
            "+/,",
            "+shape: Union[ivy.NativeShape, Sequence[int]],",
            "+*,",
            "+out: Optional[torch.Tensor] = None,"
        ],
        "core_change": "-x: torch.Tensor, shape: Union[ivy.NativeShape, Sequence[int]] +x: torch.Tensor, +/, +shape: Union[ivy.NativeShape, Sequence[int]], +*, +out: Optional[torch.Tensor] = None,",
        "core_API": "broadcast_to"
    },
    {
        "commit_hash": "e24d9433b15969ee0bd75ccbb7f3f6b88eca4a41",
        "index": "7813f4d..b9b6265 100644",
        "commit_message": "Add support for HuggingFace's TensorFlow models (#127)\n\n* added support for for HuggingFace's TensorFlow models\n\n* added notebook for HuggingFace's tensorflow bert model\n\n* change nebullvm name in logs\n\n* Add optimized model details + warning if static shape is used for HF models (#1)\n\n* add optimized model type info\n\n* fix tvm issue\n\n* edit dockerfile and add image auto building\n\n* add docker installation on azure pipeline\n\n* fix bug in neural compressor output shape\n\n* add support for openvino with python 3.10\n\n* add build docker image to azure pipelines\n\n* revert docker build from az pipelines and edit format of the optimization results\n\n* revert docker build from az pipelines\n\n* added tabulate to setup.py and general fixes\n",
        "file": "nebullvm.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFLiteBackendInferenceLearner(TensorflowBaseInferenceLearner):",
            "self.interpreter.set_tensor(i, input_tensor)",
            "self.interpreter.invoke()",
            "return tuple(",
            "-            self.interpreter.get_tensor(output_detail[\"index\"])",
            "+            tf.convert_to_tensor(",
            "+                self.interpreter.get_tensor(output_detail[\"index\"])",
            "+            )",
            "for output_detail in output_details",
            ")"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2123610)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2123611)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2123612)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2123613)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'convert_to_tensor'), position=2, insert_id=2123614)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2123615)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2123616)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 707,
        "neg_line": [
            "-self.interpreter.get_tensor(output_detail[\"index\"])"
        ],
        "pos_line": [
            "+tf.convert_to_tensor(",
            "+self.interpreter.get_tensor(output_detail[\"index\"])",
            "+)"
        ],
        "core_change": "-self.interpreter.get_tensor(output_detail[\"index\"]) +tf.convert_to_tensor( +self.interpreter.get_tensor(output_detail[\"index\"]) +)",
        "core_API": "set_tensor"
    },
    {
        "commit_hash": "be39dbdf37d57776f19e3b64a3a2bffc10d2ee93",
        "index": "81e59e2c..2103314d 100755",
        "commit_message": "fix deprecation about dropout; fix Keras compatibility in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            ".Conv2D('conv3.1', filters=128, padding='VALID') \\",
            ".Conv2D('conv3.2', filters=128, padding='VALID') \\",
            ".FullyConnected('fc0', 1024 + 512, activation=tf.nn.relu) \\",
            "-                .tf.nn.dropout(keep_prob) \\",
            "+                .Dropout(rate=drop_rate) \\",
            ".FullyConnected('fc1', 512, activation=tf.nn.relu) \\",
            ".FullyConnected('linear', out_dim=self.cifar_classnum)()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='Dropout')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2279022)",
            "Update(target_node=ASTNode(type=identifier, text=keep_prob), value='rate')",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=keep_prob), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2279023)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'drop_rate'), position=2, insert_id=2279024)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 710,
        "neg_line": [
            "-.tf.nn.dropout(keep_prob) \\"
        ],
        "pos_line": [
            "+.Dropout(rate=drop_rate) \\"
        ],
        "core_change": "-.tf.nn.dropout(keep_prob) \\ +.Dropout(rate=drop_rate) \\",
        "core_API": "dropout"
    },
    {
        "commit_hash": "21d6bb294d4d69aa2ee355bd807a1232c8e749d1",
        "index": "219325537..012538a7d 100644",
        "commit_message": "force use identity activation in branchformer conv mod + fix main_params doc\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "change value",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ConvolutionalSpatialGatingUnit(torch.nn.Module):",
            ")",
            "",
            "self.norm = norm_class(channels, **norm_args)",
            "-        self.activation = activation",
            "+        self.activation = torch.nn.Identity()",
            "",
            "self.dropout = torch.nn.Dropout(dropout_rate)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1334299)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1334300)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1334301)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1334302)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1334303)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Identity'), position=2, insert_id=1334304)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1334305)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1334306)",
            "Update(target_node=ASTNode(type=identifier, text=activation), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=activation), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1334307)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1334308)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 713,
        "neg_line": [
            "-self.activation = activation"
        ],
        "pos_line": [
            "+self.activation = torch.nn.Identity()"
        ],
        "core_change": "-self.activation = activation +self.activation = torch.nn.Identity()",
        "core_API": "Identity"
    },
    {
        "commit_hash": "242085f9ce05529315f49176091dbe36512026c0",
        "index": "cef864520..f185c2db7 100644",
        "commit_message": "Private Tensors (#2709)\n\n* ADD Private Tensor into syft lib\n\n* Allow to serialize Private Tensor\n\n* Change native allow_to_get parameters\n\n* - Overload allow_to_get method\n - Add user parameters at get method (native.py, pointer_tensor.py, object_pointer.py)\n - Modify allow_to_get method at native.py\n - Implement allow_to_get method at PrivateTensor\n\n* Fix allowed_to_get method\n\n* Update virtual_worker test\n\n* Update fit method\n\n* ADD Private Tensor experimental notebook\n\n* Register Private Tensor at hook\n\n* - Update docstrings\n- Fix allow_to_get  method\n- Fix private_tensor method\n\n* ADD get_class_attributes method\n\n* Overload torch module methods\n\n* ADD Private Tensor unit tests\n\n* Fix flake8 issues\n\n* Fix code style\n\n* Fix unit tests coverage\n\n* Change proto reference\n\n* Update PrivateTensor simplify method\n\n* Overload Private Tensor methods\n- Create custom _get_hooked_method (_get_hooked_private_method).\n- Handle PrivateTensors with _get_hooked_private_method.\n- Add parents/command attributes at private tensor.\n\n* ADD new Exception -> SendNotPermittedError\n\n* - Verify permissions during send()/get() methods.\n   We need to verify user credentials during send()/get() tensor commands to keep it safe.\n- REFACTORY/RENAME allowed_to_get(user) -> allow(user)\n   Now, we're using this method to verify permissions in an generic context, not only for get() commands.\n\n* Fix CI tests\n\n* Fix coverage\n\n* Fix detail method to enable remote operations.\n  Change data structure used to store allowed users (list -> tuple).\n\n* Fix proto requirement reference\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_obj_not_found(workers):",
            "",
            "def test_get_not_permitted(workers):",
            "bob = workers[\"bob\"]",
            "-    with patch.object(torch.Tensor, \"allowed_to_get\") as mock_allowed_to_get:",
            "+    x = torch.tensor([1, 2, 3, 4, 5]).send(bob)",
            "+    with patch.object(torch.Tensor, \"allow\") as mock_allowed_to_get:",
            "mock_allowed_to_get.return_value = False",
            "-        x = torch.tensor([1, 2, 3, 4, 5]).send(bob)",
            "with pytest.raises(GetNotPermittedError):",
            "x.get()",
            "mock_allowed_to_get.assert_called_once()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=3)",
            "Update(target_node=ASTNode(type=string, text=\"allowed_to_get\"), value='\"allow\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 716,
        "neg_line": [
            "-with patch.object(torch.Tensor, \"allowed_to_get\") as mock_allowed_to_get:",
            "-x = torch.tensor([1, 2, 3, 4, 5]).send(bob)"
        ],
        "pos_line": [
            "+x = torch.tensor([1, 2, 3, 4, 5]).send(bob)",
            "+with patch.object(torch.Tensor, \"allow\") as mock_allowed_to_get:"
        ],
        "core_change": "-with patch.object(torch.Tensor, \"allowed_to_get\") as mock_allowed_to_get: +x = torch.tensor([1, 2, 3, 4, 5]).send(bob) +with patch.object(torch.Tensor, \"allow\") as mock_allowed_to_get: -x = torch.tensor([1, 2, 3, 4, 5]).send(bob)",
        "core_API": "object"
    },
    {
        "commit_hash": "0f78e37d6f2bfa47d44fa63ac83a182be34be046",
        "index": "9933f4177..d6ad0a5e1 100644",
        "commit_message": "Fix for ST\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CustomConverter(object):",
            "xs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0).to(device, dtype=self.dtype)",
            "",
            "ilens = torch.from_numpy(ilens).to(device)",
            "-        # NOTE: this is for multi-task learning (e.g., speech translation)",
            "-        ys_pad = pad_list([torch.from_numpy(np.array(y[0]) if isinstance(y, tuple) else y).long()",
            "+        # NOTE: this is for multi-output (e.g., speech translation)",
            "+        ys_pad = pad_list([torch.from_numpy(np.array(y[0][:]) if isinstance(y, tuple) else y).long()",
            "for y in ys], self.ignore_id).to(device)",
            "",
            "return xs_pad, ilens, ys_pad"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=ASTNode(type=subscript), node=('[', '['), position=1, insert_id=163908)",
            "Insert(target_node=ASTNode(type=subscript), node=('slice', None), position=2, insert_id=163909)",
            "Insert(target_node=ASTNode(type=subscript), node=(']', ']'), position=3, insert_id=163910)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=163911)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 720,
        "neg_line": [
            "-# NOTE: this is for multi-task learning (e.g., speech translation)",
            "-ys_pad = pad_list([torch.from_numpy(np.array(y[0]) if isinstance(y, tuple) else y).long()"
        ],
        "pos_line": [
            "+# NOTE: this is for multi-output (e.g., speech translation)",
            "+ys_pad = pad_list([torch.from_numpy(np.array(y[0][:]) if isinstance(y, tuple) else y).long()"
        ],
        "core_change": "-# NOTE: this is for multi-task learning (e.g., speech translation) -ys_pad = pad_list([torch.from_numpy(np.array(y[0]) if isinstance(y, tuple) else y).long() +# NOTE: this is for multi-output (e.g., speech translation) +ys_pad = pad_list([torch.from_numpy(np.array(y[0][:]) if isinstance(y, tuple) else y).long()",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "ad7115add4e391007853fa7f465448af8e5ec306",
        "index": "846da167..36cc74f2 100644",
        "commit_message": "Scale Factor Fix (#2039)\n\n* fix (scale_factor): use new_tensor(scale_factor) in case it is numpy array\n\n* reformat (models): reformat with flake8, yapf, and isort to pass CI\n\n* fix (bbox_mapping): fix scale_factor bug in bbox_mapping\n\n* fix img_meta bug\n\nCo-authored-by: beansi <zhangwenwei@sensetime.com>\n",
        "file": "mmdetection.txt.json",
        "label": "no",
        "comments": "use custom API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BBoxHead(nn.Module):",
            "if isinstance(scale_factor, float):",
            "bboxes /= scale_factor",
            "else:",
            "-                scale_factor = torch.from_numpy(scale_factor).to(bboxes.device)",
            "+                scale_factor = bboxes.new_tensor(scale_factor)",
            "bboxes = (bboxes.view(bboxes.size(0), -1, 4) /",
            "scale_factor).view(bboxes.size()[0], -1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='bboxes')",
            "Update(target_node=ASTNode(type=identifier, text=from_numpy), value='new_tensor')",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=bboxes))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 721,
        "neg_line": [
            "-scale_factor = torch.from_numpy(scale_factor).to(bboxes.device)"
        ],
        "pos_line": [
            "+scale_factor = bboxes.new_tensor(scale_factor)"
        ],
        "core_change": "-scale_factor = torch.from_numpy(scale_factor).to(bboxes.device) +scale_factor = bboxes.new_tensor(scale_factor)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "fc2d4a7736384930044fd4ad2684f47cf6602700",
        "index": "a82fa356..4dd0237d 100644",
        "commit_message": "Fixed tests and docs (#654)\n\n* Fixed tests\n\n* Fixed doc error and warnings\n\n* Fixed lint\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "test",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestAffine2d:",
            "",
            "def test_affine_scale(self, device):",
            "torch.manual_seed(0)",
            "-        scale_factor = torch.rand(1, device=device) * 2.0",
            "+        _scale_factor = torch.rand(1, device=device) * 2.0",
            "+        scale_factor = torch.stack([_scale_factor, _scale_factor], dim=1)",
            "input = torch.rand(1, 2, 3, 4, device=device)",
            "",
            "transform = kornia.Affine(scale_factor=scale_factor).to(device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=437261)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=437262)",
            "Update(target_node=ASTNode(type=identifier, text=scale_factor), value='_scale_factor')",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'scale_factor'), position=0, insert_id=437263)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=437264)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=437265)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=437266)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=437267)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=437268)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=437269)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stack'), position=2, insert_id=437270)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=437271)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=1, insert_id=437272)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=437273)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=437274)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=437275)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=437276)",
            "Insert(target_node=IN(type=list), node=('identifier', '_scale_factor'), position=1, insert_id=437277)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=437278)",
            "Insert(target_node=IN(type=list), node=('identifier', '_scale_factor'), position=3, insert_id=437279)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=437280)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=437281)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=437282)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=437283)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 722,
        "neg_line": [
            "-scale_factor = torch.rand(1, device=device) * 2.0"
        ],
        "pos_line": [
            "+_scale_factor = torch.rand(1, device=device) * 2.0",
            "+scale_factor = torch.stack([_scale_factor, _scale_factor], dim=1)"
        ],
        "core_change": "-scale_factor = torch.rand(1, device=device) * 2.0 +_scale_factor = torch.rand(1, device=device) * 2.0 +scale_factor = torch.stack([_scale_factor, _scale_factor], dim=1)",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "ae8f9ab98dd0c418007841e51094b99df6f1a86a",
        "index": "aaec1a3..c79424b 100644",
        "commit_message": "Fix example about synthetic benchmark elastic (#2265)\n\nSigned-off-by: jiaqianjing <jiaqianjing@gmail.com>\n",
        "file": "horovod.txt.json",
        "label": "yes",
        "comments": "change API call for version fix",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def run_benchmark(state):",
            "",
            "",
            "def on_state_reset():",
            "-    tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())",
            "+    opt.lr.assign(lr * hvd.size())",
            "",
            "",
            "state = hvd.elastic.TensorFlowKerasState(model, opt, img_secs=[], iter=0, batch=0, warm=False)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=backend), value='assign')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='opt')",
            "Update(target_node=ASTNode(type=identifier, text=keras), value='lr')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=set_value))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=optimizer))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=lr))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 725,
        "neg_line": [
            "-tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())"
        ],
        "pos_line": [
            "+opt.lr.assign(lr * hvd.size())"
        ],
        "core_change": "-tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size()) +opt.lr.assign(lr * hvd.size())",
        "core_API": "set_value"
    },
    {
        "commit_hash": "1d4823c0ec446e93d00df8ca654db4b45b63b3d4",
        "index": "3017dc8512..b915bb49e6 100644",
        "commit_message": "[RLlib] Add testing framework_iterator. (#7852)\n\n* Add testing framework_iterator.\n\n* LINT.\n\n* WIP.\n\n* Fix and LINT.\n\n* LINT fix.\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "remove check",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def do_test_log_likelihood(run,",
            "prev_reward_batch=np.array([prev_r]))",
            "check(np.exp(logp), expected_prob, atol=0.2)",
            "",
            "-        if eager_ctx:",
            "-            eager_ctx.__exit__(None, None, None)",
            "-",
            "",
            "class TestComputeLogLikelihood(unittest.TestCase):",
            "def test_dqn(self):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'class'), position=4, insert_id=2146574)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=eager_ctx))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=eager_ctx))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__exit__))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parameters))",
            "Delete(target_node=ASTNode(type=class, text=class))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 726,
        "neg_line": [
            "-if eager_ctx:",
            "-eager_ctx.__exit__(None, None, None)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-if eager_ctx: -eager_ctx.__exit__(None, None, None) -",
        "core_API": "array"
    },
    {
        "commit_hash": "38fd8380f77ba5412eca1698ad9eabe8234c01ce",
        "index": "0b28e6a..7cdccd2 100644",
        "commit_message": "fix ndc/screen problem in blender/llff (#39)\n\nSummary:\nX-link: https://github.com/fairinternal/pytorch3d/pull/39\n\nBlender and LLFF cameras were sending screen space focal length and principal point to a camera init function expecting NDC\n\nReviewed By: shapovalov\n\nDifferential Revision: D37788686\n\nfbshipit-source-id: 2ddf7436248bc0d174eceb04c288b93858138582\n\n",
        "file": "pytorch3d.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _interpret_blender_cameras(",
            "",
            "Rpt3, Tpt3 = mtx[:, :3].split([3, 1], dim=0)",
            "",
            "-        focal_length_pt3 = torch.FloatTensor([[-focal, focal]])",
            "-        principal_point_pt3 = torch.FloatTensor([[W / 2, H / 2]])",
            "+        focal_length_pt3 = torch.FloatTensor([[focal, focal]])",
            "+        principal_point_pt3 = torch.FloatTensor([[0.0, 0.0]])",
            "",
            "cameras = PerspectiveCameras(",
            "focal_length=focal_length_pt3,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('float', '0.0'), position=1, insert_id=910371)",
            "Insert(target_node=ASTNode(type=list), node=('float', '0.0'), position=4, insert_id=910372)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=identifier, text=focal), position=1)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=unary_operator))",
            "Delete(target_node=ASTNode(type=identifier, text=W))",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=identifier, text=H))",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 727,
        "neg_line": [
            "-focal_length_pt3 = torch.FloatTensor([[-focal, focal]])",
            "-principal_point_pt3 = torch.FloatTensor([[W / 2, H / 2]])"
        ],
        "pos_line": [
            "+focal_length_pt3 = torch.FloatTensor([[focal, focal]])",
            "+principal_point_pt3 = torch.FloatTensor([[0.0, 0.0]])"
        ],
        "core_change": "-focal_length_pt3 = torch.FloatTensor([[-focal, focal]]) -principal_point_pt3 = torch.FloatTensor([[W / 2, H / 2]]) +focal_length_pt3 = torch.FloatTensor([[focal, focal]]) +principal_point_pt3 = torch.FloatTensor([[0.0, 0.0]])",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "f503c1d80072895089dcc23f3345d3df471ca7e6",
        "index": "a636b95f77..507d998810 100644",
        "commit_message": "Type promotion fixes (#2516)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def vector_to_skew_symmetric_matrix(",
            "a2s = vector_expanded[..., 1:2, :]",
            "a3s = vector_expanded[..., 2:3, :]",
            "# BS x 1 x 1",
            "-    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)",
            "+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device, dtype=vector.dtype)",
            "# BS x 1 x 3",
            "row1 = torch.cat((zs, -a3s, a2s), -1)",
            "row2 = torch.cat((a3s, zs, -a1s), -1)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=346287)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=346288)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=346289)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=346290)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=346291)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'vector'), position=0, insert_id=346292)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=346293)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=346294)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 728,
        "neg_line": [
            "-zs = torch.zeros(batch_shape + [1, 1], device=vector.device)"
        ],
        "pos_line": [
            "+zs = torch.zeros(batch_shape + [1, 1], device=vector.device, dtype=vector.dtype)"
        ],
        "core_change": "-zs = torch.zeros(batch_shape + [1, 1], device=vector.device) +zs = torch.zeros(batch_shape + [1, 1], device=vector.device, dtype=vector.dtype)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "bd399f82bda1ab6995603c3996d5ec2b7092646b",
        "index": "28c02d52..0102d489 100644",
        "commit_message": "Fix CUDA tests on master (#1542)\n\n* Fix CUDA tests on master\n\n* address comments\n\n* change to range(11)\n\n* remove unused import\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from pyro.ops.einsum import contract",
            "def _finfo(tensor):",
            "# This can be replaced with torch.finfo once it is available",
            "# https://github.com/pytorch/pytorch/issues/10742",
            "-    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype)",
            "+    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype)",
            "",
            "",
            "def _check_batch_dims_are_sensible(output_dims, nonoutput_ordinal):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=731265)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=731266)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=731267)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=731268)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"cpu\"'), position=2, insert_id=731269)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 730,
        "neg_line": [
            "-return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype)"
        ],
        "pos_line": [
            "+return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype)"
        ],
        "core_change": "-return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype) +return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype)",
        "core_API": "finfo"
    },
    {
        "commit_hash": "5ca81678f2f9b42287619bd803f44ffb6031195a",
        "index": "50b39528..04a82d17 100755",
        "commit_message": "[zero] fix missed subclasses partitioning bug (#1135)\n\n* fix missed subclassed partitioning bug\n\n* fix on exit\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "no",
        "comments": "custom API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class InsertPostInitMethodToModuleSubClasses(object):",
            "cls.__init__ = cls._old_init",
            "",
            "# Replace .__init__() for all existing subclasses of torch.nn.Module",
            "-        for subclass in torch.nn.modules.module.Module.__subclasses__():",
            "+        for subclass in get_all_subclasses(torch.nn.modules.module.Module):",
            "_disable_class(subclass)",
            "",
            "# Replace .__init__() for future subclasses of torch.nn.Module"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'get_all_subclasses'), position=0, insert_id=82771)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=82772)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=82773)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=82774)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__subclasses__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 731,
        "neg_line": [
            "-for subclass in torch.nn.modules.module.Module.__subclasses__():"
        ],
        "pos_line": [
            "+for subclass in get_all_subclasses(torch.nn.modules.module.Module):"
        ],
        "core_change": "-for subclass in torch.nn.modules.module.Module.__subclasses__(): +for subclass in get_all_subclasses(torch.nn.modules.module.Module):",
        "core_API": "__subclasses__"
    },
    {
        "commit_hash": "1e4055da7d14355821d2d72b40b1e6d463fe4ecd",
        "index": "78332d17..8c08c044 100644",
        "commit_message": "Fix Tracing mode TS export for LayerNorm layer\n\nSummary: When under TorchScript Tracing (instead of only doing this for Scripting) we set `export=True` for `LayerNorm` as `FusedLayerNorm `doesn't work with JIT yet (see `torch.jit.unused decorator`).\n\nReviewed By: cndn\n\nDifferential Revision: D33103054\n\nfbshipit-source-id: f8c24a4a30a89dd4c70b19362fd60c51fcb9a1f0\n\n",
        "file": "fairseq.txt.json",
        "label": "yes",
        "comments": "add condition check for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "try:",
            "with torch.cuda.device(x.device):",
            "return super().forward(x)",
            "",
            "+",
            "except ImportError:",
            "has_fused_layernorm = False",
            "",
            "",
            "def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):",
            "-    if torch.jit.is_scripting():",
            "+    if torch.jit.is_scripting() or torch.jit.is_tracing():",
            "export = True",
            "if not export and torch.cuda.is_available() and has_fused_layernorm:",
            "return FusedLayerNorm(normalized_shape, eps, elementwise_affine)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=205104)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=205105)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=2, insert_id=205106)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=205107)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=205108)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=205109)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=205110)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_tracing'), position=2, insert_id=205111)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=205112)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=205113)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=205114)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=205115)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'jit'), position=2, insert_id=205116)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 733,
        "neg_line": [
            "-if torch.jit.is_scripting():"
        ],
        "pos_line": [
            "+",
            "+if torch.jit.is_scripting() or torch.jit.is_tracing():"
        ],
        "core_change": "+ -if torch.jit.is_scripting(): +if torch.jit.is_scripting() or torch.jit.is_tracing():",
        "core_API": "device"
    },
    {
        "commit_hash": "37e98945728f9961fd33d598a954358d79988c28",
        "index": "ff19f221..412e677f 100644",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "file": "tensorpack.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def class_balanced_sigmoid_cross_entropy(logits, label, name='cross_entropy_loss",
            "cost = tf.nn.weighted_cross_entropy_with_logits(logits, y, pos_weight)",
            "cost = tf.reduce_mean(cost * (1 - beta), name=name)",
            "",
            "-    #logstable = tf.log(1 + tf.exp(-tf.abs(z)))",
            "-    # loss_pos = -beta * tf.reduce_mean(-y *",
            "-    #(logstable - tf.minimum(0.0, z)))",
            "-    # loss_neg = (1. - beta) * tf.reduce_mean((y - 1.) *",
            "-    #(logstable + tf.maximum(z, 0.0)))",
            "-    #cost = tf.sub(loss_pos, loss_neg, name=name)",
            "+    # logstable = tf.log(1 + tf.exp(-tf.abs(z)))",
            "+    # loss_pos = -beta * tf.reduce_mean(-y * (logstable - tf.minimum(0.0, z)))",
            "+    # loss_neg = (1. - beta) * tf.reduce_mean((y - 1.) * (logstable + tf.maximum(z, 0.0)))",
            "+    # cost = tf.sub(loss_pos, loss_neg, name=name)",
            "return cost"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=name), position=7)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type==, text==), position=8)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=9, insert_id=2308350)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=\", text='), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=cross_entropy_loss), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=cost), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=4)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=cost), position=5)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=6)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=7)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=return), position=8)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=default_parameter))"
        ],
        "plus_line": 4,
        "minus_line": 6,
        "AST_diff_line": 14,
        "number": 734,
        "neg_line": [
            "-#logstable = tf.log(1 + tf.exp(-tf.abs(z)))",
            "-# loss_pos = -beta * tf.reduce_mean(-y *",
            "-#(logstable - tf.minimum(0.0, z)))",
            "-# loss_neg = (1. - beta) * tf.reduce_mean((y - 1.) *",
            "-#(logstable + tf.maximum(z, 0.0)))",
            "-#cost = tf.sub(loss_pos, loss_neg, name=name)"
        ],
        "pos_line": [
            "+# logstable = tf.log(1 + tf.exp(-tf.abs(z)))",
            "+# loss_pos = -beta * tf.reduce_mean(-y * (logstable - tf.minimum(0.0, z)))",
            "+# loss_neg = (1. - beta) * tf.reduce_mean((y - 1.) * (logstable + tf.maximum(z, 0.0)))",
            "+# cost = tf.sub(loss_pos, loss_neg, name=name)"
        ],
        "core_change": "-#logstable = tf.log(1 + tf.exp(-tf.abs(z))) -# loss_pos = -beta * tf.reduce_mean(-y * -#(logstable - tf.minimum(0.0, z))) -# loss_neg = (1. - beta) * tf.reduce_mean((y - 1.) * -#(logstable + tf.maximum(z, 0.0))) -#cost = tf.sub(loss_pos, loss_neg, name=name) +# logstable = tf.log(1 + tf.exp(-tf.abs(z))) +# loss_pos = -beta * tf.reduce_mean(-y * (logstable - tf.minimum(0.0, z))) +# loss_neg = (1. - beta) * tf.reduce_mean((y - 1.) * (logstable + tf.maximum(z, 0.0))) +# cost = tf.sub(loss_pos, loss_neg, name=name)",
        "core_API": "weighted_cross_entropy_with_logits"
    },
    {
        "commit_hash": "29eb45e49a2b38dcb86918e64de93519bf0fd134",
        "index": "0817d38e..a65934a1 100755",
        "commit_message": "fixed missing super update call in TRPO\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TRPOModel(PolicyGradientModel):",
            ":param batch:",
            ":return:",
            "\"\"\"",
            "+        super(TRPOModel, self).update(batch)",
            "+",
            "self.feed_dict = {state: batch['states'][name] for name, state in self.state.items()}",
            "self.feed_dict.update({action: batch['actions'][name] for name, action in self.action.items()})",
            "self.feed_dict[self.reward] = batch['rewards']"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=2244367)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=6, insert_id=2244368)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2244369)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'self'), position=0, insert_id=2244370)",
            "Insert(target_node=IN(type=ERROR), node=('.', '.'), position=1, insert_id=2244371)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2244372)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=feed_dict), position=0)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=2244373)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'update'), position=3, insert_id=2244374)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2244375)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'batch'), position=1, insert_id=2244376)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2244377)",
            "Insert(target_node=IN(type=call), node=('identifier', 'super'), position=0, insert_id=2244378)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2244379)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2244380)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'TRPOModel'), position=1, insert_id=2244381)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2244382)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=self), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2244383)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 735,
        "neg_line": [],
        "pos_line": [
            "+super(TRPOModel, self).update(batch)",
            "+"
        ],
        "core_change": "+super(TRPOModel, self).update(batch) +",
        "core_API": "items"
    },
    {
        "commit_hash": "84194a259ead71fa9dde0e101d2da271e86d3e7e",
        "index": "69715dc5..52568a9d 100644",
        "commit_message": "fixed weight indices for root node in 3d case\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def spline_gcn(",
            "row = row.view(-1, 1).expand(row.size(0), output.size(1))",
            "output = zero.scatter_add_(0, row, output)",
            "",
            "-    # Weighten root node features by multiplying with the meaned weights at the",
            "-    # origin.",
            "-    index = torch.arange(0, kernel_size[-1]).long()",
            "+    # Weighten root node features by multiplying with the meaned weights from",
            "+    # the origin.",
            "+    index = torch.arange(0, reduce(lambda x, y: x * y, kernel_size[1:])).long()",
            "root_weight = weight[index].mean(0)",
            "output += torch.mm(features, root_weight)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=1095193)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1095194)",
            "Insert(target_node=IN(type=call), node=('identifier', 'reduce'), position=0, insert_id=1095195)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1095196)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1095197)",
            "Insert(target_node=IN(type=argument_list), node=('lambda', None), position=1, insert_id=1095198)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1095199)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=subscript), position=3)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=lambda), node=('lambda', 'lambda'), position=0, insert_id=1095200)",
            "Insert(target_node=IN(type=lambda), node=('lambda_parameters', None), position=1, insert_id=1095201)",
            "Insert(target_node=IN(type=lambda), node=(':', ':'), position=2, insert_id=1095202)",
            "Insert(target_node=IN(type=lambda), node=('binary_operator', None), position=3, insert_id=1095203)",
            "Insert(target_node=ASTNode(type=subscript), node=('slice', None), position=2, insert_id=1095204)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 'x'), position=0, insert_id=1095205)",
            "Insert(target_node=IN(type=lambda_parameters), node=(',', ','), position=1, insert_id=1095206)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 'y'), position=2, insert_id=1095207)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'x'), position=0, insert_id=1095208)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=1095209)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'y'), position=2, insert_id=1095210)",
            "Insert(target_node=IN(type=slice), node=('integer', '1'), position=0, insert_id=1095211)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=1, insert_id=1095212)",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 740,
        "neg_line": [
            "-# Weighten root node features by multiplying with the meaned weights at the",
            "-# origin.",
            "-index = torch.arange(0, kernel_size[-1]).long()"
        ],
        "pos_line": [
            "+# Weighten root node features by multiplying with the meaned weights from",
            "+# the origin.",
            "+index = torch.arange(0, reduce(lambda x, y: x * y, kernel_size[1:])).long()"
        ],
        "core_change": "-# Weighten root node features by multiplying with the meaned weights at the -# origin. -index = torch.arange(0, kernel_size[-1]).long() +# Weighten root node features by multiplying with the meaned weights from +# the origin. +index = torch.arange(0, reduce(lambda x, y: x * y, kernel_size[1:])).long()",
        "core_API": "view"
    },
    {
        "commit_hash": "4fd8977eafd658056c92ffa6294027e77cdc46c0",
        "index": "af833a8c9e..c733890ec0 100644",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def stats(policy, train_batch):",
            "\"cur_lr\": tf.cast(policy.cur_lr, tf.float64),",
            "\"policy_loss\": policy.loss.pi_loss,",
            "\"entropy\": policy.loss.entropy,",
            "-        \"var_gnorm\": tf.global_norm(policy.model.trainable_variables()),",
            "+        \"var_gnorm\": tf.linalg.global_norm(policy.model.trainable_variables()),",
            "\"vf_loss\": policy.loss.vf_loss,",
            "\"vf_explained_var\": explained_variance(",
            "tf.reshape(policy.loss.value_targets, [-1]),"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145919)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145920)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=string, text=\"var_gnorm\"), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=ERROR), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=3, insert_id=2145921)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 742,
        "neg_line": [
            "-\"var_gnorm\": tf.global_norm(policy.model.trainable_variables()),"
        ],
        "pos_line": [
            "+\"var_gnorm\": tf.linalg.global_norm(policy.model.trainable_variables()),"
        ],
        "core_change": "-\"var_gnorm\": tf.global_norm(policy.model.trainable_variables()), +\"var_gnorm\": tf.linalg.global_norm(policy.model.trainable_variables()),",
        "core_API": "cast"
    },
    {
        "commit_hash": "d1ac376595143a50074dca824ed2d0343df8c1fe",
        "index": "aae142b..92453db 100644",
        "commit_message": "Fix issues with tests\n\n",
        "file": "nebullvm.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_tensorrt_torch(",
            "res_orig = tuple(model(*inputs_example))",
            "assert all(",
            "[",
            "-                    torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01)",
            "+                    torch.allclose(",
            "+                        res_tensor.float(), res_orig_tensor, rtol=1e-01",
            "+                    )",
            "for (res_tensor, res_orig_tensor) in zip(res, res_orig)",
            "]",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=651195)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=651196)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=651197)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=res_tensor), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=651198)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=651199)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=651200)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=651201)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 743,
        "neg_line": [
            "-torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01)"
        ],
        "pos_line": [
            "+torch.allclose(",
            "+res_tensor.float(), res_orig_tensor, rtol=1e-01",
            "+)"
        ],
        "core_change": "-torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01) +torch.allclose( +res_tensor.float(), res_orig_tensor, rtol=1e-01 +)",
        "core_API": "allclose"
    },
    {
        "commit_hash": "6ed9882ddb2b6249463c855dcca6860161d91f3e",
        "index": "782812b7e..6153a8730 100644",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Attention(nn.Module):",
            "# Apply the attention mask",
            "w = w + attention_mask",
            "",
            "-        w = nn.Softmax(dim=-1)(w)",
            "+        w = nn.functional.softmax(w, dim=-1)",
            "w = self.attn_dropout(w)",
            "",
            "# Mask heads if we want to"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1536589)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1536590)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'softmax'), position=2, insert_id=1536591)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'w'), position=1, insert_id=1536592)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1536593)",
            "Update(target_node=ASTNode(type=identifier, text=Softmax), value='functional')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=w))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 744,
        "neg_line": [
            "-w = nn.Softmax(dim=-1)(w)"
        ],
        "pos_line": [
            "+w = nn.functional.softmax(w, dim=-1)"
        ],
        "core_change": "-w = nn.Softmax(dim=-1)(w) +w = nn.functional.softmax(w, dim=-1)",
        "core_API": "Softmax"
    },
    {
        "commit_hash": "127a5e2e1fac8e233492033f6bd6813b43c2bd5e",
        "index": "9215eef..30052e2 100644",
        "commit_message": "Fix typo: compat.v1\n\nPiperOrigin-RevId: 306641300\n\n",
        "file": "hub.txt.json",
        "label": "no",
        "comments": "refactor fix",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "\"from skimage.transform import AffineTransform\\n\",",
            "\"from six import BytesIO\\n\",",
            "\"\\n\",",
            "-        \"import tensorflow.compat.v2 as tf\\n\",",
            "+        \"import tensorflow.compat.v1 as tf\\n\",",
            "\"tf.disable_v2_behavior()\\n\",",
            "\"\\n\",",
            "\"import tensorflow_hub as hub\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"import tensorflow.compat.v2 as tf\\n\"), value='\"import tensorflow.compat.v1 as tf\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 745,
        "neg_line": [
            "-\"import tensorflow.compat.v2 as tf\\n\","
        ],
        "pos_line": [
            "+\"import tensorflow.compat.v1 as tf\\n\","
        ],
        "core_change": "-\"import tensorflow.compat.v2 as tf\\n\", +\"import tensorflow.compat.v1 as tf\\n\",",
        "core_API": "disable_v2_behavior"
    },
    {
        "commit_hash": "106bfb3f8dbd40febf3f96391e40ec849f912bc9",
        "index": "f38ba97..fbc32e2 100644",
        "commit_message": "Fix tests and memory issues\n\nFix issue with openvino when using large models\n\n",
        "file": "nebullvm.txt.json",
        "label": "no",
        "comments": "log update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ONNXTensorRTCompiler(TensorRTCompiler):",
            "assert os.path.isfile(onnx_model_path)",
            "except Exception:",
            "# Use original model",
            "+                self.logger.warning(",
            "+                    \"Unable to simplify model with ONNX Simplifier. \"",
            "+                    \"Original ONNX model will be used to build \"",
            "+                    \"TensorRT engine\"",
            "+                )",
            "onnx_model_path = str(model)",
            "self.simplify_model = False",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=650955)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=650956)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=Exception), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=type), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=onnx_model_path), position=0)",
            "Insert(target_node=ASTNode(type=type), node=('call', None), position=0, insert_id=650957)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=650958)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=650959)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=650960)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=650961)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'warning'), position=2, insert_id=650962)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=650963)",
            "Insert(target_node=IN(type=argument_list), node=('concatenated_string', None), position=1, insert_id=650964)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=650965)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=650966)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=650967)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logger'), position=2, insert_id=650968)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\"Unable to simplify model with ONNX Simplifier. \"'), position=0, insert_id=650969)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\"Original ONNX model will be used to build \"'), position=1, insert_id=650970)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\"TensorRT engine\"'), position=2, insert_id=650971)"
        ],
        "plus_line": 5,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 750,
        "neg_line": [],
        "pos_line": [
            "+self.logger.warning(",
            "+\"Unable to simplify model with ONNX Simplifier. \"",
            "+\"Original ONNX model will be used to build \"",
            "+\"TensorRT engine\"",
            "+)"
        ],
        "core_change": "+self.logger.warning( +\"Unable to simplify model with ONNX Simplifier. \" +\"Original ONNX model will be used to build \" +\"TensorRT engine\" +)",
        "core_API": "isfile"
    },
    {
        "commit_hash": "21fef03521eae67e4e0e52e63dd1d42887271676",
        "index": "18b26e27..a691b949 100644",
        "commit_message": "Fix parrots compatibility issues (#4143)\n\n* fix parrots compatibility\n\n* add comments\n",
        "file": "mmdetection.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistributedGroupSampler(Sampler):",
            "if size > 0:",
            "indice = np.where(self.flag == i)[0]",
            "assert len(indice) == size",
            "-                indice = indice[list(torch.randperm(int(size),",
            "-                                                    generator=g))].tolist()",
            "+                # add .numpy() to avoid bug when selecting indice in parrots.",
            "+                # TODO: check whether torch.randperm() can be replaced by",
            "+                # numpy.random.permutation().",
            "+                indice = indice[list(",
            "+                    torch.randperm(int(size), generator=g).numpy())].tolist()",
            "extra = int(",
            "math.ceil(",
            "size * 1.0 / self.samples_per_gpu / self.num_replicas)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=632678)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=632679)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=632680)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'numpy'), position=2, insert_id=632681)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=632682)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=632683)"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 752,
        "neg_line": [
            "-indice = indice[list(torch.randperm(int(size),",
            "-generator=g))].tolist()"
        ],
        "pos_line": [
            "+# add .numpy() to avoid bug when selecting indice in parrots.",
            "+# TODO: check whether torch.randperm() can be replaced by",
            "+# numpy.random.permutation().",
            "+indice = indice[list(",
            "+torch.randperm(int(size), generator=g).numpy())].tolist()"
        ],
        "core_change": "-indice = indice[list(torch.randperm(int(size), -generator=g))].tolist() +# add .numpy() to avoid bug when selecting indice in parrots. +# TODO: check whether torch.randperm() can be replaced by +# numpy.random.permutation(). +indice = indice[list( +torch.randperm(int(size), generator=g).numpy())].tolist()",
        "core_API": "where"
    },
    {
        "commit_hash": "6325142d655bac2724187fa426cb8728563c92f1",
        "index": "c8fc8075..cf8e8bd9 100644",
        "commit_message": "[keras/{backend,losses,distribute/keras_premade_models_test,mixed_precision/policy_test,premade/linear_test,premade/wide_deep_test}.py] Fix `int` given for `float` args\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def dot(x, y):",
            "",
            "If `x` is an N-D array and `y` is an M-D array (where M>=2), it is a sum",
            "product over the last axis of `x` and the second-to-last axis of `y`.",
            "-  >>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=0, high=1)",
            "+  >>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=.0, high=1.)",
            ">>> y = tf.keras.backend.ones((4, 3, 5))",
            ">>> xy = tf.keras.backend.dot(x, y)",
            ">>> tf.keras.backend.int_shape(xy)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('float', '.0'), position=2, insert_id=2084428)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('float', '1.'), position=2, insert_id=2084429)",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 754,
        "neg_line": [
            "->>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=0, high=1)"
        ],
        "pos_line": [
            "+>>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=.0, high=1.)"
        ],
        "core_change": "->>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=0, high=1) +>>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=.0, high=1.)",
        "core_API": "random_uniform_variable"
    },
    {
        "commit_hash": "8b3543fca9d811c638bb72d78601c8564f5465fd",
        "index": "a716fb93..588f3754 100644",
        "commit_message": "Fix merge_dot tests\n\n* Fix merge_dot tests\n\n* Make batch_dot unique\n\nbatch_dot is not tensordot! It only accepts one reduce dimension at a\ntime. Other reduce dimensions should be dome afterwards with K.sum\nThis means that K.batch_dot will have the same behavior in both\ntensorflow and theano. This also means that we have less parenthesis and\nless nested lists.\n\nNew usage:\n\nmerge_mode = 'dot', dot_axes=[axis1, axis2]\n\nBefore:\n\nmerge_mode = 'dot', dot_axes=[[axis1], [axis2]]\n\n* Backport sign by @the-moliver\n\n* Fix docstrings\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "add method",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def round(x):",
            "return tf.round(x)",
            "",
            "",
            "+def sign(x):",
            "+    return tf.sign(x)",
            "+",
            "+",
            "def pow(x, a):",
            "return tf.pow(x, a)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=2, insert_id=2118415)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=2118416)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'sign'), position=1, insert_id=2118417)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=2118418)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=2118419)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=2118420)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=2118421)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'x'), position=1, insert_id=2118422)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=2118423)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=2118424)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2118425)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=2118426)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2118427)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2118428)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2118429)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2118430)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sign'), position=2, insert_id=2118431)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2118432)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x'), position=1, insert_id=2118433)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2118434)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 757,
        "neg_line": [],
        "pos_line": [
            "+def sign(x):",
            "+return tf.sign(x)",
            "+",
            "+"
        ],
        "core_change": "+def sign(x): +return tf.sign(x) + +",
        "core_API": "round"
    },
    {
        "commit_hash": "6aaeb70670d4540900e55f4ac532546656447bd2",
        "index": "6aa42c82..8a914627 100644",
        "commit_message": "Add bilinear attention (#1349)\n\n* Add bilinear attention, some code cleanup, make semantic parsers use new attention\n\n* Fix pylint and docs\n\n* Increase beam size for wikitables ERM parser, so test doesn't fail\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "custom API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Attention(torch.nn.Module, Registrable):",
            "vector: torch.Tensor,",
            "matrix: torch.Tensor,",
            "matrix_mask: torch.Tensor = None) -> torch.Tensor:",
            "-        similarities = self._forward_internal(vector, matrix, matrix_mask)",
            "+        similarities = self._forward_internal(vector, matrix)",
            "if self._normalize:",
            "return masked_softmax(similarities, matrix_mask)",
            "else:",
            "return similarities",
            "",
            "-    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor,",
            "-                          matrix_mask: torch.Tensor = None) -> torch.Tensor:",
            "+    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:",
            "raise NotImplementedError",
            "",
            "@classmethod"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=function_definition), node=ASTNode(type=type), position=4)",
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=3, insert_id=36567)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=36568)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=36569)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=36570)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=36571)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=matrix_mask))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=matrix_mask))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 758,
        "neg_line": [
            "-similarities = self._forward_internal(vector, matrix, matrix_mask)",
            "-def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor,",
            "-matrix_mask: torch.Tensor = None) -> torch.Tensor:"
        ],
        "pos_line": [
            "+similarities = self._forward_internal(vector, matrix)",
            "+def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:"
        ],
        "core_change": "-similarities = self._forward_internal(vector, matrix, matrix_mask) +similarities = self._forward_internal(vector, matrix) -def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor, -matrix_mask: torch.Tensor = None) -> torch.Tensor: +def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:",
        "core_API": "_forward_internal"
    },
    {
        "commit_hash": "da2e1d089c34873fe515a815b5d843171a09f64d",
        "index": "b8808fe4..f24044ff 100644",
        "commit_message": "fix pairnorm test\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "add param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_pair_norm(scale_individually):",
            "assert out1.size() == (100, 16)",
            "",
            "out2 = norm(torch.cat([x, x], dim=0), torch.cat([batch, batch + 1], dim=0))",
            "-    assert torch.allclose(out1, out2[:100])",
            "-    assert torch.allclose(out1, out2[100:])",
            "+    assert torch.allclose(out1, out2[:100], atol=1e-6)",
            "+    assert torch.allclose(out1, out2[100:], atol=1e-6)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1005382)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1005383)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1005384)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'allclose'), position=2, insert_id=1005385)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1005386)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1005387)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1005388)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1005389)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=1005390)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1005391)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=1005392)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=1005393)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1005394)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=1005395)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=allclose))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 759,
        "neg_line": [
            "-assert torch.allclose(out1, out2[:100])",
            "-assert torch.allclose(out1, out2[100:])"
        ],
        "pos_line": [
            "+assert torch.allclose(out1, out2[:100], atol=1e-6)",
            "+assert torch.allclose(out1, out2[100:], atol=1e-6)"
        ],
        "core_change": "-assert torch.allclose(out1, out2[:100]) -assert torch.allclose(out1, out2[100:]) +assert torch.allclose(out1, out2[:100], atol=1e-6) +assert torch.allclose(out1, out2[100:], atol=1e-6)",
        "core_API": "size"
    },
    {
        "commit_hash": "cd06a4c1e5761d118c2d9d3aec7b38c512d74229",
        "index": "4046b06b..78bdd10d 100644",
        "commit_message": "linter fix\n\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(nn.Module):",
            "memories = torch.cat((memory, memories), dim=0)",
            "memories = self._update_memory(memories)",
            "if speaker_embeddings is not None:",
            "-                memories = torch.cat([memories, speaker_embeddings], dim=-1)",
            "+            memories = torch.cat([memories, speaker_embeddings], dim=-1)",
            "memories = self.prenet(memories)",
            "",
            "self._init_states(inputs, mask=mask)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 761,
        "neg_line": [
            "-memories = torch.cat([memories, speaker_embeddings], dim=-1)"
        ],
        "pos_line": [
            "+memories = torch.cat([memories, speaker_embeddings], dim=-1)"
        ],
        "core_change": "-memories = torch.cat([memories, speaker_embeddings], dim=-1) +memories = torch.cat([memories, speaker_embeddings], dim=-1)",
        "core_API": "cat"
    },
    {
        "commit_hash": "0be56647cb0cf8e290cd2bd0fe05cd229bbb56fd",
        "index": "f6cd101..c6d7504 100644",
        "commit_message": "Convert shape_list, fix softmax axis default\n\n",
        "file": "gpt-neo.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def attn(x, scope, n_state, *, past, params, block_offset=0, train=False):",
            "",
            "def merge_heads(x):",
            "# TODO: convert to mtf code",
            "-        # Reverse of split_heads",
            "+        # Reverse of split_heads : result shape [batch, sequence, features]",
            "return merge_states(tf.transpose(x, [0, 2, 1, 3]))",
            "",
            "# the old mask_attn_weights applied directly to the QK; this returns a bias that the attention code from mtf adds to the attention matrix."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 763,
        "neg_line": [
            "-# Reverse of split_heads"
        ],
        "pos_line": [
            "+# Reverse of split_heads : result shape [batch, sequence, features]"
        ],
        "core_change": "-# Reverse of split_heads +# Reverse of split_heads : result shape [batch, sequence, features]",
        "core_API": "transpose"
    },
    {
        "commit_hash": "2d2ed2cc180475833d4315dde09fc5e5a1ccc9b5",
        "index": "6a04ab349..372520bb7 100755",
        "commit_message": "[T5] Fix speed degradation bug t5 (#10496)\n\n* fix speed degradation bug t5\n\n* fix for all models\n\n* fix code quality\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add condition check for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BlenderbotSmallEncoderLayer(nn.Module):",
            "hidden_states = residual + hidden_states",
            "hidden_states = self.final_layer_norm(hidden_states)",
            "",
            "-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "+        if hidden_states.dtype == torch.float16 and (",
            "+            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()",
            "+        ):",
            "clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=1220982)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1220983)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('parenthesized_expression', None), position=2, insert_id=1220984)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1220985)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1220986)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=2, insert_id=1220987)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1220988)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=boolean_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1220989)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hidden_states'), position=0, insert_id=1220990)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1220991)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1220992)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1220993)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1220994)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float16'), position=2, insert_id=1220995)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 764,
        "neg_line": [
            "-if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():"
        ],
        "pos_line": [
            "+if hidden_states.dtype == torch.float16 and (",
            "+torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()",
            "+):"
        ],
        "core_change": "-if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any(): +if hidden_states.dtype == torch.float16 and ( +torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any() +):",
        "core_API": "final_layer_norm"
    },
    {
        "commit_hash": "145670f893f43ff70866668cf087d82fe51a22a6",
        "index": "fdb61cd9b..58a1a77c2 100644",
        "commit_message": "fix logging on rank 0 only (#2425)\n\n* fix and test for ddp block logging rank > 0\n\n* rename\n\n* use the dummy logger\n\n* dummy logger test\n\n* set the logger in  model\n\n* decorator for rank zero experiment\n\n* simplify check\n\n* simplify\n\n* fix problem with None in checkpoint path\n\n* revert configure logger\n\n* unused import\n\n* offline\n\n* try rank 0 decorator in checkpoint\n\n* try fix test\n\n* imgs\n\n* add asserts to make sure log zero only saves checkpoints\n\n* add asserts to make sure log zero only saves checkpoints\n\n* add asserts to make sure log zero only saves checkpoints\n\n* add asserts to make sure log zero only saves checkpoints\n\n* add asserts to make sure log zero only saves checkpoints\n\n* fix tpu tests\n\n* fix tpu tests\n\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "remove logger",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(",
            "",
            "\"\"\"",
            "# bind logger and other properties",
            "-        model.logger = self.logger",
            "self.copy_trainer_model_properties(model)",
            "",
            "# clean hparams"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=logger))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=logger))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 765,
        "neg_line": [
            "-model.logger = self.logger"
        ],
        "pos_line": [],
        "core_change": "-model.logger = self.logger",
        "core_API": "copy_trainer_model_properties"
    },
    {
        "commit_hash": "c88b0285d24f97530cd7ea284a06a3d7c1489f49",
        "index": "d8d0cbb..0917a44 100644",
        "commit_message": "Fix DDP unused params (#497)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/497\n\n1 DDP assumes all used params are in model.forward, however in CRF model, there's params used outside of forward, which will casue issue. This diff fixed this by setting find_unused_parameters to False\n\n2 DDP can detect unused params now, so removing the previous hack in PyText\n\nReviewed By: chenyangyu1988\n\nDifferential Revision: D15027698\n\nfbshipit-source-id: 9499cf082f7eed96ec22179076f345943de2177d\n\n",
        "file": "pytext.txt.json",
        "label": "no",
        "comments": "not clear no API",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RNNGParser(Model, Component):",
            "",
            "\"\"\"",
            "",
            "-        nn.Module.__init__(self)",
            "+        super().__init__()",
            "",
            "self.embedding = embedding",
            "# self.embedding.config: FeatureConfig object cannot be pickled but,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1472898)",
            "Update(target_node=ASTNode(type=identifier, text=nn), value='super')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=nn), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1472899)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1472900)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1472901)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Module))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 770,
        "neg_line": [
            "-nn.Module.__init__(self)"
        ],
        "pos_line": [
            "+super().__init__()"
        ],
        "core_change": "-nn.Module.__init__(self) +super().__init__()",
        "core_API": "__init__"
    },
    {
        "commit_hash": "cee5eaf659b1e52b909f931d05b64d6f25dc63e8",
        "index": "d0a5ab886..3228daae8 100644",
        "commit_message": "flake8 fixes (#3064)\n\n* flake8 fixes\n\n* fix pep8\n\n* fix pep8\n\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class ModelSummary(object):",
            "input_ = model.transfer_batch_to_device(input_, model.device)",
            "",
            "if trainer is not None and trainer.amp_backend == AMPType.NATIVE and not trainer.use_tpu:",
            "-                model.forward = torch.cuda.amp.autocast()(model.forward)",
            "+            model.forward = torch.cuda.amp.autocast()(model.forward)",
            "",
            "mode = model.training",
            "model.eval()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 773,
        "neg_line": [
            "-model.forward = torch.cuda.amp.autocast()(model.forward)"
        ],
        "pos_line": [
            "+model.forward = torch.cuda.amp.autocast()(model.forward)"
        ],
        "core_change": "-model.forward = torch.cuda.amp.autocast()(model.forward) +model.forward = torch.cuda.amp.autocast()(model.forward)",
        "core_API": "transfer_batch_to_device"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "d1da18686..281bc96df 100644",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class QuantLinear(nn.Module):",
            "x_int = x / prev_act_scaling_factor",
            "",
            "return (",
            "-            F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,",
            "+            nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,",
            "bias_scaling_factor,",
            ")"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1538329)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1538330)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=F), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1538331)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 774,
        "neg_line": [
            "-F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,"
        ],
        "pos_line": [
            "+nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,"
        ],
        "core_change": "-F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor, +nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,",
        "core_API": "linear"
    },
    {
        "commit_hash": "8c4c4d7f046a5191fa22067aaf236f7e47a1c2e7",
        "index": "431fb9c2..da978726 100755",
        "commit_message": "TRPO working, other fixes and improvements\n\n",
        "file": "tensorforce.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Gaussian(Distribution):",
            "if self.action_spec.max_value is not None:",
            "action = tf.minimum(x=self.action_spec.max_value, y=action)",
            "",
            "-        return action",
            "+            return action",
            "",
            "@tf_function(num_args=2)",
            "def log_probability(self, *, parameters, action):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 776,
        "neg_line": [
            "-return action"
        ],
        "pos_line": [
            "+return action"
        ],
        "core_change": "-return action +return action",
        "core_API": "minimum"
    },
    {
        "commit_hash": "5db74521736f6d6caef4e0cd8aba1ad624ae9390",
        "index": "936f1dd..33a2fe8 100644",
        "commit_message": "Fix visformer in_chans stem handling\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "no",
        "comments": "change value",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Visformer(nn.Module):",
            "img_size //= 8",
            "else:",
            "self.stem = nn.Sequential(",
            "-                    nn.Conv2d(3, self.init_channels, 7, stride=2, padding=3, bias=False),",
            "+                    nn.Conv2d(in_chans, self.init_channels, 7, stride=2, padding=3, bias=False),",
            "nn.BatchNorm2d(self.init_channels),",
            "nn.ReLU(inplace=True)",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'in_chans'), position=1, insert_id=1478454)",
            "Delete(target_node=ASTNode(type=integer, text=3))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 777,
        "neg_line": [
            "-nn.Conv2d(3, self.init_channels, 7, stride=2, padding=3, bias=False),"
        ],
        "pos_line": [
            "+nn.Conv2d(in_chans, self.init_channels, 7, stride=2, padding=3, bias=False),"
        ],
        "core_change": "-nn.Conv2d(3, self.init_channels, 7, stride=2, padding=3, bias=False), +nn.Conv2d(in_chans, self.init_channels, 7, stride=2, padding=3, bias=False),",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "cd4086b6646e807d07bd81b2a10f1e8adf9543ab",
        "index": "714e2ae3..35e73e79 100644",
        "commit_message": "Fix various docs formatting (#1688)\n\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Importance(TracePosterior):",
            "\"\"\"",
            "if self.log_weights:",
            "log_w_norm = self.get_normalized_weights(log_scale=True)",
            "-            ess = torch.exp(-logsumexp(2*log_w_norm, 0))",
            "+            ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))",
            "else:",
            "warnings.warn(\"The log_weights list is empty, effective sample size is zero.\")",
            "ess = 0"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 779,
        "neg_line": [
            "-ess = torch.exp(-logsumexp(2*log_w_norm, 0))"
        ],
        "pos_line": [
            "+ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))"
        ],
        "core_change": "-ess = torch.exp(-logsumexp(2*log_w_norm, 0)) +ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))",
        "core_API": "get_normalized_weights"
    },
    {
        "commit_hash": "1c19060fd6644c5f11152bb74485b537e0ce7cfd",
        "index": "da37fff..5ea0344 100644",
        "commit_message": "fix for API changes\n\n",
        "file": "examples.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "for epoch in range(opt.niter):",
            "vutils.save_image(fake.data, 'fake_samples.png')",
            "",
            "# do checkpointing",
            "-    torch.save(netG.parameter_dict(), 'netG_epoch_%d.pth' % epoch)",
            "-    torch.save(netD.parameter_dict(), 'netD_epoch_%d.pth' % epoch)",
            "+    torch.save(netG.state_dict(), 'netG_epoch_%d.pth' % epoch)",
            "+    torch.save(netD.state_dict(), 'netD_epoch_%d.pth' % epoch)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=191987)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=191988)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=191989)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'save'), position=2, insert_id=191990)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=191991)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=parameter_dict), value='state_dict')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=191992)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=191993)",
            "Update(target_node=ASTNode(type=identifier, text=parameter_dict), value='state_dict')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=save))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 780,
        "neg_line": [
            "-torch.save(netG.parameter_dict(), 'netG_epoch_%d.pth' % epoch)",
            "-torch.save(netD.parameter_dict(), 'netD_epoch_%d.pth' % epoch)"
        ],
        "pos_line": [
            "+torch.save(netG.state_dict(), 'netG_epoch_%d.pth' % epoch)",
            "+torch.save(netD.state_dict(), 'netD_epoch_%d.pth' % epoch)"
        ],
        "core_change": "-torch.save(netG.parameter_dict(), 'netG_epoch_%d.pth' % epoch) -torch.save(netD.parameter_dict(), 'netD_epoch_%d.pth' % epoch) +torch.save(netG.state_dict(), 'netG_epoch_%d.pth' % epoch) +torch.save(netD.state_dict(), 'netD_epoch_%d.pth' % epoch)",
        "core_API": "save_image"
    },
    {
        "commit_hash": "cc5f8fbdcbf6030c11eda5ba97713ee9957dd1ee",
        "index": "638b9eee..246028b4 100644",
        "commit_message": "Fix onnx unitest (#6369)\n\nFix all unitests\n",
        "file": "mmdetection.txt.json",
        "label": "no",
        "comments": "test",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def distance2bbox(points, distance, max_shape=None):",
            "bboxes = torch.stack([x1, y1, x2, y2], -1)",
            "",
            "if max_shape is not None:",
            "-        if points.dim() == 2 and not torch.onnx.is_in_onnx_export():",
            "+        if bboxes.dim() == 2 and not torch.onnx.is_in_onnx_export():",
            "# speed up",
            "bboxes[:, 0::2].clamp_(min=0, max=max_shape[1])",
            "bboxes[:, 1::2].clamp_(min=0, max=max_shape[0])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=points), value='bboxes')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 781,
        "neg_line": [
            "-if points.dim() == 2 and not torch.onnx.is_in_onnx_export():"
        ],
        "pos_line": [
            "+if bboxes.dim() == 2 and not torch.onnx.is_in_onnx_export():"
        ],
        "core_change": "-if points.dim() == 2 and not torch.onnx.is_in_onnx_export(): +if bboxes.dim() == 2 and not torch.onnx.is_in_onnx_export():",
        "core_API": "stack"
    },
    {
        "commit_hash": "32738da5b399bc9d14511a08640e83a36b745e94",
        "index": "0848945..77bf0f5 100644",
        "commit_message": "Fixed a broken test for v0.6 with importing linear function instead of module\n\n",
        "file": "skflow.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):",
            "with tf.variable_scope('dnn'):",
            "for i, n_units in enumerate(hidden_units):",
            "with tf.variable_scope('layer%d' % i):",
            "-                tensor_in = linear.linear(tensor_in, n_units, True)",
            "+                tensor_in = linear(tensor_in, n_units, True)",
            "tensor_in = activation(tensor_in)",
            "if keep_prob:",
            "tensor_in = tf.nn.dropout(tensor_in, keep_prob)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=linear), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=linear))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 782,
        "neg_line": [
            "-tensor_in = linear.linear(tensor_in, n_units, True)"
        ],
        "pos_line": [
            "+tensor_in = linear(tensor_in, n_units, True)"
        ],
        "core_change": "-tensor_in = linear.linear(tensor_in, n_units, True) +tensor_in = linear(tensor_in, n_units, True)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "299dee38..1047b47b 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class DSClipEncoder(torch.nn.Module):",
            "seq_len,",
            "seq_len,",
            "dtype=dtype,",
            "-                           device=torch.cuda.current_device())",
            "+                           device=get_accelerator().current_device_name())",
            "mask.fill_(torch.tensor(torch.finfo(dtype).min))",
            "mask.triu_(1)",
            "mask = mask.unsqueeze(1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=attribute), node=('ERROR', None), position=1, insert_id=1816210)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1816211)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=mask), position=1)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1816212)",
            "Update(target_node=ASTNode(type=identifier, text=current_device), value='current_device_name')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816213)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816214)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_accelerator')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 783,
        "neg_line": [
            "-device=torch.cuda.current_device())"
        ],
        "pos_line": [
            "+device=get_accelerator().current_device_name())"
        ],
        "core_change": "-device=torch.cuda.current_device()) +device=get_accelerator().current_device_name())",
        "core_API": "current_device"
    },
    {
        "commit_hash": "ec7f8af1061658701acae1d3e14bc50f68d8c62c",
        "index": "248566a93..fc4846272 100755",
        "commit_message": "[ConvNeXT] Fix drop_path_rate (#17280)\n\n* Fix drop_path_rate\n\n* Fix TF's drop path rate\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "change value",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ConvNextEncoder(nn.Module):",
            "out_channels=out_chs,",
            "stride=2 if i > 0 else 1,",
            "depth=config.depths[i],",
            "-                drop_path_rates=drop_path_rates[cur],",
            "+                drop_path_rates=drop_path_rates[i],",
            ")",
            "self.stages.append(stage)",
            "-            cur += config.depths[i]",
            "prev_chs = out_chs",
            "",
            "def forward("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=cur), value='i')",
            "Delete(target_node=ASTNode(type=identifier, text=cur))",
            "Delete(target_node=ASTNode(type=+=, text=+=))",
            "Delete(target_node=ASTNode(type=identifier, text=config))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=depths))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=i))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=augmented_assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 784,
        "neg_line": [
            "-drop_path_rates=drop_path_rates[cur],",
            "-cur += config.depths[i]"
        ],
        "pos_line": [
            "+drop_path_rates=drop_path_rates[i],"
        ],
        "core_change": "-drop_path_rates=drop_path_rates[cur], +drop_path_rates=drop_path_rates[i], -cur += config.depths[i]",
        "core_API": "append"
    },
    {
        "commit_hash": "d9a7362be22ca8c412bebc69f054ee546b6bc18e",
        "index": "80e5153bb..c17557123 100644",
        "commit_message": "fix some compatibility problems with PyTorch 1.3.0 in ESPnet\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CTC(torch.nn.Module):",
            "if self.ctc_type == \"warpctc\":",
            "# warpctc only supports float32",
            "ys_hat = ys_hat.to(dtype=torch.float32)",
            "+        else:",
            "+            # use GPU when using the cuDNN implementation",
            "+            ys_true = to_device(self, ys_true)",
            "self.loss = to_device(self, self.loss_fn(ys_hat, ys_true, hlens, olens)).to(dtype=dtype)",
            "if self.reduce:",
            "# NOTE: sum() is needed to keep consistency since warpctc return as tensor w/ shape (1,)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=164631)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=164632)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'else'), position=0, insert_id=164633)",
            "Insert(target_node=IN(type=assignment), node=(':', ':'), position=1, insert_id=164634)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=164635)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=3, insert_id=164636)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=4, insert_id=164637)",
            "Insert(target_node=IN(type=type), node=('identifier', 'ys_true'), position=0, insert_id=164638)",
            "Insert(target_node=IN(type=call), node=('identifier', 'to_device'), position=0, insert_id=164639)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=164640)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=164641)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'self'), position=1, insert_id=164642)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=164643)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'ys_true'), position=3, insert_id=164644)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=164645)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 785,
        "neg_line": [],
        "pos_line": [
            "+else:",
            "+# use GPU when using the cuDNN implementation",
            "+ys_true = to_device(self, ys_true)"
        ],
        "core_change": "+else: +# use GPU when using the cuDNN implementation +ys_true = to_device(self, ys_true)",
        "core_API": "to"
    },
    {
        "commit_hash": "e11e0796fc02cc2cd5b6ec2ad7cea21f77e25402",
        "index": "c45382d..46ab26c 100644",
        "commit_message": "Fix indentation to be self-consistent (#279)\n\n* Fix indentation to be self-consistent\n\nReplace 2-space with 4-space indentation\n\n* Fix indentation to be self-consistent\n\nReplace 2-space with 4-space indentation\n\n* Fix indentation\n\nReplace 3-space indentation with 4-space indentation\n\n",
        "file": "examples.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "for epoch in range(1, args.epochs + 1):",
            "test(epoch)",
            "sample = Variable(torch.randn(64, 20))",
            "if args.cuda:",
            "-       sample = sample.cuda()",
            "+        sample = sample.cuda()",
            "sample = model.decode(sample).cpu()",
            "save_image(sample.data.view(64, 1, 28, 28),",
            "'results/sample_' + str(epoch) + '.png')"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 786,
        "neg_line": [
            "-sample = sample.cuda()"
        ],
        "pos_line": [
            "+sample = sample.cuda()"
        ],
        "core_change": "-sample = sample.cuda() +sample = sample.cuda()",
        "core_API": "randn"
    },
    {
        "commit_hash": "d8002fcc0e861e2662b19c06380b80d734b15249",
        "index": "b423dbb6e..9d69a143d 100644",
        "commit_message": "Fix test for uint8\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "test",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_uint8_representation_not_allowed_with_negative_values(workers):",
            "",
            "def test_uint_representation(workers):",
            "x = torch.tensor([[1.5, 2.0, 3.0], [4.5, 5.0, 6.0]])",
            "-    enlarged = x.fix_prec(internal_type=torch.int16, precision_fractional=256)",
            "+    enlarged = x.fix_prec(internal_type=torch.uint8, precision_fractional=256)",
            "restored = enlarged.float_precision()",
            "# And now x and restored must be the same",
            "assert torch.all(torch.eq(x, restored))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=int16), value='uint8')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 788,
        "neg_line": [
            "-enlarged = x.fix_prec(internal_type=torch.int16, precision_fractional=256)"
        ],
        "pos_line": [
            "+enlarged = x.fix_prec(internal_type=torch.uint8, precision_fractional=256)"
        ],
        "core_change": "-enlarged = x.fix_prec(internal_type=torch.int16, precision_fractional=256) +enlarged = x.fix_prec(internal_type=torch.uint8, precision_fractional=256)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "008c0edb3b7b96543930c47bcaed03429503971a",
        "index": "4667d0f..bf0bfcd 100644",
        "commit_message": "Data Input Refactoring\n\n- Data input pipeline can now deal properly with missing target data.\nFixes #101\n- Moved some of the featurizer functionality into the data reader and\nmade featurizer implicit part of the model.\n- Misc: Replace concat with concat_v2\n\n",
        "file": "seq2seq.txt.json",
        "label": "yes",
        "comments": "change API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AttentionDecoder(DecoderBase):",
            "])",
            "else:",
            "attention_context = output.attention_context",
            "-    return tf.concat(1, [next_input, attention_context])",
            "+    return tf.concat_v2([next_input, attention_context], 1)",
            "",
            "def _pad_att_scores(self, scores):",
            "\"\"\"Pads attention scores to fixed length. This is a hack because raw_rnn"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=3, insert_id=2166958)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=4, insert_id=2166959)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 789,
        "neg_line": [
            "-return tf.concat(1, [next_input, attention_context])"
        ],
        "pos_line": [
            "+return tf.concat_v2([next_input, attention_context], 1)"
        ],
        "core_change": "-return tf.concat(1, [next_input, attention_context]) +return tf.concat_v2([next_input, attention_context], 1)",
        "core_API": "concat"
    },
    {
        "commit_hash": "5218c4bad80989033ec0e09002870c47df75cf0e",
        "index": "69fe5ec6..9a79c59b 100644",
        "commit_message": "fix test\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_graph_saint():",
            "assert sample.node_norm.numel() == sample.num_nodes",
            "assert sample.edge_norm.numel() == sample.num_edges",
            "",
            "+    torch.manual_seed(12345)",
            "loader = GraphSAINTRandomWalkSampler(data, batch_size=2, walk_length=1,",
            "-                                         num_steps=4, log=False)",
            "+                                         num_steps=4, sample_coverage=10,",
            "+                                         log=False)",
            "",
            "for sample in loader:",
            "assert len(sample) == 4"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1017962)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1017963)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1017964)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1017965)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1017966)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1017967)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=1017968)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1017969)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '12345'), position=1, insert_id=1017970)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1017971)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=9, insert_id=1017972)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=10, insert_id=1017973)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'sample_coverage'), position=0, insert_id=1017974)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1017975)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '10'), position=2, insert_id=1017976)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 791,
        "neg_line": [
            "-num_steps=4, log=False)"
        ],
        "pos_line": [
            "+torch.manual_seed(12345)",
            "+num_steps=4, sample_coverage=10,",
            "+log=False)"
        ],
        "core_change": "+torch.manual_seed(12345) -num_steps=4, log=False) +num_steps=4, sample_coverage=10, +log=False)",
        "core_API": "numel"
    },
    {
        "commit_hash": "c51dc4f92755c67a83f3fc8a0bd6b3e64df199e4",
        "index": "f9c49db52..14f6979d7 100755",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "remove API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPTJAttention(nn.Module):",
            "):",
            "# compute causal mask from causal mask buffer",
            "query_length, key_length = query.size(-2), key.size(-2)",
            "-        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)",
            "+        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]",
            "",
            "# Keep the attention weights computation in fp32 to avoid overflow issues",
            "query = query.to(torch.float32)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=subscript), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 792,
        "neg_line": [
            "-causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)"
        ],
        "pos_line": [
            "+causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]"
        ],
        "core_change": "-causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool) +causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]",
        "core_API": "size"
    },
    {
        "commit_hash": "9260b685d714a704b2dc8c64e3e5041b4d3a64c3",
        "index": "928fbcc5..05ecdfde 100644",
        "commit_message": "fix lint errors\n\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Tester(unittest.TestCase):",
            "",
            "# create checkerboard",
            "board = utils.create_checkerboard(height, width, 4)",
            "-        patch_src = torch.from_numpy(board).view( \\",
            "+        patch_src = torch.from_numpy(board).view(",
            "1, 1, height, width).expand(batch_size, 1, height, width)",
            "patch_src = utils.tensor_to_gradcheck_var(patch_src)  # to var"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 793,
        "neg_line": [
            "-patch_src = torch.from_numpy(board).view( \\"
        ],
        "pos_line": [
            "+patch_src = torch.from_numpy(board).view("
        ],
        "core_change": "-patch_src = torch.from_numpy(board).view( \\ +patch_src = torch.from_numpy(board).view(",
        "core_API": "create_checkerboard"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "9bdb212b..554d1ffb 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(serialization_directory, device):",
            "iterator.index_with(model.vocab)",
            "",
            "model_predictions = []",
            "-    batches = iterator(instances, num_epochs=1, shuffle=False, cuda_device=device, for_training=False)",
            "+    batches = iterator(instances, num_epochs=1, shuffle=False, cuda_device=device)",
            "for batch in Tqdm.tqdm(batches):",
            "result = model(**batch)",
            "predictions = model.decode(result)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 794,
        "neg_line": [
            "-batches = iterator(instances, num_epochs=1, shuffle=False, cuda_device=device, for_training=False)"
        ],
        "pos_line": [
            "+batches = iterator(instances, num_epochs=1, shuffle=False, cuda_device=device)"
        ],
        "core_change": "-batches = iterator(instances, num_epochs=1, shuffle=False, cuda_device=device, for_training=False) +batches = iterator(instances, num_epochs=1, shuffle=False, cuda_device=device)",
        "core_API": "index_with"
    },
    {
        "commit_hash": "7a6519f84fed06947bbf161c7b66c9099bc4ce53",
        "index": "92f5f1d6..7b0e7fcf 100644",
        "commit_message": "Bugfixes (#1159)\n\nSummary:\nSeveral bugfixes to get tests passing on OSS master\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1159\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21331993\n\nPulled By: myleott\n\nfbshipit-source-id: 327ae19f6797f92b8c6083a49d5f5edb0872223e\n\n",
        "file": "fairseq.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestExportModels(unittest.TestCase):",
            "_test_save_and_load(scripted)",
            "",
            "@unittest.skipIf(",
            "-        torch.__version__ < \"1.5.0\", \"Targeting OSS scriptability for the 1.5 release\"",
            "+        torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\"",
            ")",
            "def test_export_transformer(self):",
            "task, parser = get_dummy_task_and_parser()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"Targeting OSS scriptability for the 1.5 release\"), value='\"Targeting OSS scriptability for the 1.6 release\"')",
            "Update(target_node=ASTNode(type=string, text=\"1.5.0\"), value='\"1.6.0\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 795,
        "neg_line": [
            "-torch.__version__ < \"1.5.0\", \"Targeting OSS scriptability for the 1.5 release\""
        ],
        "pos_line": [
            "+torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\""
        ],
        "core_change": "-torch.__version__ < \"1.5.0\", \"Targeting OSS scriptability for the 1.5 release\" +torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\"",
        "core_API": "skipIf"
    },
    {
        "commit_hash": "58cb31362f27eb6502e917b091e14cc9b1c0d65a",
        "index": "de7c636..a77de29 100644",
        "commit_message": "Fix parsing Keras version identifiers like `2.2.4-tf` (#3794)\n\nSigned-off-by: Max H. Gerlach <git@maxgerlach.de>\n",
        "file": "horovod.txt.json",
        "label": "no",
        "comments": "no API check version fix",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "from horovod.tensorflow.keras import callbacks, elastic",
            "try:",
            "# In later versions of TensorFlow, optimizers are spread across multiple modules. This set is used to distinguish",
            "# stock optimizers that come with tf.keras from custom optimizers that may need to be wrapped specially.",
            "-    if version.parse(keras.__version__) < version.parse(\"2.11\"):",
            "+    if version.parse(keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):",
            "optimizer_type = tf.keras.optimizers.Optimizer",
            "else:",
            "optimizer_type = keras.optimizers.legacy.Optimizer"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2484559)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=2484560)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2484561)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2484562)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2484563)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'replace'), position=2, insert_id=2484564)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2484565)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"-tf\"'), position=1, insert_id=2484566)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2484567)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"+tf\"'), position=3, insert_id=2484568)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 796,
        "neg_line": [
            "-if version.parse(keras.__version__) < version.parse(\"2.11\"):"
        ],
        "pos_line": [
            "+if version.parse(keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):"
        ],
        "core_change": "-if version.parse(keras.__version__) < version.parse(\"2.11\"): +if version.parse(keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):",
        "core_API": "parse"
    },
    {
        "commit_hash": "95d5fddfb578674e01802f1db1820d8ac1015f67",
        "index": "18c4f24..0dcb285 100644",
        "commit_message": "Fix bug in reparameterize() (#419)\n\nThe reparameterize() function should not be deterministic for validation/test mode. It should be the same as in training mode, otherwise what is computed is not the ELBO (i.e. the correct loss) but something else. This change will slightly change the test set error.\n",
        "file": "examples.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if __name__ == \"__main__\":",
            "sample = torch.randn(64, 20).to(device)",
            "sample = model.decode(sample).cpu()",
            "save_image(sample.view(64, 1, 28, 28),",
            "-                       'results/sample_' + str(epoch) + '.png')",
            "\\ No newline at end of file",
            "+                       'results/sample_' + str(epoch) + '.png')"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=0, insert_id=190395)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=if_statement), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=expression_statement), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=expression_statement), position=2)",
            "Insert(target_node=IN(type=ERROR), node=('expression_statement', None), position=3, insert_id=190396)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('ERROR', None), position=3, insert_id=190397)",
            "Insert(target_node=IN(type=ERROR), node=('ERROR', ' '), position=0, insert_id=190398)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'No'), position=1, insert_id=190399)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'newline'), position=2, insert_id=190400)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'at'), position=3, insert_id=190401)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'end'), position=4, insert_id=190402)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'of'), position=5, insert_id=190403)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'file'), position=6, insert_id=190404)",
            "Delete(target_node=ASTNode(type=ERROR, text= ))",
            "Delete(target_node=ASTNode(type=identifier, text=No))",
            "Delete(target_node=ASTNode(type=identifier, text=newline))",
            "Delete(target_node=ASTNode(type=identifier, text=at))",
            "Delete(target_node=ASTNode(type=identifier, text=end))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=file))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 798,
        "neg_line": [
            "-'results/sample_' + str(epoch) + '.png')"
        ],
        "pos_line": [
            "+'results/sample_' + str(epoch) + '.png')"
        ],
        "core_change": "-'results/sample_' + str(epoch) + '.png') +'results/sample_' + str(epoch) + '.png')",
        "core_API": "randn"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "b1194b77..83ff4a97 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GeneralizedChannelPermute(ConditionedGeneralizedChannelPermute, TransformM",
            "self.__delattr__('permutation')",
            "",
            "# Sample a random orthogonal matrix",
            "-        W, _ = torch.qr(torch.randn(channels, channels))",
            "+        W, _ = torch.linalg.qr(torch.randn(channels, channels))",
            "",
            "# Construct the partially pivoted LU-form and the pivots",
            "LU, pivots = W.lu()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=676932)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=676933)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=676934)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 800,
        "neg_line": [
            "-W, _ = torch.qr(torch.randn(channels, channels))"
        ],
        "pos_line": [
            "+W, _ = torch.linalg.qr(torch.randn(channels, channels))"
        ],
        "core_change": "-W, _ = torch.qr(torch.randn(channels, channels)) +W, _ = torch.linalg.qr(torch.randn(channels, channels))",
        "core_API": "__delattr__"
    },
    {
        "commit_hash": "48a3208c39d9d4e5fcd5dd2942a9e8751cfe2674",
        "index": "fa41fd7a..a310693f 100644",
        "commit_message": "Fixed nightly errors (#886)\n\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "change param for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestConfusionMatrix:",
            "conf_mat = kornia.utils.metrics.confusion_matrix(",
            "predicted, actual, num_classes)",
            "conf_mat_real = torch.tensor(",
            "-            [[[3, 1],",
            "-              [0, 4]]], dtype=torch.float32)",
            "+            [",
            "+                [[3, 1], [0, 4]],",
            "+                [[3, 1], [0, 4]]",
            "+            ], dtype=torch.float32)",
            "assert_allclose(conf_mat, conf_mat_real)",
            "",
            "def test_three_classes(self):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=1, insert_id=429686)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=429687)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=4, insert_id=429688)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=429689)",
            "Insert(target_node=IN(type=list), node=('list', None), position=1, insert_id=429690)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=429691)",
            "Insert(target_node=IN(type=list), node=('list', None), position=3, insert_id=429692)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=429693)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=429694)",
            "Insert(target_node=IN(type=list), node=('integer', '3'), position=1, insert_id=429695)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=429696)",
            "Insert(target_node=IN(type=list), node=('integer', '1'), position=3, insert_id=429697)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=429698)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=429699)",
            "Insert(target_node=IN(type=list), node=('integer', '0'), position=1, insert_id=429700)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=429701)",
            "Insert(target_node=IN(type=list), node=('integer', '4'), position=3, insert_id=429702)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=429703)",
            "Delete(target_node=ASTNode(type=], text=]))"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 802,
        "neg_line": [
            "-[[[3, 1],",
            "-[0, 4]]], dtype=torch.float32)"
        ],
        "pos_line": [
            "+[",
            "+[[3, 1], [0, 4]],",
            "+[[3, 1], [0, 4]]",
            "+], dtype=torch.float32)"
        ],
        "core_change": "-[[[3, 1], -[0, 4]]], dtype=torch.float32) +[ +[[3, 1], [0, 4]], +[[3, 1], [0, 4]] +], dtype=torch.float32)",
        "core_API": "confusion_matrix"
    },
    {
        "commit_hash": "690766535a591367ad86907835b39730f4aa1dea",
        "index": "e3522b81..61a9a23e 100644",
        "commit_message": "remove graph api (#818)\n\n* remove graph api\n\n* changelog\n\n* timeout test\n\n* codacy\n\n* remove graph in tl.models\n\n* increase timeout time\n\n* remove tl in tests\n\n* Additional Cleaning\n\n* Timeout time added\n\n* tests directory refactored\n\n* Tests Fix Update python, 2.6\n\n* YAPF Cleaning\n\n* get_env fix\n\n* Python 2 fix applied\n\n* Python 2 Error fixes\n\n* TL 1.10.1rc0 released\n\n* Doc and YAPF Fix\n\n* Test YAPF Fix\n\n* RTD Lazy Import Fix\n\n* Travis config restored\n\n* Revert \"Travis config restored\"\n\nThis reverts commit 03fe83eb568c91765461012a84bde309b8d15e69.\n\n",
        "file": "TensorLayer.txt.json",
        "label": "yes",
        "comments": "update param for type fix",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def model(x, is_train, reuse):",
            "# nt = Conv2d(nin, 16, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc1')",
            "# nt = Conv2d(nt, 8, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc2')",
            "## 2. Spatial transformer module (sampler)",
            "-        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=[40, 40], name='spatial')",
            "+        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=(40, 40), name='spatial')",
            "s = n",
            "## 3. Classifier",
            "n = tl.layers.Conv2d("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('tuple', None), position=2, insert_id=2633515)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2633516)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=40), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=40), position=3)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2633517)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 803,
        "neg_line": [
            "-n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=[40, 40], name='spatial')"
        ],
        "pos_line": [
            "+n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=(40, 40), name='spatial')"
        ],
        "core_change": "-n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=[40, 40], name='spatial') +n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=(40, 40), name='spatial')",
        "core_API": "SpatialTransformer2dAffineLayer"
    },
    {
        "commit_hash": "f0b490151e4a851c0821e1f215bb7a26565d24f7",
        "index": "44110f5e4..c92d656c0 100644",
        "commit_message": "üö® üö® üö® Fix ViT parameter initialization (#19341)\n\nThis PR aims to rectify the discrepancy between the training performances of HF and Timm ViT implementations.\n\n- Initializes torch and flax ViT dense layer weights with trunc_normal instead of normal (consistent with the TF implementation.\n- Initializes cls_token and positional_embeddings with trunc_normal\n- Updates DeiT copy to reflect the changes\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change API call for shape fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DeiTPreTrainedModel(PreTrainedModel):",
            "def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:",
            "\"\"\"Initialize the weights\"\"\"",
            "if isinstance(module, (nn.Linear, nn.Conv2d)):",
            "-            # Slightly different from the TF version which uses truncated_normal for initialization",
            "-            # cf https://github.com/pytorch/pytorch/pull/5617",
            "-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)",
            "+            module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)",
            "if module.bias is not None:",
            "module.bias.data.zero_()",
            "elif isinstance(module, nn.LayerNorm):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=1533976)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=1533977)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1533978)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1533979)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1533980)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=1533981)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1533982)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'module'), position=0, insert_id=1533983)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1533984)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weight'), position=2, insert_id=1533985)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1533986)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1533987)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'trunc_normal_'), position=2, insert_id=1533988)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1533989)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1533990)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=normal_), value='init')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=normal_), position=2)",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 804,
        "neg_line": [
            "-# Slightly different from the TF version which uses truncated_normal for initialization",
            "-# cf https://github.com/pytorch/pytorch/pull/5617",
            "-module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)"
        ],
        "pos_line": [
            "+module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)"
        ],
        "core_change": "-# Slightly different from the TF version which uses truncated_normal for initialization -# cf https://github.com/pytorch/pytorch/pull/5617 -module.weight.data.normal_(mean=0.0, std=self.config.initializer_range) +module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)",
        "core_API": "normal_"
    },
    {
        "commit_hash": "8880f696b6b8368a76296126476ea020fc7c814c",
        "index": "d2bb9f7..ebc6ada 100644",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "yes",
        "comments": "change condition check for type fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SelectAdaptivePool2d(nn.Module):",
            "assert False, 'Invalid pool type: %s' % pool_type",
            "",
            "def is_identity(self):",
            "-        return self.pool_type == ''",
            "+        return not self.pool_type",
            "",
            "def forward(self, x):",
            "x = self.pool(x)",
            "-        if self.flatten:",
            "-            x = x.flatten(1)",
            "+        x = self.flatten(x)",
            "return x",
            "",
            "def feat_mult(self):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=5)",
            "Insert(target_node=ASTNode(type=return_statement), node=('not_operator', None), position=1, insert_id=1477876)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=1477877)",
            "Move(target_node=IN(type=not_operator), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'x'), position=1, insert_id=1477878)",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=string, text=''))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=flatten))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 806,
        "neg_line": [
            "-return self.pool_type == ''",
            "-if self.flatten:",
            "-x = x.flatten(1)"
        ],
        "pos_line": [
            "+return not self.pool_type",
            "+x = self.flatten(x)"
        ],
        "core_change": "-return self.pool_type == '' +return not self.pool_type -if self.flatten: -x = x.flatten(1) +x = self.flatten(x)",
        "core_API": "pool"
    },
    {
        "commit_hash": "241b1b1f7393e01528bc9bea83f7b6ee93ce3467",
        "index": "b276cd1..71d7734 100644",
        "commit_message": "Fix tests. Disable verbose test logging.\n\n",
        "file": "seq2seq.txt.json",
        "label": "no",
        "comments": "log fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AttentionDecoderTest(tf.test.TestCase, DecoderTests):",
            "\"\"\"",
            "def setUp(self):",
            "tf.test.TestCase.setUp(self)",
            "+    tf.logging.set_verbosity(tf.logging.INFO)",
            "DecoderTests.__init__(self)",
            "self.attention_dim = 64",
            "self.input_seq_len = 10"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2169120)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2169121)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2169122)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2169123)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2169124)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2169125)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_verbosity'), position=2, insert_id=2169126)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2169127)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2169128)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2169129)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2169130)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2169131)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logging'), position=2, insert_id=2169132)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2169133)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2169134)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'INFO'), position=2, insert_id=2169135)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2169136)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2169137)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logging'), position=2, insert_id=2169138)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 807,
        "neg_line": [],
        "pos_line": [
            "+tf.logging.set_verbosity(tf.logging.INFO)"
        ],
        "core_change": "+tf.logging.set_verbosity(tf.logging.INFO)",
        "core_API": "setUp"
    },
    {
        "commit_hash": "86d0b26d6c5566506b1be55002654b48d9c19ffe",
        "index": "3243ee108..1a9252a7d 100644",
        "commit_message": "Fix matmul inputs dtype (#18585)\n\n\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DisentangledSelfAttention(nn.Module):",
            "dim=-1,",
            "index=p2c_pos.squeeze(0).expand([query_layer.size(0), key_layer.size(-2), key_layer.size(-2)]),",
            ").transpose(-1, -2)",
            "-            score += p2c_att / scale",
            "+            score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)",
            "",
            "return score"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=1192994)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1192995)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1192996)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1192997)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1192998)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=1192999)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1193000)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=scale), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1193001)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1193002)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1193003)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1193004)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1193005)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1193006)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'p2c_att'), position=0, insert_id=1193007)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1193008)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1193009)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 808,
        "neg_line": [
            "-score += p2c_att / scale"
        ],
        "pos_line": [
            "+score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)"
        ],
        "core_change": "-score += p2c_att / scale +score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)",
        "core_API": "squeeze"
    },
    {
        "commit_hash": "8d47a71b00f4c7cc7542c0f7080eba8bce7b16fd",
        "index": "46fac1a1..3cd4cdb1 100644",
        "commit_message": "Fix Tutorial 9  (#734)\n\n* Add package download\n\n* Change dev to train file\n",
        "file": "haystack.txt.json",
        "label": "no",
        "comments": "config replace rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"\\n\",",
            "\"retriever.train(\\n\",",
            "\"    data_dir=doc_dir,\\n\",",
            "-    \"    train_filename=dev_filename,\\n\",",
            "+    \"    train_filename=train_filename,\\n\",",
            "\"    dev_filename=dev_filename,\\n\",",
            "\"    test_filename=dev_filename,\\n\",",
            "\"    n_epochs=1,\\n\","
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"    train_filename=dev_filename,\\n\"), value='\"    train_filename=train_filename,\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 809,
        "neg_line": [
            "-\"    train_filename=dev_filename,\\n\","
        ],
        "pos_line": [
            "+\"    train_filename=train_filename,\\n\","
        ],
        "core_change": "-\"    train_filename=dev_filename,\\n\", +\"    train_filename=train_filename,\\n\",",
        "core_API": "train"
    },
    {
        "commit_hash": "07fc4b7724f48a7fe555dcf696881525105af600",
        "index": "ce38789..0219d36 100644",
        "commit_message": "fix some code\n\n",
        "file": "deep-learning-for-image-processing.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def evaluate(model, data_loader, device):",
            "image = list(img.to(device) for img in image)",
            "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]",
            "",
            "-        torch.cuda.synchronize(device)",
            "+        # ÂΩì‰ΩøÁî®CPUÊó∂ÔºåË∑≥ËøáGPUÁõ∏ÂÖ≥Êåá‰ª§",
            "+        if device != torch.device(\"cpu\"):",
            "+            torch.cuda.synchronize(device)",
            "+",
            "model_time = time.time()",
            "outputs = model(image)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1815705)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1815706)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1815707)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1815708)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1815709)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'device'), position=0, insert_id=1815710)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=1815711)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=2, insert_id=1815712)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1815713)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1815714)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1815715)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1815716)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1815717)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1815718)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"cpu\"'), position=1, insert_id=1815719)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1815720)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 812,
        "neg_line": [
            "-torch.cuda.synchronize(device)"
        ],
        "pos_line": [
            "+# ÂΩì‰ΩøÁî®CPUÊó∂ÔºåË∑≥ËøáGPUÁõ∏ÂÖ≥Êåá‰ª§",
            "+if device != torch.device(\"cpu\"):",
            "+torch.cuda.synchronize(device)",
            "+"
        ],
        "core_change": "-torch.cuda.synchronize(device) +# ÂΩì‰ΩøÁî®CPUÊó∂ÔºåË∑≥ËøáGPUÁõ∏ÂÖ≥Êåá‰ª§ +if device != torch.device(\"cpu\"): +torch.cuda.synchronize(device) +",
        "core_API": "to"
    },
    {
        "commit_hash": "55a3aea438269d004d6b95ac9e7917d05580e849",
        "index": "1c65ab61..5d67b801 100644",
        "commit_message": "minor fixe lambda test\n\n",
        "file": "TensorLayer.txt.json",
        "label": "yes",
        "comments": "remove param for argument fix",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Layer_Lambda_Test(CustomTestCase):",
            "self.dense1 = tl.layers.Dense(in_channels=1, n_units=5)",
            "self.dense2 = tl.layers.Dense(in_channels=1, n_units=5)",
            "self.dense3 = tl.layers.Dense(in_channels=1, n_units=5)",
            "-                self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_weights=[], fn_args={'foo': 1024})",
            "+                self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_args={'foo': 1024})",
            "",
            "def forward(self, x, bar=None):",
            "noise = self.dense1(x)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=fn_weights))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 813,
        "neg_line": [
            "-self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_weights=[], fn_args={'foo': 1024})"
        ],
        "pos_line": [
            "+self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_args={'foo': 1024})"
        ],
        "core_change": "-self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_weights=[], fn_args={'foo': 1024}) +self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_args={'foo': 1024})",
        "core_API": "Dense"
    },
    {
        "commit_hash": "e386caa07107a77c7dfb2301c8ba60f05d0211da",
        "index": "030e0d13..2faccd75 100644",
        "commit_message": "mass linter fix\n\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TacotronTrainTest(unittest.TestCase):",
            "optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "for i in range(5):",
            "mel_out, mel_postnet_out, align, stop_tokens = model.forward(",
            "-                input, input_lengths, mel_spec, mel_lengths, speaker_ids)",
            "+                input_dummy, input_lengths, mel_spec, mel_lengths, speaker_ids)",
            "assert torch.sigmoid(stop_tokens).data.max() <= 1.0",
            "assert torch.sigmoid(stop_tokens).data.min() >= 0.0",
            "optimizer.zero_grad()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=input), value='input_dummy')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 814,
        "neg_line": [
            "-input, input_lengths, mel_spec, mel_lengths, speaker_ids)"
        ],
        "pos_line": [
            "+input_dummy, input_lengths, mel_spec, mel_lengths, speaker_ids)"
        ],
        "core_change": "-input, input_lengths, mel_spec, mel_lengths, speaker_ids) +input_dummy, input_lengths, mel_spec, mel_lengths, speaker_ids)",
        "core_API": "Adam"
    },
    {
        "commit_hash": "6f5b2de1a30dfba89a3508bd3a46f61a6c154c27",
        "index": "24df0a4a59..eaf3ad1a9d 100644",
        "commit_message": "lint fix\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def scatter_nd(",
            "initial_val = torch.tensor(0).type(dtype)",
            "elif reduction == \"min\":",
            "if dtype.is_floating_point:",
            "-            initial_val = min(torch.finfo(dtype).max, 1e12)",
            "+            initial_val = min(torch.finfo(dtype).max, 1e12)",
            "else:",
            "initial_val = min(torch.iinfo(dtype).max, 1e12)",
            "elif reduction == \"max\":",
            "if dtype.is_floating_point:",
            "-            initial_val = max(torch.finfo(dtype).min, 1e-12)",
            "+            initial_val = max(torch.finfo(dtype).min, 1e-12)",
            "else:",
            "initial_val = max(torch.iinfo(dtype).min, 1e-12)",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 815,
        "neg_line": [
            "-initial_val = min(torch.finfo(dtype).max, 1e12)",
            "-initial_val = max(torch.finfo(dtype).min, 1e-12)"
        ],
        "pos_line": [
            "+initial_val = min(torch.finfo(dtype).max, 1e12)",
            "+initial_val = max(torch.finfo(dtype).min, 1e-12)"
        ],
        "core_change": "-initial_val = min(torch.finfo(dtype).max, 1e12) +initial_val = min(torch.finfo(dtype).max, 1e12) -initial_val = max(torch.finfo(dtype).min, 1e-12) +initial_val = max(torch.finfo(dtype).min, 1e-12)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "b378657b8c50f49f9622281e698c62d5538b9397",
        "index": "a2fcd23..faf9cbf 100644",
        "commit_message": "Fixes #79: Added custom dropout op, that adds probability tensor to the collection DROPOUTS and zeros it on prediction\n\n",
        "file": "skflow.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):",
            "for i, n_units in enumerate(hidden_units):",
            "with tf.variable_scope('layer%d' % i):",
            "tensor_in = linear(tensor_in, n_units, True)",
            "-            tensor_in = activation(tensor_in)",
            "-            if keep_prob:",
            "-                tensor_in = tf.nn.dropout(tensor_in, keep_prob)",
            "+                tensor_in = activation(tensor_in)",
            "+                if keep_prob:",
            "+                    tensor_in = skflow.ops.dropout(tensor_in, keep_prob)",
            "return tensor_in"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='skflow')",
            "Update(target_node=ASTNode(type=identifier, text=nn), value='ops')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 2,
        "number": 816,
        "neg_line": [
            "-tensor_in = activation(tensor_in)",
            "-if keep_prob:",
            "-tensor_in = tf.nn.dropout(tensor_in, keep_prob)"
        ],
        "pos_line": [
            "+tensor_in = activation(tensor_in)",
            "+if keep_prob:",
            "+tensor_in = skflow.ops.dropout(tensor_in, keep_prob)"
        ],
        "core_change": "-tensor_in = activation(tensor_in) -if keep_prob: -tensor_in = tf.nn.dropout(tensor_in, keep_prob) +tensor_in = activation(tensor_in) +if keep_prob: +tensor_in = skflow.ops.dropout(tensor_in, keep_prob)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "8d40aefe04244be97d29886577f13d8d5779fd47",
        "index": "c8549893..d218be96 100644",
        "commit_message": "[Feature]: Evaluation with TensorRT backend (#5198)\n\n* evaluate trt models\n\n* update version of onnx\n\n* update maskrcnn results\n\n* add backend argument\n\n* update fcos results\n\n* update\n\n* fix bug\n\n* update  doc\n",
        "file": "mmdetection.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SingleStageDetector(BaseDetector):",
            "# get shape as tensor",
            "img_shape = torch._shape_as_tensor(img)[2:]",
            "img_metas[0]['img_shape_for_onnx'] = img_shape",
            "+        # get pad input shape to support onnx dynamic shape for exporting",
            "+        # `CornerNet` and `CentripetalNet`, which 'pad_shape' is used",
            "+        # for inference",
            "+        img_metas[0]['pad_shape_for_onnx'] = img_shape",
            "# TODO:move all onnx related code in bbox_head to onnx_export function",
            "det_bboxes, det_labels = self.bbox_head.get_bboxes(*outs, img_metas)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=626553)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=626554)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=0, insert_id=626555)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=626556)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'img_shape'), position=2, insert_id=626557)",
            "Insert(target_node=IN(type=subscript), node=('subscript', None), position=0, insert_id=626558)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=626559)",
            "Insert(target_node=IN(type=subscript), node=('string', \"'pad_shape_for_onnx'\"), position=2, insert_id=626560)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=626561)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'img_metas'), position=0, insert_id=626562)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=626563)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=626564)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=626565)"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 819,
        "neg_line": [],
        "pos_line": [
            "+# get pad input shape to support onnx dynamic shape for exporting",
            "+# `CornerNet` and `CentripetalNet`, which 'pad_shape' is used",
            "+# for inference",
            "+img_metas[0]['pad_shape_for_onnx'] = img_shape"
        ],
        "core_change": "+# get pad input shape to support onnx dynamic shape for exporting +# `CornerNet` and `CentripetalNet`, which 'pad_shape' is used +# for inference +img_metas[0]['pad_shape_for_onnx'] = img_shape",
        "core_API": "_shape_as_tensor"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "acc6981d2..933c88795 100644",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def convert_bort_checkpoint_to_pytorch(bort_checkpoint_path: str, pytorch_dump_f",
            "# | `encoder.transformer_cells.*.proj.weight`                      | `bert.encoder.layer.*.output.dense.weight`",
            "",
            "# Helper function to convert MXNET Arrays to PyTorch",
            "-    def to_torch(mx_array) -> torch.nn.Parameter:",
            "-        return torch.nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))",
            "+    def to_torch(mx_array) -> nn.Parameter:",
            "+        return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))",
            "",
            "# Check param shapes and map new HF param back",
            "def check_and_map_params(hf_param, gluon_param):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=nn), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=nn), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 820,
        "neg_line": [
            "-def to_torch(mx_array) -> torch.nn.Parameter:",
            "-return torch.nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))"
        ],
        "pos_line": [
            "+def to_torch(mx_array) -> nn.Parameter:",
            "+return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))"
        ],
        "core_change": "-def to_torch(mx_array) -> torch.nn.Parameter: -return torch.nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy())) +def to_torch(mx_array) -> nn.Parameter: +return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))",
        "core_API": "Parameter"
    },
    {
        "commit_hash": "93f5128509278f425afb6bcf0da574c0af0e0c16",
        "index": "739ba49f..d4ed89e3 100644",
        "commit_message": "Misc fixes (#2342)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2342\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D22601110\n\nPulled By: myleott\n\nfbshipit-source-id: 7a704c07d507692f274c31ec74b090134fa9dee3\n\n",
        "file": "fairseq.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def clip_grad_norm_(params, max_norm, aggregate_norm_fn=None) -> torch.Tensor:",
            "if multi_tensor_l2norm_available:",
            "total_norm = multi_tensor_total_norm(grads)",
            "else:",
            "-            warnings.warn(",
            "-                \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"",
            "-                \"you may get better performance by installing NVIDIA's apex library\"",
            "-            )",
            "+            if torch.cuda.is_available():",
            "+                warnings.warn(",
            "+                    \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"",
            "+                    \"you may get better performance by installing NVIDIA's apex library\"",
            "+                )",
            "total_norm = torch.norm(",
            "torch.stack([torch.norm(g, p=2, dtype=torch.float32) for g in grads])",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=4, insert_id=211951)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=total_norm), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=assignment), node=('ERROR', None), position=2, insert_id=211952)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'if'), position=0, insert_id=211953)",
            "Insert(target_node=IN(type=ERROR), node=('call', None), position=1, insert_id=211954)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=2, insert_id=211955)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=211956)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=211957)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=211958)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=211959)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_available'), position=2, insert_id=211960)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=211961)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=211962)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=211963)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=211964)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=211965)",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 5,
        "minus_line": 4,
        "AST_diff_line": 19,
        "number": 821,
        "neg_line": [
            "-warnings.warn(",
            "-\"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"",
            "-\"you may get better performance by installing NVIDIA's apex library\"",
            "-)"
        ],
        "pos_line": [
            "+if torch.cuda.is_available():",
            "+warnings.warn(",
            "+\"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"",
            "+\"you may get better performance by installing NVIDIA's apex library\"",
            "+)"
        ],
        "core_change": "-warnings.warn( -\"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \" -\"you may get better performance by installing NVIDIA's apex library\" -) +if torch.cuda.is_available(): +warnings.warn( +\"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \" +\"you may get better performance by installing NVIDIA's apex library\" +)",
        "core_API": "warn"
    },
    {
        "commit_hash": "27e123df1e871b2da228a7b1da753a94a05baa7b",
        "index": "c7849774..95af8358 100644",
        "commit_message": "fix data device type bug (#3856)\n\n\n",
        "file": "nni.txt.json",
        "label": "yes",
        "comments": "add condition check for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelSpeedupTensorRT(BaseModelSpeedup):",
            "Model input tensor",
            "\"\"\"",
            "# convert pytorch tensor to numpy darray",
            "+        if test_data.device != torch.device(\"cpu\"):",
            "+            test_data = test_data.to(\"cpu\")",
            "test_data = test_data.numpy()",
            "# Numpy dtype should be float32",
            "assert test_data.dtype == np.float32"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=(')', ')'), position=1, insert_id=663864)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 823,
        "neg_line": [],
        "pos_line": [
            "+if test_data.device != torch.device(\"cpu\"):",
            "+test_data = test_data.to(\"cpu\")"
        ],
        "core_change": "+if test_data.device != torch.device(\"cpu\"): +test_data = test_data.to(\"cpu\")",
        "core_API": "device"
    },
    {
        "commit_hash": "0c7a3b47928c6ac0d4cc5791dc20f7ee76acfaf5",
        "index": "ba4f82597..2f3e05f84 100644",
        "commit_message": "fixed the case when r > 1\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Dio(AbsFeatsExtract):",
            "",
            "return f0",
            "",
            "-    @staticmethod",
            "-    def _average_by_duration(x: torch.Tensor, d: torch.Tensor) -> torch.Tensor:",
            "-        assert d.sum() == len(x)",
            "+    def _average_by_duration(self, x: torch.Tensor, d: torch.Tensor) -> torch.Tensor:",
            "+        assert len(x) - d.sum() < self.reduction_factor",
            "d_cumsum = F.pad(d.cumsum(dim=0), (1, 0))",
            "x_avg = [",
            "x[start:end].masked_select(x[start:end].gt(0.0)).mean(dim=0)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=function_definition), position=2)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'self'), position=1, insert_id=144442)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=144443)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('binary_operator', None), position=0, insert_id=144444)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('<', '<'), position=1, insert_id=144445)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=2, insert_id=144446)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=144447)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=144448)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=144449)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reduction_factor'), position=2, insert_id=144450)",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=staticmethod))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=decorated_definition))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 824,
        "neg_line": [
            "-@staticmethod",
            "-def _average_by_duration(x: torch.Tensor, d: torch.Tensor) -> torch.Tensor:",
            "-assert d.sum() == len(x)"
        ],
        "pos_line": [
            "+def _average_by_duration(self, x: torch.Tensor, d: torch.Tensor) -> torch.Tensor:",
            "+assert len(x) - d.sum() < self.reduction_factor"
        ],
        "core_change": "-@staticmethod -def _average_by_duration(x: torch.Tensor, d: torch.Tensor) -> torch.Tensor: -assert d.sum() == len(x) +def _average_by_duration(self, x: torch.Tensor, d: torch.Tensor) -> torch.Tensor: +assert len(x) - d.sum() < self.reduction_factor",
        "core_API": "sum"
    },
    {
        "commit_hash": "9c97baf79b92e93e8a83fbaca46268fe46f12433",
        "index": "3825fa25..c6684edc 100644",
        "commit_message": "refactor of the optimization interface (#212)\n\n* initial commit\n\n* optim.py\n\n* basic thing working\n\n* got some tests to pass\n\n* fixedguidemodel test happy\n\n* more comments, default implementation for loss_and_grads, ...\n\n* change import\n\n* got a lot more tests to pass; a tiny bit inference test refactoring\n\n* flake8\n\n* fix more tests\n\n* made conjugate model tests faster; made example test pass; all tests pass now maybe?\n\n* made conjugate model tests faster; made example test pass; all tests pass now maybe?\n\n* xfail test_examples\n\n* xfail test_examples flake8\n\n* made tests/poutine/test_mapdata.py happy\n\n* fix test lambda test\n\n* more refactoring...\n\n* more refactoring...\n\n* progress on further rerefactoring\n\n* always more refactoring\n\n* linting\n\n* done with initial re-refactor?\n\n* fix bug\n\n* fl8\n\n* address tests\n\n*  make examples happy\n\n* fix tests. fixed everything?\n\n* final clean-up?\n\n* assert fix\n\n* sets not hashable error\n\n* exclusively doc string changes\n\n* batch rearrange\n\n* fix fake conflict\n\n* fl8\n\n* doc rebuild workaround\n\n* fix random module test (tags)\n\n* wrap loss\n\n* rebalance tests\n\n* whoops\n\n* fix more merge conflicts; fix test imports\n\n* fl8\n\n*  delete old tracegraph_kl_qp.py\n\n* travis workaround\n\n* better rng seeds/inits?\n\n* fix travis yml\n\n* these seeds/inits should be good\n\n* fix data\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "def main():",
            "parser.add_argument('-n', '--num-epochs', nargs='?', default=1000, type=int)",
            "args = parser.parse_args()",
            "for step in range(args.num_epochs):",
            "-        kl_optim.step(observed_data)  # loss",
            "+        svi.step(observed_data)  # loss",
            "if step % 100 == 0:",
            "if verbose:",
            "print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0]))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=kl_optim), value='svi')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 826,
        "neg_line": [
            "-kl_optim.step(observed_data)  # loss"
        ],
        "pos_line": [
            "+svi.step(observed_data)  # loss"
        ],
        "core_change": "-kl_optim.step(observed_data)  # loss +svi.step(observed_data)  # loss",
        "core_API": "add_argument"
    },
    {
        "commit_hash": "43e6143befbc3704bf99d7e2dc57e9e18d366dff",
        "index": "9f450ab..816f4ae 100755",
        "commit_message": "Fix #1712 broken support for AMP w/ PyTorch < 1.10. Disable loss scaler for bfloat16\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "yes",
        "comments": "add condition check for type fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def main():",
            "if utils.is_primary(args):",
            "_logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')",
            "elif use_amp == 'native':",
            "-        amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)",
            "-        if device.type == 'cuda':",
            "+        try:",
            "+            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)",
            "+        except (AttributeError, TypeError):",
            "+            # fallback to CUDA only AMP for PyTorch < 1.10",
            "+            assert device.type == 'cuda'",
            "+            amp_autocast = torch.cuda.amp.autocast",
            "+        if device.type == 'cuda' and amp_dtype == torch.float16:",
            "+            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it",
            "loss_scaler = NativeScaler()",
            "if utils.is_primary(args):",
            "_logger.info('Using native Torch AMP. Training in mixed precision.')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 8,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 828,
        "neg_line": [
            "-amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)",
            "-if device.type == 'cuda':"
        ],
        "pos_line": [
            "+try:",
            "+amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)",
            "+except (AttributeError, TypeError):",
            "+# fallback to CUDA only AMP for PyTorch < 1.10",
            "+assert device.type == 'cuda'",
            "+amp_autocast = torch.cuda.amp.autocast",
            "+if device.type == 'cuda' and amp_dtype == torch.float16:",
            "+# loss scaler only used for float16 (half) dtype, bfloat16 does not need it"
        ],
        "core_change": "-amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype) -if device.type == 'cuda': +try: +amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype) +except (AttributeError, TypeError): +# fallback to CUDA only AMP for PyTorch < 1.10 +assert device.type == 'cuda' +amp_autocast = torch.cuda.amp.autocast +if device.type == 'cuda' and amp_dtype == torch.float16: +# loss scaler only used for float16 (half) dtype, bfloat16 does not need it",
        "core_API": "is_primary"
    },
    {
        "commit_hash": "2cdfbd18a750cabaff1499fad7473dd91f3c7fa7",
        "index": "ae66e632..50e04760 100644",
        "commit_message": "Previously, many unit test files started with `enable_v2_behavior`, which would have caused them to run in V2 mode when executing with a V1 test flag. The correct behavior would in fact be to skip such tests when executing with a V1 test flag.\n\nThis fix significantly reduces the total V1 + V2 test load by eliminating redundancy.\n\nPiperOrigin-RevId: 424734850\n\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "remove API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):",
            "",
            "",
            "if __name__ == '__main__':",
            "-  tf.compat.v1.enable_eager_execution()",
            "tf.test.main()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=2069255)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=compat))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=v1))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=enable_eager_execution))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 829,
        "neg_line": [
            "-tf.compat.v1.enable_eager_execution()"
        ],
        "pos_line": [],
        "core_change": "-tf.compat.v1.enable_eager_execution()",
        "core_API": "enable_eager_execution"
    },
    {
        "commit_hash": "ad8b2b9d08eec5271581dabdf75aec3a83190f0b",
        "index": "dd7cc218..7df44c2e 100644",
        "commit_message": "hyperpose compatible:\n(1)maxpool and batchnorm dataformat debuged,support \"channels_first\"\n(2)vgg forward fixed\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BatchNorm(Layer):",
            "if self.axes is None:",
            "self.axes = [i for i in range(len(inputs.shape)) if i != self.channel_axis]",
            "",
            "+        mean, var = tf.nn.moments(inputs, self.axes, keepdims=False)",
            "if self.is_train:",
            "# update moving_mean and moving_var",
            "-            mean, var = tf.nn.moments(inputs, self.axes, keepdims=False)",
            "self.moving_mean = moving_averages.assign_moving_average(",
            "self.moving_mean, mean, self.decay, zero_debias=False",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=3)",
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=2249357)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2249358)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2249359)",
            "Insert(target_node=IN(type=if_statement), node=('block', ''), position=3, insert_id=2249360)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 830,
        "neg_line": [
            "-mean, var = tf.nn.moments(inputs, self.axes, keepdims=False)"
        ],
        "pos_line": [
            "+mean, var = tf.nn.moments(inputs, self.axes, keepdims=False)"
        ],
        "core_change": "+mean, var = tf.nn.moments(inputs, self.axes, keepdims=False) -mean, var = tf.nn.moments(inputs, self.axes, keepdims=False)",
        "core_API": "moments"
    },
    {
        "commit_hash": "ae85251a213600184a09b2976657d0b879105fb4",
        "index": "fac169d..1927d40 100644",
        "commit_message": "fix bug cache dataset, cache should before shuffle and batch.\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class WeightNormalization(WeightNormalizationOriginal):",
            "",
            "def build(self, input_shape):",
            "\"\"\"Build `Layer`\"\"\"",
            "-        #input_shape = tf.TensorShape(input_shape)",
            "-        #self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])",
            "+        # input_shape = tf.TensorShape(input_shape)",
            "+        # self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])",
            "",
            "# remove 2 lines above to run weight-norm on tf.function with dynamic shape"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 831,
        "neg_line": [
            "-#input_shape = tf.TensorShape(input_shape)",
            "-#self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])"
        ],
        "pos_line": [
            "+# input_shape = tf.TensorShape(input_shape)",
            "+# self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])"
        ],
        "core_change": "-#input_shape = tf.TensorShape(input_shape) -#self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:]) +# input_shape = tf.TensorShape(input_shape) +# self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])",
        "core_API": "TensorShape"
    },
    {
        "commit_hash": "e1294302795fc31b0318b513706fe143b49c35d1",
        "index": "ee9cd58560..cf73adf4d9 100644",
        "commit_message": "lint fixes (#5332)\n\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def trace(",
            "offset: int = 0,",
            "axis1: int = 0,",
            "axis2: int = 1,",
            "-    out: Optional[torch.Tensor] = None",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "ret = torch.diagonal(x, offset=offset, dim1=axis1, dim2=axis2)",
            "ret = torch.sum(ret)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=319628)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 834,
        "neg_line": [
            "-out: Optional[torch.Tensor] = None"
        ],
        "pos_line": [
            "+out: Optional[torch.Tensor] = None,"
        ],
        "core_change": "-out: Optional[torch.Tensor] = None +out: Optional[torch.Tensor] = None,",
        "core_API": "diagonal"
    },
    {
        "commit_hash": "ae985fc4f34bba9b93d65e24ae8ad0ef008b6903",
        "index": "4dbc61fd..a3421833 100644",
        "commit_message": "ugly fix of MODEL_KEY\n\n",
        "file": "tensorpack.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ValidationCallback(PeriodicCallback):",
            "self.cost_var_name = cost_var_name",
            "",
            "def _before_train(self):",
            "-        self.input_vars = tf.get_collection(MODEL_KEY)[0].get_input_vars()",
            "+        self.input_vars = tf.get_collection(INPUT_VARS_KEY)",
            "self.cost_var = self.get_tensor(self.cost_var_name)",
            "self._find_output_vars()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=MODEL_KEY), value='INPUT_VARS_KEY')",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_input_vars))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 836,
        "neg_line": [
            "-self.input_vars = tf.get_collection(MODEL_KEY)[0].get_input_vars()"
        ],
        "pos_line": [
            "+self.input_vars = tf.get_collection(INPUT_VARS_KEY)"
        ],
        "core_change": "-self.input_vars = tf.get_collection(MODEL_KEY)[0].get_input_vars() +self.input_vars = tf.get_collection(INPUT_VARS_KEY)",
        "core_API": "get_collection"
    },
    {
        "commit_hash": "d5ef64738f0437d2e0cbbe15440e974684754f23",
        "index": "a3df41f9d..c165ae0ef 100644",
        "commit_message": "fixed to keep compatibility\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Encoder(torch.nn.Module):",
            "pos_enc_class(attention_dim, positional_dropout_rate),",
            ")",
            "elif input_layer is None:",
            "-            self.embed = pos_enc_class(attention_dim, positional_dropout_rate)",
            "+            self.embed = torch.nn.Sequential(",
            "+                pos_enc_class(attention_dim, positional_dropout_rate)",
            "+            )",
            "else:",
            "raise ValueError(\"unknown input_layer: \" + input_layer)",
            "self.normalize_before = normalize_before"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=172407)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=172408)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=172409)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=172410)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Sequential'), position=2, insert_id=172411)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=172412)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=172413)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=172414)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=172415)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=172416)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 837,
        "neg_line": [
            "-self.embed = pos_enc_class(attention_dim, positional_dropout_rate)"
        ],
        "pos_line": [
            "+self.embed = torch.nn.Sequential(",
            "+pos_enc_class(attention_dim, positional_dropout_rate)",
            "+)"
        ],
        "core_change": "-self.embed = pos_enc_class(attention_dim, positional_dropout_rate) +self.embed = torch.nn.Sequential( +pos_enc_class(attention_dim, positional_dropout_rate) +)",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "98d40fed3a4515077163adab9dfd8fb2fccf1267",
        "index": "ca574f4ec..527a8a091 100644",
        "commit_message": "Cleanup the usage of `layer_norm_eps` in some models (#21336)\n\n* fix\n\n* fix\n\n* make style\n\n* For CLIP\n\n* For OwlViT\n\n* For XCLIP\n\n* For CLIPSeg\n\n* For GroupViT\n\n* fix docstrings\n\n* fix docstrings\n\n* For AltCLIP\n\n* For ChineseCLIP\n\n* For Blip\n\n* For GiT\n\n* make style\n\n* update\n\n* update\n\n* update\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for math fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ChineseCLIPVisionTransformer(nn.Module):",
            "embed_dim = config.hidden_size",
            "",
            "self.embeddings = ChineseCLIPVisionEmbeddings(config)",
            "-        self.pre_layrnorm = nn.LayerNorm(embed_dim)",
            "+        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "self.encoder = ChineseCLIPVisionEncoder(config)",
            "-        self.post_layernorm = nn.LayerNorm(embed_dim)",
            "+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "",
            "@add_start_docstrings_to_model_forward(CHINESE_CLIP_VISION_INPUTS_DOCSTRING)",
            "@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=ChineseCLIPVisionConfig)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1532713)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1532714)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1532715)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'LayerNorm'), position=2, insert_id=1532716)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1532717)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1532718)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1532719)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1532720)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=1532721)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1532722)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1532723)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=1532724)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1532725)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1532726)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=1532727)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1532728)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'layer_norm_eps'), position=2, insert_id=1532729)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=1532730)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1532731)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'layer_norm_eps'), position=2, insert_id=1532732)",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=LayerNorm))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 838,
        "neg_line": [
            "-self.pre_layrnorm = nn.LayerNorm(embed_dim)",
            "-self.post_layernorm = nn.LayerNorm(embed_dim)"
        ],
        "pos_line": [
            "+self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "+self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)"
        ],
        "core_change": "-self.pre_layrnorm = nn.LayerNorm(embed_dim) +self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) -self.post_layernorm = nn.LayerNorm(embed_dim) +self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
        "core_API": "LayerNorm"
    },
    {
        "commit_hash": "b812ffbddbbecaff61498a1ff97035a8db567e4b",
        "index": "7ffe17cb97..48acbd9669 100644",
        "commit_message": "fix issues with outer\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def svd(",
            "def outer(",
            "x1: torch.Tensor, x2: torch.Tensor, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "-    ret = torch.outer(x1, x2, out=out)",
            "-    return ret",
            "+    return torch.outer(x1, x2, out=out)",
            "",
            "",
            "def diagonal("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('return_statement', None), position=0, insert_id=351368)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=351369)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 839,
        "neg_line": [
            "-ret = torch.outer(x1, x2, out=out)",
            "-return ret"
        ],
        "pos_line": [
            "+return torch.outer(x1, x2, out=out)"
        ],
        "core_change": "-ret = torch.outer(x1, x2, out=out) -return ret +return torch.outer(x1, x2, out=out)",
        "core_API": "outer"
    },
    {
        "commit_hash": "5c18c0103ed979c3d02c55b4dacfcad02daf811a",
        "index": "567c3d33..d9b1b3b9 100644",
        "commit_message": "fix failing tests related to `solve_cast` on torch 1.9 (#2066)\n\n* update torch seed\n\n* rerun CI\n\n* update to use rescale\n\n* rerun ci\n\n* update tests cases to have hardcoded input\n\ndeleted:    test/utilities.py - not used anywhere\n\n* remove fail fast at ci\n\n* manual seed 0 TestImageStitcher::test_smoke\n\n* manual seed 0 TestHomographyTracker::test_real\n\n* manual seed 6 TestHomographyTracker::test_real\n\n* manual seed 245 TestImageStitcher::test_smoke and hardcoded case\n\n* manual seed 1 TestImageStitcher::test_smoke\n\n* manual seed 8 TestHomographyTracker::test_real\n\n- Comment the second frame test\n\n* rerun CI\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSolveCast:",
            "",
            "class TestSolveWithMask:",
            "def test_smoke(self, device, dtype):",
            "+        torch.manual_seed(0)  # issue kornia#2027",
            "A = torch.randn(2, 3, 1, 4, 4, device=device, dtype=dtype)",
            "B = torch.randn(2, 3, 1, 4, 6, device=device, dtype=dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=394418)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=394419)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=394420)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=394421)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=394422)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=394423)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=394424)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=394425)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=394426)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=1, insert_id=394427)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=394428)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 840,
        "neg_line": [],
        "pos_line": [
            "+torch.manual_seed(0)  # issue kornia#2027"
        ],
        "core_change": "+torch.manual_seed(0)  # issue kornia#2027",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "2322eb8e2f9765cb73f59b324cc46a0e9cfe803f",
        "index": "e9cf9adb7..ec7458a7e 100644",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFHubertPreTrainedModel(TFPreTrainedModel):",
            "input_signature=[",
            "{",
            "\"input_values\": tf.TensorSpec((None, None), tf.float32, name=\"input_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2361337)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2361338)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361339)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2361340)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2361341)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2361342)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2361343)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2361344)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2361345)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2361346)",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 842,
        "neg_line": [
            "-\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "-\"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),"
        ],
        "pos_line": [
            "+\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+\"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),"
        ],
        "core_change": "-\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"), -\"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"), +\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"), +\"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
        "core_API": "TensorSpec"
    },
    {
        "commit_hash": "44d2847610944f56a06b7cfa54faadb66e130a83",
        "index": "25edff6d..a6aa64c7 100644",
        "commit_message": "Metrics in distributed setting (#4525)\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* changing start method\n\n* removing unnecessary argument\n\n* adding remaining metrics, removing extra argument\n\n* allowing float values\n\n* bug fix\n\n* more bug fixes\n\n* changing average to return float\n\n* adding timeout for distributed test\n\n* testing unequal batches\n\n* adding distributed auc\n\n* adding distributed spearman correlation\n\n* adding distributed covariance and pearson correlation\n\n* changing distributed test to function, misc changes\n\n* checking batch lengths explicitly to raise errors\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "return change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Entropy(Metric):",
            "average_value = self._entropy / self._count if self._count > 0 else 0",
            "if reset:",
            "self.reset()",
            "-        return average_value",
            "+        return {\"entropy\": average_value}",
            "",
            "@overrides",
            "def reset(self):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('dictionary', None), position=1, insert_id=12036)",
            "Insert(target_node=IN(type=dictionary), node=('{', '{'), position=0, insert_id=12037)",
            "Insert(target_node=IN(type=dictionary), node=('pair', None), position=1, insert_id=12038)",
            "Insert(target_node=IN(type=dictionary), node=('}', '}'), position=2, insert_id=12039)",
            "Insert(target_node=IN(type=pair), node=('string', '\"entropy\"'), position=0, insert_id=12040)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=12041)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=identifier, text=average_value), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 844,
        "neg_line": [
            "-return average_value"
        ],
        "pos_line": [
            "+return {\"entropy\": average_value}"
        ],
        "core_change": "-return average_value +return {\"entropy\": average_value}",
        "core_API": "reset"
    },
    {
        "commit_hash": "9c1abff7105592e8ac9458b1beb4ee8536a5938a",
        "index": "1fd1b8ce3..8c451578b 100644",
        "commit_message": "fix a bug in test\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_maskctc(encoder_arch, interctc_layer_idx, interctc_use_conditioning):",
            "inputs = dict(",
            "speech=torch.randn(2, 10, 20, requires_grad=True),",
            "speech_lengths=torch.tensor([10, 8], dtype=torch.long),",
            "-        text=torch.randint(0, vocab_size + 1, [2, 4], dtype=torch.long),",
            "+        text=torch.randint(2, 4, [2, 4], dtype=torch.long),",
            "text_lengths=torch.tensor([4, 3], dtype=torch.long),",
            ")",
            "loss, *_ = model(**inputs)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=0), value='2')",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '4'), position=3, insert_id=127447)",
            "Delete(target_node=ASTNode(type=identifier, text=vocab_size))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 845,
        "neg_line": [
            "-text=torch.randint(0, vocab_size + 1, [2, 4], dtype=torch.long),"
        ],
        "pos_line": [
            "+text=torch.randint(2, 4, [2, 4], dtype=torch.long),"
        ],
        "core_change": "-text=torch.randint(0, vocab_size + 1, [2, 4], dtype=torch.long), +text=torch.randint(2, 4, [2, 4], dtype=torch.long),",
        "core_API": "randn"
    },
    {
        "commit_hash": "093e901c979c0f8b0af92f79010b53db1ddcc972",
        "index": "8e56e24..91ee8d0 100644",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _add_gradients_summaries(grads_and_vars):",
            "grad_values = grad.values",
            "else:",
            "grad_values = grad",
            "-      summaries.append(tf.histogram_summary(var.op.name + ':gradient',",
            "+      summaries.append(tf.summary.histogram(var.op.name + ':gradient',",
            "grad_values))",
            "-      summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm',",
            "+      summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',",
            "tf.global_norm([grad_values])))",
            "else:",
            "tf.logging.info('Var %s has no gradient', var.op.name)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2213697)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'summaries'), position=0, insert_id=2213698)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2213699)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'append'), position=2, insert_id=2213700)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2213701)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2213702)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2213703)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'histogram'), position=2, insert_id=2213704)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2213705)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'histogram'), position=2, insert_id=2213706)",
            "Update(target_node=ASTNode(type=identifier, text=histogram_summary), value='summary')",
            "Update(target_node=ASTNode(type=identifier, text=histogram_summary), value='summary')",
            "Delete(target_node=ASTNode(type=identifier, text=summaries))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=append))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 847,
        "neg_line": [
            "-summaries.append(tf.histogram_summary(var.op.name + ':gradient',",
            "-summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm',"
        ],
        "pos_line": [
            "+summaries.append(tf.summary.histogram(var.op.name + ':gradient',",
            "+summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',"
        ],
        "core_change": "-summaries.append(tf.histogram_summary(var.op.name + ':gradient', +summaries.append(tf.summary.histogram(var.op.name + ':gradient', -summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm', +summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',",
        "core_API": "append"
    },
    {
        "commit_hash": "3fc3058283d4c47b9bf19909919866dd49eefed6",
        "index": "6fd31c10..0f0d64e9 100644",
        "commit_message": "chore: add release candidate backwards compatibility warnings (#2512)\n\n* docs updates\n\n* remove unused import\n\n* CI - fetch depth 1\n\n* NotImplementedError for unsupported framework modules\n\n* update supported frameworks doc\n\n* reformat\n\n* fix sklearn\n\n* add backwards compatibility warning for supported ML framework modules\n\n* deprecate use of SAVE_NAMESPACE constant\n\n* framework docs MVP\n\n* template typo and notes\n\n* use a simpler example for reusable runnable class\n\n* fix f-string placeholder warning\n\n* fix sklearn integration tests\n\n* fix pickable model integration test\n",
        "file": "BentoML.txt.json",
        "label": "no",
        "comments": "value update",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "except ImportError:  # pragma: no cover",
            ")",
            "",
            "MODULE_NAME = \"bentoml.sklearn\"",
            "-MODEL_FILENAME = f\"{SAVE_NAMESPACE}.{PKL_EXT}\"",
            "+MODEL_FILENAME = \"saved_model.pkl\"",
            "",
            "logger = logging.getLogger(__name__)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"{SAVE_NAMESPACE}.{PKL_EXT}\"), value='\"saved_model.pkl\"')"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 848,
        "neg_line": [
            "-MODEL_FILENAME = f\"{SAVE_NAMESPACE}.{PKL_EXT}\""
        ],
        "pos_line": [
            "+MODEL_FILENAME = \"saved_model.pkl\""
        ],
        "core_change": "-MODEL_FILENAME = f\"{SAVE_NAMESPACE}.{PKL_EXT}\" +MODEL_FILENAME = \"saved_model.pkl\"",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "33ab3cbf74175c240bccee95029f4502667953f3",
        "index": "901ab2ce..cf884799 100644",
        "commit_message": "fix warning\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class QM9(InMemoryDataset):",
            "edge_type += 2 * [self.bonds[bond.GetBondType()]]",
            "",
            "edge_index = torch.tensor([row, col], dtype=torch.long)",
            "-            edge_type = torch.tensor(edge_type)",
            "-            edge_attr = F.one_hot(torch.tensor(edge_type),",
            "+            edge_type = torch.tensor(edge_type, dtype=torch.long)",
            "+            edge_attr = F.one_hot(edge_type,",
            "num_classes=len(self.bonds)).to(torch.float)",
            "",
            "perm = (edge_index[0] * N + edge_index[1]).argsort()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1011980)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1011981)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1011982)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1011983)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1011984)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1011985)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1011986)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=1011987)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=edge_type), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 849,
        "neg_line": [
            "-edge_type = torch.tensor(edge_type)",
            "-edge_attr = F.one_hot(torch.tensor(edge_type),"
        ],
        "pos_line": [
            "+edge_type = torch.tensor(edge_type, dtype=torch.long)",
            "+edge_attr = F.one_hot(edge_type,"
        ],
        "core_change": "-edge_type = torch.tensor(edge_type) -edge_attr = F.one_hot(torch.tensor(edge_type), +edge_type = torch.tensor(edge_type, dtype=torch.long) +edge_attr = F.one_hot(edge_type,",
        "core_API": "GetBondType"
    },
    {
        "commit_hash": "8520778652438cd4113db2b178ec12e100f63fec",
        "index": "cd571fe75..a6297d711 100644",
        "commit_message": "fix mymy issues\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def connect(",
            "self.client_type = client_type",
            "",
            "if credentials:",
            "-                metadata, _user_key = self.conn.login(credentials=credentials)",
            "+                metadata, _user_key = self.conn.login(credentials=credentials)  # type: ignore",
            "_user_key = SigningKey(_user_key.encode(\"utf-8\"), encoder=HexEncoder)",
            "else:",
            "-                metadata = self.conn._get_metadata()",
            "+                metadata = self.conn._get_metadata()  # type: ignore",
            "if not user_key:",
            "_user_key = SigningKey.generate()",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 850,
        "neg_line": [
            "-metadata, _user_key = self.conn.login(credentials=credentials)",
            "-metadata = self.conn._get_metadata()"
        ],
        "pos_line": [
            "+metadata, _user_key = self.conn.login(credentials=credentials)  # type: ignore",
            "+metadata = self.conn._get_metadata()  # type: ignore"
        ],
        "core_change": "-metadata, _user_key = self.conn.login(credentials=credentials) +metadata, _user_key = self.conn.login(credentials=credentials)  # type: ignore -metadata = self.conn._get_metadata() +metadata = self.conn._get_metadata()  # type: ignore",
        "core_API": "login"
    },
    {
        "commit_hash": "dfc76018c18db1081bb24877c2b4bafbdc6e4b00",
        "index": "da0ac8b9c..6133953ba 100644",
        "commit_message": "OPT-fix (#17229)\n\n* try fixes\n\n* Revert \"try fixes\"\n\nThis reverts commit a8ad75ef69d4fc03a402ef61bd034b018aa8555e.\n\n* add correct shape\n\n* add correct path\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "feature",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OPTEmbeddingsTest(unittest.TestCase):",
            "def test_logits(self):",
            "model = OPTForCausalLM.from_pretrained(self.path_model)",
            "model = model.eval()",
            "-        tokenizer = GPT2Tokenizer.from_pretrained(\"patrickvonplaten/opt_gpt2_tokenizer\")",
            "+        tokenizer = GPT2Tokenizer.from_pretrained(self.path_model)",
            "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})",
            "",
            "prompts = ["
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1199351)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1199352)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1199353)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'path_model'), position=2, insert_id=1199354)",
            "Delete(target_node=ASTNode(type=string, text=\"patrickvonplaten/opt_gpt2_tokenizer\"))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 851,
        "neg_line": [
            "-tokenizer = GPT2Tokenizer.from_pretrained(\"patrickvonplaten/opt_gpt2_tokenizer\")"
        ],
        "pos_line": [
            "+tokenizer = GPT2Tokenizer.from_pretrained(self.path_model)"
        ],
        "core_change": "-tokenizer = GPT2Tokenizer.from_pretrained(\"patrickvonplaten/opt_gpt2_tokenizer\") +tokenizer = GPT2Tokenizer.from_pretrained(self.path_model)",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "5593b6f772c75ae016580e2579c2af9f86b19a39",
        "index": "a8567db70..d5e3ea919 100644",
        "commit_message": "Merge pull request #7872 from PyTorchLightning/refactor/logger-poc-changes\n\nRandom fixes for logger connector PoC\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "logger doc print update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_model_checkpoint_options(tmpdir, save_top_k, save_last, expected_files)",
            "for i, loss in enumerate(losses):",
            "trainer.train_loop.current_epoch = i",
            "trainer.train_loop.global_step = i",
            "-        trainer.logger_connector.callback_metrics = {\"checkpoint_on\": torch.tensor(loss)}",
            "+        trainer.logger_connector.callback_metrics.update({\"checkpoint_on\": loss})",
            "checkpoint_callback.on_validation_end(trainer, trainer.lightning_module)",
            "",
            "file_lists = set(os.listdir(tmpdir))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('call', None), position=0, insert_id=533348)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=533349)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=533350)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=533351)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'update'), position=2, insert_id=533352)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=533353)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=dictionary), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=533354)",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=identifier, text=loss), position=2)",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 853,
        "neg_line": [
            "-trainer.logger_connector.callback_metrics = {\"checkpoint_on\": torch.tensor(loss)}"
        ],
        "pos_line": [
            "+trainer.logger_connector.callback_metrics.update({\"checkpoint_on\": loss})"
        ],
        "core_change": "-trainer.logger_connector.callback_metrics = {\"checkpoint_on\": torch.tensor(loss)} +trainer.logger_connector.callback_metrics.update({\"checkpoint_on\": loss})",
        "core_API": "tensor"
    },
    {
        "commit_hash": "1a19939ea3ea02fb977f08975c2b344ed0c333c1",
        "index": "022947b2..4e24fa19 100644",
        "commit_message": "Replace some fs operations by tf.gfile for other fs support. (fix #416)\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MinSaver(Callback):",
            "newname = os.path.join(logger.LOG_DIR,",
            "self.filename or",
            "('max-' + self.monitor_stat if self.reverse else 'min-' + self.monitor_stat))",
            "-        files_to_copy = glob.glob(path + '*')",
            "+        files_to_copy = tf.gfile.Glob(path + '*')",
            "for file_to_copy in files_to_copy:",
            "-            shutil.copy(file_to_copy, file_to_copy.replace(path, newname))",
            "+            tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)",
            "logger.info(\"Model with {} '{}' saved.\".format(",
            "'maximum' if self.reverse else 'minimum', self.monitor_stat))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2296167)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2296168)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Glob'), position=2, insert_id=2296169)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2296170)",
            "Update(target_node=ASTNode(type=identifier, text=glob), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=glob), value='gfile')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2296171)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Copy'), position=2, insert_id=2296172)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2296173)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2296174)",
            "Update(target_node=ASTNode(type=identifier, text=shutil), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=copy), value='gfile')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'overwrite'), position=0, insert_id=2296175)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2296176)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2296177)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 854,
        "neg_line": [
            "-files_to_copy = glob.glob(path + '*')",
            "-shutil.copy(file_to_copy, file_to_copy.replace(path, newname))"
        ],
        "pos_line": [
            "+files_to_copy = tf.gfile.Glob(path + '*')",
            "+tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)"
        ],
        "core_change": "-files_to_copy = glob.glob(path + '*') +files_to_copy = tf.gfile.Glob(path + '*') -shutil.copy(file_to_copy, file_to_copy.replace(path, newname)) +tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)",
        "core_API": "join"
    },
    {
        "commit_hash": "dacf4955a8adc06d9dd8b8c15c2b8448535b1f49",
        "index": "8fffe4f..d31c38d 100644",
        "commit_message": "fix prediction (apply pre-processing)\n\n",
        "file": "tflearn.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Evaluator(object):",
            "The mean average result per tensor over the entire dataset.",
            "",
            "\"\"\"",
            "+        tflearn.is_training(False, self.session)",
            "coord = tf.train.Coordinator()",
            "inputs = tf.get_collection(tf.GraphKeys.INPUTS)",
            "# Data Preprocessing"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2354809)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=3, insert_id=2354810)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2354811)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=coord), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2354812)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2354813)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tflearn'), position=0, insert_id=2354814)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2354815)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_training'), position=2, insert_id=2354816)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2354817)",
            "Insert(target_node=IN(type=argument_list), node=('false', 'False'), position=1, insert_id=2354818)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2354819)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2354820)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2354821)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2354822)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2354823)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'session'), position=2, insert_id=2354824)",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 855,
        "neg_line": [],
        "pos_line": [
            "+tflearn.is_training(False, self.session)"
        ],
        "core_change": "+tflearn.is_training(False, self.session)",
        "core_API": "is_training"
    },
    {
        "commit_hash": "9541f4963bb6188789b65b2902373ec50ccfdc8e",
        "index": "19cfd12..5308e34 100644",
        "commit_message": "One more scalar -> tensor fix for lamb optimizer\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Lamb(Optimizer):",
            "global_grad_norm.add_(grad.pow(2).sum())",
            "",
            "global_grad_norm = torch.sqrt(global_grad_norm)",
            "-        max_grad_norm = self.defaults['max_grad_norm']",
            "+        # FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes",
            "+        # scalar types properly https://github.com/pytorch/pytorch/issues/9190",
            "+        max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)",
            "clip_global_grad_norm = torch.where(",
            "global_grad_norm > max_grad_norm,",
            "global_grad_norm / max_grad_norm,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=895847)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=895848)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=895849)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=895850)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=895851)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=895852)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=895853)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=895854)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=895855)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=895856)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=895857)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=895858)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=895859)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 857,
        "neg_line": [
            "-max_grad_norm = self.defaults['max_grad_norm']"
        ],
        "pos_line": [
            "+# FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes",
            "+# scalar types properly https://github.com/pytorch/pytorch/issues/9190",
            "+max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)"
        ],
        "core_change": "-max_grad_norm = self.defaults['max_grad_norm'] +# FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes +# scalar types properly https://github.com/pytorch/pytorch/issues/9190 +max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)",
        "core_API": "add_"
    },
    {
        "commit_hash": "cac9670fe1009c90378efa04a6383d977385d194",
        "index": "5cbcfc3d..38986a1f 100644",
        "commit_message": "Fixes to tests and line lengths\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class TokenClassificationIntegrationTest(test_combinations.TestCase):",
            "layers, input_shape=(None,))",
            "model.compile(",
            "loss='sparse_categorical_crossentropy',",
            "-        optimizer='adam'",
            "+        optimizer='adam',",
            "metrics=['acc'],",
            "run_eagerly=test_utils.should_run_eagerly())",
            "history = model.fit(dataset, epochs=10,"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2512945)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2512946)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2512947)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=optimizer), position=0)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=string, text='adam'), position=2)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=metrics), position=0)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Insert(target_node=IN(type=keyword_argument), node=('list', None), position=2, insert_id=2512948)",
            "Move(target_node=IN(type=list), node=ASTNode(type=[, text=[), position=0)",
            "Move(target_node=IN(type=list), node=ASTNode(type=string, text='acc'), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=], text=]), position=2)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 858,
        "neg_line": [
            "-optimizer='adam'"
        ],
        "pos_line": [
            "+optimizer='adam',"
        ],
        "core_change": "-optimizer='adam' +optimizer='adam',",
        "core_API": "compile"
    },
    {
        "commit_hash": "01c7f3b77d99c403608b428860d93cbb24a0e391",
        "index": "c08e173..9f2f175 100644",
        "commit_message": "fix zits\n\n",
        "file": "lama-cleaner.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ZITS(InpaintModel):",
            "masks: [H, W]",
            "return: BGR IMAGE",
            "\"\"\"",
            "+        mask = mask[:, :, 0]",
            "items = load_image(image, mask, device=self.device)",
            "",
            "self.wireframe_edge_and_line(items, config.zits_wireframe)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=485009)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=5, insert_id=485010)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=485011)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=items), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'mask'), position=0, insert_id=485012)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=485013)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=2, insert_id=485014)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'mask'), position=0, insert_id=485015)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=485016)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=485017)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=485018)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=4, insert_id=485019)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=5, insert_id=485020)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=6, insert_id=485021)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=7, insert_id=485022)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=485023)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=485024)",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 859,
        "neg_line": [],
        "pos_line": [
            "+mask = mask[:, :, 0]"
        ],
        "core_change": "+mask = mask[:, :, 0]",
        "core_API": "wireframe_edge_and_line"
    },
    {
        "commit_hash": "b197c0c4044f66628c6672fe78581768a54d0e59",
        "index": "e975b1b4cc..e3865c3e79 100644",
        "commit_message": "[rllib] General RNN support (#2299)\n\n* wip\n\n* cls\n\n* re\n\n* wip\n\n* wip\n\n* a3c working\n\n* torch support\n\n* pg works\n\n* lint\n\n* rm v2\n\n* consumer id\n\n* clean up pg\n\n* clean up more\n\n* fix python 2.7\n\n* tf session management\n\n* docs\n\n* dqn wip\n\n* fix compile\n\n* dqn\n\n* apex runs\n\n* up\n\n* impotrs\n\n* ddpg\n\n* quotes\n\n* fix tests\n\n* fix last r\n\n* fix tests\n\n* lint\n\n* pass checkpoint restore\n\n* kwar\n\n* nits\n\n* policy graph\n\n* fix yapf\n\n* com\n\n* class\n\n* pyt\n\n* vectorization\n\n* update\n\n* test cpe\n\n* unit test\n\n* fix ddpg2\n\n* changes\n\n* wip\n\n* args\n\n* faster test\n\n* common\n\n* fix\n\n* add alg option\n\n* batch mode and policy serving\n\n* multi serving test\n\n* todo\n\n* wip\n\n* serving test\n\n* doc async env\n\n* num envs\n\n* comments\n\n* thread\n\n* remove init hook\n\n* update\n\n* fix ppo\n\n* comments1\n\n* fix\n\n* updates\n\n* add jenkins tests\n\n* fix\n\n* fix pytorch\n\n* fix\n\n* fixes\n\n* fix a3c policy\n\n* fix squeeze\n\n* fix trunc on apex\n\n* fix squeezing for real\n\n* update\n\n* remove horizon test for now\n\n* multiagent wip\n\n* update\n\n* fix race condition\n\n* fix ma\n\n* t\n\n* doc\n\n* st\n\n* wip\n\n* example\n\n* wip\n\n* working\n\n* cartpole\n\n* wip\n\n* batch wip\n\n* fix bug\n\n* make other_batches None default\n\n* working\n\n* debug\n\n* nit\n\n* warn\n\n* comments\n\n* fix ppo\n\n* fix obs filter\n\n* update\n\n* wip\n\n* tf\n\n* update\n\n* fix\n\n* cleanup\n\n* cleanup\n\n* spacing\n\n* model\n\n* fix\n\n* dqn\n\n* fix ddpg\n\n* doc\n\n* keep names\n\n* update\n\n* fix\n\n* com\n\n* docs\n\n* clarify model outputs\n\n* Update torch_policy_graph.py\n\n* fix obs filter\n\n* pass thru worker index\n\n* fix\n\n* rename\n\n* vlad torch comments\n\n* fix log action\n\n* debug name\n\n* fix lstm\n\n* remove unused ddpg net\n\n* remove conv net\n\n* revert lstm\n\n* wip\n\n* wip\n\n* cast\n\n* wip\n\n* works\n\n* fix a3c\n\n* works\n\n* lstm util test\n\n* doc\n\n* clean up\n\n* update\n\n* fix lstm check\n\n* move to end\n\n* fix sphinx\n\n* fix cmd\n\n* remove bad doc\n\n* clarify\n\n* copy\n\n* async sa\n\n* fix\n\n* comments\n\n* fix a3c conf\n\n* tune lstm\n\n* fix reshape\n\n* fix\n\n* back to 16\n\n* tuned a3c update\n\n* update\n\n* tuned\n\n* optional\n\n* fix catalog\n\n* remove prep\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ModelCatalogTest(unittest.TestCase):",
            "def testCustomModel(self):",
            "ray.init()",
            "ModelCatalog.register_custom_model(\"foo\", CustomModel)",
            "-        p1 = ModelCatalog.get_model(1, 5, {\"custom_model\": \"foo\"})",
            "+        p1 = ModelCatalog.get_model(",
            "+            tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})",
            "self.assertEqual(str(type(p1)), str(CustomModel))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2154558)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2154559)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2154560)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2154561)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2154562)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2154563)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'constant'), position=2, insert_id=2154564)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2154565)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=1, insert_id=2154566)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2154567)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2154568)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=1), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=list), node=('integer', '2'), position=3, insert_id=2154569)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=4, insert_id=2154570)",
            "Insert(target_node=IN(type=list), node=('integer', '3'), position=5, insert_id=2154571)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=6, insert_id=2154572)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 860,
        "neg_line": [
            "-p1 = ModelCatalog.get_model(1, 5, {\"custom_model\": \"foo\"})"
        ],
        "pos_line": [
            "+p1 = ModelCatalog.get_model(",
            "+tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})"
        ],
        "core_change": "-p1 = ModelCatalog.get_model(1, 5, {\"custom_model\": \"foo\"}) +p1 = ModelCatalog.get_model( +tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})",
        "core_API": "init"
    },
    {
        "commit_hash": "b87058c62a99012e00687b2bd34fa54e9c96ac67",
        "index": "15b39d7..80100f5 100644",
        "commit_message": "fix recent lint\n\nSummary: lint clean again\n\nReviewed By: patricklabatut\n\nDifferential Revision: D20868775\n\nfbshipit-source-id: ade4301c1012c5c6943186432465215701d635a9\n\n",
        "file": "pytorch3d.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def corresponding_points_alignment(",
            "U, S, V = torch.svd(XYcov)",
            "",
            "# identity matrix used for fixing reflections",
            "-    E = torch.eye(dim, dtype=XYcov.dtype, device=XYcov.device)[None].repeat(",
            "-        b, 1, 1",
            "-    )",
            "+    E = torch.eye(dim, dtype=XYcov.dtype, device=XYcov.device)[None].repeat(b, 1, 1)",
            "",
            "if not allow_reflection:",
            "# reflection test:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 861,
        "neg_line": [
            "-E = torch.eye(dim, dtype=XYcov.dtype, device=XYcov.device)[None].repeat(",
            "-b, 1, 1",
            "-)"
        ],
        "pos_line": [
            "+E = torch.eye(dim, dtype=XYcov.dtype, device=XYcov.device)[None].repeat(b, 1, 1)"
        ],
        "core_change": "-E = torch.eye(dim, dtype=XYcov.dtype, device=XYcov.device)[None].repeat( -b, 1, 1 -) +E = torch.eye(dim, dtype=XYcov.dtype, device=XYcov.device)[None].repeat(b, 1, 1)",
        "core_API": "svd"
    },
    {
        "commit_hash": "f89a371b9c6449d8ebbba7ab2613c1307ccc9982",
        "index": "2f48dcb..3baa355 100644",
        "commit_message": "fix a bug in DCGAN (#121)\n\nUsing single GPU in DCGAN caused an error because  'gpu_ids'  in forward function will be None .\n",
        "file": "examples.txt.json",
        "label": "yes",
        "comments": "change condition check for resource fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class _netD(nn.Module):",
            "",
            "def forward(self, input):",
            "gpu_ids = None",
            "-        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:",
            "+        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > =1:",
            "gpu_ids = range(self.ngpu)",
            "output = nn.parallel.data_parallel(self.main, input, gpu_ids)",
            "return output.view(-1, 1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('ERROR', None), position=2, insert_id=1828639)",
            "Insert(target_node=IN(type=ERROR), node=('=', '='), position=0, insert_id=1828640)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 862,
        "neg_line": [
            "-if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:"
        ],
        "pos_line": [
            "+if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > =1:"
        ],
        "core_change": "-if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1: +if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > =1:",
        "core_API": "data_parallel"
    },
    {
        "commit_hash": "06fc48666bff358f093eb9b7494d9d392915aa71",
        "index": "9d60682..8e13781 100644",
        "commit_message": "Add Dorckerfile and implement TFLite and Torchscript as model-backends (#64)\n\n* add tf and torch backeneds\n\n* add torch backend\n\n* add tqdm to requirements\n\n* avoid installation of compilers when NO_COMPILER_INSTALLATION is set\n\n* fix error with onnx installation\n\n* fix error with onnx installation\n\n* fix error with onnx installation\n\n* fix error with onnx installation\n\n* Added Dockerfile & bugfix (#63)\n\n* add dockerfile\n\n* fix tvm configs issue in the tvm installer\n\n* fix tvm issue\n\n* fix tvm\n\n* fix dockerfile & created build script for the docker images\n\n* removed redundant spaces\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n\n* add tflite to tf api\n\n* fix logging\n\n* fix error with half precision in torch\n\n* fix minor bugs\n\n* fix bugs\n\n* fix import\n\n* fix bug with tf\n\n* fix error with DeviceArrays in polygraphy\n\n* fix another bug\n\n* upgrade version\n\nCo-authored-by: morgoth95 <d.fiori@nebuly.ai>\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "file": "nebullvm.txt.json",
        "label": "yes",
        "comments": "add condition check for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from nebullvm.transformations.base import BaseTransformation",
            "",
            "",
            "class VerifyContiguity(BaseTransformation):",
            "-    def _transform(self, _input: torch.Tensor, **kwargs) -> Any:",
            "+    def _transform(self, _input: Any, **kwargs) -> Any:",
            "+        if not isinstance(_input, torch.Tensor):",
            "+            return _input",
            "if not _input.is_contiguous():",
            "_input = _input.contiguous()",
            "return _input"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=6, insert_id=654210)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=654211)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=654212)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=654213)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=654214)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=654215)",
            "Insert(target_node=ASTNode(type=type), node=('identifier', 'Any'), position=0, insert_id=654216)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=654217)",
            "Insert(target_node=IN(type=not_operator), node=('call', None), position=1, insert_id=654218)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=654219)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=654220)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=654221)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=654222)",
            "Insert(target_node=IN(type=return_statement), node=('identifier', '_input'), position=1, insert_id=654223)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=654224)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', '_input'), position=1, insert_id=654225)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=654226)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=654227)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 864,
        "neg_line": [
            "-def _transform(self, _input: torch.Tensor, **kwargs) -> Any:"
        ],
        "pos_line": [
            "+def _transform(self, _input: Any, **kwargs) -> Any:",
            "+if not isinstance(_input, torch.Tensor):",
            "+return _input"
        ],
        "core_change": "-def _transform(self, _input: torch.Tensor, **kwargs) -> Any: +def _transform(self, _input: Any, **kwargs) -> Any: +if not isinstance(_input, torch.Tensor): +return _input",
        "core_API": "is_contiguous"
    },
    {
        "commit_hash": "736078b9ea48628f5199bb627271c265ac64b53b",
        "index": "370d8c7..f491c2e 100644",
        "commit_message": "distributed strategy bug fix\n\n",
        "file": "autokeras.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "from tests import utils",
            "def test_image_classifier(tmp_path):",
            "train_x = utils.generate_data(num_instances=320, shape=(32, 32, 3))",
            "train_y = utils.generate_one_hot_labels(num_instances=320, num_classes=10)",
            "-    clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)",
            "+    clf = ak.ImageClassifier(",
            "+        directory=tmp_path,",
            "+        max_trials=2,",
            "+        seed=utils.SEED,",
            "+        distribution_strategy=tf.distribute.MirroredStrategy(),",
            "+    )",
            "clf.fit(train_x, train_y, epochs=1, validation_split=0.2)",
            "keras_model = clf.export_model()",
            "clf.evaluate(train_x, train_y)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1893015)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=1893016)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=1893017)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=9, insert_id=1893018)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'distribution_strategy'), position=0, insert_id=1893019)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1893020)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=1893021)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1893022)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1893023)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1893024)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1893025)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'MirroredStrategy'), position=2, insert_id=1893026)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1893027)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1893028)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1893029)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distribute'), position=2, insert_id=1893030)"
        ],
        "plus_line": 6,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 865,
        "neg_line": [
            "-clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)"
        ],
        "pos_line": [
            "+clf = ak.ImageClassifier(",
            "+directory=tmp_path,",
            "+max_trials=2,",
            "+seed=utils.SEED,",
            "+distribution_strategy=tf.distribute.MirroredStrategy(),",
            "+)"
        ],
        "core_change": "-clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED) +clf = ak.ImageClassifier( +directory=tmp_path, +max_trials=2, +seed=utils.SEED, +distribution_strategy=tf.distribute.MirroredStrategy(), +)",
        "core_API": "generate_data"
    },
    {
        "commit_hash": "7d57c58bf5b404e0b91bedc4bc2215555312662a",
        "index": "d7f085fe..363a8b19 100644",
        "commit_message": "Fix conv2d_transpose\n\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "remove API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),",
            "output_shape[3],",
            "output_shape[1])",
            "if output_shape[0] is None:",
            "-        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])",
            "-        output_shape = tf.stack(list(output_shape))",
            "+        output_shape = (shape(x)[0],) + tuple(output_shape[1:])",
            "+",
            "+    output_shape = tf.stack(list(output_shape))",
            "",
            "padding = _preprocess_padding(padding)",
            "if tf_data_format == 'NHWC':"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=shape), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 867,
        "neg_line": [
            "-output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])",
            "-output_shape = tf.stack(list(output_shape))"
        ],
        "pos_line": [
            "+output_shape = (shape(x)[0],) + tuple(output_shape[1:])",
            "+",
            "+output_shape = tf.stack(list(output_shape))"
        ],
        "core_change": "-output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:]) -output_shape = tf.stack(list(output_shape)) +output_shape = (shape(x)[0],) + tuple(output_shape[1:]) + +output_shape = tf.stack(list(output_shape))",
        "core_API": "shape"
    },
    {
        "commit_hash": "0eca74fa5f7bf82f3b93e3e38dd1d84cfedc5630",
        "index": "a253020..28e9ff7 100644",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "file": "pytorch3d.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRasterizeMeshes(TestCaseMixin, unittest.TestCase):",
            "], dtype=torch.int64, device=device)",
            "# fmt: on",
            "",
            "-        pix_to_face_padded = -torch.ones_like(pix_to_face_frontface)",
            "+        pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))",
            "# Run with and without culling",
            "# Without culling, for k=0, the front face (i.e. face 2) is",
            "# rasterized and for k=1, the back face (i.e. face 3) is"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=unary_operator), node=('parenthesized_expression', None), position=1, insert_id=925825)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=925826)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=925827)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 868,
        "neg_line": [
            "-pix_to_face_padded = -torch.ones_like(pix_to_face_frontface)"
        ],
        "pos_line": [
            "+pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))"
        ],
        "core_change": "-pix_to_face_padded = -torch.ones_like(pix_to_face_frontface) +pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "fea52ab8bd2c181a22456164b92c2d3079986a7c",
        "index": "fd13988b..9fa96fc6 100644",
        "commit_message": "Fit `RGATConv` different device error (#5187)\n\n* fix rgatconv device\n\n* changelog\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "customized API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RGATConv(MessagePassing):",
            "alpha = torch.where(alpha > 0, alpha + 1, alpha)",
            "",
            "elif self.mod == \"f-scaled\":",
            "-            ones = torch.ones(index.size())",
            "+            ones = alpha.new_ones(index.size())",
            "degree = scatter_add(ones, index,",
            "dim_size=size_i)[index].unsqueeze(-1)",
            "alpha = alpha * degree"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='alpha')",
            "Update(target_node=ASTNode(type=identifier, text=ones), value='new_ones')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 870,
        "neg_line": [
            "-ones = torch.ones(index.size())"
        ],
        "pos_line": [
            "+ones = alpha.new_ones(index.size())"
        ],
        "core_change": "-ones = torch.ones(index.size()) +ones = alpha.new_ones(index.size())",
        "core_API": "where"
    },
    {
        "commit_hash": "149a703884ee126d20a9df187abe9176bb4a581d",
        "index": "e848ae0..c2aa98d 100644",
        "commit_message": "add compat.is_gpu_available(); fix test failure\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PipelineTest(test.SparkTest):",
            "import tensorflow as tf",
            "from tensorflowonspark import TFNode",
            "",
            "+      tf.compat.v1.disable_eager_execution()",
            "tf.compat.v1.reset_default_graph()",
            "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2210939)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2210940)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2210941)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2210942)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2210943)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2210944)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'disable_eager_execution'), position=2, insert_id=2210945)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2210946)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2210947)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2210948)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2210949)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2210950)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2210951)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2210952)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2210953)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 871,
        "neg_line": [],
        "pos_line": [
            "+tf.compat.v1.disable_eager_execution()"
        ],
        "core_change": "+tf.compat.v1.disable_eager_execution()",
        "core_API": "disable_eager_execution"
    },
    {
        "commit_hash": "58cb31362f27eb6502e917b091e14cc9b1c0d65a",
        "index": "fb5a302..bc067c0 100644",
        "commit_message": "Fix parsing Keras version identifiers like `2.2.4-tf` (#3794)\n\nSigned-off-by: Max H. Gerlach <git@maxgerlach.de>\n",
        "file": "horovod.txt.json",
        "label": "yes",
        "comments": "change condition check for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SparkKerasTests(tf.test.TestCase):",
            "",
            "def test_fit_model_multiclass(self):",
            "model = create_mnist_model()",
            "-        if version.parse(tf.keras.__version__) < version.parse(\"2.11\"):",
            "+        if version.parse(tf.keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):",
            "optimizer = tf.keras.optimizers.Adadelta(1.0)",
            "else:",
            "optimizer = tf.keras.optimizers.legacy.Adadelta(1.0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1943826)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=1943827)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1943828)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1943829)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1943830)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'replace'), position=2, insert_id=1943831)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1943832)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"-tf\"'), position=1, insert_id=1943833)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1943834)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"+tf\"'), position=3, insert_id=1943835)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 875,
        "neg_line": [
            "-if version.parse(tf.keras.__version__) < version.parse(\"2.11\"):"
        ],
        "pos_line": [
            "+if version.parse(tf.keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):"
        ],
        "core_change": "-if version.parse(tf.keras.__version__) < version.parse(\"2.11\"): +if version.parse(tf.keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):",
        "core_API": "parse"
    },
    {
        "commit_hash": "bc59c00277a30e5a03702773cc7b7b6dc17f6168",
        "index": "3989283b..1d089190 100644",
        "commit_message": "Fix docs links to PyTorch documentation (#856)\n\n\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def torch_sum(x):",
            "",
            "def torch_backward(x):",
            "\"\"\"",
            "-    Like ``x.backward()`` for a ``torch.autograd.Variable``, but also accepts",
            "+    Like ``x.backward()`` for a :class:`~torch.autograd.Variable`, but also accepts",
            "numbers (a no-op if given a number).",
            "\"\"\"",
            "if isinstance(x, torch.autograd.Variable):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n    Like ``x.backward()`` for a ``torch.autograd.Variable``, but also accepts\nnumbers (a no-op if given a number).\n\"\"\"), value='\"\"\"\\n    Like ``x.backward()`` for a :class:`~torch.autograd.Variable`, but also accepts\\nnumbers (a no-op if given a number).\\n\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 877,
        "neg_line": [
            "-Like ``x.backward()`` for a ``torch.autograd.Variable``, but also accepts"
        ],
        "pos_line": [
            "+Like ``x.backward()`` for a :class:`~torch.autograd.Variable`, but also accepts"
        ],
        "core_change": "-Like ``x.backward()`` for a ``torch.autograd.Variable``, but also accepts +Like ``x.backward()`` for a :class:`~torch.autograd.Variable`, but also accepts",
        "core_API": "backward"
    },
    {
        "commit_hash": "97e850cbab75668c409dc71800f012c84c949a8f",
        "index": "730534f10..64e403c60 100644",
        "commit_message": "fix amp+multi gpu in asr\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "change param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(args):",
            "dtype = torch.float32",
            "model = model_class(args.n_vocab, args).to(dtype=dtype)",
            "if args.ngpu > 0:",
            "-        model.to(\"cuda:0\")",
            "+        model.to(\"cuda\")",
            "gpu_id = list(range(args.ngpu))",
            "else:",
            "gpu_id = [-1]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"cuda:0\"), value='\"cuda\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 878,
        "neg_line": [
            "-model.to(\"cuda:0\")"
        ],
        "pos_line": [
            "+model.to(\"cuda\")"
        ],
        "core_change": "-model.to(\"cuda:0\") +model.to(\"cuda\")",
        "core_API": "to"
    },
    {
        "commit_hash": "335a2791d8125622baaba01d20bce54d5dc4c977",
        "index": "005e3b49..9907e148 100644",
        "commit_message": "fix create_meshgrid indexing and refactor tensor_to_image\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "change param for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_meshgrid(height, width, normalized_coordinates=True):",
            "else:",
            "xs = torch.linspace(0, width - 1, width)",
            "ys = torch.linspace(0, height - 1, height)",
            "-    return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)",
            "+    return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)[:, (1, 0), :]",
            "",
            "",
            "class HomographyWarper(nn.Module):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('subscript', None), position=1, insert_id=481420)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=481421)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=481422)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=481423)",
            "Insert(target_node=IN(type=subscript), node=('tuple', None), position=4, insert_id=481424)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=5, insert_id=481425)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=6, insert_id=481426)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=7, insert_id=481427)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=481428)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=481429)",
            "Insert(target_node=IN(type=tuple), node=('integer', '1'), position=1, insert_id=481430)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=481431)",
            "Insert(target_node=IN(type=tuple), node=('integer', '0'), position=3, insert_id=481432)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=481433)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=481434)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 880,
        "neg_line": [
            "-return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)"
        ],
        "pos_line": [
            "+return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)[:, (1, 0), :]"
        ],
        "core_change": "-return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1) +return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)[:, (1, 0), :]",
        "core_API": "linspace"
    },
    {
        "commit_hash": "27b51f5c8db9930e83de2eecd6b6f958e0720d9f",
        "index": "d0cd7ee..904d967 100644",
        "commit_message": "Fix tensorflow bugs\n\n",
        "file": "nebullvm.txt.json",
        "label": "no",
        "comments": "change control flow",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "try:",
            "\"GPU\"",
            ")",
            "if len(physical_devices) > 0:",
            "-        tensorflow.config.experimental.set_memory_growth(",
            "-            physical_devices[0], True",
            "-        )",
            "+        for physical_device in physical_devices:",
            "+            tensorflow.config.experimental.set_memory_growth(",
            "+                physical_device, True",
            "+            )",
            "",
            "tensorflow.get_logger().setLevel(\"ERROR\")",
            "tensorflow.autograph.set_verbosity(0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('for_statement', None), position=0, insert_id=2453071)",
            "Insert(target_node=IN(type=for_statement), node=('for', 'for'), position=0, insert_id=2453072)",
            "Insert(target_node=IN(type=for_statement), node=('identifier', 'physical_device'), position=1, insert_id=2453073)",
            "Insert(target_node=IN(type=for_statement), node=('in', 'in'), position=2, insert_id=2453074)",
            "Insert(target_node=IN(type=for_statement), node=('identifier', 'physical_devices'), position=3, insert_id=2453075)",
            "Insert(target_node=IN(type=for_statement), node=(':', ':'), position=4, insert_id=2453076)",
            "Insert(target_node=IN(type=for_statement), node=('block', None), position=5, insert_id=2453077)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=physical_devices), value='physical_device')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=physical_devices), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 881,
        "neg_line": [
            "-tensorflow.config.experimental.set_memory_growth(",
            "-physical_devices[0], True",
            "-)"
        ],
        "pos_line": [
            "+for physical_device in physical_devices:",
            "+tensorflow.config.experimental.set_memory_growth(",
            "+physical_device, True",
            "+)"
        ],
        "core_change": "-tensorflow.config.experimental.set_memory_growth( -physical_devices[0], True -) +for physical_device in physical_devices: +tensorflow.config.experimental.set_memory_growth( +physical_device, True +)",
        "core_API": "set_memory_growth"
    },
    {
        "commit_hash": "d383f5f8964c021e1d70a6f29a8bd4e88a7f02af",
        "index": "2f8befd..310b462 100644",
        "commit_message": "fix device\n\n",
        "file": "deep-learning-for-image-processing.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth",
            "model_weight_path = \"./resnet34-pre.pth\"",
            "assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)",
            "-    net.load_state_dict(torch.load(model_weight_path, map_location=device))",
            "+    net.load_state_dict(torch.load(model_weight_path, map_location='cpu'))",
            "# for param in net.parameters():",
            "#     param.requires_grad = False"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', \"'cpu'\"), position=2, insert_id=69024)",
            "Delete(target_node=ASTNode(type=identifier, text=device))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 882,
        "neg_line": [
            "-net.load_state_dict(torch.load(model_weight_path, map_location=device))"
        ],
        "pos_line": [
            "+net.load_state_dict(torch.load(model_weight_path, map_location='cpu'))"
        ],
        "core_change": "-net.load_state_dict(torch.load(model_weight_path, map_location=device)) +net.load_state_dict(torch.load(model_weight_path, map_location='cpu'))",
        "core_API": "exists"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "35e44cab..f4386500 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def triangular_solve(x, y, upper=False, transpose=False):",
            "",
            "",
            "def precision_to_scale_tril(P):",
            "-    Lf = torch.cholesky(torch.flip(P, (-2, -1)))",
            "+    Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))",
            "L_inv = torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)",
            "L = torch.triangular_solve(torch.eye(P.shape[-1], dtype=P.dtype, device=P.device),",
            "L_inv, upper=False)[0]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=677040)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=677041)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677042)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 883,
        "neg_line": [
            "-Lf = torch.cholesky(torch.flip(P, (-2, -1)))"
        ],
        "pos_line": [
            "+Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))"
        ],
        "core_change": "-Lf = torch.cholesky(torch.flip(P, (-2, -1))) +Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))",
        "core_API": "cholesky"
    },
    {
        "commit_hash": "47d5c244706ab01859a3931f66066407a9d5efbb",
        "index": "570a3e9a..1da9febf 100755",
        "commit_message": "Summaries completely changed, distributed mode incompletely changed, various fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "remove API call for type fixfor distributed bug",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NaturalGradient(Optimizer):",
            "applied = self.apply_step(variables=variables, diffs=estimated_diffs)",
            "",
            "with tf.control_dependencies(control_inputs=(applied,)):",
            "-                return [tf.identity(input=estimated_diff) for estimated_diff in estimated_diffs]",
            "+                return [estimated_diff + 0.0 for estimated_diff in estimated_diffs]",
            "",
            "def false_fn():",
            "return [tf.zeros_like(tensor=diff) for diff in diffs]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list_comprehension), node=('binary_operator', None), position=1, insert_id=2242670)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='estimated_diff')",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=tf), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2242671)",
            "Insert(target_node=IN(type=binary_operator), node=('float', '0.0'), position=2, insert_id=2242672)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=input))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=estimated_diff))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 884,
        "neg_line": [
            "-return [tf.identity(input=estimated_diff) for estimated_diff in estimated_diffs]"
        ],
        "pos_line": [
            "+return [estimated_diff + 0.0 for estimated_diff in estimated_diffs]"
        ],
        "core_change": "-return [tf.identity(input=estimated_diff) for estimated_diff in estimated_diffs] +return [estimated_diff + 0.0 for estimated_diff in estimated_diffs]",
        "core_API": "apply_step"
    },
    {
        "commit_hash": "9e45cd0352ac6ed6499314548f78048c8c3ad63f",
        "index": "1db3bd6..fd6ae72 100644",
        "commit_message": "üêõ  fix path, entrypoint and logging\n\n",
        "file": "spleeter.txt.json",
        "label": "no",
        "comments": "refactor fix error",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def configure_logger(verbose: bool) -> None:",
            "verbose (bool):",
            "`True` to use verbose logger, `False` otherwise.",
            "\"\"\"",
            "-    tf_logger = tf.get_logger()",
            "+    from tensorflow import get_logger",
            "+    from tensorflow.compat.v1 import logging as tf_logging",
            "+    tf_logger = get_logger()",
            "tf_logger.handlers = [handler]",
            "if verbose:",
            "tf_logging.set_verbosity(tf_logging.INFO)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('import_from_statement', None), position=5, insert_id=2454834)",
            "Insert(target_node=ASTNode(type=module), node=('import_from_statement', None), position=6, insert_id=2454835)",
            "Insert(target_node=IN(type=import_from_statement), node=('from', 'from'), position=0, insert_id=2454836)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=1, insert_id=2454837)",
            "Insert(target_node=IN(type=import_from_statement), node=('import', 'import'), position=2, insert_id=2454838)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=2454839)",
            "Insert(target_node=IN(type=import_from_statement), node=('from', 'from'), position=0, insert_id=2454840)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=1, insert_id=2454841)",
            "Insert(target_node=IN(type=import_from_statement), node=('import', 'import'), position=2, insert_id=2454842)",
            "Insert(target_node=IN(type=import_from_statement), node=('aliased_import', None), position=3, insert_id=2454843)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorflow'), position=0, insert_id=2454844)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'get_logger'), position=0, insert_id=2454845)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorflow'), position=0, insert_id=2454846)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=1, insert_id=2454847)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'compat'), position=2, insert_id=2454848)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=3, insert_id=2454849)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'v1'), position=4, insert_id=2454850)",
            "Insert(target_node=IN(type=aliased_import), node=('dotted_name', None), position=0, insert_id=2454851)",
            "Insert(target_node=IN(type=aliased_import), node=('as', 'as'), position=1, insert_id=2454852)",
            "Insert(target_node=IN(type=aliased_import), node=('identifier', 'tf_logging'), position=2, insert_id=2454853)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'logging'), position=0, insert_id=2454854)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=get_logger), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 885,
        "neg_line": [
            "-tf_logger = tf.get_logger()"
        ],
        "pos_line": [
            "+from tensorflow import get_logger",
            "+from tensorflow.compat.v1 import logging as tf_logging",
            "+tf_logger = get_logger()"
        ],
        "core_change": "-tf_logger = tf.get_logger() +from tensorflow import get_logger +from tensorflow.compat.v1 import logging as tf_logging +tf_logger = get_logger()",
        "core_API": "get_logger"
    },
    {
        "commit_hash": "6d309462c3b3a5e678b8e1369cf94e53c33db17e",
        "index": "9f323f48de..56fef82b3d 100644",
        "commit_message": "Fix argsort test for tensorflow (#2389)\n\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def argsort(",
            "ret = tf.argsort(",
            "tf.convert_to_tensor(x), axis=axis, direction=\"ASCENDING\", stable=stable",
            ")",
            "-    return ret",
            "+    return tf.cast(ret, dtype=tf.int64)",
            "",
            "",
            "def sort("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('call', None), position=5, insert_id=2004245)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=6, insert_id=2004246)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2004247)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2004248)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'def'), position=0, insert_id=2004249)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=sort), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=2004250)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=2004251)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=3, insert_id=2004252)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2004253)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=ret), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2004254)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2004255)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2004256)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=return), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'tf'), position=1, insert_id=2004257)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2004258)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2004259)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2004260)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2004261)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2004262)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int64'), position=2, insert_id=2004263)",
            "Delete(target_node=ASTNode(type=def, text=def))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 886,
        "neg_line": [
            "-return ret"
        ],
        "pos_line": [
            "+return tf.cast(ret, dtype=tf.int64)"
        ],
        "core_change": "-return ret +return tf.cast(ret, dtype=tf.int64)",
        "core_API": "argsort"
    },
    {
        "commit_hash": "f6758020c913ff88f8b0c150dab542d2d3326c13",
        "index": "53d86e0923..822e8ad1c3 100644",
        "commit_message": "remove redundant recursive calls in _to_native and fix _to_ivy_array\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _to_ivy(x: Any) -> Any:",
            "",
            "",
            "def _to_ivy_array(x: Any) -> ivy.Array:",
            "-    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.numpy.DeviceArray, numpy.ndarray)):",
            "+    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.DeviceArray, numpy.ndarray)):",
            "return ivy.array(numpy.array(x))",
            "return x"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=numpy))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 887,
        "neg_line": [
            "-if isinstance(x, (torch.Tensor, tf.Tensor, jnp.numpy.DeviceArray, numpy.ndarray)):"
        ],
        "pos_line": [
            "+if isinstance(x, (torch.Tensor, tf.Tensor, jnp.DeviceArray, numpy.ndarray)):"
        ],
        "core_change": "-if isinstance(x, (torch.Tensor, tf.Tensor, jnp.numpy.DeviceArray, numpy.ndarray)): +if isinstance(x, (torch.Tensor, tf.Tensor, jnp.DeviceArray, numpy.ndarray)):",
        "core_API": "array"
    },
    {
        "commit_hash": "8eb323656c7cf0d66d74f789c9946d96eeea3f80",
        "index": "0576f380b1..531a914d53 100644",
        "commit_message": "Another fix to vecdot (#4639)\n\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "add condition check for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def vecdot(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
            "-    x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)",
            "+    if dtype != \"float64\":",
            "+        x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)",
            "ret = tf.cast(tf.tensordot(x1, x2, axes=(axis, axis)), dtype)",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=1993028)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1993029)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1993030)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1993031)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1993032)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'dtype'), position=0, insert_id=1993033)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=1993034)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"float64\"'), position=2, insert_id=1993035)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 888,
        "neg_line": [
            "-x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)"
        ],
        "pos_line": [
            "+if dtype != \"float64\":",
            "+x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)"
        ],
        "core_change": "-x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32) +if dtype != \"float64\": +x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)",
        "core_API": "as_native_dtype"
    },
    {
        "commit_hash": "3df10a9529e2d91dc9d4c1068d401b4935938b58",
        "index": "32bb712c..c194fc50 100644",
        "commit_message": "Add save and load tests to fairseq export test (#1653)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1653\n\nEarlier we had some issues at pickling. Type information gets lost. Fixed in https://github.com/pytorch/pytorch/pull/32569.\n\nThese save_and_load tests are added for protection in the future.\n\nReviewed By: myleott\n\nDifferential Revision: D19435988\n\nfbshipit-source-id: 560ea65ed3493bebcf394327818364b3fcd6fc92\n\n",
        "file": "fairseq.txt.json",
        "label": "no",
        "comments": "customize API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestExportModels(unittest.TestCase):",
            "TransformerModel.add_args(parser)",
            "args = parser.parse_args([])",
            "model = TransformerModel.build_model(args, task)",
            "-        torch.jit.script(model)",
            "+        scripted = torch.jit.script(model)",
            "+        self._test_save_and_load(scripted)",
            "",
            "",
            "if __name__ == \"__main__\":"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=216346)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=216347)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=216348)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'scripted'), position=0, insert_id=216349)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=216350)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=216351)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=216352)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=216353)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=216354)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_test_save_and_load'), position=2, insert_id=216355)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=216356)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'scripted'), position=1, insert_id=216357)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=216358)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 890,
        "neg_line": [
            "-torch.jit.script(model)"
        ],
        "pos_line": [
            "+scripted = torch.jit.script(model)",
            "+self._test_save_and_load(scripted)"
        ],
        "core_change": "-torch.jit.script(model) +scripted = torch.jit.script(model) +self._test_save_and_load(scripted)",
        "core_API": "add_args"
    },
    {
        "commit_hash": "f4a057201876fd29872d722390e2e13d2a4127ab",
        "index": "f523fedd..59cd60e4 100644",
        "commit_message": "Test documentation (#511)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* test documentation added\n\n* Missing requirement added: sphinx\n\n* Allow test on documentation to pass on warning\n\n* Fix travis dependencies install\n\n* Travis install script fixed\n\n* Travis install command fixed\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* shorten codes\n\n* Various Cleaning\n\n* Trailing Slashes removed\n\n* Test Recurrent Fixed\n\n* Line Width Fix\n\n* docs requirements updated\n\n* fix example docs style\n\n* Codacy Issue Fixed\n\n* Merge Errors fixed\n\n* YAPF Style Applied\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "print update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.device('/cpu:0'):",
            "n_batch += 1",
            "",
            "if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:",
            "-            print(\"Epoch %d : Step %d-%d of %d took %fs\" % \\",
            "-                (epoch, step, step + n_step_epoch, n_step, time.time() - start_time))",
            "+            print(",
            "+                \"Epoch %d : Step %d-%d of %d took %fs\" %",
            "+                (epoch, step, step + n_step_epoch, n_step, time.time() - start_time)",
            "+            )",
            "print(\"   train loss: %f\" % (train_loss / n_batch))",
            "print(\"   train acc: %f\" % (train_acc / n_batch))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 892,
        "neg_line": [
            "-print(\"Epoch %d : Step %d-%d of %d took %fs\" % \\",
            "-(epoch, step, step + n_step_epoch, n_step, time.time() - start_time))"
        ],
        "pos_line": [
            "+print(",
            "+\"Epoch %d : Step %d-%d of %d took %fs\" %",
            "+(epoch, step, step + n_step_epoch, n_step, time.time() - start_time)",
            "+)"
        ],
        "core_change": "-print(\"Epoch %d : Step %d-%d of %d took %fs\" % \\ -(epoch, step, step + n_step_epoch, n_step, time.time() - start_time)) +print( +\"Epoch %d : Step %d-%d of %d took %fs\" % +(epoch, step, step + n_step_epoch, n_step, time.time() - start_time) +)",
        "core_API": "device"
    },
    {
        "commit_hash": "81b47e7ee6836f999ef027e41f07bd473b4d6170",
        "index": "7d97e385..697717de 100644",
        "commit_message": "Fix buffers in sinusoidal positional embeddings\n\n",
        "file": "fairseq.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SinusoidalPositionalEmbedding(nn.Module):",
            "self.embedding_dim,",
            "self.padding_idx,",
            ").type_as(self.weights)",
            "+        self.weights = self.weights.type_as(self._float_tensor)",
            "weights = Variable(self.weights)",
            "",
            "if incremental_state is not None:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=224242)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=224243)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=224244)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=224245)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=224246)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=224247)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=224248)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weights'), position=2, insert_id=224249)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=224250)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=224251)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=224252)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=224253)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type_as'), position=2, insert_id=224254)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=224255)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=224256)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=224257)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=224258)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=224259)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weights'), position=2, insert_id=224260)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=224261)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=224262)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_float_tensor'), position=2, insert_id=224263)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 895,
        "neg_line": [],
        "pos_line": [
            "+self.weights = self.weights.type_as(self._float_tensor)"
        ],
        "core_change": "+self.weights = self.weights.type_as(self._float_tensor)",
        "core_API": "type_as"
    },
    {
        "commit_hash": "0655e4a2bd89ccc20f4f1157f65b3e5a61f140e1",
        "index": "81720b43..3a388230 100755",
        "commit_message": "fixed lstm, added gru, other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "no",
        "comments": "custom API",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def shape(x, unknown=-1):",
            "",
            "",
            "def no_operation():",
            "-    return tf.constant(value=False, dtype=tf_dtype(dtype='bool'))",
            "+    return identity_operation(x=tf.constant(value=False, dtype=tf_dtype(dtype='bool')))",
            "",
            "",
            "def identity_operation(x, operation_name=None):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'identity_operation'), position=0, insert_id=2231606)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2231607)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2231608)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2231609)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2231610)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'x'), position=0, insert_id=2231611)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2231612)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 897,
        "neg_line": [
            "-return tf.constant(value=False, dtype=tf_dtype(dtype='bool'))"
        ],
        "pos_line": [
            "+return identity_operation(x=tf.constant(value=False, dtype=tf_dtype(dtype='bool')))"
        ],
        "core_change": "-return tf.constant(value=False, dtype=tf_dtype(dtype='bool')) +return identity_operation(x=tf.constant(value=False, dtype=tf_dtype(dtype='bool')))",
        "core_API": "constant"
    },
    {
        "commit_hash": "ced4cfdbbf4496ec0d95483e470933b4fed4f95a",
        "index": "f606c649..2260b781 100644",
        "commit_message": "Allow saving / loading checkpoints from cloud paths (#683)\n\n* Allow saving / loading checkpoints from cloud paths\n\nAllows saving and loading checkpoints directly from cloud paths like\nAmazon S3 (s3://) and Google Cloud Storage (gs://) by using fsspec.\n\nNote: The user will have to install the relevant dependency for each\nprotocol. Otherwise fsspec will fail and specify which dependency is\nmissing.\n\n* Append suffix _fsspec to save/load function names\n\n* Add a lower bound to the fsspec dependency\n\nSkips the 0 major version.\n\n* Add missing changes from refactor\n\n* Use fsspec for remaining artifacts\n\n* Add test case with path requiring fsspec\n\n* Avoid writing logs to file unless output_path is local\n\n* Document the possibility of using paths supported by fsspec\n\n* Fix style and lint\n\n* Add missing lint fixes\n\n* Add type annotations to new functions\n\n* Use Coqpit method for converting config to dict\n\n* Fix type annotation in semi-new function\n\n* Add return type for load_fsspec\n\n* Fix bug where fs not always created\n\n* Restore the experiment removal functionality\n",
        "file": "TTS.txt.json",
        "label": "no",
        "comments": "customize API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HifiganGenerator(torch.nn.Module):",
            "def load_checkpoint(",
            "self, config, checkpoint_path, eval=False",
            "):  # pylint: disable=unused-argument, redefined-builtin",
            "-        state = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))",
            "+        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"))",
            "self.load_state_dict(state[\"model\"])",
            "if eval:",
            "self.eval()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='load_fsspec')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=load))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 900,
        "neg_line": [
            "-state = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))"
        ],
        "pos_line": [
            "+state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"))"
        ],
        "core_change": "-state = torch.load(checkpoint_path, map_location=torch.device(\"cpu\")) +state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"))",
        "core_API": "load"
    },
    {
        "commit_hash": "2a97601cffe71ed56b7c6fb35d904ecc0ddbd4d3",
        "index": "254f2c7a6..9b2350703 100644",
        "commit_message": "fix generate_share range and reduce field to 62\n\n",
        "file": "PySyft.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class AdditiveSharingTensor(AbstractTensor):",
            "random_shares = [random_type(secret.shape) for _ in range(n_workers - 1)]",
            "",
            "for share in random_shares:",
            "-            share.random_(-field, field)",
            "+            share.random_(int(-field/2), int(field/2)-1)",
            "",
            "shares = []",
            "for i in range(n_workers):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1456187)",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=4, insert_id=1456188)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=5, insert_id=1456189)",
            "Insert(target_node=IN(type=call), node=('identifier', 'int'), position=0, insert_id=1456190)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1456191)",
            "Insert(target_node=IN(type=binary_operator), node=('call', None), position=0, insert_id=1456192)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1456193)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1456194)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1456195)",
            "Insert(target_node=IN(type=argument_list), node=('binary_operator', None), position=1, insert_id=1456196)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1456197)",
            "Insert(target_node=IN(type=call), node=('identifier', 'int'), position=0, insert_id=1456198)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1456199)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=unary_operator), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=1456200)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=1456201)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1456202)",
            "Insert(target_node=IN(type=argument_list), node=('binary_operator', None), position=1, insert_id=1456203)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=field), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=1456204)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=1456205)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 901,
        "neg_line": [
            "-share.random_(-field, field)"
        ],
        "pos_line": [
            "+share.random_(int(-field/2), int(field/2)-1)"
        ],
        "core_change": "-share.random_(-field, field) +share.random_(int(-field/2), int(field/2)-1)",
        "core_API": "random_"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "5ad9b9bb..a080559b 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineEngine(DeepSpeedEngine):",
            "mem_cached = new_cached",
            "mem_alloced = new_alloced",
            "",
            "-        max_alloced = torch.cuda.max_memory_allocated()",
            "-        max_cached = torch.cuda.max_memory_cached()",
            "+        max_alloced = get_accelerator().max_memory_allocated()",
            "+        max_cached = get_accelerator().max_memory_cached()",
            "",
            "# convert to GB for printing",
            "new_alloced /= 1024**3"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=75080)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=75081)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=75082)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=75083)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=75084)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_accelerator')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=75085)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_accelerator')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=75086)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=75087)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 902,
        "neg_line": [
            "-max_alloced = torch.cuda.max_memory_allocated()",
            "-max_cached = torch.cuda.max_memory_cached()"
        ],
        "pos_line": [
            "+max_alloced = get_accelerator().max_memory_allocated()",
            "+max_cached = get_accelerator().max_memory_cached()"
        ],
        "core_change": "-max_alloced = torch.cuda.max_memory_allocated() -max_cached = torch.cuda.max_memory_cached() +max_alloced = get_accelerator().max_memory_allocated() +max_cached = get_accelerator().max_memory_cached()",
        "core_API": "max_memory_allocated"
    },
    {
        "commit_hash": "5eedf2997d1fc0e7cd1d09ef75c0ac20dee89d67",
        "index": "2448ff42..2a9749c2 100644",
        "commit_message": "Fix gpu placement bug in enum_discrete (#672)\n\n\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def torch_multinomial(input, num_samples, replacement=False):",
            "Does not support keyword argument `out`.",
            "\"\"\"",
            "if input.is_cuda:",
            "-        return torch_multinomial(input.cpu(), num_samples, replacement).cuda()",
            "+        return torch.multinomial(input.cpu(), num_samples, replacement).cuda(input.get_device())",
            "else:",
            "return torch.multinomial(input, num_samples, replacement)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=757110)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=757111)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=757112)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=757113)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=757114)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=757115)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch_multinomial), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=torch_multinomial), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=757116)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'multinomial'), position=2, insert_id=757117)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input'), position=0, insert_id=757118)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=757119)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_device'), position=2, insert_id=757120)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 903,
        "neg_line": [
            "-return torch_multinomial(input.cpu(), num_samples, replacement).cuda()"
        ],
        "pos_line": [
            "+return torch.multinomial(input.cpu(), num_samples, replacement).cuda(input.get_device())"
        ],
        "core_change": "-return torch_multinomial(input.cpu(), num_samples, replacement).cuda() +return torch.multinomial(input.cpu(), num_samples, replacement).cuda(input.get_device())",
        "core_API": "cpu"
    },
    {
        "commit_hash": "c664423caee20d946a3d9adc0fc55121dbd5fcc7",
        "index": "faddc7206..ea9ba650a 100644",
        "commit_message": "add redis test marker to conftest.py\nadd redis database tests to tox and .github flows\nuse print instead of std.write\nfix datasets tests to work with the new skip_checks flag\nadd skip_checks flag on dataset purge\n\n",
        "file": "PySyft.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_delete_entire_dataset(domain_owner, cleanup_storage):",
            "assert domain_owner.datasets[0].name == \"Dataset_1\"",
            "assert domain_owner.datasets[1].name == \"Dataset_2\"",
            "",
            "-    domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id)",
            "+    domain_owner.datasets.delete(",
            "+        dataset_id=domain_owner.datasets[0].id, skip_checks=True",
            "+    )",
            "",
            "# Check if the number of available datasets has been decreased",
            "assert len(domain_owner.datasets) == 1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1797526)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1797527)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'skip_checks'), position=0, insert_id=1797528)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1797529)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=1797530)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 904,
        "neg_line": [
            "-domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id)"
        ],
        "pos_line": [
            "+domain_owner.datasets.delete(",
            "+dataset_id=domain_owner.datasets[0].id, skip_checks=True",
            "+)"
        ],
        "core_change": "-domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id) +domain_owner.datasets.delete( +dataset_id=domain_owner.datasets[0].id, skip_checks=True +)",
        "core_API": "delete"
    },
    {
        "commit_hash": "8d6320087879b2974ad19b0a969d1dff599c4bd4",
        "index": "1ab4a9dfda..f27bb5d3f3 100644",
        "commit_message": "modified logspace to also include the dtype argument, fixing the failing unit test.\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def logspace(",
            "base=10.0,",
            "axis=None,",
            "*,",
            "+    dtype: torch.dtype,",
            "device: torch.device,",
            "out: Optional[torch.Tensor] = None,",
            "):",
            "-    power_seq = linspace(",
            "-        start, stop, num, axis, dtype=None, device=default_device(device)",
            "+    power_seq = ivy.linspace(",
            "+        start, stop, num, axis, dtype=dtype, device=ivy.default_device(device)",
            ")",
            "return base**power_seq"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=7, insert_id=347830)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=347831)",
            "Insert(target_node=IN(type=typed_parameter), node=('identifier', 'dtype'), position=0, insert_id=347832)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=347833)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=347834)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=347835)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=347836)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=347837)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=347838)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=347839)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=347840)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=347841)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=linspace), position=2)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=347842)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=347843)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=347844)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=347845)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=default_device), position=2)",
            "Delete(target_node=ASTNode(type=none, text=None))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 905,
        "neg_line": [
            "-power_seq = linspace(",
            "-start, stop, num, axis, dtype=None, device=default_device(device)"
        ],
        "pos_line": [
            "+dtype: torch.dtype,",
            "+power_seq = ivy.linspace(",
            "+start, stop, num, axis, dtype=dtype, device=ivy.default_device(device)"
        ],
        "core_change": "+dtype: torch.dtype, -power_seq = linspace( -start, stop, num, axis, dtype=None, device=default_device(device) +power_seq = ivy.linspace( +start, stop, num, axis, dtype=dtype, device=ivy.default_device(device)",
        "core_API": "linspace"
    },
    {
        "commit_hash": "18b21f0fd289565d1168c3b3ef49729d597900cf",
        "index": "126dc1b62..e804793d0 100644",
        "commit_message": "fix crypto lr test (#2691)\n\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_crypto_lr(fit_intercept, hook, workers):",
            "",
            "K = 2  # number of features",
            "",
            "-    beta = torch.Tensor([1.0, 10.0]).view(-1, 1)  # \"real\" coefficients",
            "-    intercept = 3.0 if fit_intercept else 0  # \"real\" intercept",
            "+    beta = torch.Tensor([1.0, 2.0]).view(-1, 1)  # \"real\" coefficients",
            "+    intercept = 0.5 if fit_intercept else 0  # \"real\" intercept",
            "",
            "# Alice's data",
            "torch.manual_seed(0)  # Truncation might not always work so we set the random seed"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=3.0), value='0.5')",
            "Update(target_node=ASTNode(type=float, text=10.0), value='2.0')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 906,
        "neg_line": [
            "-beta = torch.Tensor([1.0, 10.0]).view(-1, 1)  # \"real\" coefficients",
            "-intercept = 3.0 if fit_intercept else 0  # \"real\" intercept"
        ],
        "pos_line": [
            "+beta = torch.Tensor([1.0, 2.0]).view(-1, 1)  # \"real\" coefficients",
            "+intercept = 0.5 if fit_intercept else 0  # \"real\" intercept"
        ],
        "core_change": "-beta = torch.Tensor([1.0, 10.0]).view(-1, 1)  # \"real\" coefficients -intercept = 3.0 if fit_intercept else 0  # \"real\" intercept +beta = torch.Tensor([1.0, 2.0]).view(-1, 1)  # \"real\" coefficients +intercept = 0.5 if fit_intercept else 0  # \"real\" intercept",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "a1a04e05a828353c74775327095f78273faba4f3",
        "index": "dade3e067..ffa807e23 100644",
        "commit_message": "Add pytorch=1.10.0 to CI configuration (#3749)\n\n* Add pytorch=1.10.0 to CI configuration\n\n* fix:   test/espnet2/bin/test_k2_asr_inference.py\n\n* fix:   espnet2/main_funcs/pack_funcs.py\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def pack(",
            "try:",
            "import torch",
            "",
            "-        meta_objs.update(torch=torch.__version__)",
            "+        meta_objs.update(torch=str(torch.__version__))",
            "except ImportError:",
            "pass",
            "try:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=131936)",
            "Insert(target_node=IN(type=call), node=('identifier', 'str'), position=0, insert_id=131937)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=131938)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=131939)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=131940)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 908,
        "neg_line": [
            "-meta_objs.update(torch=torch.__version__)"
        ],
        "pos_line": [
            "+meta_objs.update(torch=str(torch.__version__))"
        ],
        "core_change": "-meta_objs.update(torch=torch.__version__) +meta_objs.update(torch=str(torch.__version__))",
        "core_API": "update"
    },
    {
        "commit_hash": "786be2a3f8b8160640614638f2fddc11572e2bc8",
        "index": "aff4daf8b..794d8055b 100755",
        "commit_message": "Should be fixed finally\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# recog",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.lmchainer.asr_chainer import recog",
            "+        from espnet.asr.chainer.asr_chainer import recog",
            "recog(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.lmpytorch.asr_pytorch import recog",
            "+        from espnet.asr.pytorch.asr_pytorch import recog",
            "recog(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=import_from_statement), node=ASTNode(type=dotted_name), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=lmpytorch), value='asr')",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'pytorch'), position=4, insert_id=178786)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=5, insert_id=178787)",
            "Insert(target_node=ASTNode(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=178788)",
            "Update(target_node=ASTNode(type=identifier, text=lmchainer), value='asr')",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'chainer'), position=4, insert_id=178789)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=5, insert_id=178790)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'recog'), position=0, insert_id=178791)",
            "Delete(target_node=ASTNode(type=identifier, text=recog))",
            "Delete(target_node=ASTNode(type=dotted_name))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 909,
        "neg_line": [
            "-from espnet.lmchainer.asr_chainer import recog",
            "-from espnet.lmpytorch.asr_pytorch import recog"
        ],
        "pos_line": [
            "+from espnet.asr.chainer.asr_chainer import recog",
            "+from espnet.asr.pytorch.asr_pytorch import recog"
        ],
        "core_change": "-from espnet.lmchainer.asr_chainer import recog +from espnet.asr.chainer.asr_chainer import recog -from espnet.lmpytorch.asr_pytorch import recog +from espnet.asr.pytorch.asr_pytorch import recog",
        "core_API": "info"
    },
    {
        "commit_hash": "83e06cd30a45245c2cb0e9f4bd924224b1581554",
        "index": "63e3a0f2bc..f35d42e0f7 100644",
        "commit_message": "[RLlib] DDPG refactor and Exploration API action noise classes. (#7314)\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix\n\n* WIP.\n\n* Add TD3 quick Pendulum regresison.\n\n* Cleanup.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Sort quick_learning test cases, add TD3.\n\n* Sort quick_learning test cases, add TD3.\n\n* Revert test_checkpoint_restore.py (debugging) changes.\n\n* Fix old soft_q settings in documentation and test configs.\n\n* More doc fixes.\n\n* Fix test case.\n\n* Fix test case.\n\n* Lower test load.\n\n* WIP.\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "change API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Schedule(metaclass=ABCMeta):",
            "raise NotImplementedError",
            "",
            "def value(self, t):",
            "-        if self.framework == \"tf\" and tf.executing_eagerly() is False:",
            "+        if self.framework == \"tf\":",
            "return tf.cast(",
            "-                tf.py_func(self._value, [t], tf.float64),",
            "+                tf.py_function(self._value, [t], tf.float64),",
            "tf.float32,",
            "-                name=\"schedule-value\")",
            "+                name=\"schedule_value\")",
            "return self._value(t)",
            "",
            "def __call__(self, t):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"schedule-value\"), value='\"schedule_value\"')",
            "Update(target_node=ASTNode(type=identifier, text=py_func), value='py_function')",
            "Delete(target_node=ASTNode(type=and, text=and))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=executing_eagerly))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=is, text=is))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=boolean_operator))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 910,
        "neg_line": [
            "-if self.framework == \"tf\" and tf.executing_eagerly() is False:",
            "-tf.py_func(self._value, [t], tf.float64),",
            "-name=\"schedule-value\")"
        ],
        "pos_line": [
            "+if self.framework == \"tf\":",
            "+tf.py_function(self._value, [t], tf.float64),",
            "+name=\"schedule_value\")"
        ],
        "core_change": "-if self.framework == \"tf\" and tf.executing_eagerly() is False: +if self.framework == \"tf\": -tf.py_func(self._value, [t], tf.float64), +tf.py_function(self._value, [t], tf.float64), -name=\"schedule-value\") +name=\"schedule_value\")",
        "core_API": "executing_eagerly"
    },
    {
        "commit_hash": "99a7d749961fc6e2bb529d97a9e505ba1816bcca",
        "index": "8fd8cb27..b5315c1c 100644",
        "commit_message": "misc small changes and fix #688\n\n",
        "file": "tensorpack.txt.json",
        "label": "no",
        "comments": "conditional warning",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def Dropout(x, *args, **kwargs):",
            "if 'is_training' in kwargs:",
            "kwargs['training'] = kwargs.pop('is_training')",
            "if len(args) > 0:",
            "-        logger.warn(",
            "-            \"The first positional argument to tensorpack.Dropout is the probability to keep rather than to drop. \"",
            "-            \"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \"",
            "-            \"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")",
            "+        if args[0] != 0.5:",
            "+            logger.warn(",
            "+                \"The first positional argument to tensorpack.Dropout is the probability to keep, rather than to drop. \"",
            "+                \"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \"",
            "+                \"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")",
            "rate = 1 - args[0]",
            "elif 'keep_prob' in kwargs:",
            "assert 'rate' not in kwargs, \"Cannot set both keep_prob and rate!\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=2639049)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2639050)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=2639051)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2639052)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2639053)",
            "Insert(target_node=IN(type=comparison_operator), node=('subscript', None), position=0, insert_id=2639054)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=2639055)",
            "Insert(target_node=IN(type=comparison_operator), node=('float', '0.5'), position=2, insert_id=2639056)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'args'), position=0, insert_id=2639057)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2639058)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=2639059)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2639060)",
            "Update(target_node=ASTNode(type=string, text=\"The first positional argument to tensorpack.Dropout is the probability to keep rather than to drop. \"), value='\"The first positional argument to tensorpack.Dropout is the probability to keep, rather than to drop. \"')"
        ],
        "plus_line": 5,
        "minus_line": 4,
        "AST_diff_line": 14,
        "number": 911,
        "neg_line": [
            "-logger.warn(",
            "-\"The first positional argument to tensorpack.Dropout is the probability to keep rather than to drop. \"",
            "-\"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \"",
            "-\"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")"
        ],
        "pos_line": [
            "+if args[0] != 0.5:",
            "+logger.warn(",
            "+\"The first positional argument to tensorpack.Dropout is the probability to keep, rather than to drop. \"",
            "+\"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \"",
            "+\"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")"
        ],
        "core_change": "-logger.warn( -\"The first positional argument to tensorpack.Dropout is the probability to keep rather than to drop. \" -\"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \" -\"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\") +if args[0] != 0.5: +logger.warn( +\"The first positional argument to tensorpack.Dropout is the probability to keep, rather than to drop. \" +\"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \" +\"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")",
        "core_API": "pop"
    },
    {
        "commit_hash": "7640dd7b8814feddf4c55ec8e84e62a2500db678",
        "index": "f407137f7..7a73ef71b 100644",
        "commit_message": "fixed a bug of overwriting the variable\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FeedForwardTransformer(TTSInterface, torch.nn.Module):",
            "",
            "# concat speaker embedding",
            "if self.spk_embed_dim is not None:",
            "-            spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)",
            "-            hs = self.projection(torch.cat([hs, spembs], dim=-1))",
            "+            spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)",
            "+            hs = self.projection(torch.cat([hs, spembs_], dim=-1))",
            "",
            "# forward duration predictor and length regulator",
            "d_masks = make_pad_mask(ilens).to(xs.device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=spembs), value='spembs_')",
            "Update(target_node=ASTNode(type=identifier, text=spembs), value='spembs_')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='F')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=functional))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 912,
        "neg_line": [
            "-spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)",
            "-hs = self.projection(torch.cat([hs, spembs], dim=-1))"
        ],
        "pos_line": [
            "+spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)",
            "+hs = self.projection(torch.cat([hs, spembs_], dim=-1))"
        ],
        "core_change": "-spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1) -hs = self.projection(torch.cat([hs, spembs], dim=-1)) +spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1) +hs = self.projection(torch.cat([hs, spembs_], dim=-1))",
        "core_API": "normalize"
    },
    {
        "commit_hash": "76b266bfbf8a4a01dd42708f70df36accc66f97d",
        "index": "324ea714..93dc9a4c 100644",
        "commit_message": " Upgrade to modern Python syntax (#1213)\n\n* Upgrade to modern Python syntax\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Use f-strings\n\n* Placate DeepSource Python\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def symmetrical_epipolar_distance(",
            "",
            "\"\"\"",
            "if not isinstance(Fm, torch.Tensor):",
            "-        raise TypeError(\"Fm type is not a torch.Tensor. Got {}\".format(type(Fm)))",
            "+        raise TypeError(f\"Fm type is not a torch.Tensor. Got {type(Fm)}\")",
            "",
            "if (len(Fm.shape) != 3) or not Fm.shape[-2:] == (3, 3):",
            "-        raise ValueError(\"Fm must be a (*, 3, 3) tensor. Got {}\".format(Fm.shape))",
            "+        raise ValueError(f\"Fm must be a (*, 3, 3) tensor. Got {Fm.shape}\")",
            "",
            "if pts1.size(-1) == 2:",
            "pts1 = kornia.convert_points_to_homogeneous(pts1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=0, insert_id=419652)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=def, text=def), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=symmetrical_epipolar_distance), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=(, text=(), position=2)",
            "Insert(target_node=IN(type=ERROR), node=('\"', '\"\"\"'), position=3, insert_id=419653)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=string, text=\"\"\"\nif not isinstance(Fm, torch.Tensor):\n        raise TypeError(\"Fm type is not a torch.Tensor. Got {}\".format(type(Fm)))\n\nif (len(Fm.shape) != 3) or not Fm.shape[-2:] == (3, 3):\n        raise ValueError(\"Fm must be a (*, 3, 3) tensor. Got {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=Fm))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=shape))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 916,
        "neg_line": [
            "-raise TypeError(\"Fm type is not a torch.Tensor. Got {}\".format(type(Fm)))",
            "-raise ValueError(\"Fm must be a (*, 3, 3) tensor. Got {}\".format(Fm.shape))"
        ],
        "pos_line": [
            "+raise TypeError(f\"Fm type is not a torch.Tensor. Got {type(Fm)}\")",
            "+raise ValueError(f\"Fm must be a (*, 3, 3) tensor. Got {Fm.shape}\")"
        ],
        "core_change": "-raise TypeError(\"Fm type is not a torch.Tensor. Got {}\".format(type(Fm))) +raise TypeError(f\"Fm type is not a torch.Tensor. Got {type(Fm)}\") -raise ValueError(\"Fm must be a (*, 3, 3) tensor. Got {}\".format(Fm.shape)) +raise ValueError(f\"Fm must be a (*, 3, 3) tensor. Got {Fm.shape}\")",
        "core_API": "size"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "61fd69f0..a0a71a45 100755",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Replay(Queue):",
            "",
            "zero = tf.constant(value=0, dtype=util.tf_dtype(dtype='long'))",
            "indices = tf.zeros(shape=(0,), dtype=util.tf_dtype(dtype='long'))",
            "-        indices, _ = tf.while_loop(",
            "+        indices, _ = self.while_loop(",
            "cond=cond, body=reduce_range_concat, loop_vars=(indices, zero),",
            "shape_invariants=(tf.TensorShape(dims=(None,)), zero.get_shape()), back_prop=False",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 917,
        "neg_line": [
            "-indices, _ = tf.while_loop("
        ],
        "pos_line": [
            "+indices, _ = self.while_loop("
        ],
        "core_change": "-indices, _ = tf.while_loop( +indices, _ = self.while_loop(",
        "core_API": "constant"
    },
    {
        "commit_hash": "32c7521c55be7691b5b10388897dc97ee670f31a",
        "index": "3eccb939eb..8b34e9439b 100644",
        "commit_message": "Superset Behaviour - linalg submodule (#4768)\n\n* added superset behaviour to cross, eigh, eighvalsh\n\n* fixed tests to support superset behaviour\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "eigh.unsupported_dtypes = (",
            "eigh.support_native_out = True",
            "",
            "",
            "-def eigvalsh(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "-    return torch.linalg.eigvalsh(x, out=out)",
            "+def eigvalsh(",
            "+    x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "+    return torch.linalg.eigvalsh(x, UPLO=UPLO, out=out)",
            "",
            "",
            "eigvalsh.unsupported_dtypes = ("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=7, insert_id=323480)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=323481)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('identifier', 'UPLO'), position=0, insert_id=323482)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=323483)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=323484)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=323485)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('string', '\"L\"'), position=4, insert_id=323486)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=323487)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Optional'), position=0, insert_id=323488)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=323489)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'str'), position=2, insert_id=323490)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=323491)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=323492)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=323493)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'UPLO'), position=0, insert_id=323494)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=323495)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'UPLO'), position=2, insert_id=323496)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 918,
        "neg_line": [
            "-def eigvalsh(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "-return torch.linalg.eigvalsh(x, out=out)"
        ],
        "pos_line": [
            "+def eigvalsh(",
            "+x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "+return torch.linalg.eigvalsh(x, UPLO=UPLO, out=out)"
        ],
        "core_change": "-def eigvalsh(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor: -return torch.linalg.eigvalsh(x, out=out) +def eigvalsh( +x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None +) -> torch.Tensor: +return torch.linalg.eigvalsh(x, UPLO=UPLO, out=out)",
        "core_API": "eigvalsh"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "eaeb5f1e..62534192 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "update param for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def image_histogram2d(",
            "hist = hist.squeeze()",
            "elif image.dim() == 3:",
            "hist = hist.squeeze(0)",
            "-    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=device)",
            "+    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=hist.device)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=419224)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hist'), position=0, insert_id=419225)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=419226)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=device), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 919,
        "neg_line": [
            "-return hist, torch.zeros_like(hist, dtype=hist.dtype, device=device)"
        ],
        "pos_line": [
            "+return hist, torch.zeros_like(hist, dtype=hist.dtype, device=hist.device)"
        ],
        "core_change": "-return hist, torch.zeros_like(hist, dtype=hist.dtype, device=device) +return hist, torch.zeros_like(hist, dtype=hist.dtype, device=hist.device)",
        "core_API": "squeeze"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "ba7e8bd6..4b644275 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "remove API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FeedForwardEncoder(Seq2SeqEncoder):",
            "return self._feedforward(inputs)",
            "else:",
            "outputs = self._feedforward(inputs)",
            "-            return outputs * mask.unsqueeze(dim=-1).float()",
            "+            return outputs * mask.unsqueeze(dim=-1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 920,
        "neg_line": [
            "-return outputs * mask.unsqueeze(dim=-1).float()"
        ],
        "pos_line": [
            "+return outputs * mask.unsqueeze(dim=-1)"
        ],
        "core_change": "-return outputs * mask.unsqueeze(dim=-1).float() +return outputs * mask.unsqueeze(dim=-1)",
        "core_API": "_feedforward"
    },
    {
        "commit_hash": "976fd81847918ff8942fdc58edebbdba7b0d7a07",
        "index": "aff69da8..4d3c1553 100644",
        "commit_message": "Small refactor on `filters` module: Dropping JIT support (#2187)\n\n* add suport to Tensor for sigmas\n\n- Removed the functions `gaussian_blur2d_t`, `get_gaussian_kernel1d_t`, `get_gaussian_kernel2d_t`, and\n`get_gaussian_kernel3d_t` in favor of support sigmas as `Tensors`, floats, or tuple of floats on the functions without\nthe suffix `_t`\n\n* Remove crashing JIT tests related to filters\n\n* fix typing\n\n* add support to kernel size as tuple or integer\n\n* update laplacian test\n\n* update DTYPES atol and rtol\n\n- added bfloat16\n- use same values for BaseTester class and the function\n\n* add device and dtype for default gaussian\n\n* add device and dtype for discrete gaussian by erf\n\n* add device, dtype, batched for discrete gaussian\n\n- add support to compute batched kernerl for discrete gaussian by bessel functions\n- add support to pass device and dtype when sigma is a float\n\n* add device and dtype for laplacian kernels\n\n* add device and dtype for binary and box kernels\n\n* add new typping annotations\n\n- by adding `from __future__ import annotations` we can use the pyupgrade\nto update the typing annotations to be like on python 3.10\n\n* add device and dtype for static kernels (sobel)\n\n* add dtype and device for canny\n\n- fix gaussian_blur2d sigma shape\n\n* add KORNIA_CHECK API to filters\n\n* fix typing and docs\n\n* fix laplacian module kernel_size typing\n\n* remove unsqueeze in favor of pythonic slicing\n\n* update kernel size canny module\n\n* add depreciation for `*_t` functions\n\n* update blur tests\n\n* add noncontiguous tests to blur\n\n* update canny tests\n\n- add BaseTester\n- add Dynamo test\n\n* update gaussian tests\n\n- Add basetester\n- add dynamo tests\n\n* update hanning tests\n\n* update laplacian tests\n\n- Add BaseTester\n- Add dynamo test\n\n* update median tests\n\n- add BaseTester\n- add dynamo test\n\n* remove border parameter\n\n- this should be tested just on `filter2d`, `filter3d`, etc\n\n* update motion tests\n\n- Add BaseTester\n- Add dynamo\n- Add tests for 3D\n\n* update sobel tests\n\n- Add basetester\n- add dynamo tests\n\n* update unsharp mask test\n\n- add basetester\n- add dynamo test\n\n* update filter 2d and 3d tests\n\n- add basetester\n- add dynamo\n\n* fix typing\n\n* remove Pyr down and up jit tests\n\n* update atol for fp64 based on old _DTYPE_PRECISIONS values\n\n* add TODO note about dtype precision for fp64\n\n* skip filter3d with reflect border for < 1.9 torch\n\n* small fix\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "change API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def elastic_transform2d(",
            "sigma_t = sigma.to(device=device, dtype=dtype)",
            "",
            "# Get Gaussian kernel for 'y' and 'x' displacement",
            "-    kernel_x: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[0].expand(2).unsqueeze(0))",
            "-    kernel_y: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[1].expand(2).unsqueeze(0))",
            "+    kernel_x: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[0].expand(2).unsqueeze(0))",
            "+    kernel_y: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[1].expand(2).unsqueeze(0))",
            "",
            "# Convolve over a random displacement matrix and scale them with 'alpha'",
            "disp_x: torch.Tensor = noise[:, :1]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=type), node=ASTNode(type=ERROR), position=8)",
            "Insert(target_node=ASTNode(type=ERROR), node=('type', None), position=4, insert_id=389287)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=389288)",
            "Update(target_node=ASTNode(type=identifier, text=get_gaussian_kernel2d_t), value='get_gaussian_kernel2d')",
            "Update(target_node=ASTNode(type=identifier, text=get_gaussian_kernel2d_t), value='get_gaussian_kernel2d')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=389289)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=389290)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=389291)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 921,
        "neg_line": [
            "-kernel_x: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[0].expand(2).unsqueeze(0))",
            "-kernel_y: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[1].expand(2).unsqueeze(0))"
        ],
        "pos_line": [
            "+kernel_x: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[0].expand(2).unsqueeze(0))",
            "+kernel_y: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[1].expand(2).unsqueeze(0))"
        ],
        "core_change": "-kernel_x: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[0].expand(2).unsqueeze(0)) -kernel_y: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[1].expand(2).unsqueeze(0)) +kernel_x: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[0].expand(2).unsqueeze(0)) +kernel_y: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[1].expand(2).unsqueeze(0))",
        "core_API": "to"
    },
    {
        "commit_hash": "d67a43aa8cf2a6618bbd103869c477e449e7428d",
        "index": "42f7ec7b..ba3c828f 100644",
        "commit_message": "Fix typos\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TpuStrategyTest(tf.test.TestCase):",
            "self.assertIn(prediction1, (\"yes\", \"no\"))",
            "",
            "prediction2 = loaded_serving_fn(",
            "-        tf.constant([\"ironman\", \"ironman\", \"unkonwn\"]))[\"output_0\"]",
            "+        tf.constant([\"ironman\", \"ironman\", \"unknown\"]))[\"output_0\"]",
            "self.assertIn(prediction2, (\"yes\", \"no\"))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"unkonwn\"), value='\"unknown\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 923,
        "neg_line": [
            "-tf.constant([\"ironman\", \"ironman\", \"unkonwn\"]))[\"output_0\"]"
        ],
        "pos_line": [
            "+tf.constant([\"ironman\", \"ironman\", \"unknown\"]))[\"output_0\"]"
        ],
        "core_change": "-tf.constant([\"ironman\", \"ironman\", \"unkonwn\"]))[\"output_0\"] +tf.constant([\"ironman\", \"ironman\", \"unknown\"]))[\"output_0\"]",
        "core_API": "assertIn"
    },
    {
        "commit_hash": "bd168d2f1254047013e679bc72f24d22e4f0f207",
        "index": "16776600..efd5a086 100644",
        "commit_message": "Barber-Agakov, and OED refactoring (#1361)\n\n* Merge changes from oed-master excluding sequential sigmoid example\n\n* Some flake8 and Python 2 changes\n\n* Indentation and noqa for lambdas\n\n* Blank __init__ file\n\n* Better docstrings, fix some tests\n\n* Revert Makefile\n\n* Some cosmetic tune-ups\n\n* Correct ewma formula in doc\n\n* A better docstring\n\n* Add from __future__ import... to all files\n\n* Better treatment of intial values\n\n* Rename variables\n\n* Rename shuffled properly\n\n* Better docstring\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "customized API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(N, M):",
            "item_thetas = torch.tensor([[0., 0.], [0., .5], [0., 1.]])",
            "design_tensor = build_design_tensor(item_thetas, individual_assignment)",
            "print(\"Design tensor\", design_tensor)",
            "-    y = naive_rainforth(model, design_tensor, target_labels=[\"w_global\", \"w_local\"], N=N, M=M)",
            "+    y = naive_rainforth_eig(model, design_tensor, observation_labels=\"y\",",
            "+                            target_labels=[\"w\", \"u\", \"G_u\"], N=N, M=M)",
            "print(\"EIG\", y)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=naive_rainforth), value='naive_rainforth_eig')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=734703)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=734704)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'observation_labels'), position=0, insert_id=734705)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=734706)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"y\"'), position=2, insert_id=734707)",
            "Update(target_node=ASTNode(type=string, text=\"w_global\"), value='\"w\"')",
            "Update(target_node=ASTNode(type=string, text=\"w_local\"), value='\"u\"')",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=4, insert_id=734708)",
            "Insert(target_node=ASTNode(type=list), node=('string', '\"G_u\"'), position=5, insert_id=734709)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 925,
        "neg_line": [
            "-y = naive_rainforth(model, design_tensor, target_labels=[\"w_global\", \"w_local\"], N=N, M=M)"
        ],
        "pos_line": [
            "+y = naive_rainforth_eig(model, design_tensor, observation_labels=\"y\",",
            "+target_labels=[\"w\", \"u\", \"G_u\"], N=N, M=M)"
        ],
        "core_change": "-y = naive_rainforth(model, design_tensor, target_labels=[\"w_global\", \"w_local\"], N=N, M=M) +y = naive_rainforth_eig(model, design_tensor, observation_labels=\"y\", +target_labels=[\"w\", \"u\", \"G_u\"], N=N, M=M)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "92f96f3e584138d41be6d2c114d410286cd7ecaa",
        "index": "c9cd3786e5..4c903ef0f0 100644",
        "commit_message": "fixing typo in torch backend subtract() | numpy-array-api test should pass\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def subtract(",
            "return torch.subtract(x1, x2, out=out)",
            "return torch.subtract(",
            "x1 if isinstance(x1, torch.Tensor) else torch.tensor(x1, dtype=x2.dtype),",
            "-        x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2, dtype=x2.dtype),",
            "+        x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2, dtype=x1.dtype),",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=x2), value='x1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 926,
        "neg_line": [
            "-x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2, dtype=x2.dtype),"
        ],
        "pos_line": [
            "+x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2, dtype=x1.dtype),"
        ],
        "core_change": "-x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2, dtype=x2.dtype), +x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2, dtype=x1.dtype),",
        "core_API": "subtract"
    },
    {
        "commit_hash": "38dfd07a4c85df28e777d34911e8a63209bf648a",
        "index": "9a9c9c1e4..f28198ed7 100755",
        "commit_message": "Revert \"attempt to fix media types test in pytoch (#2339)\"\n\nThis reverts commit c4ede069c33adc380ce5c3223f1acca035a2756e.\n\n",
        "file": "wandb.txt.json",
        "label": "no",
        "comments": "add graph",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def main():",
            "'tensorflow-variable-single-summary': tensorflow_variable_single,",
            "'tensorflow-variable-multi-summary': tensorflow_variable_multi,",
            "",
            "-            #'graph-summary': graph,",
            "+            'graph-summary': graph,",
            "})",
            "",
            "#history.add({"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=3, insert_id=2691538)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2691539)",
            "Insert(target_node=IN(type=ERROR), node=('string', \"'graph-summary'\"), position=0, insert_id=2691540)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=1, insert_id=2691541)",
            "Insert(target_node=IN(type=expression_statement), node=('identifier', 'graph'), position=0, insert_id=2691542)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=2691543)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 928,
        "neg_line": [
            "-#'graph-summary': graph,"
        ],
        "pos_line": [
            "+'graph-summary': graph,"
        ],
        "core_change": "-#'graph-summary': graph, +'graph-summary': graph,",
        "core_API": "add"
    },
    {
        "commit_hash": "a966834755378c6fdb92d1b7d0aebf818de7479f",
        "index": "a8b72714..24ab8d93 100644",
        "commit_message": "[BugFix] fix compression bugs (#5140)\n\n\n",
        "file": "nni.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rand_like_with_shape(shape, ori_t):",
            "higher_bound = torch.max(ori_t)",
            "",
            "if dtype in [torch.uint8, torch.int16, torch.short, torch.int16, torch.long, torch.bool]:",
            "-        return torch.randint(lower_bound, higher_bound+1, shape, dtype=dtype, device=device)",
            "+        return torch.randint(lower_bound.long(), higher_bound.long() + 1, shape, dtype=dtype, device=device)",
            "else:",
            "return torch.rand(shape, dtype=dtype, device=device, requires_grad=require_grad)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=654902)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=654903)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=654904)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=654905)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=lower_bound), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=654906)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=654907)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=654908)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=654909)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=654910)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=654911)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=higher_bound), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=654912)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=654913)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=654914)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=654915)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 929,
        "neg_line": [
            "-return torch.randint(lower_bound, higher_bound+1, shape, dtype=dtype, device=device)"
        ],
        "pos_line": [
            "+return torch.randint(lower_bound.long(), higher_bound.long() + 1, shape, dtype=dtype, device=device)"
        ],
        "core_change": "-return torch.randint(lower_bound, higher_bound+1, shape, dtype=dtype, device=device) +return torch.randint(lower_bound.long(), higher_bound.long() + 1, shape, dtype=dtype, device=device)",
        "core_API": "max"
    },
    {
        "commit_hash": "6643ade2f7ebd3c91ac0228d69867a48d4a83e40",
        "index": "ff45277d..d9dca377 100644",
        "commit_message": "More cuda fixes (#379)\n\nDefault use_cuda=torch.Tensor.is_cuda in irange and friends\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "add condition check for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def map_data_vector_model(subsample_size):",
            "pyro.sample(\"x\", dist.normal, mu[batch], sigma[batch])",
            "return batch",
            "",
            "-    ind = Variable(torch.LongTensor(range(20)))",
            "+    LongTensor = torch.cuda.LongTensor if torch.Tensor.is_cuda else torch.LongTensor",
            "+    ind = Variable(LongTensor(range(20)))",
            "batch = pyro.map_data('mapdata', ind, local_model, batch_size=subsample_size)",
            "return list(batch.data)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=760616)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=760617)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'LongTensor'), position=0, insert_id=760618)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=760619)",
            "Insert(target_node=IN(type=assignment), node=('conditional_expression', None), position=2, insert_id=760620)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=0, insert_id=760621)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=760622)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=2, insert_id=760623)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=760624)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=attribute), position=4)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=760625)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=760626)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'LongTensor'), position=2, insert_id=760627)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=760628)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=760629)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_cuda'), position=2, insert_id=760630)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=760631)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=760632)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=760633)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=760634)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=760635)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=760636)",
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'LongTensor'), position=0, insert_id=760637)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 930,
        "neg_line": [
            "-ind = Variable(torch.LongTensor(range(20)))"
        ],
        "pos_line": [
            "+LongTensor = torch.cuda.LongTensor if torch.Tensor.is_cuda else torch.LongTensor",
            "+ind = Variable(LongTensor(range(20)))"
        ],
        "core_change": "-ind = Variable(torch.LongTensor(range(20))) +LongTensor = torch.cuda.LongTensor if torch.Tensor.is_cuda else torch.LongTensor +ind = Variable(LongTensor(range(20)))",
        "core_API": "sample"
    },
    {
        "commit_hash": "13e81483ef36a73a739747fab8667d317fbdbbce",
        "index": "2f92834..9bcd45a 100644",
        "commit_message": "fixed small bug in style transfer solution\n\n",
        "file": "stanford-tensorflow-tutorials.txt.json",
        "label": "no",
        "comments": "print update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def train(model, generated_image, initial_image):",
            "## 2. create writer to write your graph",
            "saver = tf.train.Saver()",
            "sess.run(tf.global_variables_initializer())",
            "+        writer = tf.summary.FileWriter(EXP + '/graphs', sess.graph)",
            "###############################",
            "sess.run(generated_image.assign(initial_image))",
            "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2205639)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2205640)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'writer'), position=0, insert_id=2205641)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2205642)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2205643)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2205644)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2205645)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2205646)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2205647)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'FileWriter'), position=2, insert_id=2205648)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2205649)",
            "Insert(target_node=IN(type=argument_list), node=('binary_operator', None), position=1, insert_id=2205650)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2205651)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2205652)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2205653)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2205654)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2205655)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'summary'), position=2, insert_id=2205656)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'EXP'), position=0, insert_id=2205657)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2205658)",
            "Insert(target_node=IN(type=binary_operator), node=('string', \"'/graphs'\"), position=2, insert_id=2205659)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sess'), position=0, insert_id=2205660)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2205661)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'graph'), position=2, insert_id=2205662)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 932,
        "neg_line": [],
        "pos_line": [
            "+writer = tf.summary.FileWriter(EXP + '/graphs', sess.graph)"
        ],
        "core_change": "+writer = tf.summary.FileWriter(EXP + '/graphs', sess.graph)",
        "core_API": "Saver"
    },
    {
        "commit_hash": "aad9300bb04bed59f00b5a804980d2fab873260c",
        "index": "f3bba00..c00d139 100644",
        "commit_message": "fix dropout scaling from p to 1/(1-p) (#816)\n\nCo-authored-by: Sukru Eryilmaz <seryilmaz@computelab-dgx1v-32.nvidia.com>\n",
        "file": "apex.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SelfAttnFunc(torch.autograd.Function):",
            "values_grads   = torch.bmm(dropout_results.transpose(1,2), output_lin_grads, out=values_grads.transpose(0,1))",
            "",
            "# Mask and Scaling for Dropout (not a publically documented op)",
            "-        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])",
            "+        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0]))",
            "",
            "# Softmax Grad (not a publically documented op)",
            "softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=5, insert_id=52253)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=6, insert_id=52254)",
            "Insert(target_node=IN(type=binary_operator), node=('float', '1.0'), position=0, insert_id=52255)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=52256)",
            "Insert(target_node=IN(type=binary_operator), node=('parenthesized_expression', None), position=2, insert_id=52257)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=52258)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=52259)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=binary_operator), node=('float', '1.0'), position=0, insert_id=52260)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=52261)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=subscript), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 933,
        "neg_line": [
            "-dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])"
        ],
        "pos_line": [
            "+dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0]))"
        ],
        "core_change": "-dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0]) +dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0]))",
        "core_API": "bmm"
    },
    {
        "commit_hash": "595ec65796fa6cbb6ed9479e0871be3d6b208241",
        "index": "89bdd5198..ab14ed10e 100644",
        "commit_message": "refactor trainer checks (#1651)\n\n* refactor trainer checks\n\n* opt\n\n* none\n\n* Apply suggestions from code review\n\n* imports\n\n* fix tensors\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "add condition check for null fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ValidationEpochEndVariations(ABC):",
            "",
            "val_acc_mean += val_acc",
            "",
            "-        val_loss_mean /= len(outputs)",
            "-        val_acc_mean /= len(outputs)",
            "+        if outputs:  # skip zero divisions",
            "+            val_loss_mean /= len(outputs)",
            "+            val_acc_mean /= len(outputs)",
            "",
            "metrics_dict = {'val_loss': val_loss_mean.item(), 'val_acc': val_acc_mean.item()}",
            "results = {'progress_bar': metrics_dict, 'log': metrics_dict}"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=582573)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=582574)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', 'outputs'), position=1, insert_id=582575)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=582576)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=582577)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=1)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 934,
        "neg_line": [
            "-val_loss_mean /= len(outputs)",
            "-val_acc_mean /= len(outputs)"
        ],
        "pos_line": [
            "+if outputs:  # skip zero divisions",
            "+val_loss_mean /= len(outputs)",
            "+val_acc_mean /= len(outputs)"
        ],
        "core_change": "-val_loss_mean /= len(outputs) -val_acc_mean /= len(outputs) +if outputs:  # skip zero divisions +val_loss_mean /= len(outputs) +val_acc_mean /= len(outputs)",
        "core_API": "item"
    },
    {
        "commit_hash": "950a85d9f625ee7cb897e6f6a87aa6cc45bdea27",
        "index": "d59ee97..5119881 100644",
        "commit_message": "TensorRT PyTorch Hub inference fix (#7560)\n\nSolution proposed in https://github.com/ultralytics/yolov5/issues/7128 to TRT PyTorch Hub CUDA illegal memory errors.\n",
        "file": "yolov5.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AutoShape(nn.Module):",
            "#   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images",
            "",
            "t = [time_sync()]",
            "-        p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type",
            "+        p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type",
            "autocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference",
            "if isinstance(imgs, torch.Tensor):  # torch",
            "with amp.autocast(autocast):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1294734)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1294735)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1294736)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1294737)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1294738)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1294739)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1294740)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1294741)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1294742)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1294743)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=2, insert_id=1294744)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 935,
        "neg_line": [
            "-p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type"
        ],
        "pos_line": [
            "+p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type"
        ],
        "core_change": "-p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type +p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type",
        "core_API": "open"
    },
    {
        "commit_hash": "29021090614641d2509155ca0021497896228999",
        "index": "287fd74b..c191e67e 100644",
        "commit_message": "Fix all stable diffusion (#1415)\n\n* up\n\n* uP\n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "add condition check for null fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class CycleDiffusionPipeline(DiffusionPipeline):",
            "",
            "device = torch.device(f\"cuda:{gpu_id}\")",
            "",
            "-        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:",
            "+        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:",
            "if cpu_offloaded_model is not None:",
            "cpu_offload(cpu_offloaded_model, device)",
            "",
            "+        if self.safety_checker is not None:",
            "+            # TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate",
            "+            # fix by only offloading self.safety_checker for now",
            "+            cpu_offload(self.safety_checker.vision_model)",
            "+",
            "@property",
            "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline._execution_device",
            "def _execution_device(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=5, insert_id=1328668)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1328669)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1328670)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1328671)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1328672)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1328673)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1328674)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1328675)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1328676)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1328677)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1328678)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1328679)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'safety_checker'), position=2, insert_id=1328680)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1328681)",
            "Insert(target_node=IN(type=call), node=('identifier', 'cpu_offload'), position=0, insert_id=1328682)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1328683)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1328684)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1328685)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1328686)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1328687)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'vision_model'), position=2, insert_id=1328688)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 936,
        "neg_line": [
            "-for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:"
        ],
        "pos_line": [
            "+for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:",
            "+if self.safety_checker is not None:",
            "+# TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate",
            "+# fix by only offloading self.safety_checker for now",
            "+cpu_offload(self.safety_checker.vision_model)",
            "+"
        ],
        "core_change": "-for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]: +for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]: +if self.safety_checker is not None: +# TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate +# fix by only offloading self.safety_checker for now +cpu_offload(self.safety_checker.vision_model) +",
        "core_API": "device"
    },
    {
        "commit_hash": "b8385d8a118f19ff31a49fd463561e8d2d7f098d",
        "index": "ef5768800..9d5363cbd 100644",
        "commit_message": "TF Seq2Seq int dtype fix (#13496)\n\nFixes problems with passing int64 input to TF Seq2Seq models.\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def shift_tokens_right(input_ids: tf.Tensor, pad_token_id: int, decoder_start_to",
            "",
            "if tf.executing_eagerly():",
            "# \"Verify that `labels` has only positive values and -100\"",
            "-        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))",
            "+        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0, dtype=input_ids.dtype))",
            "",
            "# Make sure the assertion op is called by wrapping the result in an identity no-op",
            "with tf.control_dependencies([assert_gte0]):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2369205)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2369206)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2369207)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2369208)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2369209)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input_ids'), position=0, insert_id=2369210)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2369211)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2369212)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 937,
        "neg_line": [
            "-assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))"
        ],
        "pos_line": [
            "+assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0, dtype=input_ids.dtype))"
        ],
        "core_change": "-assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0)) +assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0, dtype=input_ids.dtype))",
        "core_API": "executing_eagerly"
    },
    {
        "commit_hash": "d2c82881aaa13202075a08a2b20eb9170401f2ca",
        "index": "ae43d8d01..746b3cce5 100644",
        "commit_message": "fix for alphabet import\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ConvEncoder(AbsEncoder):",
            "return self._output_dim",
            "",
            "def forward(self, input: torch.Tensor, ilens: torch.Tensor):",
            "-        \"\"\"",
            "+        \"\"\"Forward.",
            "+",
            "Args:",
            "-            input (torch.Tensor): mixed speech [Batch, sample]",
            "-            ilens (torch.Tensor): input lengths [Batch]",
            "+        input (torch.Tensor): mixed speech [Batch, sample]",
            "+        ilens (torch.Tensor): input lengths [Batch]",
            "\"\"\"",
            "assert input.dim() == 2, \"Currently only support single channle input\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nArgs:\n            input (torch.Tensor): mixed speech [Batch, sample]\n            ilens (torch.Tensor): input lengths [Batch]\n\"\"\"), value='\"\"\"Forward.\\n\\nArgs:\\n        input (torch.Tensor): mixed speech [Batch, sample]\\n        ilens (torch.Tensor): input lengths [Batch]\\n\"\"\"')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 939,
        "neg_line": [
            "-\"\"\"",
            "-input (torch.Tensor): mixed speech [Batch, sample]",
            "-ilens (torch.Tensor): input lengths [Batch]"
        ],
        "pos_line": [
            "+\"\"\"Forward.",
            "+",
            "+input (torch.Tensor): mixed speech [Batch, sample]",
            "+ilens (torch.Tensor): input lengths [Batch]"
        ],
        "core_change": "-\"\"\" +\"\"\"Forward. + -input (torch.Tensor): mixed speech [Batch, sample] -ilens (torch.Tensor): input lengths [Batch] +input (torch.Tensor): mixed speech [Batch, sample] +ilens (torch.Tensor): input lengths [Batch]",
        "core_API": "dim"
    },
    {
        "commit_hash": "e2e0c71f7e72a0048fa811e70f374b276f85baf2",
        "index": "0c7ff43f4c..4e7d7dc380 100644",
        "commit_message": "fixes to arange\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def arange(start, stop=None, step=1, dtype=None, dev=None):",
            "if dtype in [torch.int8, torch.uint8, torch.int16]:",
            "return torch.arange(start, stop, step=step, dtype=torch.int64, device=dev).to(dtype)",
            "else:",
            "-            return torch.range(start, stop, step=step, dtype=dtype, device=dev)",
            "+            return torch.arange(start, stop, step=step, dtype=dtype, device=dev)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=range), value='arange')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 940,
        "neg_line": [
            "-return torch.range(start, stop, step=step, dtype=dtype, device=dev)"
        ],
        "pos_line": [
            "+return torch.arange(start, stop, step=step, dtype=dtype, device=dev)"
        ],
        "core_change": "-return torch.range(start, stop, step=step, dtype=dtype, device=dev) +return torch.arange(start, stop, step=step, dtype=dtype, device=dev)",
        "core_API": "arange"
    },
    {
        "commit_hash": "c84eb77e2765ef0bc3899f6a9dcb880e30328326",
        "index": "0000000..b9eea34",
        "commit_message": "Api (#679)\n\n* legacy\n\n* new files\n\n* Update demo.py\n\n* test tunner bug fixed\n\n* classes and funtions in the demo created\n\n* network implemented not tested\n\n* travis to ignore legacy tests\n\n* test connectedHyperparameter\n\n* test fixed\n\n* some basic blocks implemented\n\n* still debuging\n\n* basic test for hypergraph passed\n\n* merge layer implemented\n\n* hyper_graph fully tested\n\n* more args added to auto model\n\n* test fixed\n\n* local changes\n\n* test auto_model\n\n* auto_model fit signature changed for validation data\n\n* Refactor (#646)\n\n* super classes extending object\n\n* change n_ to num_\n\n* refactored automodel to extend hypermodel and removed tuner from signature\n\n* rename HyperNode to Node. TextInput added\n\n* rename HyperGraph to GraphAutoModel extending AutoModel\n\n* refactor hyperhead, removed tensor heads\n\n* removed unnecessary blocks, rename build_output to build\n\n* changed some functions and attributes to private\n\n* remove trails from AutoModel public API\n\n* test cases changed accordingly\n\n* import modules instead of objects\n\n* tuner deleted from AutoModel contructor\n\n* change trails to num_trials\n\n* use the same quote sign\n\n* revised auto pipeline docs\n\n* removed compile from AutoModel\n\n* loss and metrics moved to hyper heads\n\n* do not flatten by default in hyperheads\n\n* inputs and outputs down to GraphAutoModel\n\n* changed AutoModel\n\n* name_scope changed to tf 2.0 and moved to hyperparameters and hypermodel\n\n* remove HierarchicalHyperParameters\n\n* renaming some variables and make private some members\n\n* remove legacy\n\n* test fixed\n\n* changed setup.py\n\n",
        "file": "autokeras.txt.json",
        "label": "no",
        "comments": "add method",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "+import tensorflow as tf",
            "+from autokeras.hyperparameters import HyperParameters",
            "+",
            "+",
            "+def test_hierarchical_hyperparameters():",
            "+    hp = HyperParameters()",
            "+    with tf.name_scope('abc'):",
            "+        hp.Choice('num_layers', [1, 2, 3], default=1)",
            "+    assert 'abc/num_layers' in hp.values"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 941,
        "neg_line": [],
        "pos_line": [
            "+import tensorflow as tf",
            "+from autokeras.hyperparameters import HyperParameters",
            "+",
            "+",
            "+def test_hierarchical_hyperparameters():",
            "+hp = HyperParameters()",
            "+with tf.name_scope('abc'):",
            "+hp.Choice('num_layers', [1, 2, 3], default=1)",
            "+assert 'abc/num_layers' in hp.values"
        ],
        "core_change": "+import tensorflow as tf +from autokeras.hyperparameters import HyperParameters + + +def test_hierarchical_hyperparameters(): +hp = HyperParameters() +with tf.name_scope('abc'): +hp.Choice('num_layers', [1, 2, 3], default=1) +assert 'abc/num_layers' in hp.values",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "225a85f81..68651abbc 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ≈†a≈°ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "doc string print error log update",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class TimesOfIndiaNewsHeadlines(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), _FILENAME)",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"path\": path_to_manual_file})]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'times_of_india_news_headlines\\', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"'), position=1, insert_id=1781655)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {}. Manual download instructions: {})\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_FILENAME))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 942,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format( -path_to_manual_file, _FILENAME, self.manual_download_instructions -) +f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
        "core_API": "join"
    },
    {
        "commit_hash": "701d4d3c4681810a48a122cff3d11f8b2dd7af78",
        "index": "716843da..ec7f0121 100644",
        "commit_message": "Issue #642 - Add `AtrousDeConv2dLayer` (#662)\n\n* Update visualize.py\n\n* Update README.md\n\nAdd an example for Adversarial Learning\n\n* Update more.rst\n\nUpdate the URLs\n\n* Update more.rst\n\n* Update example.rst\n\nAdd the same link of BEGAN implementation.\n\n* Update example.rst\n\n* Update example.rst\n\n* Create tutorial_tfslim.py\n\nfixes #552\n\n* Update tutorial_tfslim.py\n\n* Update utils.py\n\nFix #565\n\n* Update utils.py\n\n* Create test_utils_predict.py\n\nrelated with #288, #565, #566\n\n* Create test_utils_predict.py\n\n* Update utils.py\n\n* Update test_utils_predict.py\n\n* Update CHANGELOG.md\n\nrelated to #566\n\n* Update test_utils_predict.py\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py (fix Bad Coding Style)\n\n* Update test_utils_predict.py\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update convolution.py (Add AtrousConv2dTransLayer)\n\n* Add AtrousConv2dTransLayer\n\n* Fix some mistakes\n\n* Follow protocols\n\n* Fix coding style (yapf)\n\n* AtrousConv2dLayer fixed\n\n* AtrousConv2dTransposeLayer refactored\n\n* Fix coding style (yapf)\n\n* Fix error\n\n* Bias Add using premade tf func\n\n* Old TF Code Removed\n\n* Renamed to AtrousDeConv2dLayer\n\n* Update CHANGELOG.md\n\n* Release 1.8.6rc2\n\n* Documentation Fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main(_):",
            "# n_examples = batch_size * num_steps",
            "# so",
            "# cost is the averaged cost of each mini-batch (concurrent process).",
            "-        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(  # loss = tf.nn.seq2seq.sequence_loss_by_example( # TF0.12",
            "-            [outputs], [tf.reshape(targets, [-1])], [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)])",
            "+        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(",
            "+            [outputs], [tf.reshape(targets, [-1])], [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)]",
            "+        )",
            "# [tf.ones([batch_size * num_steps])])",
            "cost = tf.reduce_sum(loss) / batch_size",
            "return cost"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 943,
        "neg_line": [
            "-loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(  # loss = tf.nn.seq2seq.sequence_loss_by_example( # TF0.12",
            "-[outputs], [tf.reshape(targets, [-1])], [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)])"
        ],
        "pos_line": [
            "+loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(",
            "+[outputs], [tf.reshape(targets, [-1])], [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)]",
            "+)"
        ],
        "core_change": "-loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(  # loss = tf.nn.seq2seq.sequence_loss_by_example( # TF0.12 -[outputs], [tf.reshape(targets, [-1])], [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)]) +loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example( +[outputs], [tf.reshape(targets, [-1])], [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)] +)",
        "core_API": "sequence_loss_by_example"
    },
    {
        "commit_hash": "20a9acca2636512522116601ae09c6be0408b486",
        "index": "7a8dc91a..a38538c8 100644",
        "commit_message": "Speed-up warp_affine and fix bugs in RandomAffine [WIP: to add tests] (#474)\n\n* speed-up warp_affine, rotate, random_crop\n\n* added basic speed test for warp_affine\n\n* fixed centerization for random shear and bug (radians instead of degrees)\n\n* add test versus torchvision\n\n* added convert_affinematrix_to_homography function\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "change API call for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _apply_affine(input: torch.Tensor,",
            "",
            "height, width = x_data.shape[-2:]",
            "transform: torch.Tensor = params['transform'].to(device, dtype)",
            "-",
            "-    out_data: torch.Tensor = warp_perspective(x_data, transform, (height, width))",
            "+    out_data: torch.Tensor = warp_affine(x_data, transform[:, :2, :], (height, width))",
            "",
            "if return_transform:",
            "return out_data.view_as(input), transform"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=warp_perspective), value='warp_affine')",
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=3, insert_id=442854)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=442855)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=transform), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=442856)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=442857)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=,, text=,), position=3)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=4, insert_id=442858)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=5, insert_id=442859)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=6, insert_id=442860)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=7, insert_id=442861)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=442862)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=442863)",
            "Insert(target_node=IN(type=slice), node=('integer', '2'), position=1, insert_id=442864)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=442865)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 944,
        "neg_line": [
            "-",
            "-out_data: torch.Tensor = warp_perspective(x_data, transform, (height, width))"
        ],
        "pos_line": [
            "+out_data: torch.Tensor = warp_affine(x_data, transform[:, :2, :], (height, width))"
        ],
        "core_change": "- -out_data: torch.Tensor = warp_perspective(x_data, transform, (height, width)) +out_data: torch.Tensor = warp_affine(x_data, transform[:, :2, :], (height, width))",
        "core_API": "view_as"
    },
    {
        "commit_hash": "035d5eb68e5ecb24bc148abda9c3957b9e756632",
        "index": "646ae67170..fd6aca1a90 100644",
        "commit_message": "added docstrings examples for general:unstack (#2594)\n\n* added docstrings examples for general:unstack\n\n* fixed bugs in unstack docstrings\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def floormod(",
            "return ret",
            "",
            "",
            "-def unstack(x, axis: int, keepdims: bool = False) -> List[torch.Tensor]:",
            "+def unstack(",
            "+    x: torch.Tensor,",
            "+    axis: int,",
            "+    keepdims: bool = False",
            "+) -> List[torch.Tensor]:",
            "if x.shape == ():",
            "return [x]",
            "ret = list(torch.unbind(x, axis))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=1, insert_id=341552)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=identifier, text=x), position=0)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=341553)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=341554)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=341555)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=341556)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=341557)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=341558)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 8,
        "number": 945,
        "neg_line": [
            "-def unstack(x, axis: int, keepdims: bool = False) -> List[torch.Tensor]:"
        ],
        "pos_line": [
            "+def unstack(",
            "+x: torch.Tensor,",
            "+axis: int,",
            "+keepdims: bool = False",
            "+) -> List[torch.Tensor]:"
        ],
        "core_change": "-def unstack(x, axis: int, keepdims: bool = False) -> List[torch.Tensor]: +def unstack( +x: torch.Tensor, +axis: int, +keepdims: bool = False +) -> List[torch.Tensor]:",
        "core_API": "unbind"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "5dfd8151e..44c6dd658 100755",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class GPTNeoAttentionMixin:",
            "else:",
            "raise ValueError(f\"Input tensor rank should be one of [2, 3], but is: {len(tensor.shape)}\")",
            "",
            "-        padded_tensor = F.pad(tensor, padding_side, value=pad_value)",
            "+        padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)",
            "padded_tensor = padded_tensor.unfold(dimension=1, size=window_size + block_length, step=block_length)",
            "",
            "if is_key_value:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1755663)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1755664)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=F), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1755665)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 946,
        "neg_line": [
            "-padded_tensor = F.pad(tensor, padding_side, value=pad_value)"
        ],
        "pos_line": [
            "+padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)"
        ],
        "core_change": "-padded_tensor = F.pad(tensor, padding_side, value=pad_value) +padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)",
        "core_API": "pad"
    },
    {
        "commit_hash": "017eeed143917da5d43d30a1cb028028965258c0",
        "index": "aa53df9901..5ad4eb05b7 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "import ivy",
            "from typing import Optional, Union",
            "",
            "",
            "-def logit(x: Union[tf.Tensor, tf.Variable],",
            "-          /,",
            "-          *,",
            "-          eps: Optional[float] = None,",
            "-          out=None):",
            "+def logit(",
            "+    x: Union[tf.Tensor, tf.Variable], /, *, eps: Optional[float] = None, out=None",
            "+):",
            "x_dtype = x.dtype",
            "if eps is None:",
            "x = tf.where(tf.math.logical_or(x > 1, x < 0), ivy.nan, x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 947,
        "neg_line": [
            "-def logit(x: Union[tf.Tensor, tf.Variable],",
            "-/,",
            "-*,",
            "-eps: Optional[float] = None,",
            "-out=None):"
        ],
        "pos_line": [
            "+def logit(",
            "+x: Union[tf.Tensor, tf.Variable], /, *, eps: Optional[float] = None, out=None",
            "+):"
        ],
        "core_change": "-def logit(x: Union[tf.Tensor, tf.Variable], -/, -*, -eps: Optional[float] = None, -out=None): +def logit( +x: Union[tf.Tensor, tf.Variable], /, *, eps: Optional[float] = None, out=None +):",
        "core_API": "where"
    },
    {
        "commit_hash": "2322eb8e2f9765cb73f59b324cc46a0e9cfe803f",
        "index": "b0d26e6ed..53370c1d3 100644",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTransfoXLPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 949,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "27561892..324daee0 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "change param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BiattentiveClassificationNetwork(Model):",
            "# Create ELMo embeddings if applicable",
            "if self._elmo:",
            "if elmo_tokens is not None:",
            "-                elmo_representations = self._elmo(elmo_tokens)[\"elmo_representations\"]",
            "+                elmo_representations = self._elmo(elmo_tokens[\"tokens\"])[\"elmo_representations\"]",
            "# Pop from the end is more performant with list",
            "if self._use_integrator_output_elmo:",
            "integrator_output_elmo = elmo_representations.pop()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=1, insert_id=22358)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=elmo_tokens), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=22359)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"tokens\"'), position=2, insert_id=22360)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=22361)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 951,
        "neg_line": [
            "-elmo_representations = self._elmo(elmo_tokens)[\"elmo_representations\"]"
        ],
        "pos_line": [
            "+elmo_representations = self._elmo(elmo_tokens[\"tokens\"])[\"elmo_representations\"]"
        ],
        "core_change": "-elmo_representations = self._elmo(elmo_tokens)[\"elmo_representations\"] +elmo_representations = self._elmo(elmo_tokens[\"tokens\"])[\"elmo_representations\"]",
        "core_API": "_elmo"
    },
    {
        "commit_hash": "af6ede0afd5e9e4157436c6737ae68e551f193d0",
        "index": "f5c4090b..cec03a18 100644",
        "commit_message": "fix some unit tests\n\n",
        "file": "flair.txt.json",
        "label": "no",
        "comments": "test fix",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_tagged_corpus_downsample():",
            "",
            "assert 10 == len(corpus.train)",
            "",
            "-    corpus.downsample(percentage=0.3, only_downsample_train=True)",
            "+    corpus.downsample(percentage=0.3, downsample_dev=False, downsample_test=False)",
            "",
            "assert 3 == len(corpus.train)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1793918)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1793919)",
            "Update(target_node=ASTNode(type=identifier, text=only_downsample_train), value='downsample_dev')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1793920)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'downsample_test'), position=0, insert_id=1793921)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1793922)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1793923)",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 953,
        "neg_line": [
            "-corpus.downsample(percentage=0.3, only_downsample_train=True)"
        ],
        "pos_line": [
            "+corpus.downsample(percentage=0.3, downsample_dev=False, downsample_test=False)"
        ],
        "core_change": "-corpus.downsample(percentage=0.3, only_downsample_train=True) +corpus.downsample(percentage=0.3, downsample_dev=False, downsample_test=False)",
        "core_API": "downsample"
    },
    {
        "commit_hash": "68495894306acfef6b5dc9c812abdca01ddc54a7",
        "index": "43104179..4f88ae88 100644",
        "commit_message": "Fix LiL sparse matrix on Tensorflow (#4173)\n\nLiL sparse matrices would not work correctly due to dtype being\ndifferent. Using the sparse_coo data fixes it.\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "change param for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Function(object):",
            "if is_sparse(tensor):",
            "sparse_coo = value.tocoo()",
            "indices = np.concatenate((np.expand_dims(sparse_coo.row, 1), np.expand_dims(sparse_coo.col, 1)), 1)",
            "-                value = (indices, value.data, value.shape)",
            "+                value = (indices, sparse_coo.data, sparse_coo.shape)",
            "feed_dict[tensor] = value",
            "session = get_session()",
            "updated = session.run(self.outputs + [self.updates_op], feed_dict=feed_dict)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=value), value='sparse_coo')",
            "Update(target_node=ASTNode(type=identifier, text=value), value='sparse_coo')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 957,
        "neg_line": [
            "-value = (indices, value.data, value.shape)"
        ],
        "pos_line": [
            "+value = (indices, sparse_coo.data, sparse_coo.shape)"
        ],
        "core_change": "-value = (indices, value.data, value.shape) +value = (indices, sparse_coo.data, sparse_coo.shape)",
        "core_API": "tocoo"
    },
    {
        "commit_hash": "9b0b579e876cd7d3ee115a8da94b5ffb182fcf3f",
        "index": "cd8d7cb5..0076433a 100644",
        "commit_message": "Adds model configs to ludwig.datasets (#2540)\n\n* Adds README and stub for reading dataset configs.\n\n* Adds __init__.py for configs, moves circular import into function scope in ludwig/datasets/__init__.py\n\n* Print config files in datasets folder.\n\n* First pass at automatic archive extraction.\n\n* Implemented downloading and extract.\n\n* Refactor DatasetConfig into its own file.\n\n* Fixed bugs downloading kaggle dataset.\n\n* Makes registry store dataset instances, not classes. Also comments out import_submodules for testing.\n\n* Typo fix.\n\n* Only pass data files on to load_unprocessed_dataframe, symlink directories.\n\n* Downloading dataset files into existing directory if exists.\n\n* Refactor: make datasets fully config-first, lazy load dataset loaders.\n\n* Implemented agnews custom loader.\n\n* Implements train/validation/test split by files, and globbing support\n\n* Adds _glob_multiple\n\n* Adds adult_census_income, agnews, allstate_claims_severity.\n\n* Implements sha256 verification, adds more datasets up to creditcard_fraud.\n\n* Adds checksums, dbpedia, electricity\n\n* Fixes gzip file name returned as string not list, adds up to forest_cover dataset.\n\n* Adds datasets up to reuters_r8\n\n* Adds all datasets which don't require a custom class.\n\n* Restore dataset import behavior by implementing module __getattr__\n\n* Adds KDD datasets.\n\n* Adds ieee_fraud.\n\n* Adds imbalanced_insurance, insurance_lite.\n\n* Adds mnist.\n\n* Completes implementation of all of the built-in datasets.\n\n* Made cache_dir optional, read from environment variable if set.\n\n* Upgrades datasets tests.\n\n* Adds test for new dataset config API.  Also adds scripts for dataset link checking.\n\n* Fixes loading allstate claims severity dataset.\n\n* Use @lru_cache(1), @cache not supported in python < 3.9\n\n* Deletes dataset registry, updates automl test utils\n\n* Fix imports of datasets API.\n\n* Adds more detail to sha256: docstring and basic README\n\n* Copy-paste link oops.\n\n* Fixes handling of nested archive types like .tar.bz  Also adds a LUDWIG_CACHE and export to the README\n\n* Adds link for twitter bots.\n\n* Fix order of splits in README.md\n\n* typo\n\n* Adds verify as a phase in doc string.\n\n* Support .pqt, .pq extensions for parquet.\n\n* Handle nested archives with longer file extensions like .csv.zip\n\n* Handle nested .gz types properly too.  Check all extensions with .endswith\n\n* Handle all archive types with .endswith\n\n* Update ludwig/datasets/loaders/split_loaders.py\n\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\n\n* Adds explanation for export, fixes preserve_paths (should be relative to processed_dataset_dir)\n\n* Resolve preserved paths relative to raw dataset dir before move.\n\n* Catch runtime exception from extracting sub-archives.\n\n* Started adding info to README about dataset model configs.\n\n* Adds method to get model configs for datasets.\n\n* Adds mnist, titanic examples as default configs.\n\n* Export dataset before training.\n\n* Adds multiprocessing version of train_all_model_configs, and adds a few configs mosttly from automl experiments.\n\n* Adds a few more configs, removes AG news, training is too slow.\n\n* Default to only 4 processes due to memory constraints.\n\n* Visualize learning curves.\n\n* Started documenting API functions in readme.\n\n* Adds test for model configs API, updates tests to mock protected _load_dataset_config\n\n* Clear dataset cache after testing with mock datasets.\n\n* Adds best configs, improved README\n\n* higgs_best consistent formatting.\n\n* Update ludwig/datasets/README.md\n\nCo-authored-by: abidwael <103003638+abidwael@users.noreply.github.com>\n\n* Adds model commit hash to results.\n\n* Adds MSE, MAE to metric list.\n\n* Don't printout ludwig commit.\n\n* Increase display width to show more columns in print output.\n\n* Fix error in get_commit_hash\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\nCo-authored-by: abidwael <103003638+abidwael@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_download_mnist_dataset(tmpdir):",
            ")",
            "",
            "ludwig.datasets._get_dataset_configs.cache_clear()",
            "-    with mock.patch(\"ludwig.datasets.load_dataset_config\", return_value=config):",
            "+    with mock.patch(\"ludwig.datasets._load_dataset_config\", return_value=config):",
            "dataset = ludwig.datasets.get_dataset(\"mnist\", cache_dir=tmpdir)",
            "assert not dataset.state == DatasetState.DOWNLOADED",
            "assert not dataset.state == DatasetState.TRANSFORMED",
            "dataset.download()",
            "",
            "assert dataset.state == DatasetState.DOWNLOADED",
            "+    ludwig.datasets._get_dataset_configs.cache_clear()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=8)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1795986)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1795987)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1795988)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1795989)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1795990)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1795991)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cache_clear'), position=2, insert_id=1795992)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1795993)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1795994)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1795995)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1795996)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_get_dataset_configs'), position=2, insert_id=1795997)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ludwig'), position=0, insert_id=1795998)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1795999)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=2, insert_id=1796000)",
            "Update(target_node=ASTNode(type=string, text=\"ludwig.datasets.load_dataset_config\"), value='\"ludwig.datasets._load_dataset_config\"')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 958,
        "neg_line": [
            "-with mock.patch(\"ludwig.datasets.load_dataset_config\", return_value=config):"
        ],
        "pos_line": [
            "+with mock.patch(\"ludwig.datasets._load_dataset_config\", return_value=config):",
            "+ludwig.datasets._get_dataset_configs.cache_clear()"
        ],
        "core_change": "-with mock.patch(\"ludwig.datasets.load_dataset_config\", return_value=config): +with mock.patch(\"ludwig.datasets._load_dataset_config\", return_value=config): +ludwig.datasets._get_dataset_configs.cache_clear()",
        "core_API": "cache_clear"
    },
    {
        "commit_hash": "bc26c18201b053373911559ad45771b14b11f13b",
        "index": "f81682ab3..be0246ce1 100644",
        "commit_message": "minor fix of li52\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_transformer_trainable_and_decodable(model_dict):",
            "attn_dict = model.calculate_all_attentions(",
            "x[0:1], ilens[0:1], y_tgt[0:1], y_src[0:1]",
            ")",
            "-    plot.plot_multi_head_attention(data, attn_dict, \"\", savefn=_savefn)",
            "+    plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"\", savefn=_savefn)",
            "",
            "# test CTC plot",
            "ctc_probs = model.calculate_all_ctc_probs("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'uttid_list'), position=3, insert_id=141801)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=141802)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 960,
        "neg_line": [
            "-plot.plot_multi_head_attention(data, attn_dict, \"\", savefn=_savefn)"
        ],
        "pos_line": [
            "+plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"\", savefn=_savefn)"
        ],
        "core_change": "-plot.plot_multi_head_attention(data, attn_dict, \"\", savefn=_savefn) +plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"\", savefn=_savefn)",
        "core_API": "calculate_all_attentions"
    },
    {
        "commit_hash": "2322eb8e2f9765cb73f59b324cc46a0e9cfe803f",
        "index": "418ab2b14..6bb3eb54e 100644",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFLayoutLMv3PreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "-                \"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"),",
            "\"pixel_values\": tf.TensorSpec((None, None, None, None), tf.float32, name=\"pixel_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2361349)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2361350)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361351)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2361352)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2361353)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2361354)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2361355)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2361356)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2361357)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2361358)",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 961,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "-\"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"),",
            "-\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+\"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"),",
            "+\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), -\"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), +\"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"), -\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"), +\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "521da6518f8f5a5b691cde562b59c62e1f4b7a82",
        "index": "149b5d5ff..5008c7ed7 100755",
        "commit_message": "Fix gpt2 fp16 training when tracing is enabled (#20656)\n\n* ONNX tracing fix\n\n* Remove conditional\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DecisionTransformerGPT2Attention(nn.Module):",
            "# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.",
            "# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`",
            "mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)",
            "-            attn_weights = torch.where(causal_mask, attn_weights, mask_value)",
            "+            attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)",
            "",
            "if attention_mask is not None:",
            "# Apply the attention mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=1181489)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=7, insert_id=1181490)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1181491)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1181492)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=attn_weights), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1181493)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1181494)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1181495)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1181496)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'attn_weights'), position=0, insert_id=1181497)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1181498)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1181499)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 962,
        "neg_line": [
            "-attn_weights = torch.where(causal_mask, attn_weights, mask_value)"
        ],
        "pos_line": [
            "+attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)"
        ],
        "core_change": "-attn_weights = torch.where(causal_mask, attn_weights, mask_value) +attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)",
        "core_API": "full"
    },
    {
        "commit_hash": "a7e5a5f7b4e70c1df705c0ece4f046e44fc22525",
        "index": "0a7102c3..c32f09b4 100644",
        "commit_message": "Fix and enable some tfcoreml converter convent tests\n\n",
        "file": "coremltools.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def load_tf_graph(graph_file):",
            "\"\"\"",
            "# We load the protobuf file from the disk and parse it to retrieve the",
            "# unserialized graph_def",
            "-    with tf.gfile.GFile(graph_file, \"rb\") as f:",
            "-        graph_def = tf.GraphDef()",
            "+    with tf.io.gfile.GFile(graph_file, \"rb\") as f:",
            "+        graph_def = tf.compat.v1.GraphDef()",
            "graph_def.ParseFromString(f.read())",
            "",
            "# Then, we import the graph_def into a new Graph and returns it"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1916086)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1916087)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1916088)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1916089)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=1916090)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=1916091)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 963,
        "neg_line": [
            "-with tf.gfile.GFile(graph_file, \"rb\") as f:",
            "-graph_def = tf.GraphDef()"
        ],
        "pos_line": [
            "+with tf.io.gfile.GFile(graph_file, \"rb\") as f:",
            "+graph_def = tf.compat.v1.GraphDef()"
        ],
        "core_change": "-with tf.gfile.GFile(graph_file, \"rb\") as f: -graph_def = tf.GraphDef() +with tf.io.gfile.GFile(graph_file, \"rb\") as f: +graph_def = tf.compat.v1.GraphDef()",
        "core_API": "GFile"
    },
    {
        "commit_hash": "aa2ec42da619f8f044e57686ea52b2df6da5a924",
        "index": "5931d115..d3bb232f 100644",
        "commit_message": "stop gradients (#3221)\n\n* stop gradients\n\n* fix stop grad test\n\n* stop gradients\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "add method",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def gradients(loss, variables):",
            "return tf.gradients(loss, variables)",
            "",
            "",
            "+def stop_gradient(variables):",
            "+    '''Returns `variables` but with zero gradient with respect to every other",
            "+    variables.",
            "+    '''",
            "+    return tf.stop_gradient(variables)",
            "+",
            "+",
            "# CONTROL FLOW",
            "",
            "def rnn(step_function, inputs, initial_states,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=2, insert_id=2116945)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=2116946)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'stop_gradient'), position=1, insert_id=2116947)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=2116948)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=2116949)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=2116950)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=2116951)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'variables'), position=1, insert_id=2116952)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=2116953)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2116954)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=1, insert_id=2116955)",
            "Insert(target_node=IN(type=expression_statement), node=('string', \"'''Returns `variables` but with zero gradient with respect to every other\\n    variables.\\n    '''\"), position=0, insert_id=2116956)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2116957)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=2116958)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2116959)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2116960)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2116961)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2116962)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stop_gradient'), position=2, insert_id=2116963)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2116964)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'variables'), position=1, insert_id=2116965)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2116966)"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 964,
        "neg_line": [],
        "pos_line": [
            "+def stop_gradient(variables):",
            "+'''Returns `variables` but with zero gradient with respect to every other",
            "+variables.",
            "+'''",
            "+return tf.stop_gradient(variables)",
            "+",
            "+"
        ],
        "core_change": "+def stop_gradient(variables): +'''Returns `variables` but with zero gradient with respect to every other +variables. +''' +return tf.stop_gradient(variables) + +",
        "core_API": "gradients"
    },
    {
        "commit_hash": "a966834755378c6fdb92d1b7d0aebf818de7479f",
        "index": "18f0c019..aa76c6cd 100644",
        "commit_message": "[BugFix] fix compression bugs (#5140)\n\n\n",
        "file": "nni.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "},",
            "\"outputs\": [],",
            "\"source\": [",
            "-        \"import torch\\nimport torch.nn.functional as F\\nfrom torch.optim import SGD\\n\\nfrom scripts.compression_mnist_model import TorchModel, trainer, evaluator, device, test_trt\\n\\n# define the model\\nmodel = TorchModel().to(device)\\n\\n# define the optimizer and criterion for pre-training\\n\\noptimizer = SGD(model.parameters(), 1e-2)\\ncriterion = F.nll_loss\\n\\n# pre-train and evaluate the model on MNIST dataset\\nfor epoch in range(3):\\n    trainer(model, optimizer, criterion)\\n    evaluator(model)\"",
            "+        \"import torch\\nimport torch.nn.functional as F\\nfrom torch.optim import SGD\\n\\nfrom nni_assets.compression.mnist_model import TorchModel, trainer, evaluator, device, test_trt\\n\\n# define the model\\nmodel = TorchModel().to(device)\\n\\n# define the optimizer and criterion for pre-training\\n\\noptimizer = SGD(model.parameters(), 1e-2)\\ncriterion = F.nll_loss\\n\\n# pre-train and evaluate the model on MNIST dataset\\nfor epoch in range(3):\\n    trainer(model, optimizer, criterion)\\n    evaluator(model)\"",
            "]",
            "},",
            "{"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"import torch\\nimport torch.nn.functional as F\\nfrom torch.optim import SGD\\n\\nfrom scripts.compression_mnist_model import TorchModel, trainer, evaluator, device, test_trt\\n\\n# define the model\\nmodel = TorchModel().to(device)\\n\\n# define the optimizer and criterion for pre-training\\n\\noptimizer = SGD(model.parameters(), 1e-2)\\ncriterion = F.nll_loss\\n\\n# pre-train and evaluate the model on MNIST dataset\\nfor epoch in range(3):\\n    trainer(model, optimizer, criterion)\\n    evaluator(model)\"), value='\"import torch\\\\nimport torch.nn.functional as F\\\\nfrom torch.optim import SGD\\\\n\\\\nfrom nni_assets.compression.mnist_model import TorchModel, trainer, evaluator, device, test_trt\\\\n\\\\n# define the model\\\\nmodel = TorchModel().to(device)\\\\n\\\\n# define the optimizer and criterion for pre-training\\\\n\\\\noptimizer = SGD(model.parameters(), 1e-2)\\\\ncriterion = F.nll_loss\\\\n\\\\n# pre-train and evaluate the model on MNIST dataset\\\\nfor epoch in range(3):\\\\n    trainer(model, optimizer, criterion)\\\\n    evaluator(model)\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 965,
        "neg_line": [
            "-\"import torch\\nimport torch.nn.functional as F\\nfrom torch.optim import SGD\\n\\nfrom scripts.compression_mnist_model import TorchModel, trainer, evaluator, device, test_trt\\n\\n# define the model\\nmodel = TorchModel().to(device)\\n\\n# define the optimizer and criterion for pre-training\\n\\noptimizer = SGD(model.parameters(), 1e-2)\\ncriterion = F.nll_loss\\n\\n# pre-train and evaluate the model on MNIST dataset\\nfor epoch in range(3):\\n    trainer(model, optimizer, criterion)\\n    evaluator(model)\""
        ],
        "pos_line": [
            "+\"import torch\\nimport torch.nn.functional as F\\nfrom torch.optim import SGD\\n\\nfrom nni_assets.compression.mnist_model import TorchModel, trainer, evaluator, device, test_trt\\n\\n# define the model\\nmodel = TorchModel().to(device)\\n\\n# define the optimizer and criterion for pre-training\\n\\noptimizer = SGD(model.parameters(), 1e-2)\\ncriterion = F.nll_loss\\n\\n# pre-train and evaluate the model on MNIST dataset\\nfor epoch in range(3):\\n    trainer(model, optimizer, criterion)\\n    evaluator(model)\""
        ],
        "core_change": "-\"import torch\\nimport torch.nn.functional as F\\nfrom torch.optim import SGD\\n\\nfrom scripts.compression_mnist_model import TorchModel, trainer, evaluator, device, test_trt\\n\\n# define the model\\nmodel = TorchModel().to(device)\\n\\n# define the optimizer and criterion for pre-training\\n\\noptimizer = SGD(model.parameters(), 1e-2)\\ncriterion = F.nll_loss\\n\\n# pre-train and evaluate the model on MNIST dataset\\nfor epoch in range(3):\\n    trainer(model, optimizer, criterion)\\n    evaluator(model)\" +\"import torch\\nimport torch.nn.functional as F\\nfrom torch.optim import SGD\\n\\nfrom nni_assets.compression.mnist_model import TorchModel, trainer, evaluator, device, test_trt\\n\\n# define the model\\nmodel = TorchModel().to(device)\\n\\n# define the optimizer and criterion for pre-training\\n\\noptimizer = SGD(model.parameters(), 1e-2)\\ncriterion = F.nll_loss\\n\\n# pre-train and evaluate the model on MNIST dataset\\nfor epoch in range(3):\\n    trainer(model, optimizer, criterion)\\n    evaluator(model)\"",
        "core_API": "parameters"
    },
    {
        "commit_hash": "b5e2b183af5e40e33a4dc7659e697d137259d56e",
        "index": "9d28d4980..2fd354eba 100755",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Wav2Vec2ForMaskedLM(Wav2Vec2PreTrainedModel):",
            ">>> processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")",
            ">>> model = Wav2Vec2ForMaskedLM.from_pretrained(\"facebook/wav2vec2-base-960h\")",
            "",
            "+",
            ">>> def map_to_array(batch):",
            "-        >>>     speech, _ = sf.read(batch[\"file\"])",
            "-        >>>     batch[\"speech\"] = speech",
            "-        >>>     return batch",
            "+        ...     speech, _ = sf.read(batch[\"file\"])",
            "+        ...     batch[\"speech\"] = speech",
            "+        ...     return batch",
            "+",
            "",
            ">>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")",
            ">>> ds = ds.map(map_to_array)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('ellipsis', '...'), position=0, insert_id=1208562)",
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=4, insert_id=1208563)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ellipsis', '...'), position=0, insert_id=1208564)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'return'), position=1, insert_id=1208565)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=identifier, text=batch), position=0)",
            "Insert(target_node=ASTNode(type=subscript), node=('ellipsis', '...'), position=0, insert_id=1208566)",
            "Insert(target_node=ASTNode(type=subscript), node=('ERROR', None), position=1, insert_id=1208567)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=batch), position=0)",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 966,
        "neg_line": [
            "->>>     speech, _ = sf.read(batch[\"file\"])",
            "->>>     batch[\"speech\"] = speech",
            "->>>     return batch"
        ],
        "pos_line": [
            "+",
            "+...     speech, _ = sf.read(batch[\"file\"])",
            "+...     batch[\"speech\"] = speech",
            "+...     return batch",
            "+"
        ],
        "core_change": "+ ->>>     speech, _ = sf.read(batch[\"file\"]) ->>>     batch[\"speech\"] = speech ->>>     return batch +...     speech, _ = sf.read(batch[\"file\"]) +...     batch[\"speech\"] = speech +...     return batch +",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "abb269c3..c3331d18 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestScalarMix(AllenNlpTestCase):",
            "tensors = [torch.randn([3, 4, 5]) for _ in range(3)]",
            "numpy_mask = numpy.ones((3, 4), dtype=\"int32\")",
            "numpy_mask[1, 2:] = 0",
            "-        mask = torch.from_numpy(numpy_mask)",
            "+        mask = torch.from_numpy(numpy_mask).bool()",
            "",
            "weights = [0.1, 0.2, 0.3]",
            "for k in range(3):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=19760)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=19761)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19762)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=19763)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=19764)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=19765)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 967,
        "neg_line": [
            "-mask = torch.from_numpy(numpy_mask)"
        ],
        "pos_line": [
            "+mask = torch.from_numpy(numpy_mask).bool()"
        ],
        "core_change": "-mask = torch.from_numpy(numpy_mask) +mask = torch.from_numpy(numpy_mask).bool()",
        "core_API": "randn"
    },
    {
        "commit_hash": "f60989d325474ba7c89861dd0973d6ba73236b1a",
        "index": "0814786b..1a0727ed 100644",
        "commit_message": "fix bn and rewrite saverrestore with var.load\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_global_step_var():",
            "with tf.variable_scope(scope, reuse=False), \\",
            "tf.name_scope(None):",
            "var = tf.get_variable(GLOBAL_STEP_OP_NAME,",
            "-                                  initializer=0,",
            "-                                  trainable=False, dtype=tf.int32)",
            "+                                  initializer=tf.constant(0, dtype=tf.int64),",
            "+                                  trainable=False, dtype=tf.int64)",
            "return var"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=2305201)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2305202)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2305203)",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2305204)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2305205)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'constant'), position=2, insert_id=2305206)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2305207)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=0), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2305208)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2305209)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2305210)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2305211)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2305212)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2305213)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2305214)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2305215)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int64'), position=2, insert_id=2305216)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 968,
        "neg_line": [
            "-initializer=0,",
            "-trainable=False, dtype=tf.int32)"
        ],
        "pos_line": [
            "+initializer=tf.constant(0, dtype=tf.int64),",
            "+trainable=False, dtype=tf.int64)"
        ],
        "core_change": "-initializer=0, -trainable=False, dtype=tf.int32) +initializer=tf.constant(0, dtype=tf.int64), +trainable=False, dtype=tf.int64)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "d26b37e744ea980977e266adf48736451b73c583",
        "index": "8c1777553..f375e10e1 100644",
        "commit_message": "Speech2TextTransformer (#10175)\n\n* s2t\n\n* fix config\n\n* conversion script\n\n* fix import\n\n* add tokenizer\n\n* fix tok init\n\n* fix tokenizer\n\n* first version working\n\n* fix embeds\n\n* fix lm head\n\n* remove extra heads\n\n* fix convert script\n\n* handle encoder attn mask\n\n* style\n\n* better enc attn mask\n\n* override _prepare_attention_mask_for_generation\n\n* handle attn_maks in encoder and decoder\n\n* input_ids => input_features\n\n* enable use_cache\n\n* remove old code\n\n* expand embeddings if needed\n\n* remove logits bias\n\n* masked_lm_loss => loss\n\n* hack tokenizer to support feature processing\n\n* fix model_input_names\n\n* style\n\n* fix error message\n\n* doc\n\n* remove inputs_embeds\n\n* remove input_embeds\n\n* remove unnecessary docstring\n\n* quality\n\n* SpeechToText => Speech2Text\n\n* style\n\n* remove shared_embeds\n\n* subsample => conv\n\n* remove Speech2TextTransformerDecoderWrapper\n\n* update output_lengths formula\n\n* fix table\n\n* remove max_position_embeddings\n\n* update conversion scripts\n\n* add possibility to do upper case for now\n\n* add FeatureExtractor and Processor\n\n* add tests for extractor\n\n* require_torch_audio => require_torchaudio\n\n* add processor test\n\n* update import\n\n* remove classification head\n\n* attention mask is now 1D\n\n* update docstrings\n\n* attention mask should be of type long\n\n* handle attention mask from generate\n\n* alwyas return attention_mask\n\n* fix test\n\n* style\n\n* doc\n\n* Speech2TextTransformer => Speech2Text\n\n* Speech2TextTransformerConfig => Speech2TextConfig\n\n* remove dummy_inputs\n\n* nit\n\n* style\n\n* multilinguial tok\n\n* fix tokenizer\n\n* add tgt_lang setter\n\n* save lang_codes\n\n* fix tokenizer\n\n* add forced_bos_token_id to tokenizer\n\n* apply review suggestions\n\n* add torchaudio to extra deps\n\n* add speech deps to CI\n\n* fix dep\n\n* add libsndfile to ci\n\n* libsndfile1\n\n* add speech to extras all\n\n* libsndfile1 -> libsndfile1\n\n* libsndfile\n\n* libsndfile1-dev\n\n* apt update\n\n* add sudo to install\n\n* update deps table\n\n* install libsndfile1-dev on CI\n\n* tuple to list\n\n* init conv layer\n\n* add model tests\n\n* quality\n\n* add integration tests\n\n* skip_special_tokens\n\n* add speech_to_text_transformer in toctree\n\n* fix tokenizer\n\n* fix fp16 tests\n\n* add tokenizer tests\n\n* fix copyright\n\n* input_values => input_features\n\n* doc\n\n* add model in readme\n\n* doc\n\n* change checkpoint names\n\n* fix copyright\n\n* fix code example\n\n* add max_model_input_sizes in tokenizer\n\n* fix integration tests\n\n* add do_lower_case to tokenizer\n\n* remove clamp trick\n\n* fix \"Add modeling imports here\"\n\n* fix copyrights\n\n* fix tests\n\n* SpeechToTextTransformer => SpeechToText\n\n* fix naming\n\n* fix table formatting\n\n* fix typo\n\n* style\n\n* fix typos\n\n* remove speech dep from extras[testing]\n\n* fix copies\n\n* rename doc file,\n\n* put imports under is_torch_available\n\n* run feat extract tests when torch is available\n\n* dummy objects for processor and extractor\n\n* fix imports in tests\n\n* fix import in modeling test\n\n* fxi imports\n\n* fix torch import\n\n* fix imports again\n\n* fix positional embeddings\n\n* fix typo in import\n\n* adapt new extractor refactor\n\n* style\n\n* fix torchscript test\n\n* doc\n\n* doc\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix docs, copied from, style\n\n* fix docstring\n\n* handle imports\n\n* remove speech from all extra deps\n\n* remove s2t from seq2seq lm mapping\n\n* better names\n\n* skip training tests\n\n* add install instructions\n\n* List => Tuple\n\n* doc\n\n* fix conversion script\n\n* fix urls\n\n* add instruction for libsndfile\n\n* fix fp16 test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SequenceFeatureExtractionTestMixin(FeatureExtractionSavingTestMixin):",
            "input_np = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"np\")[input_name]",
            "input_tf = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"tf\")[input_name]",
            "",
            "-        self.assertTrue(abs(input_np.sum() - input_tf.numpy().sum()) < 1e-2)",
            "+        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)",
            "",
            "def test_attention_mask(self):",
            "feat_dict = self.feat_extract_dict"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=2371298)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2371299)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2371300)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2371301)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=input_np), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'astype'), position=2, insert_id=2371302)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2371303)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2371304)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2371305)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=2371306)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2371307)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2371308)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 969,
        "neg_line": [
            "-self.assertTrue(abs(input_np.sum() - input_tf.numpy().sum()) < 1e-2)"
        ],
        "pos_line": [
            "+self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)"
        ],
        "core_change": "-self.assertTrue(abs(input_np.sum() - input_tf.numpy().sum()) < 1e-2) +self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)",
        "core_API": "pad"
    },
    {
        "commit_hash": "726aba089d12503249d824bbaf4070f47d0fe44d",
        "index": "12ed1a1b..9dda30e3 100644",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScoreSdeVeScheduler(SchedulerMixin, ConfigMixin):",
            "\"\"\"",
            "sampling_eps = sampling_eps if sampling_eps is not None else self.config.sampling_eps",
            "",
            "-        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps)",
            "+        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps, device=device)",
            "",
            "def set_sigmas(",
            "self, num_inference_steps: int, sigma_min: float = None, sigma_max: float = None, sampling_eps: float = None"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=104103)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=104104)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=104105)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=104106)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=104107)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 971,
        "neg_line": [
            "-self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps)"
        ],
        "pos_line": [
            "+self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps, device=device)"
        ],
        "core_change": "-self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps) +self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps, device=device)",
        "core_API": "linspace"
    },
    {
        "commit_hash": "7db18ce0c28c0b7c93e9a61af55433f2d88d4213",
        "index": "9b582c2f..fe379b6c 100644",
        "commit_message": "fix mypy errors\n\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SSIM(nn.Module):",
            "ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\",
            "((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))",
            "",
            "-        loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.",
            "+        loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.",
            "",
            "if self.reduction == 'mean':",
            "loss = torch.mean(loss)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=470489)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=470490)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=470491)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=470492)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=470493)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=470494)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=470495)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=float, text=1.), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=470496)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 972,
        "neg_line": [
            "-loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2."
        ],
        "pos_line": [
            "+loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2."
        ],
        "core_change": "-loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2. +loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.",
        "core_API": "clamp"
    },
    {
        "commit_hash": "b244fd0aa1bf6cb65bcbe3ac274546472048928d",
        "index": "e0c9e120..ee23fa59 100644",
        "commit_message": "bugfix\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):",
            "placeholder = 1.",
            "label_loss = tf.nn.sigmoid_cross_entropy_with_logits(",
            "labels=tf.to_float(valid_anchor_labels), logits=valid_label_logits)",
            "-    label_loss = label_loss * (1. / config.RPN_BATCH_PER_IM)",
            "+    label_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)",
            "label_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name='label_loss')",
            "",
            "pos_anchor_boxes = tf.boolean_mask(anchor_boxes, pos_mask)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=2284529)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2284530)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2284531)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2284532)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2284533)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reduce_sum'), position=2, insert_id=2284534)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2284535)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=label_loss), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2284536)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 974,
        "neg_line": [
            "-label_loss = label_loss * (1. / config.RPN_BATCH_PER_IM)"
        ],
        "pos_line": [
            "+label_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)"
        ],
        "core_change": "-label_loss = label_loss * (1. / config.RPN_BATCH_PER_IM) +label_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)",
        "core_API": "sigmoid_cross_entropy_with_logits"
    },
    {
        "commit_hash": "5f22884cc97db1dde0f03530d47c844f22c13dcf",
        "index": "1cfd4c6e2..f59682966 100644",
        "commit_message": "bug fix about calculation precision\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_beamformer_net_wpe_output(ch, num_spk, use_dnn_mask_for_wpe):",
            "def test_beamformer_net_bf_output(num_spk):",
            "ch = 3",
            "inputs = torch.randn(2, 16, ch)",
            "+    inputs = inputs.float()",
            "ilens = torch.LongTensor([16, 12])",
            "model = BeamformerNet(",
            "n_fft=8,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=152004)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=152005)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'inputs'), position=0, insert_id=152006)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=152007)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=152008)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=152009)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=152010)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'inputs'), position=0, insert_id=152011)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=152012)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=152013)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=152014)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=152015)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 975,
        "neg_line": [],
        "pos_line": [
            "+inputs = inputs.float()"
        ],
        "core_change": "+inputs = inputs.float()",
        "core_API": "randn"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "cd99d002..a139fbcc 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "remove API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from allennlp.common.testing import AllenNlpTestCase",
            "",
            "class TestElmoLstmCell(AllenNlpTestCase):",
            "def test_elmo_lstm(self):",
            "-        input_tensor = Variable(torch.rand(4, 5, 3))",
            "+        input_tensor = torch.rand(4, 5, 3)",
            "input_tensor[1, 4:, :] = 0.",
            "input_tensor[2, 2:, :] = 0.",
            "input_tensor[3, 1:, :] = 0.",
            "-        mask = Variable(torch.ones([4, 5]))",
            "+        mask = torch.ones([4, 5])",
            "mask[1, 4:] = 0.",
            "mask[2, 2:] = 0.",
            "mask[3, 1:] = 0."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 976,
        "neg_line": [
            "-input_tensor = Variable(torch.rand(4, 5, 3))",
            "-mask = Variable(torch.ones([4, 5]))"
        ],
        "pos_line": [
            "+input_tensor = torch.rand(4, 5, 3)",
            "+mask = torch.ones([4, 5])"
        ],
        "core_change": "-input_tensor = Variable(torch.rand(4, 5, 3)) +input_tensor = torch.rand(4, 5, 3) -mask = Variable(torch.ones([4, 5])) +mask = torch.ones([4, 5])",
        "core_API": "rand"
    },
    {
        "commit_hash": "a120c621ee694a7583bdd270e9e755d08fa219c6",
        "index": "8bc6d305..473151a8 100644",
        "commit_message": "Remove object metadata when saving SavedModel.\n\nThis change also fixes a few bugs when loading the metadata file, and fixes Keras tests so that they use model.save instead of tf.saved_model.save\n\nPiperOrigin-RevId: 378963258\n\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TpuStrategyTest(tf.test.TestCase):",
            "serving_fn = create_serving_signature(model)",
            "",
            "saved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())",
            "-      tf.saved_model.save(",
            "-          model, saved_model_dir, signatures={\"serving_default\": serving_fn})",
            "+      model.save(saved_model_dir, save_format=\"tf\",",
            "+                 signatures={\"serving_default\": serving_fn})",
            "",
            "# Test the saved_model.",
            "loaded_serving_fn = tf.keras.models.load_model("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='model')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=model), value='saved_model_dir')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2090793)",
            "Update(target_node=ASTNode(type=identifier, text=saved_model_dir), value='save_format')",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=saved_model_dir), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2090794)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"tf\"'), position=2, insert_id=2090795)",
            "Delete(target_node=ASTNode(type=identifier, text=saved_model))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 977,
        "neg_line": [
            "-tf.saved_model.save(",
            "-model, saved_model_dir, signatures={\"serving_default\": serving_fn})"
        ],
        "pos_line": [
            "+model.save(saved_model_dir, save_format=\"tf\",",
            "+signatures={\"serving_default\": serving_fn})"
        ],
        "core_change": "-tf.saved_model.save( -model, saved_model_dir, signatures={\"serving_default\": serving_fn}) +model.save(saved_model_dir, save_format=\"tf\", +signatures={\"serving_default\": serving_fn})",
        "core_API": "mkdtemp"
    },
    {
        "commit_hash": "3ef2c43bbff12bdb0a26630e64394dc0617d0b7d",
        "index": "72ce3b7fe..8f0fdb0d5 100644",
        "commit_message": "fix errors in test/espnet2/enh and wsj0_2mix_spatialized_data_prep\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BeamformerNet(torch.nn.Module):",
            "def forward_rawwav(self, input: torch.Tensor, ilens: torch.Tensor):",
            "\"\"\"",
            "Args:",
            "-            input (torch.Tensor): mixed speech [Batch, sample]",
            "+            input (torch.Tensor): mixed speech [Batch, Nsample, Channel]",
            "ilens (torch.Tensor): input lengths [Batch]",
            "",
            "Returns:",
            "predcited speech wavs (single-channel):",
            "-                torch.Tensor(Batch, sample), or List[torch.Tensor(Batch, sample)]",
            "+                torch.Tensor(Batch, Nsamples), or List[torch.Tensor(Batch, Nsamples)]",
            "output lengths",
            "predcited masks: OrderedDict[",
            "'dereverb': torch.Tensor(Batch, Frames, Channel, Freq),"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sample), value='Nsample')",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=6, insert_id=153255)",
            "Insert(target_node=ASTNode(type=subscript), node=('identifier', 'Channel'), position=7, insert_id=153256)",
            "Update(target_node=ASTNode(type=identifier, text=sample), value='Nsamples')",
            "Update(target_node=ASTNode(type=identifier, text=sample), value='Nsamples')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 979,
        "neg_line": [
            "-input (torch.Tensor): mixed speech [Batch, sample]",
            "-torch.Tensor(Batch, sample), or List[torch.Tensor(Batch, sample)]"
        ],
        "pos_line": [
            "+input (torch.Tensor): mixed speech [Batch, Nsample, Channel]",
            "+torch.Tensor(Batch, Nsamples), or List[torch.Tensor(Batch, Nsamples)]"
        ],
        "core_change": "-input (torch.Tensor): mixed speech [Batch, sample] +input (torch.Tensor): mixed speech [Batch, Nsample, Channel] -torch.Tensor(Batch, sample), or List[torch.Tensor(Batch, sample)] +torch.Tensor(Batch, Nsamples), or List[torch.Tensor(Batch, Nsamples)]",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "7fe73f6395bf5fb51e380d02303573a15e0e5293",
        "index": "1fb218644..9af85ed3c 100644",
        "commit_message": "remove unused import + fix LM zero_state\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "change param for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequentialRNNLM(AbsLM):",
            "c = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)",
            "state = h, c",
            "else:",
            "-            state = torch.zeros((nlayers, nhid), dtype=torch.float)",
            "+            state = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)",
            "",
            "return state"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=('attribute', None), position=1, insert_id=146638)",
            "Insert(target_node=ASTNode(type=tuple), node=('attribute', None), position=4, insert_id=146639)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=146640)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=146641)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=nlayers), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=146642)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=146643)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=nhid), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 980,
        "neg_line": [
            "-state = torch.zeros((nlayers, nhid), dtype=torch.float)"
        ],
        "pos_line": [
            "+state = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)"
        ],
        "core_change": "-state = torch.zeros((nlayers, nhid), dtype=torch.float) +state = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "a0ffd6f464913825ec6cf2c4f141aba4ef2294f0",
        "index": "a5956efd..834d19d7 100644",
        "commit_message": "Add TorchScript support for `Node2Vec` (#6726)\n\nThe original jit test of `Node2Vec` model is not really using\nTorchScript, just fix this.\n\n---------\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "change API feature change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_node2vec():",
            "assert 0 <= acc and acc <= 1",
            "",
            "if is_full_test():",
            "-        jit = torch.jit.export(model)",
            "+        jit = torch.jit.script(model)",
            "",
            "assert jit(torch.arange(3)).size() == (3, 16)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=export), value='script')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 981,
        "neg_line": [
            "-jit = torch.jit.export(model)"
        ],
        "pos_line": [
            "+jit = torch.jit.script(model)"
        ],
        "core_change": "-jit = torch.jit.export(model) +jit = torch.jit.script(model)",
        "core_API": "export"
    },
    {
        "commit_hash": "7a195b64ea02e02698083b0b795287ffa6e72cf8",
        "index": "21d15963..0eec41f1 100644",
        "commit_message": "Convolutional layer supports float64 dtype after tensorflow 1.8.0 (#10977)\n\n* Support float64 dtype after tensorflow 1.8.0\n\n* Fix explanation message.\n\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "add condition check for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _preprocess_conv3d_input(x, data_format):",
            "# Returns",
            "A tensor.",
            "\"\"\"",
            "-    if dtype(x) == 'float64':",
            "+    # tensorflow doesn't support float64 for conv layer before 1.8.0",
            "+    if (dtype(x) == 'float64'",
            "+            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):",
            "x = tf.cast(x, 'float32')",
            "tf_data_format = 'NDHWC'",
            "if data_format == 'channels_first':"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('parenthesized_expression', None), position=1, insert_id=2111908)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=2111909)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=2111910)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=2111911)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2111912)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=2111913)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=2111914)",
            "Insert(target_node=IN(type=comparison_operator), node=('<', '<'), position=1, insert_id=2111915)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=2, insert_id=2111916)",
            "Insert(target_node=IN(type=call), node=('identifier', 'StrictVersion'), position=0, insert_id=2111917)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2111918)",
            "Insert(target_node=IN(type=call), node=('identifier', 'StrictVersion'), position=0, insert_id=2111919)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2111920)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2111921)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2111922)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2111923)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2111924)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'1.8.0'\"), position=1, insert_id=2111925)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2111926)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2111927)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2111928)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__version__'), position=2, insert_id=2111929)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 984,
        "neg_line": [
            "-if dtype(x) == 'float64':"
        ],
        "pos_line": [
            "+# tensorflow doesn't support float64 for conv layer before 1.8.0",
            "+if (dtype(x) == 'float64'",
            "+and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):"
        ],
        "core_change": "-if dtype(x) == 'float64': +# tensorflow doesn't support float64 for conv layer before 1.8.0 +if (dtype(x) == 'float64' +and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):",
        "core_API": "cast"
    },
    {
        "commit_hash": "7ea781c9a030fb2d49574d8d324e562e58dd49fb",
        "index": "6e529081d..8c4255378 100644",
        "commit_message": "CI fix3\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TextEncoder(torch.nn.Module):",
            "",
            "# define modules",
            "self.emb = torch.nn.Embedding(vocabs, attention_dim)",
            "-        torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim**-0.5)",
            "+        torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim ** -0.5)",
            "self.encoder = Encoder(",
            "idim=-1,",
            "input_layer=None,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 985,
        "neg_line": [
            "-torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim**-0.5)"
        ],
        "pos_line": [
            "+torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim ** -0.5)"
        ],
        "core_change": "-torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim**-0.5) +torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim ** -0.5)",
        "core_API": "Embedding"
    },
    {
        "commit_hash": "23a537e2ad1ee9af7e8016054208d5ce1cc572fd",
        "index": "8c4255378..6e529081d 100644",
        "commit_message": "black fix\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TextEncoder(torch.nn.Module):",
            "",
            "# define modules",
            "self.emb = torch.nn.Embedding(vocabs, attention_dim)",
            "-        torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim ** -0.5)",
            "+        torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim**-0.5)",
            "self.encoder = Encoder(",
            "idim=-1,",
            "input_layer=None,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 986,
        "neg_line": [
            "-torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim ** -0.5)"
        ],
        "pos_line": [
            "+torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim**-0.5)"
        ],
        "core_change": "-torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim ** -0.5) +torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim**-0.5)",
        "core_API": "Embedding"
    },
    {
        "commit_hash": "eb66e0d01ecded59160ab8d5c0e8e302eb1836a1",
        "index": "a52857c7..9fed5ac2 100644",
        "commit_message": "add pat change (#3414)\n\n* add pat change\n\n* fix grid roi head\n\n* fix comments\n\n* clean\n\n* revert change\n",
        "file": "mmdetection.txt.json",
        "label": "yes",
        "comments": "add condition check for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SingleRoIExtractor(BaseRoIExtractor):",
            "num_levels = len(feats)",
            "roi_feats = feats[0].new_zeros(",
            "rois.size(0), self.out_channels, *out_size)",
            "+        # TODO: remove this when parrots supports",
            "+        if torch.__version__ == 'parrots':",
            "+            roi_feats.requires_grad = True",
            "",
            "if num_levels == 1:",
            "if len(rois) == 0:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=636167)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=636168)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=636169)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=636170)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=636171)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=636172)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=636173)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', \"'parrots'\"), position=2, insert_id=636174)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=636175)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=636176)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=636177)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__version__'), position=2, insert_id=636178)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=636179)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=636180)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=636181)",
            "Insert(target_node=IN(type=assignment), node=('true', 'True'), position=2, insert_id=636182)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'roi_feats'), position=0, insert_id=636183)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=636184)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'requires_grad'), position=2, insert_id=636185)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 987,
        "neg_line": [],
        "pos_line": [
            "+# TODO: remove this when parrots supports",
            "+if torch.__version__ == 'parrots':",
            "+roi_feats.requires_grad = True"
        ],
        "core_change": "+# TODO: remove this when parrots supports +if torch.__version__ == 'parrots': +roi_feats.requires_grad = True",
        "core_API": "size"
    },
    {
        "commit_hash": "a66f2baeb782e091dde4e1e6394e46f169e5ba58",
        "index": "2c2bff97..0ae8f992 100644",
        "commit_message": "Dreambooth: reduce VRAM usage (#2039)\n\n* Dreambooth: use `optimizer.zero_grad(set_to_none=True)` to reduce VRAM usage\n\n* Allow the user to control `optimizer.zero_grad(set_to_none=True)` with --set_grads_to_none\n\n* Update Dreambooth readme\n\n* Fix link in readme\n\n* Fix header size in readme\n",
        "file": "diffusers.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args):",
            "accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)",
            "optimizer.step()",
            "lr_scheduler.step()",
            "-                optimizer.zero_grad()",
            "+                optimizer.zero_grad(set_to_none=args.set_grads_to_none)",
            "",
            "# Checks if the accelerator has performed an optimization step behind the scenes",
            "if accelerator.sync_gradients:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=90133)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'set_to_none'), position=0, insert_id=90134)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=90135)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=90136)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=90137)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=90138)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_grads_to_none'), position=2, insert_id=90139)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 988,
        "neg_line": [
            "-optimizer.zero_grad()"
        ],
        "pos_line": [
            "+optimizer.zero_grad(set_to_none=args.set_grads_to_none)"
        ],
        "core_change": "-optimizer.zero_grad() +optimizer.zero_grad(set_to_none=args.set_grads_to_none)",
        "core_API": "clip_grad_norm_"
    },
    {
        "commit_hash": "a2900a39bd7dd5682268582c9cba39e4f2a43fbc",
        "index": "7035fdc3..908c8bf8 100644",
        "commit_message": "Fix dropout by temporarily replacing with nn.Dropout\n\n",
        "file": "stanza.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Tagger(nn.Module):",
            "# criterion",
            "self.crit = nn.CrossEntropyLoss(ignore_index=0) # ignore padding",
            "",
            "-        self.drop = Dropout(args['dropout'])",
            "+        self.drop = nn.Dropout(args['dropout'])",
            "self.worddrop = WordDropout(args['word_dropout'])",
            "",
            "def forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1517131)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1517132)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1517133)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Dropout), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 989,
        "neg_line": [
            "-self.drop = Dropout(args['dropout'])"
        ],
        "pos_line": [
            "+self.drop = nn.Dropout(args['dropout'])"
        ],
        "core_change": "-self.drop = Dropout(args['dropout']) +self.drop = nn.Dropout(args['dropout'])",
        "core_API": "CrossEntropyLoss"
    },
    {
        "commit_hash": "ac315b7a4a20f72bceedf9a43e9b673c83dd6bc1",
        "index": "628ba4a3..752eac8a 100644",
        "commit_message": "fixed scatter_sum call\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "feature change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)",
            "def train():",
            "model.train()",
            "optimizer.zero_grad()",
            "-    pos_z, neg_z, summary = model(data.x, data.edge_index, data.edge_attr)",
            "-    loss = model.loss(pos_z, neg_z, summary)",
            "+    y = model(data.x, data.edge_index, data.edge_attr)",
            "+    loss = torch.sum(y) #TODO: actual loss function",
            "loss.backward()",
            "optimizer.step()",
            "return loss.item()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=pos_z), value='y')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=pos_z), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=model), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=loss), value='sum')",
            "Update(target_node=ASTNode(type=identifier, text=pos_z), value='y')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=neg_z))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=summary))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=neg_z))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=summary))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 990,
        "neg_line": [
            "-pos_z, neg_z, summary = model(data.x, data.edge_index, data.edge_attr)",
            "-loss = model.loss(pos_z, neg_z, summary)"
        ],
        "pos_line": [
            "+y = model(data.x, data.edge_index, data.edge_attr)",
            "+loss = torch.sum(y) #TODO: actual loss function"
        ],
        "core_change": "-pos_z, neg_z, summary = model(data.x, data.edge_index, data.edge_attr) -loss = model.loss(pos_z, neg_z, summary) +y = model(data.x, data.edge_index, data.edge_attr) +loss = torch.sum(y) #TODO: actual loss function",
        "core_API": "Adam"
    },
    {
        "commit_hash": "3b91f96fc94e9e201a2c637b2f654cbdc6a21ee1",
        "index": "04cf80b9f..8ba049869 100755",
        "commit_message": "Fix torch meshgrid warnings (#20475)\n\n* fix torch meshgrid warnings\n\n* support lower torch versions\n\n* don't edit examples\n\n* dont edit examples\n\n* fix ci\n\n* fix style\n\n* rebase cleanup\n\n* fix ci again\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def build_linear_positions(index_dims, output_range=(-1.0, 1.0)):",
            "return torch.linspace(start=output_range[0], end=output_range[1], steps=n_xels_per_dim, dtype=torch.float32)",
            "",
            "dim_ranges = [_linspace(n_xels_per_dim) for n_xels_per_dim in index_dims]",
            "-    array_index_grid = torch.meshgrid(*dim_ranges)",
            "+    array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")",
            "",
            "return torch.stack(array_index_grid, dim=-1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=meshgrid), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1182485)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1182486)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'indexing'), position=0, insert_id=1182487)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1182488)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"ij\"'), position=2, insert_id=1182489)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 991,
        "neg_line": [
            "-array_index_grid = torch.meshgrid(*dim_ranges)"
        ],
        "pos_line": [
            "+array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")"
        ],
        "core_change": "-array_index_grid = torch.meshgrid(*dim_ranges) +array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")",
        "core_API": "linspace"
    },
    {
        "commit_hash": "5b57f61ba47f8b11d19a5b46e7fb5a52458abae5",
        "index": "1d5e3a32..3036e48a 100644",
        "commit_message": "fix pin_memory with different latent sampling method\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "no",
        "comments": "remove print",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train_embedding(embedding_name, learn_rate, batch_size, gradient_step, data_",
            "# go back until we reach gradient accumulation steps",
            "if (j + 1) % gradient_step != 0:",
            "continue",
            "-                #print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\")",
            "-                #scaler.unscale_(optimizer)",
            "-                #print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\")",
            "-                #torch.nn.utils.clip_grad_norm_(embedding.vec, max_norm=1.0)",
            "-                #print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\")",
            "scaler.step(optimizer)",
            "scaler.update()",
            "embedding.step += 1"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=13, insert_id=1137411)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=binary_operator), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=!=, text=!=), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=integer, text=0), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=continue, text=continue), position=4)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=5)",
            "Insert(target_node=IN(type=ERROR), node=('call', None), position=6, insert_id=1137412)",
            "Insert(target_node=IN(type=ERROR), node=('attribute', None), position=7, insert_id=1137413)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=+=, text=+=), position=8)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1137414)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=embedding), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=step), position=2)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=scaler), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=update), position=2)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 0,
        "minus_line": 5,
        "AST_diff_line": 24,
        "number": 994,
        "neg_line": [
            "-#print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\")",
            "-#scaler.unscale_(optimizer)",
            "-#print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\")",
            "-#torch.nn.utils.clip_grad_norm_(embedding.vec, max_norm=1.0)",
            "-#print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\")"
        ],
        "pos_line": [],
        "core_change": "-#print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\") -#scaler.unscale_(optimizer) -#print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\") -#torch.nn.utils.clip_grad_norm_(embedding.vec, max_norm=1.0) -#print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\")",
        "core_API": "detach"
    },
    {
        "commit_hash": "5ea2eab7090c2f1c0449d1ab93c0fba318f61628",
        "index": "18b04c49..e7e13d62 100644",
        "commit_message": "implement regularization (#347)\n\n* make regularization happen\n\n* have model.from_params call initialize\n\n* better defaults\n\n* fix docs\n\n* add test for regularization\n\n* remove initializer from model subclass constructors\n\n* fix comment\n\n* add missing test fixture\n\n* address PR feedback\n\n* add regularization tests\n\n* use InitializerApplicator() instead of None\n\n* update docstrings\n\n* update doscstrings more\n\n* change variable return type to float\n\n* fix docstring that was breaking sphinx\n\n* remove regularizer from simple_tagger\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "class Model(torch.nn.Module, Registrable):",
            "@classmethod",
            "def from_params(cls, vocab: Vocabulary, params: Params) -> 'Model':",
            "choice = params.pop_choice(\"type\", cls.list_available())",
            "-        return cls.by_name(choice).from_params(vocab, params)",
            "+        model = cls.by_name(choice).from_params(vocab, params)",
            "+        return model",
            "",
            "@classmethod",
            "def load(cls,"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1573627)",
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=4, insert_id=1573628)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1573629)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1573630)",
            "Insert(target_node=IN(type=return_statement), node=('identifier', 'model'), position=1, insert_id=1573631)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'model'), position=0, insert_id=1573632)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1573633)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 995,
        "neg_line": [
            "-return cls.by_name(choice).from_params(vocab, params)"
        ],
        "pos_line": [
            "+model = cls.by_name(choice).from_params(vocab, params)",
            "+return model"
        ],
        "core_change": "-return cls.by_name(choice).from_params(vocab, params) +model = cls.by_name(choice).from_params(vocab, params) +return model",
        "core_API": "pop_choice"
    },
    {
        "commit_hash": "c317aaa3de289fc0a69e6b1001ffc586bb4d9c9a",
        "index": "be158d16..ecc3c76a 100644",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestInverseWithMask:",
            "assert_close(y, y_expected)",
            "assert torch.equal(mask, torch.ones_like(mask))",
            "",
            "-    @pytest.mark.skipif((int(torch.__version__.split('.')[0]) == 1) and (int(torch.__version__.split('.')[1]) < 9),",
            "-                        reason='<1.9.0 not supporting')",
            "+    @pytest.mark.skipif(",
            "+        (int(torch.__version__.split('.')[0]) == 1) and (int(torch.__version__.split('.')[1]) < 9),",
            "+        reason='<1.9.0 not supporting',",
            "+    )",
            "def test_all_bad(self, device, dtype):",
            "A = torch.ones(10, 3, 3, device=device, dtype=dtype)",
            "X, mask = safe_inverse_with_mask(A)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=413322)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 999,
        "neg_line": [
            "-@pytest.mark.skipif((int(torch.__version__.split('.')[0]) == 1) and (int(torch.__version__.split('.')[1]) < 9),",
            "-reason='<1.9.0 not supporting')"
        ],
        "pos_line": [
            "+@pytest.mark.skipif(",
            "+(int(torch.__version__.split('.')[0]) == 1) and (int(torch.__version__.split('.')[1]) < 9),",
            "+reason='<1.9.0 not supporting',",
            "+)"
        ],
        "core_change": "-@pytest.mark.skipif((int(torch.__version__.split('.')[0]) == 1) and (int(torch.__version__.split('.')[1]) < 9), -reason='<1.9.0 not supporting') +@pytest.mark.skipif( +(int(torch.__version__.split('.')[0]) == 1) and (int(torch.__version__.split('.')[1]) < 9), +reason='<1.9.0 not supporting', +)",
        "core_API": "equal"
    },
    {
        "commit_hash": "3d878f2765bf7742a234e9d47c7e5fb31d1d6148",
        "index": "9767bdb..bbd3477 100644",
        "commit_message": "Readability and documentation (#67)\n\n* Readability and documentation\n\n* Add unit tests and other fixes\n\n* Typo\n\n",
        "file": "tensorflow-wavenet.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def batch_to_time(value, dilation, name=None):",
            "return tf.reshape(transposed, [tf.div(shape[0], dilation), -1, shape[2]])",
            "",
            "",
            "-def causal_conv(value, filter_, dilation, name=None):",
            "-    with tf.name_scope('causal_conv'):",
            "+def causal_conv(value, filter_, dilation, name='causal_conv'):",
            "+    with tf.name_scope(name):",
            "# Pad beforehand to preserve causality",
            "filter_width = tf.shape(filter_)[0]",
            "padded = tf.pad(value, [[0, 0], [(filter_width - 1) * dilation, 0], [0, 0]])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=default_parameter), node=('string', \"'causal_conv'\"), position=2, insert_id=2208515)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'name'), position=1, insert_id=2208516)",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=string, text='causal_conv'))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1000,
        "neg_line": [
            "-def causal_conv(value, filter_, dilation, name=None):",
            "-with tf.name_scope('causal_conv'):"
        ],
        "pos_line": [
            "+def causal_conv(value, filter_, dilation, name='causal_conv'):",
            "+with tf.name_scope(name):"
        ],
        "core_change": "-def causal_conv(value, filter_, dilation, name=None): -with tf.name_scope('causal_conv'): +def causal_conv(value, filter_, dilation, name='causal_conv'): +with tf.name_scope(name):",
        "core_API": "reshape"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "76fe45100..948ce6fe5 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ≈†a≈°ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "no",
        "comments": "string fix",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Matinf(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('matinf', data_dir=...)` that includes files unzipped from the MATINF zip. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('matinf', data_dir=...)` that includes files unzipped from the MATINF zip. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'matinf\\', data_dir=...)` that includes files unzipped from the MATINF zip. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781610)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('matinf', data_dir=...)` that includes files unzipped from the MATINF zip. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=data_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 1001,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('matinf', data_dir=...)` that includes files unzipped from the MATINF zip. Manual download instructions: {}\".format(",
            "-data_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('matinf', data_dir=...)` that includes files unzipped from the MATINF zip. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('matinf', data_dir=...)` that includes files unzipped from the MATINF zip. Manual download instructions: {}\".format( -data_dir, self.manual_download_instructions -) +f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('matinf', data_dir=...)` that includes files unzipped from the MATINF zip. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "7ddf0769a1c2bb02af69cc5a4b6db38d9525c85b",
        "index": "129a8fa9..bfaf6ba4 100755",
        "commit_message": "Quick fix\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def SubpixelConv2d(net, scale=2, n_out_channel=None, act=tf.identity, name='subp",
            "bsize = tf.shape(X)[0] # Handling Dimension(None) type for undefined batch dim",
            "Xs=tf.split(X,r,3) #b*h*w*r*r",
            "Xr=tf.concat(Xs,2) #b*h*(r*w)*r",
            "-            X=tf.reshape(Xr,(b,r*a,r*b,c)) # b*(r*h)*(r*w)*c",
            "+            X=tf.reshape(Xr,(bsize,r*a,r*b,c)) # b*(r*h)*(r*w)*c",
            "else:",
            "print(_err_log)",
            "return X"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=b), value='bsize')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1003,
        "neg_line": [
            "-X=tf.reshape(Xr,(b,r*a,r*b,c)) # b*(r*h)*(r*w)*c"
        ],
        "pos_line": [
            "+X=tf.reshape(Xr,(bsize,r*a,r*b,c)) # b*(r*h)*(r*w)*c"
        ],
        "core_change": "-X=tf.reshape(Xr,(b,r*a,r*b,c)) # b*(r*h)*(r*w)*c +X=tf.reshape(Xr,(bsize,r*a,r*b,c)) # b*(r*h)*(r*w)*c",
        "core_API": "shape"
    },
    {
        "commit_hash": "f7833fd3877bf3fe55eceb3bc581d84811384ae9",
        "index": "bbc1ae6a98..55ad8f0968 100644",
        "commit_message": "Implicit out (#2158)\n\n* add missing Nones\n\n* fix linting in backend tf set\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def unique_inverse(",
            "",
            "",
            "def unique_values(",
            "-    x: Union[tf.Tensor, tf.Variable], *, out: Optional[Union[tf.Tensor, tf.Variable]]",
            "+    x: Union[tf.Tensor, tf.Variable],",
            "+    *,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "ret = tf.unique(tf.reshape(x, [-1]))[0]",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=6, insert_id=2005487)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=out), position=0)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=type), position=2)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=2005488)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('none', 'None'), position=4, insert_id=2005489)",
            "Delete(target_node=ASTNode(type=typed_parameter))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1006,
        "neg_line": [
            "-x: Union[tf.Tensor, tf.Variable], *, out: Optional[Union[tf.Tensor, tf.Variable]]"
        ],
        "pos_line": [
            "+x: Union[tf.Tensor, tf.Variable],",
            "+*,",
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None"
        ],
        "core_change": "-x: Union[tf.Tensor, tf.Variable], *, out: Optional[Union[tf.Tensor, tf.Variable]] +x: Union[tf.Tensor, tf.Variable], +*, +out: Optional[Union[tf.Tensor, tf.Variable]] = None",
        "core_API": "unique"
    },
    {
        "commit_hash": "6ed9882ddb2b6249463c855dcca6860161d91f3e",
        "index": "77ef0386e..1fbee0999 100644",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class GPT2Attention(nn.Module):",
            "# Apply the attention mask",
            "attn_weights = attn_weights + attention_mask",
            "",
            "-        attn_weights = nn.Softmax(dim=-1)(attn_weights)",
            "+        attn_weights = nn.functional.softmax(attn_weights, dim=-1)",
            "",
            "# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise",
            "if attn_weights.dtype != torch.float32:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1536512)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1536513)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'softmax'), position=2, insert_id=1536514)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'attn_weights'), position=1, insert_id=1536515)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1536516)",
            "Update(target_node=ASTNode(type=identifier, text=Softmax), value='functional')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=attn_weights))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 1007,
        "neg_line": [
            "-attn_weights = nn.Softmax(dim=-1)(attn_weights)"
        ],
        "pos_line": [
            "+attn_weights = nn.functional.softmax(attn_weights, dim=-1)"
        ],
        "core_change": "-attn_weights = nn.Softmax(dim=-1)(attn_weights) +attn_weights = nn.functional.softmax(attn_weights, dim=-1)",
        "core_API": "Softmax"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "c3edc94b..ac83acbe 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BidirectionalEndpointSpanExtractor(SpanExtractor):",
            "sequence_lengths = util.get_lengths_from_binary_sequence_mask(sequence_mask)",
            "else:",
            "# shape (batch_size), filled with the sequence length size of the sequence_tensor.",
            "-            sequence_lengths = util.ones_like(sequence_tensor[:, 0, 0]).long() * sequence_tensor.size(1)",
            "+            sequence_lengths = (torch.ones_like(sequence_tensor[:, 0, 0], dtype=torch.long) *",
            "+                                sequence_tensor.size(1))",
            "",
            "# shape (batch_size, num_spans, 1)",
            "end_sentinel_mask = (exclusive_span_ends == sequence_lengths.unsqueeze(-1)).long().unsqueeze(-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=4, insert_id=36840)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=36841)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=36842)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=util), value='torch')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=36843)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=36844)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=36845)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=36846)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=36847)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=36848)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=long), position=2)",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 1008,
        "neg_line": [
            "-sequence_lengths = util.ones_like(sequence_tensor[:, 0, 0]).long() * sequence_tensor.size(1)"
        ],
        "pos_line": [
            "+sequence_lengths = (torch.ones_like(sequence_tensor[:, 0, 0], dtype=torch.long) *",
            "+sequence_tensor.size(1))"
        ],
        "core_change": "-sequence_lengths = util.ones_like(sequence_tensor[:, 0, 0]).long() * sequence_tensor.size(1) +sequence_lengths = (torch.ones_like(sequence_tensor[:, 0, 0], dtype=torch.long) * +sequence_tensor.size(1))",
        "core_API": "get_lengths_from_binary_sequence_mask"
    },
    {
        "commit_hash": "a9acc3ab0d9b2141b6f9e982670c9b9294ff86ee",
        "index": "3864643c0..592d4c98d 100644",
        "commit_message": "Fixed issue with build_proto and isort\n\n- Added torchvision>=0.5 to requirements\n- Removed unused imports in .proto files\n\n",
        "file": "PySyft.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "_SHARETENSOR = _descriptor.Descriptor(",
            "syntax=\"proto3\",",
            "extension_ranges=[],",
            "oneofs=[],",
            "-    serialized_start=154,",
            "-    serialized_end=257,",
            "+    serialized_start=115,",
            "+    serialized_end=218,",
            ")",
            "",
            "_SHARETENSOR.fields_by_name["
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=154), value='115')",
            "Update(target_node=ASTNode(type=integer, text=257), value='218')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 1009,
        "neg_line": [
            "-serialized_start=154,",
            "-serialized_end=257,"
        ],
        "pos_line": [
            "+serialized_start=115,",
            "+serialized_end=218,"
        ],
        "core_change": "-serialized_start=154, -serialized_end=257, +serialized_start=115, +serialized_end=218,",
        "core_API": "Descriptor"
    },
    {
        "commit_hash": "9ee81e054efef3c80778ba44b2f571161da48fff",
        "index": "a7bc29d..c4b7eb1 100644",
        "commit_message": "Fix layer combination bug\n\nThe raw outputs of each layer need to be combined at the end, not the\nskipped inputs for the next layer.\nThanks to @keskival for pointing this out!\n\n",
        "file": "tensorflow-wavenet.txt.json",
        "label": "no",
        "comments": "doc",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class WaveNet(object):",
            "tf.histogram_summary('postprocess2_weights', w2)",
            "",
            "# We skip connections from the outputs of each layer, adding them all up here",
            "-            # We perform pairwise addition instead of using tf.add_n, so TensorFlow can free",
            "-            # the memory of previous layers",
            "total = outputs[0]",
            "for out in outputs[1:]:",
            "total += out"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1010,
        "neg_line": [
            "-# We perform pairwise addition instead of using tf.add_n, so TensorFlow can free",
            "-# the memory of previous layers"
        ],
        "pos_line": [],
        "core_change": "-# We perform pairwise addition instead of using tf.add_n, so TensorFlow can free -# the memory of previous layers",
        "core_API": "histogram_summary"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "e04118681b..6c2f380225 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "update API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ARSTFPolicy:",
            "self.num_params = sum(",
            "np.prod(variable.shape.as_list())",
            "for _, variable in self.variables.variables.items())",
            "-        self.sess.run(tf.global_variables_initializer())",
            "+        self.sess.run(tf1.global_variables_initializer())",
            "",
            "def compute_actions(self,",
            "observation,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1013,
        "neg_line": [
            "-self.sess.run(tf.global_variables_initializer())"
        ],
        "pos_line": [
            "+self.sess.run(tf1.global_variables_initializer())"
        ],
        "core_change": "-self.sess.run(tf.global_variables_initializer()) +self.sess.run(tf1.global_variables_initializer())",
        "core_API": "prod"
    },
    {
        "commit_hash": "cd51772bbe4eec0addd4e149b2f47f999020e7a0",
        "index": "75969ad9..e4631f0d 100644",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestOpening:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            opening(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-4, rtol=1e-4",
            "+            opening(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-4,",
            "+            rtol=1e-4,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=423228)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 1014,
        "neg_line": [
            "-opening(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-atol=1e-4, rtol=1e-4"
        ],
        "pos_line": [
            "+opening(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+expected,",
            "+atol=1e-4,",
            "+rtol=1e-4,"
        ],
        "core_change": "-opening(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected, -atol=1e-4, rtol=1e-4 +opening(tensor, torch.ones_like(structural_element), structuring_element=structural_element), +expected, +atol=1e-4, +rtol=1e-4,",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "d49f8d31893534ace807e756567fab18ff516490",
        "index": "266ee2f53..f57f914d1 100644",
        "commit_message": "Added type hints for Pytorch Marian calls (#16200)\n\n* Added type hinting for forward functions in pytorch marian\n\n* typo correction\n\n* Removed type hints on functions from BART per Suraj Patil request\n\n* fix import pb\n\n* fix typo\n\n* corrected tuple call\n\n* ran black\n\n* after fix-copies\nSome optional tags on primitives were removed, past_key_values in MarianForCausalLM changed from Tuple of Tuple to List\n\n* Fixing copies to roformer and pegasus\n\nCo-authored-by: Clementine Fourrier <cfourrie@inria.fr>\nCo-authored-by: matt <rocketknight1@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RoFormerSinusoidalPositionalEmbedding(nn.Embedding):",
            "return out",
            "",
            "@torch.no_grad()",
            "-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):",
            "+    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:",
            "\"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"",
            "bsz, seq_len = input_ids_shape[:2]",
            "positions = torch.arange("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('->', '->'), position=3, insert_id=1535713)",
            "Insert(target_node=ASTNode(type=function_definition), node=('type', None), position=4, insert_id=1535714)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=1535715)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1535716)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1535717)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1535718)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1015,
        "neg_line": [
            "-def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):"
        ],
        "pos_line": [
            "+def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:"
        ],
        "core_change": "-def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0): +def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "c72b262088e2c929b8a25ed00b696b414d4baea2",
        "index": "9d795356..9084d3f7 100644",
        "commit_message": "GH-464: fix text generation on cuda:1\n\n",
        "file": "flair.txt.json",
        "label": "yes",
        "comments": "change API call for resource fix",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class LanguageModel(nn.Module):",
            "",
            "for i in range(number_of_characters):",
            "",
            "-                if torch.cuda.is_available():",
            "-                    input = input.cuda()",
            "+                input = input.to(flair.device)",
            "",
            "# get predicted weights",
            "prediction, _, hidden = self.forward(input, hidden)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=cuda), value='to')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1833454)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flair'), position=0, insert_id=1833455)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1833456)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=is_available))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 1016,
        "neg_line": [
            "-if torch.cuda.is_available():",
            "-input = input.cuda()"
        ],
        "pos_line": [
            "+input = input.to(flair.device)"
        ],
        "core_change": "-if torch.cuda.is_available(): -input = input.cuda() +input = input.to(flair.device)",
        "core_API": "is_available"
    },
    {
        "commit_hash": "934d9fa8aa341bfcfd3106dbf77d5378a21c3f18",
        "index": "b453a84c41..c1c977c07d 100644",
        "commit_message": "Fix optional typehint (continued) (#11923)\n\nCo-authored-by: @AnnaTz\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def instance_norm(",
            "",
            "",
            "def lp_normalize(",
            "-    x: Union[tf.Tensor, tf.Variable], /, *, p: float = 2, axis: int = None, out=None",
            "+    x: Union[tf.Tensor, tf.Variable],",
            "+    /,",
            "+    *,",
            "+    p: float = 2,",
            "+    axis: Optional[int] = None,",
            "+    out: Optional[tf.Tensor] = None,",
            ") -> tf.Tensor:",
            "denorm = tf.norm(x, ord=p, axis=axis, keepdims=True)",
            "denorm = tf.math.maximum(denorm, 1e-12)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=12, insert_id=1959324)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=13, insert_id=1959325)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=out), position=0)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=1959326)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=1959327)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type==, text==), position=3)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=none, text=None), position=4)",
            "Insert(target_node=ASTNode(type=type), node=('subscript', None), position=0, insert_id=1959328)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=1959329)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Optional'), position=0, insert_id=1959330)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1959331)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=int), position=2)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1959332)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Optional'), position=0, insert_id=1959333)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1959334)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=1959335)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1959336)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1959337)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1959338)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1959339)",
            "Delete(target_node=ASTNode(type=default_parameter))"
        ],
        "plus_line": 6,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 1017,
        "neg_line": [
            "-x: Union[tf.Tensor, tf.Variable], /, *, p: float = 2, axis: int = None, out=None"
        ],
        "pos_line": [
            "+x: Union[tf.Tensor, tf.Variable],",
            "+/,",
            "+*,",
            "+p: float = 2,",
            "+axis: Optional[int] = None,",
            "+out: Optional[tf.Tensor] = None,"
        ],
        "core_change": "-x: Union[tf.Tensor, tf.Variable], /, *, p: float = 2, axis: int = None, out=None +x: Union[tf.Tensor, tf.Variable], +/, +*, +p: float = 2, +axis: Optional[int] = None, +out: Optional[tf.Tensor] = None,",
        "core_API": "norm"
    },
    {
        "commit_hash": "8aa196ce08007aa1033b0e42931c247e1e491321",
        "index": "7ac3a4a..c30c8ee 100644",
        "commit_message": "Add dilated conv support (#9347)\n\n* added dilate conv support\n\n* added dilate conv support\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update common.py\n\n* Update common.py\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "file": "yolov5.txt.json",
        "label": "no",
        "comments": "doc",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Conv(nn.Module):",
            "",
            "",
            "class DWConv(Conv):",
            "-    # Depth-wise convolution class",
            "+    # Depth-wise convolution",
            "def __init__(self, c1, c2, k=1, s=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups",
            "super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), act=act)",
            "",
            "",
            "class DWConvTranspose2d(nn.ConvTranspose2d):",
            "-    # Depth-wise transpose convolution class",
            "+    # Depth-wise transpose convolution",
            "def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # ch_in, ch_out, kernel, stride, padding, padding_out",
            "super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1020,
        "neg_line": [
            "-# Depth-wise convolution class",
            "-# Depth-wise transpose convolution class"
        ],
        "pos_line": [
            "+# Depth-wise convolution",
            "+# Depth-wise transpose convolution"
        ],
        "core_change": "-# Depth-wise convolution class +# Depth-wise convolution -# Depth-wise transpose convolution class +# Depth-wise transpose convolution",
        "core_API": "gcd"
    },
    {
        "commit_hash": "a54cdb6fcb3be229614c03cd680b36d00983a8b0",
        "index": "534632ff..d1f630a6 100644",
        "commit_message": "Fix mnist_synthetic_dataset\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def reduce_per_replica(values, strategy, reduction):",
            "else:",
            "return concat(strategy.experimental_local_results(v))",
            "elif reduction == \"sum\":",
            "-            values = strategy.experimental_local_results(v)",
            "-            return tf.reduce_sum(values)",
            "+            return tf.reduce_sum(strategy.experimental_local_results(v))",
            "else:",
            "raise ValueError(",
            "'`reduction` must be \"first\", \"concat\", \"sum\", or \"auto\". '"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=3, insert_id=2048367)",
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=2, insert_id=2048368)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2048369)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=2048370)",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=string, text=\"sum\"), position=2)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2048371)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2048372)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2048373)",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=values))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=values))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 1021,
        "neg_line": [
            "-values = strategy.experimental_local_results(v)",
            "-return tf.reduce_sum(values)"
        ],
        "pos_line": [
            "+return tf.reduce_sum(strategy.experimental_local_results(v))"
        ],
        "core_change": "-values = strategy.experimental_local_results(v) -return tf.reduce_sum(values) +return tf.reduce_sum(strategy.experimental_local_results(v))",
        "core_API": "experimental_local_results"
    },
    {
        "commit_hash": "bc33fbf956eef62d0ba8d3cd67ee955ad5defcdb",
        "index": "630e30b7e..edb1b8349 100755",
        "commit_message": "[CI] Fix ci  (#21940)\n\n* fix `get_proposal_pos_embed`\n\n* fix order\n\n* style\n\n* zero shot simplify test\n\n* add approximate values for zero shot audio classification\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "custom API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeformableDetrModel(DeformableDetrPreTrainedModel):",
            "scale = 2 * math.pi",
            "",
            "dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)",
            "-        dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)",
            "+        dim_t = temperature ** (2 * torch_int_div(dim_t, 2) / num_pos_feats)",
            "# batch_size, num_queries, 4",
            "proposals = proposals.sigmoid() * scale",
            "# batch_size, num_queries, 4, 128"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='torch_int_div')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=div))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1022,
        "neg_line": [
            "-dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)"
        ],
        "pos_line": [
            "+dim_t = temperature ** (2 * torch_int_div(dim_t, 2) / num_pos_feats)"
        ],
        "core_change": "-dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats) +dim_t = temperature ** (2 * torch_int_div(dim_t, 2) / num_pos_feats)",
        "core_API": "arange"
    },
    {
        "commit_hash": "f41a9b981353fd4e161e6019d757424e4fe88a64",
        "index": "5f25e4516a..ed6858e2ef 100644",
        "commit_message": "[RLlib] Fix KL method of MultiCategorial tf distribution (issue #7009). (#7119)\n\n* Fix KL method of MultiCategorial tf distribution.\n\n* Fix KL method of MultiCategorial tf distribution.\n\n* Merge AsyncReplayOptimizer fixes into this branch.\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "add API call for shape fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MultiCategorical(TFActionDistribution):",
            "",
            "@override(ActionDistribution)",
            "def multi_kl(self, other):",
            "-        return [cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)]",
            "+        return tf.stack(",
            "+            [cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)],",
            "+            axis=1)",
            "",
            "@override(ActionDistribution)",
            "def kl(self, other):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=2147349)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2147350)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2147351)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2147352)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2147353)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stack'), position=2, insert_id=2147354)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2147355)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=list_comprehension), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2147356)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2147357)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2147358)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2147359)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2147360)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=2147361)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1024,
        "neg_line": [
            "-return [cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)]"
        ],
        "pos_line": [
            "+return tf.stack(",
            "+[cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)],",
            "+axis=1)"
        ],
        "core_change": "-return [cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)] +return tf.stack( +[cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)], +axis=1)",
        "core_API": "kl"
    },
    {
        "commit_hash": "a5ba53aa75920842a78aa2380576168b349da5b3",
        "index": "77d005f..395b65c 100644",
        "commit_message": "Adapt to tf-nightly (#1634)\n\n* fixed tests in keras_layers_test\n\n* fixed image classifier test\n\n* fix bert tokenizer\n\n* multi branch arch adapt preprocessing layers\n\n* depending on tf-nightly\n\n* coverage\n\nCo-authored-by: Haifeng Jin <haifeng-jin@users.noreply.github.com>\n",
        "file": "autokeras.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class SelfAttentionMask(tf.keras.layers.Layer):",
            "",
            "return mask  # pragma: no cover",
            "",
            "+    def get_config(self):",
            "+        return super().get_config()",
            "+",
            "",
            "@tf.keras.utils.register_keras_serializable()",
            "class Transformer(tf.keras.layers.Layer):"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=2, insert_id=2559171)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=2559172)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'get_config'), position=1, insert_id=2559173)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=2559174)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=2559175)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=2559176)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=2559177)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'self'), position=1, insert_id=2559178)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=2559179)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=2559180)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2559181)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=2559182)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2559183)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2559184)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=2559185)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2559186)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_config'), position=2, insert_id=2559187)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2559188)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2559189)",
            "Insert(target_node=IN(type=call), node=('identifier', 'super'), position=0, insert_id=2559190)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2559191)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2559192)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2559193)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 23,
        "number": 1025,
        "neg_line": [],
        "pos_line": [
            "+def get_config(self):",
            "+return super().get_config()",
            "+"
        ],
        "core_change": "+def get_config(self): +return super().get_config() +",
        "core_API": "register_keras_serializable"
    },
    {
        "commit_hash": "35cb101eae8a9678e91b809a8df77d34c259831d",
        "index": "23333f49c..566dd54a9 100755",
        "commit_message": "DataParallel fixes (#5733)\n\n* DataParallel fixes:\n\n1. switched to a more precise check\n-        if self.args.n_gpu > 1:\n+        if isinstance(model, nn.DataParallel):\n\n2. fix tests - require the same fixup under DataParallel as the training module\n\n* another fix\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change condition check for type fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Trainer:",
            "if self.args.past_index >= 0:",
            "inputs[\"mems\"] = past",
            "# Our model outputs do not work with DataParallel, so forcing return tuple.",
            "-            if self.args.n_gpu > 1:",
            "+            if isinstance(model, nn.DataParallel):",
            "inputs[\"return_tuple\"] = True",
            "",
            "with torch.no_grad():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('call', None), position=1, insert_id=1544074)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=1544075)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1544076)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1544077)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'model'), position=1, insert_id=1544078)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1544079)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1544080)",
            "Update(target_node=ASTNode(type=identifier, text=self), value='nn')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=args), value='DataParallel')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=n_gpu))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1027,
        "neg_line": [
            "-if self.args.n_gpu > 1:"
        ],
        "pos_line": [
            "+if isinstance(model, nn.DataParallel):"
        ],
        "core_change": "-if self.args.n_gpu > 1: +if isinstance(model, nn.DataParallel):",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "eea50784c4facbedd15345796973e2281e59bdf7",
        "index": "65e2db41..980959a4 100644",
        "commit_message": "Dev pylint (#1697)\n\nFix pylint errors\n",
        "file": "nni.txt.json",
        "label": "yes",
        "comments": "change param for math fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PolicyWithValue:",
            "def sample(logits, mask_npinf):",
            "new_logits = tf.math.add(logits, mask_npinf)",
            "u = tf.random_uniform(tf.shape(new_logits), dtype=logits.dtype)",
            "-            return tf.argmax(new_logits - tf.log(-tf.log(u)), axis=-1)",
            "+            return tf.argmax(new_logits - tf.log(-1*tf.log(u)), axis=-1)",
            "",
            "def neglogp(logits, x):",
            "# return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=2129512)",
            "Insert(target_node=IN(type=binary_operator), node=('unary_operator', '-1'), position=0, insert_id=2129513)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=2129514)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1028,
        "neg_line": [
            "-return tf.argmax(new_logits - tf.log(-tf.log(u)), axis=-1)"
        ],
        "pos_line": [
            "+return tf.argmax(new_logits - tf.log(-1*tf.log(u)), axis=-1)"
        ],
        "core_change": "-return tf.argmax(new_logits - tf.log(-tf.log(u)), axis=-1) +return tf.argmax(new_logits - tf.log(-1*tf.log(u)), axis=-1)",
        "core_API": "add"
    },
    {
        "commit_hash": "9e29080439f02af7cfbb4ab165bf7a9524ef8904",
        "index": "0a70fdcb4..0e9826d78 100644",
        "commit_message": "[X-CLIP] Fix doc tests (#19523)\n\n* Fix XCLIP doc tests\n\n* Add model to doc test list\n\n* Fix tests\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class XCLIPModelIntegrationTest(unittest.TestCase):",
            "torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),",
            ")",
            "",
            "-        expected_logits = torch.tensor([[14.3819, 20.6031, 15.0526]], device=torch_device)",
            "+        expected_logits = torch.tensor([[14.0181, 20.2771, 14.4776]], device=torch_device)",
            "",
            "self.assertTrue(torch.allclose(outputs.logits_per_video, expected_logits, atol=1e-3))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=14.3819), value='14.0181')",
            "Update(target_node=ASTNode(type=float, text=20.6031), value='20.2771')",
            "Update(target_node=ASTNode(type=float, text=15.0526), value='14.4776')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 1029,
        "neg_line": [
            "-expected_logits = torch.tensor([[14.3819, 20.6031, 15.0526]], device=torch_device)"
        ],
        "pos_line": [
            "+expected_logits = torch.tensor([[14.0181, 20.2771, 14.4776]], device=torch_device)"
        ],
        "core_change": "-expected_logits = torch.tensor([[14.3819, 20.6031, 15.0526]], device=torch_device) +expected_logits = torch.tensor([[14.0181, 20.2771, 14.4776]], device=torch_device)",
        "core_API": "Size"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "db821f2b..1c8e7e47 100644",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def Conv2DTranspose(",
            "if get_tf_version_tuple() <= (1, 12):",
            "kernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0),",
            "else:",
            "-            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)",
            "+            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')",
            "",
            "with rename_get_variable({'kernel': 'W', 'bias': 'b'}):",
            "layer = tf.layers.Conv2DTranspose("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278996)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2278997)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'distribution'), position=0, insert_id=2278998)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2278999)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'untruncated_normal'\"), position=2, insert_id=2279000)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1030,
        "neg_line": [
            "-kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)"
        ],
        "pos_line": [
            "+kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')"
        ],
        "core_change": "-kernel_initializer = tf.keras.initializers.VarianceScaling(2.0) +kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')",
        "core_API": "variance_scaling_initializer"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "a4dd344d..1faac575 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def binary_focal_loss_with_logits(",
            "",
            "probs_pos = torch.sigmoid(input)",
            "probs_neg = torch.sigmoid(-input)",
            "-    loss_tmp = -alpha * torch.pow(probs_neg, gamma) * target * F.logsigmoid(input) - (",
            "-        1 - alpha",
            "-    ) * torch.pow(probs_pos, gamma) * (1.0 - target) * F.logsigmoid(-input)",
            "+    loss_tmp = -alpha * torch.pow(probs_neg, gamma) * target * F.logsigmoid(input) - (1 - alpha) * torch.pow(",
            "+        probs_pos, gamma",
            "+    ) * (1.0 - target) * F.logsigmoid(-input)",
            "",
            "if reduction == 'none':",
            "loss = loss_tmp"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 1031,
        "neg_line": [
            "-loss_tmp = -alpha * torch.pow(probs_neg, gamma) * target * F.logsigmoid(input) - (",
            "-1 - alpha",
            "-) * torch.pow(probs_pos, gamma) * (1.0 - target) * F.logsigmoid(-input)"
        ],
        "pos_line": [
            "+loss_tmp = -alpha * torch.pow(probs_neg, gamma) * target * F.logsigmoid(input) - (1 - alpha) * torch.pow(",
            "+probs_pos, gamma",
            "+) * (1.0 - target) * F.logsigmoid(-input)"
        ],
        "core_change": "-loss_tmp = -alpha * torch.pow(probs_neg, gamma) * target * F.logsigmoid(input) - ( -1 - alpha -) * torch.pow(probs_pos, gamma) * (1.0 - target) * F.logsigmoid(-input) +loss_tmp = -alpha * torch.pow(probs_neg, gamma) * target * F.logsigmoid(input) - (1 - alpha) * torch.pow( +probs_pos, gamma +) * (1.0 - target) * F.logsigmoid(-input)",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "72bcb9dcdb15bdf04c1198e3711ebd05be56f3a3",
        "index": "126e4c7b..8e9555b0 100644",
        "commit_message": "data augmentation pipeline\n\nSummary:\nSeq2Seq Data augmentation pipeline based on prefix with the following workflow\n\n{F207337357}\n\nThe pipeline takes about 5 hours to process music domain, the music domain output is about 100k augmented data.\n\nExample:\nf136633622\n\nfix 2 bugs:\n\n* sequence_generator max_len assertion\n* lengths in interactive.py should be python list instead of tensor\n\nReviewed By: myleott\n\nDifferential Revision: D17138089\n\nfbshipit-source-id: eaeeadd5ba81e02930a45f8873069137469925b6\n\n",
        "file": "fairseq.txt.json",
        "label": "yes",
        "comments": "remove API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def make_batches(lines, args, task, max_positions, encode_fn):",
            ").long()",
            "for src_str in lines",
            "]",
            "-    lengths = torch.LongTensor([t.numel() for t in tokens])",
            "+    lengths = [t.numel() for t in tokens]",
            "itr = task.get_batch_iterator(",
            "dataset=task.build_dataset_for_inference(tokens, lengths),",
            "max_tokens=args.max_tokens,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=list_comprehension), position=4)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=LongTensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1033,
        "neg_line": [
            "-lengths = torch.LongTensor([t.numel() for t in tokens])"
        ],
        "pos_line": [
            "+lengths = [t.numel() for t in tokens]"
        ],
        "core_change": "-lengths = torch.LongTensor([t.numel() for t in tokens]) +lengths = [t.numel() for t in tokens]",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "3910ad033074367f6abfe0001562db725a75cb73",
        "index": "5b3fc1c9d..88225ffc0 100644",
        "commit_message": "bugfix/3185 transpose (#3252)\n\n* change t() to transpose() as xla devices do not support .t() on 1-dim tensor\n\n* detach tensor before copying\n\n* Revert \"detach tensor before copying\"\n\nThis reverts commit 37cc7bbe\n\n* changed dims\n\n* added test_result_obj_on_tpu\n\n* detach before copying\n\n* detach before copying\n\n* detach before copying\n\n* replace torch.cat with sum\n",
        "file": "lightning.txt.json",
        "label": "yes",
        "comments": "add param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EarlyStopping(Callback):",
            "",
            "if trainer.use_tpu:",
            "stop = torch.tensor(int(trainer.should_stop), device=pl_module.device, dtype=torch.int32)",
            "-            stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)",
            "+            stop = xm.mesh_reduce(\"stop_signal\", stop, sum)",
            "torch_xla.core.xla_model.rendezvous(\"pl.EarlyStoppingCallback.stop_distributed_training_check\")",
            "trainer.should_stop = int(stop.item()) == trainer.world_size"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='sum')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=torch), position=5)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cat))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1034,
        "neg_line": [
            "-stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)"
        ],
        "pos_line": [
            "+stop = xm.mesh_reduce(\"stop_signal\", stop, sum)"
        ],
        "core_change": "-stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat) +stop = xm.mesh_reduce(\"stop_signal\", stop, sum)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "ddebbdc53544c64a1957cf0ec04a34578e9b57ba",
        "index": "716c66c4..72c2d603 100644",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "change API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Entropy(Metric):",
            "mask : `torch.Tensor`, optional (default = None).",
            "A masking tensor of shape (batch_size, ...).",
            "\"\"\"",
            "-        logits, mask = self.unwrap_to_tensors(logits, mask)",
            "+        logits, mask = self.detach_tensors(logits, mask)",
            "",
            "if mask is None:",
            "-            mask = torch.ones(logits.size()[:-1])",
            "+            mask = torch.ones(logits.size()[:-1], device=logits.device)",
            "",
            "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)",
            "probabilities = torch.exp(log_probs) * mask.unsqueeze(-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=20643)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=20644)",
            "Update(target_node=ASTNode(type=identifier, text=unwrap_to_tensors), value='detach_tensors')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=20645)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=20646)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=20647)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logits'), position=0, insert_id=20648)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=20649)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=20650)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 1035,
        "neg_line": [
            "-logits, mask = self.unwrap_to_tensors(logits, mask)",
            "-mask = torch.ones(logits.size()[:-1])"
        ],
        "pos_line": [
            "+logits, mask = self.detach_tensors(logits, mask)",
            "+mask = torch.ones(logits.size()[:-1], device=logits.device)"
        ],
        "core_change": "-logits, mask = self.unwrap_to_tensors(logits, mask) +logits, mask = self.detach_tensors(logits, mask) -mask = torch.ones(logits.size()[:-1]) +mask = torch.ones(logits.size()[:-1], device=logits.device)",
        "core_API": "unwrap_to_tensors"
    },
    {
        "commit_hash": "625520998fe989156d0c840654080bc5ba3059ee",
        "index": "93c53f74a..8175792e8 100644",
        "commit_message": "Fixed run resuming summary, deprecated warn -> warning, better keras error message\n\n",
        "file": "wandb.txt.json",
        "label": "no",
        "comments": "doc fix",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "from itertools import chain",
            "if \"keras\" in sys.modules:",
            "if \"tensorflow.python.keras\" in sys.modules:",
            "wandb.termlog(",
            "-            \"WARNING: found both keras and tensorflow.python.keras. Use `from tensorflow import keras` and remove `import keras` to use the latest W&B features.\")",
            "+            \"Found keras and tensorflow.keras. WandbCallback will be configured for keras not tensorflow.keras.\")",
            "import keras",
            "import keras.backend as K",
            "elif \"tensorflow.python.keras\" in sys.modules:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"WARNING: found both keras and tensorflow.python.keras. Use `from tensorflow import keras` and remove `import keras` to use the latest W&B features.\"), value='\"Found keras and tensorflow.keras. WandbCallback will be configured for keras not tensorflow.keras.\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1036,
        "neg_line": [
            "-\"WARNING: found both keras and tensorflow.python.keras. Use `from tensorflow import keras` and remove `import keras` to use the latest W&B features.\")"
        ],
        "pos_line": [
            "+\"Found keras and tensorflow.keras. WandbCallback will be configured for keras not tensorflow.keras.\")"
        ],
        "core_change": "-\"WARNING: found both keras and tensorflow.python.keras. Use `from tensorflow import keras` and remove `import keras` to use the latest W&B features.\") +\"Found keras and tensorflow.keras. WandbCallback will be configured for keras not tensorflow.keras.\")",
        "core_API": "termlog"
    },
    {
        "commit_hash": "5699ed7d139062020d1394f0e85a07f706c87c09",
        "index": "4bb637f..b894a2f 100644",
        "commit_message": "double down on dual patch norm, fix MAE and Simmim to be compatible with dual patchnorm\n\n",
        "file": "vit-pytorch.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ImageEmbedder(nn.Module):",
            "",
            "self.to_patch_embedding = nn.Sequential(",
            "Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),",
            "+            nn.LayerNorm(patch_dim),",
            "nn.Linear(patch_dim, dim),",
            "+            nn.LayerNorm(dim)",
            ")",
            "",
            "self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=1560822)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1560823)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=7, insert_id=1560824)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=8, insert_id=1560825)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1560826)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1560827)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1560828)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1560829)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1560830)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1560831)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'LayerNorm'), position=2, insert_id=1560832)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1560833)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'patch_dim'), position=1, insert_id=1560834)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1560835)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1560836)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1560837)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'LayerNorm'), position=2, insert_id=1560838)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1560839)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dim'), position=1, insert_id=1560840)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 1039,
        "neg_line": [],
        "pos_line": [
            "+nn.LayerNorm(patch_dim),",
            "+nn.LayerNorm(dim)"
        ],
        "core_change": "+nn.LayerNorm(patch_dim), +nn.LayerNorm(dim)",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "7f9193ef09902f2f85262402abf07b66464c60a2",
        "index": "dafcaf368..3e409cfb7 100644",
        "commit_message": "Fixed Style Inconsistency (#3976)\n\n\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change param for refactor fix",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class BertForSequenceClassification(BertPreTrainedModel):",
            "",
            "self.bert = BertModel(config)",
            "self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "-        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)",
            "+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)",
            "",
            "self.init_weights()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=., text=.), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1043,
        "neg_line": [
            "-self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)"
        ],
        "pos_line": [
            "+self.classifier = nn.Linear(config.hidden_size, config.num_labels)"
        ],
        "core_change": "-self.classifier = nn.Linear(config.hidden_size, self.config.num_labels) +self.classifier = nn.Linear(config.hidden_size, config.num_labels)",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "aa90091f9bea1c62757c0e0c64212a16f3d2b348",
        "index": "1f729f4147..61b7dabc28 100644",
        "commit_message": "fix tensorflow's fmod\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def fmod(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "result = tf.math.floormod(x1, x2, name=None)",
            "-    temp = (result, x1)",
            "-    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp)",
            "+    temp = [result, x1]",
            "+    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)",
            "",
            "",
            "def fmax("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('list', None), position=2, insert_id=1974031)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1974032)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=result), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=x1), position=3)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=1974033)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1974034)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1974035)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'fn_output_signature'), position=0, insert_id=1974036)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1974037)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1974038)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'result'), position=0, insert_id=1974039)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1974040)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1974041)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1044,
        "neg_line": [
            "-temp = (result, x1)",
            "-return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp)"
        ],
        "pos_line": [
            "+temp = [result, x1]",
            "+return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)"
        ],
        "core_change": "-temp = (result, x1) -return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp) +temp = [result, x1] +return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)",
        "core_API": "floormod"
    },
    {
        "commit_hash": "ae9230af40ecc8ccb940765830b2f8727049a845",
        "index": "771850690..951a68cb7 100644",
        "commit_message": "[`T5`] Fix torchquant issue (#21843)\n\n* fix torchquant issue\n\n* add tests\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change condition check for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MT5DenseGatedActDense(nn.Module):",
            "# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.",
            "# See https://github.com/huggingface/transformers/issues/20287",
            "# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``",
            "-        if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:",
            "+        if (",
            "+            isinstance(self.wo.weight, torch.Tensor)",
            "+            and hidden_states.dtype != self.wo.weight.dtype",
            "+            and self.wo.weight.dtype != torch.int8",
            "+        ):",
            "hidden_states = hidden_states.to(self.wo.weight.dtype)",
            "",
            "hidden_states = self.wo(hidden_states)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('parenthesized_expression', None), position=1, insert_id=1175588)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1175589)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=boolean_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1175590)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('boolean_operator', None), position=0, insert_id=1175591)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=1175592)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1175593)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=1175594)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1175595)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1175596)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1175597)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1175598)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=1175599)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1175600)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1175601)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1175602)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weight'), position=2, insert_id=1175603)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1175604)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1175605)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1175606)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1175607)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1175608)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'wo'), position=2, insert_id=1175609)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 1046,
        "neg_line": [
            "-if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:"
        ],
        "pos_line": [
            "+if (",
            "+isinstance(self.wo.weight, torch.Tensor)",
            "+and hidden_states.dtype != self.wo.weight.dtype",
            "+and self.wo.weight.dtype != torch.int8",
            "+):"
        ],
        "core_change": "-if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8: +if ( +isinstance(self.wo.weight, torch.Tensor) +and hidden_states.dtype != self.wo.weight.dtype +and self.wo.weight.dtype != torch.int8 +):",
        "core_API": "to"
    },
    {
        "commit_hash": "da3aceb876e0f92b65405a20198836c08ee80d10",
        "index": "852b9bf8..a0ed4be4 100644",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "file": "kornia.txt.json",
        "label": "no",
        "comments": "use custom api",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _scale_channel(im: torch.Tensor) -> torch.Tensor:",
            "",
            "im = im * 255",
            "# Compute the histogram of the image channel.",
            "-    histo = torch.histc(im, bins=256, min=0, max=255)",
            "+    histo = _torch_histc_cast(im, bins=256, min=0, max=255)",
            "# For the purposes of computing the step, filter out the nonzeros.",
            "nonzero_histo = torch.reshape(histo[histo != 0], [-1])",
            "step = (torch.sum(nonzero_histo) - nonzero_histo[-1]) // 255"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_histc_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=histc))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1047,
        "neg_line": [
            "-histo = torch.histc(im, bins=256, min=0, max=255)"
        ],
        "pos_line": [
            "+histo = _torch_histc_cast(im, bins=256, min=0, max=255)"
        ],
        "core_change": "-histo = torch.histc(im, bins=256, min=0, max=255) +histo = _torch_histc_cast(im, bins=256, min=0, max=255)",
        "core_API": "histc"
    },
    {
        "commit_hash": "109fc779a7136ebcb321dc4d96d88114218525a1",
        "index": "d49e69f0ef..4953f9353b 100644",
        "commit_message": "small fixes regarding device placement.\n\n",
        "file": "ivy.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def vector_to_skew_symmetric_matrix(vector):",
            "a2s = vector_expanded[..., 1:2, :]",
            "a3s = vector_expanded[..., 2:3, :]",
            "# BS x 1 x 1",
            "-    zs = torch.zeros(batch_shape + [1, 1])",
            "+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)",
            "# BS x 1 x 3",
            "row1 = torch.cat((zs, -a3s, a2s), -1)",
            "row2 = torch.cat((a3s, zs, -a1s), -1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=381268)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=381269)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=381270)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=381271)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=381272)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'vector'), position=0, insert_id=381273)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=381274)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=381275)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1048,
        "neg_line": [
            "-zs = torch.zeros(batch_shape + [1, 1])"
        ],
        "pos_line": [
            "+zs = torch.zeros(batch_shape + [1, 1], device=vector.device)"
        ],
        "core_change": "-zs = torch.zeros(batch_shape + [1, 1]) +zs = torch.zeros(batch_shape + [1, 1], device=vector.device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "bef1fdf2f2492957121f04d0361e33684c135b02",
        "index": "c96d51b..a6d0783 100644",
        "commit_message": "Fix misstyping\n",
        "file": "PyTorch-YOLOv3.txt.json",
        "label": "no",
        "comments": "add comment",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class YOLOLayer(nn.Module):",
            "w = prediction[..., 2]  # Width",
            "h = prediction[..., 3]  # Height",
            "pred_conf = torch.sigmoid(prediction[..., 4])  # Conf",
            "-        pred_cls = torch.sigmoid(prediction[..., 5:]        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor)  # Cls pred.",
            "+        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.",
            "",
            "# If grid size does not match current we compute new offsets",
            "if grid_size != self.grid_size:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=ByteTensor))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ByteTensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ByteTensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=conditional_expression))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 1049,
        "neg_line": [
            "-pred_cls = torch.sigmoid(prediction[..., 5:]        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor)  # Cls pred."
        ],
        "pos_line": [
            "+pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred."
        ],
        "core_change": "-pred_cls = torch.sigmoid(prediction[..., 5:]        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor)  # Cls pred. +pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "037ffcd5..51453c3a 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PointAssigner(BaseAssigner):",
            "",
            "if gt_labels is not None:",
            "assigned_labels = assigned_gt_inds.new_full((num_points, ), -1)",
            "-            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()",
            "+            pos_inds = torch.nonzero(",
            "+                assigned_gt_inds > 0, as_tuple=False).squeeze()",
            "if pos_inds.numel() > 0:",
            "assigned_labels[pos_inds] = gt_labels[",
            "assigned_gt_inds[pos_inds] - 1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638713)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638714)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638715)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638716)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638717)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1050,
        "neg_line": [
            "-pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()"
        ],
        "pos_line": [
            "+pos_inds = torch.nonzero(",
            "+assigned_gt_inds > 0, as_tuple=False).squeeze()"
        ],
        "core_change": "-pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze() +pos_inds = torch.nonzero( +assigned_gt_inds > 0, as_tuple=False).squeeze()",
        "core_API": "new_full"
    },
    {
        "commit_hash": "9480ffea988893699122b828eaeb03fd29cf9192",
        "index": "1e50a4ec5..fab61d4a7 100644",
        "commit_message": "fixed att_to_numpy() function for AttCov, AttCovLoc\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "change param for shape fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def att_to_numpy(att_ws, att):",
            "att_ws = torch.stack([aw[:, -1] for aw in att_ws], dim=1).cpu().numpy()",
            "elif isinstance(att, (AttCov, AttCovLoc)):",
            "# att_ws => list of list of previous attentions",
            "-        att_ws = torch.stack([aw[-1] for aw in att_ws], dim=1).cpu().numpy()",
            "+        att_ws = torch.stack([aw[idx] for idx, aw in enumerate(att_ws)], dim=1).cpu().numpy()",
            "elif isinstance(att, AttLocRec):",
            "# att_ws => list of tuple of attention and hidden states",
            "att_ws = torch.stack([aw[0] for aw in att_ws], dim=1).cpu().numpy()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('identifier', 'idx'), position=2, insert_id=166294)",
            "Insert(target_node=ASTNode(type=for_in_clause), node=('pattern_list', None), position=1, insert_id=166295)",
            "Insert(target_node=ASTNode(type=for_in_clause), node=('call', None), position=4, insert_id=166296)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'idx'), position=0, insert_id=166297)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=1, insert_id=166298)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=identifier, text=aw), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'enumerate'), position=0, insert_id=166299)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=166300)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=166301)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=att_ws), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=166302)",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 1052,
        "neg_line": [
            "-att_ws = torch.stack([aw[-1] for aw in att_ws], dim=1).cpu().numpy()"
        ],
        "pos_line": [
            "+att_ws = torch.stack([aw[idx] for idx, aw in enumerate(att_ws)], dim=1).cpu().numpy()"
        ],
        "core_change": "-att_ws = torch.stack([aw[-1] for aw in att_ws], dim=1).cpu().numpy() +att_ws = torch.stack([aw[idx] for idx, aw in enumerate(att_ws)], dim=1).cpu().numpy()",
        "core_API": "stack"
    },
    {
        "commit_hash": "34263ab196cbfab65f2afa1150e6cf6db5ffd8b6",
        "index": "9328968e6..81e2b94f7 100755",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "file": "espnet.txt.json",
        "label": "no",
        "comments": "refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# train",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.asr.chain.asr_chainer import train",
            "+        from espnet.asr.chain.asr import train",
            "train(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.asr.pytorch.asr_pytorch import train",
            "+        from espnet.asr.pytorch.asr import train",
            "train(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=import_from_statement), node=ASTNode(type=dotted_name), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=asr_pytorch), value='asr')",
            "Insert(target_node=ASTNode(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=178751)",
            "Update(target_node=ASTNode(type=identifier, text=asr_chainer), value='asr')",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'train'), position=0, insert_id=178752)",
            "Delete(target_node=ASTNode(type=identifier, text=train))",
            "Delete(target_node=ASTNode(type=dotted_name))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 1053,
        "neg_line": [
            "-from espnet.asr.chain.asr_chainer import train",
            "-from espnet.asr.pytorch.asr_pytorch import train"
        ],
        "pos_line": [
            "+from espnet.asr.chain.asr import train",
            "+from espnet.asr.pytorch.asr import train"
        ],
        "core_change": "-from espnet.asr.chain.asr_chainer import train +from espnet.asr.chain.asr import train -from espnet.asr.pytorch.asr_pytorch import train +from espnet.asr.pytorch.asr import train",
        "core_API": "info"
    },
    {
        "commit_hash": "02c9c545579ec2054cb38b8a5193f26f11540fd9",
        "index": "f22b187e..cd5c498d 100644",
        "commit_message": "Fix default dtype in HJM, HullWhite, and Heston model, as well as in PiecewiseConstant class.\n\nPiperOrigin-RevId: 387639335\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "yes",
        "comments": "change param for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class HestonModel(generic_ito_process.GenericItoProcess):",
            "drift = tf.stack([log_spot_drift, var_drift], -1)",
            "return drift",
            "",
            "-    super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, dtype, name)",
            "+    super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, self._dtype, name)",
            "",
            "def sample_paths(self,",
            "times: types.RealTensor,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=7, insert_id=2336755)",
            "Update(target_node=ASTNode(type=identifier, text=dtype), value='self')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dtype), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2336756)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_dtype'), position=2, insert_id=2336757)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1054,
        "neg_line": [
            "-super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, dtype, name)"
        ],
        "pos_line": [
            "+super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, self._dtype, name)"
        ],
        "core_change": "-super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, dtype, name) +super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, self._dtype, name)",
        "core_API": "stack"
    },
    {
        "commit_hash": "ac59f458247451ec8e2400f81c37f93ab7f75026",
        "index": "b3658e7d..53dce68a 100644",
        "commit_message": "Test fixes for compatibility with PyTorch master (#1416)\n\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "value change",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_gaussian_mixture_model():",
            "cluster_assignments = dist.Categorical(true_mix_proportions).sample(torch.Size((N,)))",
            "data = dist.Normal(true_cluster_means[cluster_assignments], 1.0).sample()",
            "hmc_kernel = HMC(gmm, trajectory_length=1, adapt_step_size=True, max_iarange_nesting=1)",
            "-    mcmc_run = MCMC(hmc_kernel, num_samples=600, warmup_steps=200).run(data)",
            "+    mcmc_run = MCMC(hmc_kernel, num_samples=300, warmup_steps=100).run(data)",
            "posterior = EmpiricalMarginal(mcmc_run, sites=[\"phi\", \"cluster_means\"]).mean.sort()[0]",
            "assert_equal(posterior[0], true_mix_proportions, prec=0.05)",
            "assert_equal(posterior[1], true_cluster_means, prec=0.2)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=600), value='300')",
            "Update(target_node=ASTNode(type=integer, text=200), value='100')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1055,
        "neg_line": [
            "-mcmc_run = MCMC(hmc_kernel, num_samples=600, warmup_steps=200).run(data)"
        ],
        "pos_line": [
            "+mcmc_run = MCMC(hmc_kernel, num_samples=300, warmup_steps=100).run(data)"
        ],
        "core_change": "-mcmc_run = MCMC(hmc_kernel, num_samples=600, warmup_steps=200).run(data) +mcmc_run = MCMC(hmc_kernel, num_samples=300, warmup_steps=100).run(data)",
        "core_API": "Categorical"
    },
    {
        "commit_hash": "8fcb93074f3764eaef464c74dcdda6a7ed5fcfeb",
        "index": "367fb62e..d64a1d71 100644",
        "commit_message": "Fix the evaluate() method in the SimilarityLearner class\n\nThe evaluate method does not have a similar function signature as other models, which is causing errors when training it.\n",
        "file": "flair.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SimilarityLearner(flair.nn.Model):",
            "epoch_results_str,",
            "detailed_results,",
            "),",
            "-            0,",
            "+            torch.tensor(0),",
            ")",
            "",
            "def _get_state_dict(self):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('call', None), position=0, insert_id=237089)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=237090)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=237091)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=237092)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=237093)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=237094)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=237095)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=0), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=237096)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1056,
        "neg_line": [
            "-0,"
        ],
        "pos_line": [
            "+torch.tensor(0),"
        ],
        "core_change": "-0, +torch.tensor(0),",
        "core_API": "tensor"
    },
    {
        "commit_hash": "abc400b06a8ab26cd438b6e9add3aad082ffc48f",
        "index": "435302048..89c731b4d 100644",
        "commit_message": "Add final_layer_norm to OPT model (#17785)\n\n* Add final_layer_norm to OPT model\n\n* Add JAX and TF version\n\n* Fix Keras name\n\n* Woops\n\n* Allow for non breaking change\n\n* Apply suggestions from code review\n\n* add tests\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add condition check for null fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFOPTDecoder(tf.keras.layers.Layer):",
            "if output_attentions:",
            "all_self_attns += (layer_self_attn,)",
            "",
            "+        if self.final_layer_norm is not None:",
            "+            hidden_states = self.final_layer_norm(hidden_states)",
            "+",
            "if self.project_out is not None:",
            "hidden_states = self.project_out(hidden_states)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=2363905)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2363906)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=2363907)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2363908)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2363909)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=2363910)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=2363911)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=2363912)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=2363913)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2363914)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2363915)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2363916)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'final_layer_norm'), position=2, insert_id=2363917)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2363918)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'hidden_states'), position=0, insert_id=2363919)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2363920)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2363921)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2363922)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2363923)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2363924)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2363925)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'final_layer_norm'), position=2, insert_id=2363926)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2363927)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'hidden_states'), position=1, insert_id=2363928)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2363929)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 1059,
        "neg_line": [],
        "pos_line": [
            "+if self.final_layer_norm is not None:",
            "+hidden_states = self.final_layer_norm(hidden_states)",
            "+"
        ],
        "core_change": "+if self.final_layer_norm is not None: +hidden_states = self.final_layer_norm(hidden_states) +",
        "core_API": "final_layer_norm"
    },
    {
        "commit_hash": "a6fbc50983f5e318480601631e534edfd0072377",
        "index": "68f5894..b7df51e 100644",
        "commit_message": "Fixing python2 logging for extract_features.py\n\n",
        "file": "bert.txt.json",
        "label": "yes",
        "comments": "change API call for type fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def convert_examples_to_features(examples, seq_length, tokenizer):",
            "if ex_index < 5:",
            "tf.logging.info(\"*** Example ***\")",
            "tf.logging.info(\"unique_id: %s\" % (example.unique_id))",
            "-      tf.logging.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))",
            "+      tf.logging.info(\"tokens: %s\" % \" \".join(",
            "+          [tokenization.printable_text(x) for x in tokens]))",
            "tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))",
            "tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))",
            "tf.logging.info("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1907781)",
            "Update(target_node=ASTNode(type=identifier, text=str), value='tokenization')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=str), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1907782)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'printable_text'), position=2, insert_id=1907783)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1060,
        "neg_line": [
            "-tf.logging.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))"
        ],
        "pos_line": [
            "+tf.logging.info(\"tokens: %s\" % \" \".join(",
            "+[tokenization.printable_text(x) for x in tokens]))"
        ],
        "core_change": "-tf.logging.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens])) +tf.logging.info(\"tokens: %s\" % \" \".join( +[tokenization.printable_text(x) for x in tokens]))",
        "core_API": "info"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "d86dbc4bf..73df985e3 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "doc",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "logger = logging.getLogger(__name__)",
            "# for the pretrained weights provided with the models",
            "####################################################",
            "T5_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-pytorch_model.bin\",",
            "-    \"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-pytorch_model.bin\",",
            "-    \"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-pytorch_model.bin\",",
            "-    \"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-pytorch_model.bin\",",
            "-    \"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-pytorch_model.bin\",",
            "+    \"t5-small\": \"https://cdn.huggingface.co/t5-small-pytorch_model.bin\",",
            "+    \"t5-base\": \"https://cdn.huggingface.co/t5-base-pytorch_model.bin\",",
            "+    \"t5-large\": \"https://cdn.huggingface.co/t5-large-pytorch_model.bin\",",
            "+    \"t5-3b\": \"https://cdn.huggingface.co/t5-3b-pytorch_model.bin\",",
            "+    \"t5-11b\": \"https://cdn.huggingface.co/t5-11b-pytorch_model.bin\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=dictionary), position=9)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689777)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689778)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/t5-small-pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/t5-base-pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/t5-large-pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/t5-3b-pytorch_model.bin\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/t5-11b-pytorch_model.bin\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 11,
        "number": 1062,
        "neg_line": [
            "-\"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-pytorch_model.bin\",",
            "-\"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-pytorch_model.bin\",",
            "-\"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-pytorch_model.bin\",",
            "-\"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-pytorch_model.bin\",",
            "-\"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-pytorch_model.bin\","
        ],
        "pos_line": [
            "+\"t5-small\": \"https://cdn.huggingface.co/t5-small-pytorch_model.bin\",",
            "+\"t5-base\": \"https://cdn.huggingface.co/t5-base-pytorch_model.bin\",",
            "+\"t5-large\": \"https://cdn.huggingface.co/t5-large-pytorch_model.bin\",",
            "+\"t5-3b\": \"https://cdn.huggingface.co/t5-3b-pytorch_model.bin\",",
            "+\"t5-11b\": \"https://cdn.huggingface.co/t5-11b-pytorch_model.bin\","
        ],
        "core_change": "-\"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-pytorch_model.bin\", -\"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-pytorch_model.bin\", -\"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-pytorch_model.bin\", -\"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-pytorch_model.bin\", -\"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-pytorch_model.bin\", +\"t5-small\": \"https://cdn.huggingface.co/t5-small-pytorch_model.bin\", +\"t5-base\": \"https://cdn.huggingface.co/t5-base-pytorch_model.bin\", +\"t5-large\": \"https://cdn.huggingface.co/t5-large-pytorch_model.bin\", +\"t5-3b\": \"https://cdn.huggingface.co/t5-3b-pytorch_model.bin\", +\"t5-11b\": \"https://cdn.huggingface.co/t5-11b-pytorch_model.bin\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "c0b2040e38c5135c65dd2b5e9b24f00ff3f31548",
        "index": "ef34290..ff4e10a 100644",
        "commit_message": "Fixed module resolution for tf.keras optimizers and added unit tests (#1935)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n",
        "file": "horovod.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def load_model(filepath, custom_optimizers=None, custom_objects=None, compressio",
            "\"\"\"",
            "def wrap_optimizer(cls):",
            "return lambda **kwargs: DistributedOptimizer(cls(**kwargs), compression=compression)",
            "-    return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects)",
            "+    optimizer_modules = {keras.optimizers.Optimizer.__module__}",
            "+    return _impl.load_model(keras, wrap_optimizer, optimizer_modules, filepath, custom_optimizers, custom_objects)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2486138)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2486139)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'optimizer_modules'), position=0, insert_id=2486140)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2486141)",
            "Insert(target_node=IN(type=assignment), node=('set', None), position=2, insert_id=2486142)",
            "Insert(target_node=IN(type=set), node=('{', '{'), position=0, insert_id=2486143)",
            "Insert(target_node=IN(type=set), node=('attribute', None), position=1, insert_id=2486144)",
            "Insert(target_node=IN(type=set), node=('}', '}'), position=2, insert_id=2486145)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'optimizer_modules'), position=5, insert_id=2486146)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=2486147)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2486148)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2486149)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__module__'), position=2, insert_id=2486150)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2486151)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2486152)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Optimizer'), position=2, insert_id=2486153)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'keras'), position=0, insert_id=2486154)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2486155)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'optimizers'), position=2, insert_id=2486156)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 1064,
        "neg_line": [
            "-return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects)"
        ],
        "pos_line": [
            "+optimizer_modules = {keras.optimizers.Optimizer.__module__}",
            "+return _impl.load_model(keras, wrap_optimizer, optimizer_modules, filepath, custom_optimizers, custom_objects)"
        ],
        "core_change": "-return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects) +optimizer_modules = {keras.optimizers.Optimizer.__module__} +return _impl.load_model(keras, wrap_optimizer, optimizer_modules, filepath, custom_optimizers, custom_objects)",
        "core_API": "load_model"
    },
    {
        "commit_hash": "cc276f0ac7badca2093e27e3d3143f2e9cd3fc17",
        "index": "c8a1029d..0fcb7650 100644",
        "commit_message": "1. Add time dependent boundary condition to the PDE solver\n2. Remove numpy dependency of the PDE solver implementation.\n2. Fix a bug in 1-d PDE solver for the default boundary to work with a batch of PDEs.\n\nPiperOrigin-RevId: 371112748\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "yes",
        "comments": "add API call for shape fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):",
            "second_order_coeff_fn=second_order_coeff_fn,",
            "inner_first_order_coeff_fn=inner_first_order_coeff_fn)[0]",
            "",
            "-    true_values = tf.math.exp(final_t + grid[0])",
            "+    true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)",
            "self.assertAllClose(",
            "est_values, true_values, atol=1e-2, rtol=1e-2)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2338418)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2338419)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2338420)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2338421)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'expand_dims'), position=2, insert_id=2338422)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2338423)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2338424)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2338425)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2338426)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2338427)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2338428)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '0'), position=2, insert_id=2338429)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 1065,
        "neg_line": [
            "-true_values = tf.math.exp(final_t + grid[0])"
        ],
        "pos_line": [
            "+true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)"
        ],
        "core_change": "-true_values = tf.math.exp(final_t + grid[0]) +true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)",
        "core_API": "exp"
    },
    {
        "commit_hash": "aa014af85b551d37bc77179b3aaeafccc848aec0",
        "index": "c79bf25620..56ed1a1c49 100644",
        "commit_message": "[rllib] Fix atari reward calculations, add LR annealing, explained var stat for A2C / impala (#2700)\n\nChanges needed to reproduce Atari plots in IMPALA / A2C: https://github.com/ray-project/rl-experiments\n\n\n",
        "file": "ray.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LocalMultiGPUOptimizer(PolicyOptimizer):",
            "else:",
            "rnn_inputs = []",
            "self.par_opt = LocalSyncParallelOptimizer(",
            "-                        tf.train.AdamOptimizer(",
            "-                            self.sgd_stepsize), self.devices,",
            "+                        self.policy.optimizer(), self.devices,",
            "[v for _, v in self.policy.loss_inputs()], rnn_inputs,",
            "self.per_device_batch_size, self.policy.copy,",
            "os.getcwd())"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=AdamOptimizer), value='optimizer')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=train), value='policy')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sgd_stepsize))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 1066,
        "neg_line": [
            "-tf.train.AdamOptimizer(",
            "-self.sgd_stepsize), self.devices,"
        ],
        "pos_line": [
            "+self.policy.optimizer(), self.devices,"
        ],
        "core_change": "-tf.train.AdamOptimizer( -self.sgd_stepsize), self.devices, +self.policy.optimizer(), self.devices,",
        "core_API": "AdamOptimizer"
    },
    {
        "commit_hash": "ce709fa3a6380a5d4c9c03deb88047710e2a5600",
        "index": "bb6617a1..b1ee6e1b 100644",
        "commit_message": "fix multigpu training\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DistributedReplicatedBuilder(DataParallelBuilder):",
            "return grads",
            "",
            "# Ngpu * Nvar * 2",
            "-        grad_list = self.build_on_multi_tower(",
            "-            get_grads,",
            "+        grad_list = DataParallelBuilder.build_on_towers(",
            "+            self.towers, get_grads,",
            "devices=self.raw_devices,",
            "use_vs=[True] * len(self.towers))  # open vs at each tower",
            "DataParallelBuilder._check_grad_list(grad_list)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=self), value='DataParallelBuilder')",
            "Update(target_node=ASTNode(type=identifier, text=build_on_multi_tower), value='build_on_towers')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=2295569)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2295570)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2295571)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2295572)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'towers'), position=2, insert_id=2295573)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 1067,
        "neg_line": [
            "-grad_list = self.build_on_multi_tower(",
            "-get_grads,"
        ],
        "pos_line": [
            "+grad_list = DataParallelBuilder.build_on_towers(",
            "+self.towers, get_grads,"
        ],
        "core_change": "-grad_list = self.build_on_multi_tower( -get_grads, +grad_list = DataParallelBuilder.build_on_towers( +self.towers, get_grads,",
        "core_API": "build_on_multi_tower"
    },
    {
        "commit_hash": "e3cc4487fe66e03ec85970ea2db8e5fb34c455f4",
        "index": "b37e0cca3..ec090c932 100644",
        "commit_message": "Fix CIs for PyTorch 1.13 (#20686)\n\n* fix 1\n\n* fix 2\n\n* fix 3\n\n* fix 4\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add API call for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OPTForSequenceClassification(OPTPreTrainedModel):",
            "sequence_lengths = -1",
            "else:",
            "if input_ids is not None:",
            "-                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1",
            "+                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)",
            "else:",
            "sequence_lengths = -1",
            "logger.warning("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=5, insert_id=1181407)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1181408)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1181409)",
            "Insert(target_node=IN(type=attribute), node=('parenthesized_expression', None), position=0, insert_id=1181410)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1181411)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1181412)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1181413)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1181414)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1181415)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1181416)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1181417)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logits'), position=0, insert_id=1181418)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1181419)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1181420)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 1069,
        "neg_line": [
            "-sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1"
        ],
        "pos_line": [
            "+sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)"
        ],
        "core_change": "-sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1 +sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)",
        "core_API": "ne"
    },
    {
        "commit_hash": "291c01d175f749985c2ffbbb87547e59bc4f6e10",
        "index": "c328578534..793da69888 100644",
        "commit_message": "formatting fixes for Array API submodule in TensorFlow backend.\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def vector_norm(x: Tensor,",
            "tn_normalized_vector = tf.linalg.norm(x,p,axis,keepdims)",
            "",
            "if tn_normalized_vector.shape  == tuple():",
            "-        return  tf.expand_dims(tn_normalized_vector, 0)",
            "+        return tf.expand_dims(tn_normalized_vector, 0)",
            "return tn_normalized_vector"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1071,
        "neg_line": [
            "-return  tf.expand_dims(tn_normalized_vector, 0)"
        ],
        "pos_line": [
            "+return tf.expand_dims(tn_normalized_vector, 0)"
        ],
        "core_change": "-return  tf.expand_dims(tn_normalized_vector, 0) +return tf.expand_dims(tn_normalized_vector, 0)",
        "core_API": "norm"
    },
    {
        "commit_hash": "4d938bca820a8a0d6ebf48af2a025322e64042e4",
        "index": "a8132f86..ff2fc6af 100644",
        "commit_message": "Rename .reshape(s,n) -> .expand_by(s).independent(n) (#1016)\n\n* Start to rename .reshape()\n\n* Remove the .reshape() method entirely\n\n* Add .reshape() with informative error message\n\n* Fix test\n\n* Fix failing test\n\n* Fix failing test\n\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_discrete_parallel(continuous_class):",
            "",
            "def model(data):",
            "weights = pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))",
            "-        locs = pyro.sample('locs', dist.Normal(0, 10).reshape([K], extra_event_dims=1))",
            "+        locs = pyro.sample('locs', dist.Normal(0, 10).expand_by([K]).independent(1))",
            "scale = pyro.sample('scale', dist.LogNormal(0, 1))",
            "",
            "with pyro.iarange('data', len(data)):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=745183)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=745184)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=745185)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=745186)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'independent'), position=2, insert_id=745187)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=745188)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=1), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=745189)",
            "Update(target_node=ASTNode(type=identifier, text=reshape), value='expand_by')",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=list), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=745190)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=extra_event_dims))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 1072,
        "neg_line": [
            "-locs = pyro.sample('locs', dist.Normal(0, 10).reshape([K], extra_event_dims=1))"
        ],
        "pos_line": [
            "+locs = pyro.sample('locs', dist.Normal(0, 10).expand_by([K]).independent(1))"
        ],
        "core_change": "-locs = pyro.sample('locs', dist.Normal(0, 10).reshape([K], extra_event_dims=1)) +locs = pyro.sample('locs', dist.Normal(0, 10).expand_by([K]).independent(1))",
        "core_API": "sample"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "a87f156f..32b97f9d 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "no",
        "comments": "refactor",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FBetaMeasure(Metric):",
            "self._total_sum = torch.zeros(num_classes, device=predictions.device)",
            "",
            "if mask is None:",
            "-            mask = torch.ones_like(gold_labels)",
            "-        mask = mask.to(dtype=torch.bool)",
            "+            mask = torch.ones_like(gold_labels).bool()",
            "gold_labels = gold_labels.float()",
            "",
            "argmax_predictions = predictions.max(dim=-1)[1].float()",
            "-        true_positives = (gold_labels == argmax_predictions) * mask",
            "+        true_positives = (gold_labels == argmax_predictions) & mask",
            "true_positives_bins = gold_labels[true_positives]",
            "",
            "# Watch it:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('&', '&'), position=1, insert_id=20215)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=20216)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=20217)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=to), value='bool')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=to), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=mask))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=mask))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=*, text=*))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 1073,
        "neg_line": [
            "-mask = torch.ones_like(gold_labels)",
            "-mask = mask.to(dtype=torch.bool)",
            "-true_positives = (gold_labels == argmax_predictions) * mask"
        ],
        "pos_line": [
            "+mask = torch.ones_like(gold_labels).bool()",
            "+true_positives = (gold_labels == argmax_predictions) & mask"
        ],
        "core_change": "-mask = torch.ones_like(gold_labels) -mask = mask.to(dtype=torch.bool) +mask = torch.ones_like(gold_labels).bool() -true_positives = (gold_labels == argmax_predictions) * mask +true_positives = (gold_labels == argmax_predictions) & mask",
        "core_API": "zeros"
    },
    {
        "commit_hash": "cd4086b6646e807d07bd81b2a10f1e8adf9543ab",
        "index": "06866477..bc9dc6a1 100644",
        "commit_message": "Fix various docs formatting (#1688)\n\n\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RenyiELBO(ELBO):",
            "surrogate_elbo_particles = torch.stack(surrogate_elbo_particles)",
            "",
            "log_weights = (1. - self.alpha) * elbo_particles",
            "-        log_mean_weight = logsumexp(log_weights, dim=0) - math.log(self.num_particles)",
            "+        log_mean_weight = torch.logsumexp(log_weights, dim=0) - math.log(self.num_particles)",
            "elbo = log_mean_weight.sum().item() / (1. - self.alpha)",
            "",
            "# collect parameters to train from model and guide"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=725642)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=725643)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=725644)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=logsumexp), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1075,
        "neg_line": [
            "-log_mean_weight = logsumexp(log_weights, dim=0) - math.log(self.num_particles)"
        ],
        "pos_line": [
            "+log_mean_weight = torch.logsumexp(log_weights, dim=0) - math.log(self.num_particles)"
        ],
        "core_change": "-log_mean_weight = logsumexp(log_weights, dim=0) - math.log(self.num_particles) +log_mean_weight = torch.logsumexp(log_weights, dim=0) - math.log(self.num_particles)",
        "core_API": "stack"
    },
    {
        "commit_hash": "5218a7c970308a7b807548c05d0f20cb1ab37bdd",
        "index": "6c64f66d..ecbf29d2 100644",
        "commit_message": "Fix compatibility with PyTorch 1.0.x (Fixes #906)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/910\n\nDifferential Revision: D16536532\n\nPulled By: myleott\n\nfbshipit-source-id: 56bb5570e70b5670ad87c64d9dd20c64c1fa9f5c\n\n",
        "file": "fairseq.txt.json",
        "label": "yes",
        "comments": "add API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MaskTokensDataset(BaseWrapperDataset):",
            "if self.mask_whole_words is not None:",
            "mask = np.repeat(mask, word_lens)",
            "new_item = np.full(len(mask), self.pad_idx)",
            "-                new_item[mask] = item[torch.from_numpy(mask)]",
            "+                new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8))]",
            "return torch.from_numpy(new_item)",
            "",
            "# decide unmasking and random replacement"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=219161)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=219162)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=219163)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=219164)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=mask), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=219165)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'astype'), position=2, insert_id=219166)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=219167)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=219168)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=219169)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=219170)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'uint8'), position=2, insert_id=219171)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 1077,
        "neg_line": [
            "-new_item[mask] = item[torch.from_numpy(mask)]"
        ],
        "pos_line": [
            "+new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8))]"
        ],
        "core_change": "-new_item[mask] = item[torch.from_numpy(mask)] +new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8))]",
        "core_API": "repeat"
    },
    {
        "commit_hash": "47d663f0fa97919cce60a64ffa977d9aed6cff45",
        "index": "7948c203..a068ce85 100644",
        "commit_message": "Add docstrings of core modules and methods (#3120)\n\n* Add docstrings of core modules and methods\n\n* Update docs and fix comments\n\n* Complete docstrings\n\n* Resolve comments\n\n* reformat docstrings\n\n* resolve comments\n",
        "file": "mmdetection.txt.json",
        "label": "no",
        "comments": "doc",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class IoUBalancedNegSampler(RandomSampler):",
            "return sampled_inds",
            "",
            "def _sample_neg(self, assign_result, num_expected, **kwargs):",
            "+        \"\"\"Sample negative boxes",
            "+",
            "+        Args:",
            "+            assign_result (:obj:`AssignResult`): The assigned results of boxes.",
            "+            num_expected (int): The number of expected negative samples",
            "+",
            "+        Returns:",
            "+            Tensor or ndarray: sampled indices.",
            "+        \"\"\"",
            "neg_inds = torch.nonzero(assign_result.gt_inds == 0, as_tuple=False)",
            "if neg_inds.numel() != 0:",
            "neg_inds = neg_inds.squeeze(1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=636643)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=636644)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Sample negative boxes\\n\\n        Args:\\n            assign_result (:obj:`AssignResult`): The assigned results of boxes.\\n            num_expected (int): The number of expected negative samples\\n\\n        Returns:\\n            Tensor or ndarray: sampled indices.\\n        \"\"\"'), position=0, insert_id=636645)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 7,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 1078,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Sample negative boxes",
            "+",
            "+Args:",
            "+assign_result (:obj:`AssignResult`): The assigned results of boxes.",
            "+num_expected (int): The number of expected negative samples",
            "+",
            "+Returns:",
            "+Tensor or ndarray: sampled indices.",
            "+\"\"\""
        ],
        "core_change": "+\"\"\"Sample negative boxes + +Args: +assign_result (:obj:`AssignResult`): The assigned results of boxes. +num_expected (int): The number of expected negative samples + +Returns: +Tensor or ndarray: sampled indices. +\"\"\"",
        "core_API": "nonzero"
    },
    {
        "commit_hash": "5e0a5f5a5e6625696eaf67113ab4db955324076c",
        "index": "666308c6..8b93163a 100644",
        "commit_message": "Add a OneTwoMatching distribution (#2697)\n\n* Implement OneTwoMatching.enumerate_support()\n\n* Clarify docs\n\n* Sketch BP computation of Bethe free energy\n\n* Add large-scale smoke test\n\n* Fix some bugs\n\n* Fix sign error\n\n* Simplify\n\n* Add float32 tests\n\n* Clarify; add debug statements\n\n* Relax test, clamp logits\n\n* Optimize clamping logic\n\n* Simplify Z2 computation\n\n* Avoid splitting logits in half\n\n* Add naive sampling algorithm\n\n* Add a more accurate mean field algorithm\n\n* Fix tests\n\n* Simplify\n\n* Change temperature, tighten tests\n\n* Remove unused algorithms\n\n* Update docs and comments\n\n* Numerically stabilize\n\n* Improve accuraccy\n\n* Remove heuristic\n\n* Reduce memory footprint\n\n* Improve accuracy of free energy formula\n\n* Fuse ops\n\n* Implement OneTwoMatching.mode()\n\n* Simplify\n\n* Simplify test\n\n* Simplify and add a test\n\n* Fix docs\n\n* Add Sinkhorn preconditioner\n\n* Add a .mode() method using lap\n\n* Fix in-place op\n\n* Implement .sample() via perturb-and-map\n\n* Remove bad .sample() implementation\n\n* Add link to docs\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "change API call for refactor fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _sample_coalescent_times(leaf_times):",
            "coal_times.append(t)",
            "coal_times.reverse()",
            "",
            "-    return torch.tensor(coal_times)",
            "+    return proto.new_tensor(coal_times)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='proto')",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='new_tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1079,
        "neg_line": [
            "-return torch.tensor(coal_times)"
        ],
        "pos_line": [
            "+return proto.new_tensor(coal_times)"
        ],
        "core_change": "-return torch.tensor(coal_times) +return proto.new_tensor(coal_times)",
        "core_API": "append"
    },
    {
        "commit_hash": "e527fc234859591d7ffaa1be720fec391f8e8c1f",
        "index": "099f9ac..738e753 100644",
        "commit_message": "python2 fixes\n\n",
        "file": "face-alignment.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def transform(point, center, scale, resolution, invert=False):",
            "return new_point.int()",
            "",
            "",
            "-def crop(image, center, scale, resolution=256):",
            "+def crop(image, center, scale, resolution=256.0):",
            "# Crop around the center point",
            "\"\"\" Crops the image around the center. Input is expected to be an np.ndarray \"\"\"",
            "ul = transform([1, 1], center, scale, resolution, True)",
            "br = transform([resolution, resolution], center, scale, resolution, True)",
            "-    pad = math.ceil(torch.norm((ul - br).float()) / 2 - (br[0] - ul[0]) / 2)",
            "+    pad = math.ceil(torch.norm((ul - br).float()) / 2.0 - (br[0] - ul[0]) / 2.0)",
            "if image.ndim > 2:",
            "newDim = np.array([br[1] - ul[1], br[0] - ul[0],",
            "image.shape[2]], dtype=np.int32)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=default_parameter), node=('float', '256.0'), position=2, insert_id=201847)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '2.0'), position=2, insert_id=201848)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '2.0'), position=2, insert_id=201849)",
            "Delete(target_node=ASTNode(type=integer, text=256))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=integer, text=2))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1080,
        "neg_line": [
            "-def crop(image, center, scale, resolution=256):",
            "-pad = math.ceil(torch.norm((ul - br).float()) / 2 - (br[0] - ul[0]) / 2)"
        ],
        "pos_line": [
            "+def crop(image, center, scale, resolution=256.0):",
            "+pad = math.ceil(torch.norm((ul - br).float()) / 2.0 - (br[0] - ul[0]) / 2.0)"
        ],
        "core_change": "-def crop(image, center, scale, resolution=256): +def crop(image, center, scale, resolution=256.0): -pad = math.ceil(torch.norm((ul - br).float()) / 2 - (br[0] - ul[0]) / 2) +pad = math.ceil(torch.norm((ul - br).float()) / 2.0 - (br[0] - ul[0]) / 2.0)",
        "core_API": "int"
    },
    {
        "commit_hash": "04a17f8550c686e339dfd77ccfdbda9ee168b112",
        "index": "5fd5469a2..464e79b06 100755",
        "commit_message": "Doc fixes in preparation for the docstyle PR (#8061)\n\n* Fixes in preparation for doc styling\n\n* More fixes\n\n* Better syntax\n\n* Fixes\n\n* Style\n\n* More fixes\n\n* More fixes\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "doc",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LongformerEmbeddings(nn.Module):",
            "\"\"\"We are provided embeddings directly. We cannot infer which are padded so just generate",
            "sequential position ids.",
            "",
            "-        :param torch.Tensor inputs_embeds:",
            "-        :return torch.Tensor:",
            "+        Args:",
            "+            inputs_embeds: torch.Tensor inputs_embeds:",
            "+",
            "+        Returns: torch.Tensor",
            "\"\"\"",
            "input_shape = inputs_embeds.size()[:-1]",
            "sequence_length = input_shape[1]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"We are provided embeddings directly. We cannot infer which are padded so just generate\nsequential position ids.\n\n        :param torch.Tensor inputs_embeds:\n        :return torch.Tensor:\n\"\"\"), value='\"\"\"We are provided embeddings directly. We cannot infer which are padded so just generate\\nsequential position ids.\\n\\n        Args:\\n            inputs_embeds: torch.Tensor inputs_embeds:\\n\\n        Returns: torch.Tensor\\n\"\"\"')"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 1082,
        "neg_line": [
            "-:param torch.Tensor inputs_embeds:",
            "-:return torch.Tensor:"
        ],
        "pos_line": [
            "+Args:",
            "+inputs_embeds: torch.Tensor inputs_embeds:",
            "+",
            "+Returns: torch.Tensor"
        ],
        "core_change": "-:param torch.Tensor inputs_embeds: -:return torch.Tensor: +Args: +inputs_embeds: torch.Tensor inputs_embeds: + +Returns: torch.Tensor",
        "core_API": "size"
    },
    {
        "commit_hash": "ddebbdc53544c64a1957cf0ec04a34578e9b57ba",
        "index": "a1bc038e..cc8aac66 100644",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "file": "allennlp.txt.json",
        "label": "yes",
        "comments": "add param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CategoricalAccuracy(Metric):",
            "# ith entry in gold_labels points to index (0-num_classes) for ith row in max_predictions",
            "# For each row check if index pointed by gold_label is was 1 or not (among max scored classes)",
            "correct = max_predictions_mask[",
            "-                torch.arange(gold_labels.numel()).long(), gold_labels",
            "+                torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels",
            "].float()",
            "tie_counts = max_predictions_mask.sum(-1)",
            "correct /= tie_counts.float()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=20623)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=20624)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=20625)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=20626)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=20627)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'gold_labels'), position=0, insert_id=20628)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=20629)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=20630)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1083,
        "neg_line": [
            "-torch.arange(gold_labels.numel()).long(), gold_labels"
        ],
        "pos_line": [
            "+torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels"
        ],
        "core_change": "-torch.arange(gold_labels.numel()).long(), gold_labels +torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels",
        "core_API": "arange"
    },
    {
        "commit_hash": "da9584357e327cdb2b8649ed4c27dbb042b7dc34",
        "index": "2a0de2e..754d71a 100644",
        "commit_message": "circleci fixes\n\nSummary:\nMisc fixes.\n\n- most important: the mac image is gone so switch to a newer one.\n- torch.concat is new; was used accidentally\n- remove lpips from testing in meta.yaml as it is breaking the conda test. Better to leave the relevant tests failing in OSS.\n- TypedDict usage is breaking implicitron on Python 3.7.\n\nReviewed By: patricklabatut\n\nDifferential Revision: D38458164\n\nfbshipit-source-id: b16c26453a743b9a771e2a6787b9a4d2a52e41c2\n\n",
        "file": "pytorch3d.txt.json",
        "label": "no",
        "comments": "typo fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DoublePoolBatchSampler(Sampler[List[int]]):",
            "torch.randperm(len(self.first_indices), generator=self.generator)",
            "for _ in range(n_copies)",
            "]",
            "-            i_first = torch.concat(raw_indices)[:num_batches]",
            "+            i_first = torch.cat(raw_indices)[:num_batches]",
            "else:",
            "i_first = torch.randperm(len(self.first_indices), generator=self.generator)",
            "first_indices = [self.first_indices[i] for i in i_first]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=concat), value='cat')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1086,
        "neg_line": [
            "-i_first = torch.concat(raw_indices)[:num_batches]"
        ],
        "pos_line": [
            "+i_first = torch.cat(raw_indices)[:num_batches]"
        ],
        "core_change": "-i_first = torch.concat(raw_indices)[:num_batches] +i_first = torch.cat(raw_indices)[:num_batches]",
        "core_API": "randperm"
    },
    {
        "commit_hash": "ce4a8acc617d2b9665135a569e74186bb79a2457",
        "index": "d8e8dae..d7f4a86 100644",
        "commit_message": "Fix tensorflow implementation\n\n",
        "file": "tf-faster-rcnn.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def proposal_layer_tf(rpn_cls_prob, rpn_bbox_pred, im_info, cfg_key, _feat_strid",
            "proposals = bbox_transform_inv_tf(anchors, rpn_bbox_pred)",
            "proposals = clip_boxes_tf(proposals, im_info[:2])",
            "",
            "-  indices = tf.image.non_max_suppression(rpn_bbox_pred, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)",
            "+  indices = tf.image.non_max_suppression(proposals, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)",
            "",
            "-  boxes = tf.gather(rpn_bbox_pred, indices)",
            "+  boxes = tf.gather(proposals, indices)",
            "boxes = tf.to_float(boxes)",
            "scores = tf.gather(scores, indices)",
            "scores = tf.reshape(scores, shape=(-1, 1))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rpn_bbox_pred), value='proposals')",
            "Update(target_node=ASTNode(type=identifier, text=rpn_bbox_pred), value='proposals')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 1087,
        "neg_line": [
            "-indices = tf.image.non_max_suppression(rpn_bbox_pred, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)",
            "-boxes = tf.gather(rpn_bbox_pred, indices)"
        ],
        "pos_line": [
            "+indices = tf.image.non_max_suppression(proposals, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)",
            "+boxes = tf.gather(proposals, indices)"
        ],
        "core_change": "-indices = tf.image.non_max_suppression(rpn_bbox_pred, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh) +indices = tf.image.non_max_suppression(proposals, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh) -boxes = tf.gather(rpn_bbox_pred, indices) +boxes = tf.gather(proposals, indices)",
        "core_API": "non_max_suppression"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "a37ce680..52a91304 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "doc",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class MultiplexerLayer(Layer):",
            ">>> network = tl.layers.ReshapeLayer(net_mux, shape=(-1, 800), name='reshape')",
            ">>> network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')",
            ">>> # output layer",
            "-    >>> network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')",
            "+    >>> network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')",
            "",
            "\"\"\"",
            "",
            "def __init__(self, layers, name='mux_layer'):",
            "super(MultiplexerLayer, self).__init__(prev_layer=layers, name=name)",
            "+",
            "self.n_inputs = len(layers)",
            "",
            "self.inputs = []",
            "+",
            "for l in layers:",
            "self.inputs.append(l.outputs)",
            "+",
            "try:  # TF1.0",
            "all_inputs = tf.stack(self.inputs, name=name)  # pack means concat a list of tensor in a new dim  # 1.2",
            "except Exception:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('none', 'None'), position=2, insert_id=2634208)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1089,
        "neg_line": [
            "->>> network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')"
        ],
        "pos_line": [
            "+>>> network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')",
            "+",
            "+",
            "+"
        ],
        "core_change": "->>> network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output') +>>> network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output') + + +",
        "core_API": "ReshapeLayer"
    },
    {
        "commit_hash": "84790b78f60221fe6819b5b01ed27c65da12e37a",
        "index": "e185735e..da8b5e2c 100755",
        "commit_message": "re-organize predict/; fix TF incompatibile change of sparse_softmax_cross_entropy_loss\n\n",
        "file": "tensorpack.txt.json",
        "label": "yes",
        "comments": "add param for argument fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            ".GlobalAvgPooling('gap')",
            ".FullyConnected('linear', 1000, nl=tf.identity)())",
            "",
            "-        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)",
            "+        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "loss = tf.reduce_mean(loss, name='xentropy-loss')",
            "",
            "wrong = prediction_incorrect(logits, label, 1, name='wrong-top1')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2307835)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=2307836)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=logits), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2307837)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'logits'), position=2, insert_id=2307838)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'labels'), position=0, insert_id=2307839)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2307840)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=label), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1091,
        "neg_line": [
            "-loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)"
        ],
        "pos_line": [
            "+loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)"
        ],
        "core_change": "-loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label) +loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
        "core_API": "sparse_softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "071bcc81a62b2d7756184518599721d0c52851c7",
        "index": "d42aa889..75a89af4 100644",
        "commit_message": "Fix the //third_party/py/keras/distribute:minimize_loss_test that fails on local.\n\nThe compat.v1.layer is populated from keras/legacy_tf_layers, which need to be imported differently.\n\nPiperOrigin-RevId: 399749315\n\n",
        "file": "keras.txt.json",
        "label": "yes",
        "comments": "change API call for version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def batchnorm_example(optimizer_fn,",
            "for z in range(batch_per_epoch)]).repeat()",
            "",
            "optimizer = optimizer_fn()",
            "-  batchnorm = tf.compat.v1.layers.BatchNormalization(",
            "+  batchnorm = normalization.BatchNormalization(",
            "renorm=renorm, momentum=momentum, fused=False)",
            "-  layer = tf.compat.v1.layers.Dense(1, use_bias=False)",
            "+  layer = core.Dense(1, use_bias=False)",
            "",
            "def model_fn(x):",
            "\"\"\"A model that uses batchnorm.\"\"\""
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=parameters), node=ASTNode(type=ERROR), position=6)",
            "Insert(target_node=ASTNode(type=ERROR), node=('parameters', None), position=3, insert_id=2078617)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='normalization')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='core')",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=2078618)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=1, insert_id=2078619)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parameters))",
            "Delete(target_node=ASTNode(type=identifier, text=compat))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=v1))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=layers))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=compat))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=v1))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=layers))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 1094,
        "neg_line": [
            "-batchnorm = tf.compat.v1.layers.BatchNormalization(",
            "-layer = tf.compat.v1.layers.Dense(1, use_bias=False)"
        ],
        "pos_line": [
            "+batchnorm = normalization.BatchNormalization(",
            "+layer = core.Dense(1, use_bias=False)"
        ],
        "core_change": "-batchnorm = tf.compat.v1.layers.BatchNormalization( +batchnorm = normalization.BatchNormalization( -layer = tf.compat.v1.layers.Dense(1, use_bias=False) +layer = core.Dense(1, use_bias=False)",
        "core_API": "BatchNormalization"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "4531dd00f3..3a4bebd13b 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TestDistributions(unittest.TestCase):",
            "def test_categorical(self):",
            "\"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"",
            "num_samples = 100000",
            "-        logits = tf.placeholder(tf.float32, shape=(None, 10))",
            "+        logits = tf1.placeholder(tf.float32, shape=(None, 10))",
            "z = 8 * (np.random.rand(10) - 0.5)",
            "data = np.tile(z, (num_samples, 1))",
            "c = Categorical(logits, {})  # dummy config dict",
            "sample_op = c.sample()",
            "-        sess = tf.Session()",
            "-        sess.run(tf.global_variables_initializer())",
            "+        sess = tf1.Session()",
            "+        sess.run(tf1.global_variables_initializer())",
            "samples = sess.run(sample_op, feed_dict={logits: data})",
            "counts = np.zeros(10)",
            "for sample in samples:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2145690)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2145691)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2145692)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 10,
        "number": 1096,
        "neg_line": [
            "-logits = tf.placeholder(tf.float32, shape=(None, 10))",
            "-sess = tf.Session()",
            "-sess.run(tf.global_variables_initializer())"
        ],
        "pos_line": [
            "+logits = tf1.placeholder(tf.float32, shape=(None, 10))",
            "+sess = tf1.Session()",
            "+sess.run(tf1.global_variables_initializer())"
        ],
        "core_change": "-logits = tf.placeholder(tf.float32, shape=(None, 10)) +logits = tf1.placeholder(tf.float32, shape=(None, 10)) -sess = tf.Session() -sess.run(tf.global_variables_initializer()) +sess = tf1.Session() +sess.run(tf1.global_variables_initializer())",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "ab47abd6980a60c67ddfec8e81bf9c9b38c166bc",
        "index": "7057804536..338181e925 100644",
        "commit_message": "Docstring fixes and bugs (#5380)\n\n* docstring and framework bug fixes\n\n* docstring fixes and bugs\n\n* docstring failures fixed and bugs removed\n\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "value update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def trace(",
            "[7., 8.]]])",
            ">>> y = ivy.trace(x, offset=1)",
            ">>> print(y)",
            "-    ivy.array([2., 6.])",
            "+    ivy.array([3., 4.])",
            "",
            "With :class:`ivy.NativeArray` inputs:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=2.), value='3.')",
            "Update(target_node=ASTNode(type=float, text=6.), value='4.')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1097,
        "neg_line": [
            "-ivy.array([2., 6.])"
        ],
        "pos_line": [
            "+ivy.array([3., 4.])"
        ],
        "core_change": "-ivy.array([2., 6.]) +ivy.array([3., 4.])",
        "core_API": "trace"
    },
    {
        "commit_hash": "38f7461df3fe51308a62a81e4a0e7770a38d7125",
        "index": "767fa3a2d..f8b2ca8e3 100644",
        "commit_message": "[TFT5, Cache] Add cache to TFT5 (#3772)\n\n* correct gpt2 test inputs\n\n* make style\n\n* delete modeling_gpt2 change in test file\n\n* translate from pytorch\n\n* correct tests\n\n* fix conflicts\n\n* fix conflicts\n\n* fix conflicts\n\n* fix conflicts\n\n* make tensorflow t5 caching work\n\n* make style\n\n* clean reorder cache\n\n* remove unnecessary spaces\n\n* fix test\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "value update",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGPT2ModelTest(TFModelTesterMixin, unittest.TestCase):",
            "output_from_past_slice = output_from_past[:, 0, random_slice_idx]",
            "",
            "# test that outputs are equal for slice",
            "-            tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-12)",
            "+            tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)",
            "",
            "def create_and_check_gpt2_model_attention_mask_past(",
            "self, config, input_ids, input_mask, head_mask, token_type_ids, *args"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1e-12), value='1e-6')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1099,
        "neg_line": [
            "-tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-12)"
        ],
        "pos_line": [
            "+tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)"
        ],
        "core_change": "-tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-12) +tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)",
        "core_API": "assert_near"
    },
    {
        "commit_hash": "9d1a6a2911e0f2543f4b29aed00182bc398b5b9e",
        "index": "294783e1d..79d64d8f5 100644",
        "commit_message": "Various fix including:\n- Forbid several Torchhook\n- Fix or alter tests\n- Fix create_pointer()\nTODO: There is an issue with the .get() function (just run unittest to see...)\n\n",
        "file": "PySyft.txt.json",
        "label": "yes",
        "comments": "add condition check for state fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchHook(object):",
            "",
            "self._hook_torch_module()",
            "",
            "+        if torch.torch_hooked > 0:",
            "+            raise Exception('Torch was already hooked')",
            "+",
            "def _hook_native_tensors_and_variables(self, tensor_type):",
            "\"\"\"Overloading a given tensor_type\"\"\"",
            "# Overload 'special' methods here"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=857801)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=857802)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=857803)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=857804)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=857805)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=857806)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=857807)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '0'), position=2, insert_id=857808)",
            "Insert(target_node=IN(type=block), node=('raise_statement', None), position=0, insert_id=857809)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=857810)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=857811)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch_hooked'), position=2, insert_id=857812)",
            "Insert(target_node=IN(type=raise_statement), node=('raise', 'raise'), position=0, insert_id=857813)",
            "Insert(target_node=IN(type=raise_statement), node=('call', None), position=1, insert_id=857814)",
            "Insert(target_node=IN(type=call), node=('identifier', 'Exception'), position=0, insert_id=857815)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=857816)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=857817)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'Torch was already hooked'\"), position=1, insert_id=857818)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=857819)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 1100,
        "neg_line": [],
        "pos_line": [
            "+if torch.torch_hooked > 0:",
            "+raise Exception('Torch was already hooked')",
            "+"
        ],
        "core_change": "+if torch.torch_hooked > 0: +raise Exception('Torch was already hooked') +",
        "core_API": "_hook_torch_module"
    },
    {
        "commit_hash": "8891193e83b14ebdcfea939fe4b0897177985048",
        "index": "92bda4f81..c0aee8b2d 100644",
        "commit_message": "[Pipeline] fix failing bloom `pipeline` test (#20778)\n\nfix failing `pipeline` test\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TextGenerationPipelineTests(unittest.TestCase, metaclass=PipelineTestCaseM",
            "],",
            ")",
            "",
            "-        # torch_dtype not necessary",
            "+        # torch_dtype will be automatically set to float32 if not provided - check: https://github.com/huggingface/transformers/pull/20602",
            "pipe = pipeline(model=\"hf-internal-testing/tiny-random-bloom\", device_map=\"auto\")",
            "self.assertEqual(pipe.model.device, torch.device(0))",
            "-        self.assertEqual(pipe.model.lm_head.weight.dtype, torch.bfloat16)",
            "+        self.assertEqual(pipe.model.lm_head.weight.dtype, torch.float32)",
            "out = pipe(\"This is a test\")",
            "self.assertEqual(",
            "out,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=bfloat16), value='float32')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 1101,
        "neg_line": [
            "-# torch_dtype not necessary",
            "-self.assertEqual(pipe.model.lm_head.weight.dtype, torch.bfloat16)"
        ],
        "pos_line": [
            "+# torch_dtype will be automatically set to float32 if not provided - check: https://github.com/huggingface/transformers/pull/20602",
            "+self.assertEqual(pipe.model.lm_head.weight.dtype, torch.float32)"
        ],
        "core_change": "-# torch_dtype not necessary +# torch_dtype will be automatically set to float32 if not provided - check: https://github.com/huggingface/transformers/pull/20602 -self.assertEqual(pipe.model.lm_head.weight.dtype, torch.bfloat16) +self.assertEqual(pipe.model.lm_head.weight.dtype, torch.float32)",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "e850d36cab88360177f85d98d73c5efb4d9925ba",
        "index": "= adj._indices()",
        "commit_message": "fixed graclus and following transforms\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SphericalAdj(object):",
            "phi = torch.acos(direction[:, 2]) / PI",
            "spherical = torch.stack([rho, theta, phi], dim=1)",
            "",
            "-        # Modify data and return.",
            "-        data.adj = SparseTensor(index, spherical, torch.Size([n, n, 3]))",
            "-        return data",
            "+        return SparseTensor(index, spherical, torch.Size([n, n, 3]))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=3, insert_id=1088412)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1088413)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=adj))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 1102,
        "neg_line": [
            "-# Modify data and return.",
            "-data.adj = SparseTensor(index, spherical, torch.Size([n, n, 3]))",
            "-return data"
        ],
        "pos_line": [
            "+return SparseTensor(index, spherical, torch.Size([n, n, 3]))"
        ],
        "core_change": "-# Modify data and return. -data.adj = SparseTensor(index, spherical, torch.Size([n, n, 3])) -return data +return SparseTensor(index, spherical, torch.Size([n, n, 3]))",
        "core_API": "acos"
    },
    {
        "commit_hash": "fff62f0ae5af52bdcd7e65b82ecbc5f13bda7715",
        "index": "06f088e87..dc53fb5e3 100644",
        "commit_message": "Fix TPU testing and collect all tests (#11098)\n\nCo-authored-by: Carlos Mochol√≠ <carlossmocholi@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Kaushik B <45285388+kaushikb11@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_devices_auto_choice_mps():",
            "",
            "@pytest.mark.parametrize(",
            "[\"parallel_devices\", \"accelerator\"],",
            "-    [([torch.device(\"cpu\")], \"cuda\"), ([torch.device(\"cuda\", i) for i in range(8)], (\"tpu\"))],",
            "+    [([torch.device(\"cpu\")], \"cuda\"), ([torch.device(\"cuda\", i) for i in range(8)], \"tpu\")],",
            ")",
            "def test_parallel_devices_in_strategy_confilict_with_accelerator(parallel_devices, accelerator):",
            "with pytest.raises(MisconfigurationException, match=r\"parallel_devices set through\"):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=string, text=\"tpu\"), position=3)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1103,
        "neg_line": [
            "-[([torch.device(\"cpu\")], \"cuda\"), ([torch.device(\"cuda\", i) for i in range(8)], (\"tpu\"))],"
        ],
        "pos_line": [
            "+[([torch.device(\"cpu\")], \"cuda\"), ([torch.device(\"cuda\", i) for i in range(8)], \"tpu\")],"
        ],
        "core_change": "-[([torch.device(\"cpu\")], \"cuda\"), ([torch.device(\"cuda\", i) for i in range(8)], (\"tpu\"))], +[([torch.device(\"cpu\")], \"cuda\"), ([torch.device(\"cuda\", i) for i in range(8)], \"tpu\")],",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "a3e55be685fb52ac8c7caafd6835f521bcbcff9d",
        "index": "533ba85b..1b4a69f0 100755",
        "commit_message": "fixed and improved shape handling for exploration\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "change API call for math fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EpsilonDecay(Exploration):",
            "",
            "pred = tf.logical_or(x=(timestep < self.start_timestep),",
            "y=(timestep > self.start_timestep + int(self.timesteps)))",
            "-        return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)",
            "+        return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2234373)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2234374)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2234375)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2234376)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'fill'), position=2, insert_id=2234377)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2234378)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2234379)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2234380)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2234381)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2234382)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dims'), position=0, insert_id=2234383)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2234384)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'shape'), position=2, insert_id=2234385)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'value'), position=0, insert_id=2234386)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2234387)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 1104,
        "neg_line": [
            "-return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)"
        ],
        "pos_line": [
            "+return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ],
        "core_change": "-return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn) +return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))",
        "core_API": "logical_or"
    },
    {
        "commit_hash": "03421e87c0aa212384abddd0a1aef44dafa7e8c3",
        "index": "e76e289..0590cd7 100644",
        "commit_message": "fix https://github.com/facebookresearch/maskrcnn-benchmark/issues/802 (#516)\n\n\n",
        "file": "apex.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import itertools",
            "",
            "import torch",
            "",
            "+def is_cuda_enabled():",
            "+    return torch.version.cuda is not None",
            "+",
            "def get_cuda_version():",
            "return tuple(int(x) for x in torch.version.cuda.split('.'))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=2, insert_id=52872)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=52873)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'is_cuda_enabled'), position=1, insert_id=52874)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=52875)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=52876)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=52877)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=52878)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=1, insert_id=52879)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=52880)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=52881)",
            "Insert(target_node=IN(type=return_statement), node=('comparison_operator', None), position=1, insert_id=52882)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=52883)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=52884)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=52885)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=52886)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=52887)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=52888)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=52889)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=52890)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=52891)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'version'), position=2, insert_id=52892)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 1105,
        "neg_line": [],
        "pos_line": [
            "+def is_cuda_enabled():",
            "+return torch.version.cuda is not None",
            "+"
        ],
        "core_change": "+def is_cuda_enabled(): +return torch.version.cuda is not None +",
        "core_API": "split"
    },
    {
        "commit_hash": "11cef19fccbf343d37925833e09c0e2efbcc1107",
        "index": "8cc3f865b1..3bde08ef75 100644",
        "commit_message": "docstring fix\n\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "doc",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def fft(",
            "*,",
            "norm: Optional[str] = \"backward\",",
            "n: Union[int, Tuple[int]] = None,",
            "-    out: Optional[torch.Tensor] = None",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if not isinstance(dim, int):",
            "raise ivy.exceptions.IvyError(f\"Expecting <class 'int'> instead of {type(dim)}\")",
            "if n is None:",
            "n = x.shape[dim]",
            "-    if n < -len(x.shape) :",
            "+    if n < -len(x.shape):",
            "raise ivy.exceptions.IvyError(",
            "f\"Invalid dim {dim}, expecting ranging\"",
            "\" from {-len(x.shape)} to {len(x.shape)-1}  \"",
            ")",
            "if not isinstance(n, int):",
            "raise ivy.exceptions.IvyError(f\"Expecting <class 'int'> instead of {type(n)}\")",
            "-    if n <= 1 :",
            "+    if n <= 1:",
            "raise ivy.exceptions.IvyError(f\"Invalid data points {n}, expecting more than 1\")",
            "if norm != \"backward\" and norm != \"ortho\" and norm != \"forward\":",
            "raise ivy.exceptions.IvyError(f\"Unrecognized normalization mode {norm}\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=298802)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 1106,
        "neg_line": [
            "-out: Optional[torch.Tensor] = None",
            "-if n < -len(x.shape) :",
            "-if n <= 1 :"
        ],
        "pos_line": [
            "+out: Optional[torch.Tensor] = None,",
            "+if n < -len(x.shape):",
            "+if n <= 1:"
        ],
        "core_change": "-out: Optional[torch.Tensor] = None +out: Optional[torch.Tensor] = None, -if n < -len(x.shape) : +if n < -len(x.shape): -if n <= 1 : +if n <= 1:",
        "core_API": "IvyError"
    },
    {
        "commit_hash": "b51ab2af66d6e6af890542df386ebe78e1b3cb65",
        "index": "c0eeb11daf..6b3c9efa13 100644",
        "commit_message": "[RLlib] Offline Type Annotations (#9676)\n\n* Offline Annotations\n\n* Modifications\n\n* Fixed circular dependencies\n\n* Linter fix\n",
        "file": "ray.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class _QueueRunner(threading.Thread):",
            "self.placeholders = [tf1.placeholder(dtype) for dtype in dtypes]",
            "self.enqueue_op = queue.enqueue(dict(zip(keys, self.placeholders)))",
            "",
            "-    def enqueue(self, batch):",
            "+    def enqueue(self, batch: SampleBatchType):",
            "data = {",
            "self.placeholders[i]: batch[key]",
            "for i, key in enumerate(self.keys)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=3, insert_id=2144780)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=identifier, text=batch), position=0)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=2144781)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=2144782)",
            "Insert(target_node=IN(type=type), node=('identifier', 'SampleBatchType'), position=0, insert_id=2144783)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1108,
        "neg_line": [
            "-def enqueue(self, batch):"
        ],
        "pos_line": [
            "+def enqueue(self, batch: SampleBatchType):"
        ],
        "core_change": "-def enqueue(self, batch): +def enqueue(self, batch: SampleBatchType):",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "ba1e08a858f839a41866c0b400fbcbc22778dd2b",
        "index": "df89df21..69124635 100644",
        "commit_message": "Add xfailing jit tests to many anchor examples (#1296)\n\n* Fix Categorical.enumerate_support to make JitTraceEnum_ELBO work\n* Add xfailing examples that use --jit\n* Enable jit in most SVI examples\n",
        "file": "pyro.txt.json",
        "label": "no",
        "comments": "add argument for param",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "if __name__ == '__main__':",
            "help='number of steps between parameter saves')",
            "parser.add_argument('--cuda', action='store_true', default=False,",
            "help='use cuda')",
            "+    parser.add_argument('--jit', action='store_true', default=False,",
            "+                        help='use PyTorch jit')",
            "parser.add_argument('-t', '--model-steps', type=int, default=3,",
            "help='number of time steps')",
            "parser.add_argument('--rnn-hidden-size', type=int, default=256,"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1608355)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1608356)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1608357)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1608358)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'parser'), position=0, insert_id=1608359)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1608360)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'add_argument'), position=2, insert_id=1608361)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1608362)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'--jit'\"), position=1, insert_id=1608363)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1608364)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1608365)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=1608366)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1608367)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=6, insert_id=1608368)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=7, insert_id=1608369)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=8, insert_id=1608370)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'action'), position=0, insert_id=1608371)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1608372)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'store_true'\"), position=2, insert_id=1608373)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'default'), position=0, insert_id=1608374)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1608375)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1608376)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'help'), position=0, insert_id=1608377)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1608378)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'use PyTorch jit'\"), position=2, insert_id=1608379)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 1109,
        "neg_line": [],
        "pos_line": [
            "+parser.add_argument('--jit', action='store_true', default=False,",
            "+help='use PyTorch jit')"
        ],
        "core_change": "+parser.add_argument('--jit', action='store_true', default=False, +help='use PyTorch jit')",
        "core_API": "add_argument"
    },
    {
        "commit_hash": "94373fefce87226db29f8270eca75408aa085996",
        "index": "da99bdfa..305822d7 100644",
        "commit_message": "Fix various typos in error messages.\n\nPiperOrigin-RevId: 410160508\n\n",
        "file": "keras.txt.json",
        "label": "no",
        "comments": "format",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Sequential(functional.Functional):",
            "# invalid use case of Sequential, but we tolerate it for backwards",
            "# compatibility.",
            "self._use_legacy_deferred_behavior = True",
            "-        self._build_input_shape = tf.nest.map_structure(_get_shape_tuple, inputs)",
            "+        self._build_input_shape = tf.nest.map_structure(",
            "+            _get_shape_tuple, inputs)",
            "if tf.__internal__.tf2.enabled():",
            "logging.warning('Layers in a Sequential model should only have a '",
            "-                          'single input tensor, but we receive a %s input: %s'",
            "-                          '\\nConsider rewriting this model with the Functional '",
            "-                          'API.' % (type(inputs), inputs))",
            "+                          f'single input tensor. Received: inputs={inputs}. '",
            "+                          'Consider rewriting this model with the Functional '",
            "+                          'API.')",
            "else:",
            "self._build_graph_network_for_inferred_shape(inputs.shape, inputs.dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=concatenated_string), position=1)",
            "Update(target_node=ASTNode(type=string, text='single input tensor, but we receive a %s input: %s'), value=\"f'single input tensor. Received: inputs={inputs}. '\")",
            "Update(target_node=ASTNode(type=string, text='\\nConsider rewriting this model with the Functional '), value=\"'Consider rewriting this model with the Functional '\")",
            "Delete(target_node=ASTNode(type=%, text=%))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=type))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=inputs))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=inputs))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 5,
        "minus_line": 4,
        "AST_diff_line": 16,
        "number": 1110,
        "neg_line": [
            "-self._build_input_shape = tf.nest.map_structure(_get_shape_tuple, inputs)",
            "-'single input tensor, but we receive a %s input: %s'",
            "-'\\nConsider rewriting this model with the Functional '",
            "-'API.' % (type(inputs), inputs))"
        ],
        "pos_line": [
            "+self._build_input_shape = tf.nest.map_structure(",
            "+_get_shape_tuple, inputs)",
            "+f'single input tensor. Received: inputs={inputs}. '",
            "+'Consider rewriting this model with the Functional '",
            "+'API.')"
        ],
        "core_change": "-self._build_input_shape = tf.nest.map_structure(_get_shape_tuple, inputs) +self._build_input_shape = tf.nest.map_structure( +_get_shape_tuple, inputs) -'single input tensor, but we receive a %s input: %s' -'\\nConsider rewriting this model with the Functional ' -'API.' % (type(inputs), inputs)) +f'single input tensor. Received: inputs={inputs}. ' +'Consider rewriting this model with the Functional ' +'API.')",
        "core_API": "map_structure"
    },
    {
        "commit_hash": "e850d36cab88360177f85d98d73c5efb4d9925ba",
        "index": "= adj._indices()",
        "commit_message": "fixed graclus and following transforms\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CartesianAdj(object):",
            "cartesian *= 1 / (2 * cartesian.abs().max())",
            "cartesian += 0.5",
            "",
            "-        # Modify data and return.",
            "-        data.adj = SparseTensor(index, cartesian, torch.Size([n, n, dim]))",
            "-        return data",
            "+        return SparseTensor(index, cartesian, torch.Size([n, n, dim]))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=3, insert_id=1088396)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1088397)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=adj))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 1111,
        "neg_line": [
            "-# Modify data and return.",
            "-data.adj = SparseTensor(index, cartesian, torch.Size([n, n, dim]))",
            "-return data"
        ],
        "pos_line": [
            "+return SparseTensor(index, cartesian, torch.Size([n, n, dim]))"
        ],
        "core_change": "-# Modify data and return. -data.adj = SparseTensor(index, cartesian, torch.Size([n, n, dim])) -return data +return SparseTensor(index, cartesian, torch.Size([n, n, dim]))",
        "core_API": "abs"
    },
    {
        "commit_hash": "af8425b749439eec00a886e635827a65b302a54d",
        "index": "50e3232d8..1bdbee870 100644",
        "commit_message": "Refactoring the TF activations functions (#7150)\n\n* Refactoring the activations functions into a common file\n\n* Apply style\n\n* remove unused import\n\n* fix tests\n\n* Fix tests.\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "customize method",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class TFXLNetFeedForward(tf.keras.layers.Layer):",
            ")",
            "self.dropout = tf.keras.layers.Dropout(config.dropout)",
            "if isinstance(config.ff_activation, str):",
            "-            self.activation_function = ACT2FN[config.ff_activation]",
            "+            self.activation_function = get_tf_activation(config.ff_activation)",
            "else:",
            "self.activation_function = config.ff_activation"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2555927)",
            "Update(target_node=ASTNode(type=identifier, text=ACT2FN), value='get_tf_activation')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=ACT2FN), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2555928)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2555929)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2555930)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1113,
        "neg_line": [
            "-self.activation_function = ACT2FN[config.ff_activation]"
        ],
        "pos_line": [
            "+self.activation_function = get_tf_activation(config.ff_activation)"
        ],
        "core_change": "-self.activation_function = ACT2FN[config.ff_activation] +self.activation_function = get_tf_activation(config.ff_activation)",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "7977bc56439dcea2ee0c4f27b7ae7919ab620151",
        "index": "65439e4cc..80c0a1085 100644",
        "commit_message": "fix\n\n",
        "file": "espnet.txt.json",
        "label": "yes",
        "comments": "remove param for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Stft(torch.nn.Module, InversibleInterface):",
            "pad = self.n_fft // 2",
            "ilens = ilens + 2 * pad",
            "",
            "-            olens = (",
            "-                torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")",
            "-                + 1",
            "-            )",
            "+            olens = torch.div((ilens - self.n_fft), self.hop_length) + 1",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=rounding_mode))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"floor\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 9,
        "number": 1114,
        "neg_line": [
            "-olens = (",
            "-torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")",
            "-+ 1",
            "-)"
        ],
        "pos_line": [
            "+olens = torch.div((ilens - self.n_fft), self.hop_length) + 1"
        ],
        "core_change": "-olens = ( -torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\") -+ 1 -) +olens = torch.div((ilens - self.n_fft), self.hop_length) + 1",
        "core_API": "div"
    },
    {
        "commit_hash": "f043b4d934a9454b5db32e7ea7331307506a1a6f",
        "index": "6e59653c..87a25a4e 100644",
        "commit_message": "feat: update openclip loader (#782)\n\n* feat: update openclip loader\n\nto support independent download process and make precision adapted to device to solve VRAM issue\n\n* fix: changes for comments\n\n* fix: error\n\n* fix: error\n\n* fix: use openai loader\n\n* fix: address comments\n\n* fix: openclip compatable\n\nCo-authored-by: numb3r3 <wangfelix87@gmail.com>\n",
        "file": "clip-as-service.txt.json",
        "label": "no",
        "comments": "def",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultilingualCLIPModel(CLIPModel):",
            "input_ids=input_ids, attention_mask=attention_mask, **kwargs",
            ")",
            "",
            "-    def encode_image(self, pixel_values: torch.Tensor, **kwargs):",
            "+    def encode_image(self, pixel_values: torch.Tensor):",
            "return self._model.encode_image(pixel_values)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=**, text=**))",
            "Delete(target_node=ASTNode(type=identifier, text=kwargs))",
            "Delete(target_node=ASTNode(type=dictionary_splat_pattern))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1117,
        "neg_line": [
            "-def encode_image(self, pixel_values: torch.Tensor, **kwargs):"
        ],
        "pos_line": [
            "+def encode_image(self, pixel_values: torch.Tensor):"
        ],
        "core_change": "-def encode_image(self, pixel_values: torch.Tensor, **kwargs): +def encode_image(self, pixel_values: torch.Tensor):",
        "core_API": "encode_image"
    },
    {
        "commit_hash": "a4fe3a3fea83a260ad22f45e928d0eec6862fadc",
        "index": "5bd220ab..a7600254 100644",
        "commit_message": "[Feat] Enabled Torch1.5.1 cpu support (#796)\n\n* Added py151 support\n\n* Enabled 1.5.1 CI\n\n* Typo fix\n\n* Fixed Equalize\n\n* Update setup.py\n\n* Bug fix\n\n* typo fix\n\n* Fixed mypy check\n\n* Update tests_cpu_versions.yml\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _scale_channel(im: torch.Tensor) -> torch.Tensor:",
            "# and then normalization by step.",
            "lut = (torch.cumsum(histo, 0) + (step // 2)) // step",
            "# Shift lut, prepending with 0.",
            "-        lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])",
            "+        lut = torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])",
            "# Clip the counts to be in range.  This is done",
            "# in the C code for image.point.",
            "return torch.clamp(lut, 0, 255)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=433550)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=433551)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=433552)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=433553)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=433554)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'lut'), position=0, insert_id=433555)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=433556)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=433557)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1118,
        "neg_line": [
            "-lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])"
        ],
        "pos_line": [
            "+lut = torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])"
        ],
        "core_change": "-lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]]) +lut = torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])",
        "core_API": "cumsum"
    },
    {
        "commit_hash": "e6252b2442ab116a223341f14f6d8ee2d0ea00f2",
        "index": "b153857397..082799ce91 100644",
        "commit_message": "fixes for transpilation (#7102)\n\nadd private classes for transpiling operations and minor fix for compilation\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "typo fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def scatter_nd(",
            "*[",
            "torch.range(0, s - 1)",
            "if idx == slice(None, None, None)",
            "-                                else torch.Tensor([idx % s])",
            "+                                else torch.tensor([idx % s])",
            "for s, idx in zip(shape, index)",
            "],",
            "indexing=\"xy\","
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1121,
        "neg_line": [
            "-else torch.Tensor([idx % s])"
        ],
        "pos_line": [
            "+else torch.tensor([idx % s])"
        ],
        "core_change": "-else torch.Tensor([idx % s]) +else torch.tensor([idx % s])",
        "core_API": "range"
    },
    {
        "commit_hash": "7320d95d98f51ba056cbfe58f08146420c7ec8af",
        "index": "78e5cc81c..4f23d9793 100644",
        "commit_message": "[Swin, Swinv2] Fix attn_mask dtype (#18803)\n\n* Add dtype\n\n* Fix Swinv2 as well\n\nCo-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DonutSwinLayer(nn.Module):",
            "# partition windows",
            "hidden_states_windows = window_partition(shifted_hidden_states, self.window_size)",
            "hidden_states_windows = hidden_states_windows.view(-1, self.window_size * self.window_size, channels)",
            "-        attn_mask = self.get_attn_mask(height_pad, width_pad)",
            "+        attn_mask = self.get_attn_mask(height_pad, width_pad, dtype=hidden_states.dtype)",
            "if attn_mask is not None:",
            "attn_mask = attn_mask.to(hidden_states_windows.device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1192942)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1192943)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1192944)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1192945)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1192946)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hidden_states'), position=0, insert_id=1192947)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1192948)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1192949)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1122,
        "neg_line": [
            "-attn_mask = self.get_attn_mask(height_pad, width_pad)"
        ],
        "pos_line": [
            "+attn_mask = self.get_attn_mask(height_pad, width_pad, dtype=hidden_states.dtype)"
        ],
        "core_change": "-attn_mask = self.get_attn_mask(height_pad, width_pad) +attn_mask = self.get_attn_mask(height_pad, width_pad, dtype=hidden_states.dtype)",
        "core_API": "view"
    },
    {
        "commit_hash": "5a133aebcd37b68bdb6bc01d5e775523b220ba63",
        "index": "9cbe98fb7d..7f00a924a2 100644",
        "commit_message": "fix `unravel_index` test defintion\n",
        "file": "ivy.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def max_value_as_shape_prod(draw):",
            "",
            "",
            "@handle_test(",
            "-    fn_tree=\"functional.ivy.experimental.nanmean\",",
            "+    fn_tree=\"functional.ivy.experimental.unravel_index\",",
            "dtype_x_shape=max_value_as_shape_prod(),",
            "test_gradients=st.just(False),",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"functional.ivy.experimental.nanmean\"), value='\"functional.ivy.experimental.unravel_index\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1125,
        "neg_line": [
            "-fn_tree=\"functional.ivy.experimental.nanmean\","
        ],
        "pos_line": [
            "+fn_tree=\"functional.ivy.experimental.unravel_index\","
        ],
        "core_change": "-fn_tree=\"functional.ivy.experimental.nanmean\", +fn_tree=\"functional.ivy.experimental.unravel_index\",",
        "core_API": "just"
    },
    {
        "commit_hash": "31ffa3bb1c89d3ed2e7964f7ccfec73880896b8d",
        "index": "f9d721b..a99797e 100644",
        "commit_message": "Fix mode regularized gan loss\n\n",
        "file": "generative-models.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "for it in range(1000000):",
            "D_reg = D(G_sample_reg)",
            "",
            "mse = torch.sum((X - G_sample_reg)**2, 1)",
            "-    E_loss = torch.mean(lam1 * mse + lam2 * D_reg)",
            "+    E_loss = torch.mean(lam1 * mse + lam2 * log(D_reg))",
            "",
            "E_loss.backward()",
            "E_solver.step()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=244587)",
            "Insert(target_node=IN(type=call), node=('identifier', 'log'), position=0, insert_id=244588)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=244589)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=244590)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=D_reg), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=244591)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1126,
        "neg_line": [
            "-E_loss = torch.mean(lam1 * mse + lam2 * D_reg)"
        ],
        "pos_line": [
            "+E_loss = torch.mean(lam1 * mse + lam2 * log(D_reg))"
        ],
        "core_change": "-E_loss = torch.mean(lam1 * mse + lam2 * D_reg) +E_loss = torch.mean(lam1 * mse + lam2 * log(D_reg))",
        "core_API": "sum"
    },
    {
        "commit_hash": "c4e1586db8ef6b4102016fc5cb038940fde45325",
        "index": "bb3996ec5..f7aea7851 100755",
        "commit_message": "Fix VisualBert Embeddings (#13017)\n\n\n",
        "file": "transformers.txt.json",
        "label": "yes",
        "comments": "change param for resource fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VisualBertEmbeddings(nn.Module):",
            "inputs_embeds = self.word_embeddings(input_ids)",
            "",
            "if token_type_ids is None:",
            "-            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device)",
            "+            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)",
            "",
            "token_type_embeddings = self.token_type_embeddings(token_type_ids)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=input_embeds), value='position_ids')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1127,
        "neg_line": [
            "-token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device)"
        ],
        "pos_line": [
            "+token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)"
        ],
        "core_change": "-token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device) +token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)",
        "core_API": "word_embeddings"
    },
    {
        "commit_hash": "a6f73a18cb14ad598bdb57581524b334784426ff",
        "index": "325791c7..50c61d67 100644",
        "commit_message": "Fix BCELoss adressing  #1192\n\n",
        "file": "TTS.txt.json",
        "label": "yes",
        "comments": "remove API call for type fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BCELossMasked(nn.Module):",
            "Returns:",
            "loss: An average loss value in range [0, 1] masked by the length.",
            "\"\"\"",
            "-        # mask: (batch, max_len, 1)",
            "target.requires_grad = False",
            "if length is not None:",
            "-            mask = sequence_mask(sequence_length=length, max_len=target.size(1)).float()",
            "-            x = x * mask",
            "-            target = target * mask",
            "+            # mask: (batch, max_len, 1)",
            "+            mask = sequence_mask(sequence_length=length, max_len=target.size(1))",
            "num_items = mask.sum()",
            "+            loss = functional.binary_cross_entropy_with_logits(x.masked_select(mask), target.masked_select(mask), pos_weight=self.pos_weight, reduction=\"sum\")",
            "else:",
            "+            loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")",
            "num_items = torch.numel(x)",
            "-        loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")",
            "loss = loss / num_items",
            "return loss"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1252753)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1252754)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=5, insert_id=1252755)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'loss'), position=0, insert_id=1252756)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1252757)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1252758)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1252759)",
            "Update(target_node=ASTNode(type=identifier, text=loss), value='num_items')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1252760)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1252761)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'numel'), position=2, insert_id=1252762)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1252763)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x'), position=1, insert_id=1252764)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1252765)"
        ],
        "plus_line": 4,
        "minus_line": 5,
        "AST_diff_line": 15,
        "number": 1128,
        "neg_line": [
            "-# mask: (batch, max_len, 1)",
            "-mask = sequence_mask(sequence_length=length, max_len=target.size(1)).float()",
            "-x = x * mask",
            "-target = target * mask",
            "-loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")"
        ],
        "pos_line": [
            "+# mask: (batch, max_len, 1)",
            "+mask = sequence_mask(sequence_length=length, max_len=target.size(1))",
            "+loss = functional.binary_cross_entropy_with_logits(x.masked_select(mask), target.masked_select(mask), pos_weight=self.pos_weight, reduction=\"sum\")",
            "+loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")"
        ],
        "core_change": "-# mask: (batch, max_len, 1) -mask = sequence_mask(sequence_length=length, max_len=target.size(1)).float() -x = x * mask -target = target * mask +# mask: (batch, max_len, 1) +mask = sequence_mask(sequence_length=length, max_len=target.size(1)) +loss = functional.binary_cross_entropy_with_logits(x.masked_select(mask), target.masked_select(mask), pos_weight=self.pos_weight, reduction=\"sum\") +loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\") -loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")",
        "core_API": "size"
    },
    {
        "commit_hash": "77534faf1a43447bf66b243dbdee792baca7d6c6",
        "index": "69b2812d..1f858c39 100644",
        "commit_message": "summaries updated and fixed\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "change API call for refactor fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PrioritizedReplay(Memory):",
            "))",
            "",
            "with tf.control_dependencies(control_inputs=assignments):",
            "-            return tf.no_op()",
            "+            return util.no_operation()",
            "",
            "# These are not supported for prioritized replay currently.",
            "def tf_retrieve_episodes(self, n):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='util')",
            "Update(target_node=ASTNode(type=identifier, text=no_op), value='no_operation')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1129,
        "neg_line": [
            "-return tf.no_op()"
        ],
        "pos_line": [
            "+return util.no_operation()"
        ],
        "core_change": "-return tf.no_op() +return util.no_operation()",
        "core_API": "control_dependencies"
    },
    {
        "commit_hash": "85fd39a74d4c8f3e671c23901099db0bd2c7c085",
        "index": "c28209f1..8cd2ad80 100644",
        "commit_message": "[Retiarii] Serializer and experiment status fixes (#3421)\n\n\n",
        "file": "nni.txt.json",
        "label": "no",
        "comments": "no API",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def get_module_name(cls):",
            "f'please launch the experiment under the directory where \"{main_file_path.name}\" is located.')",
            "module_name = main_file_path.stem",
            "break",
            "+    if module_name == '__main__':",
            "+        warnings.warn('Callstack exhausted but main module still not found. This will probably cause issues that the '",
            "+                      'function/class cannot be imported.')",
            "",
            "# NOTE: this is hacky. As torchscript retrieves LSTM's source code to do something.",
            "# to make LSTM's source code can be found, we should assign original LSTM's __module__ to",
            "# the wrapped LSTM's __module__",
            "# TODO: find out all the modules that have the same requirement as LSTM",
            "-    if f'{cls.__module__}.{cls.__name__}' == 'torch.nn.modules.rnn.LSTM':",
            "-        module_name = cls.__module__",
            "+    if f'{cls_or_func.__module__}.{cls_or_func.__name__}' == 'torch.nn.modules.rnn.LSTM':",
            "+        module_name = cls_or_func.__module__",
            "",
            "return module_name",
            "",
            "",
            "-def get_full_class_name(cls, relocate_module=False):",
            "+def get_importable_name(cls, relocate_module=False):",
            "module_name = get_module_name(cls) if relocate_module else cls.__module__",
            "return module_name + '.' + cls.__name__"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=1437408)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1437409)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1437410)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1437411)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1437412)",
            "Update(target_node=ASTNode(type=identifier, text=get_full_class_name), value='get_importable_name')",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'module_name'), position=0, insert_id=1437413)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1437414)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', \"'__main__'\"), position=2, insert_id=1437415)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1437416)",
            "Update(target_node=ASTNode(type=string, text=f'{cls.__module__}.{cls.__name__}'), value=\"f'{cls_or_func.__module__}.{cls_or_func.__name__}'\")",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1437417)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1437418)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1437419)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'warnings'), position=0, insert_id=1437420)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1437421)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'warn'), position=2, insert_id=1437422)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1437423)",
            "Insert(target_node=IN(type=argument_list), node=('concatenated_string', None), position=1, insert_id=1437424)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1437425)",
            "Update(target_node=ASTNode(type=identifier, text=cls), value='cls_or_func')",
            "Insert(target_node=IN(type=concatenated_string), node=('string', \"'Callstack exhausted but main module still not found. This will probably cause issues that the '\"), position=0, insert_id=1437426)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', \"'function/class cannot be imported.'\"), position=1, insert_id=1437427)"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 1130,
        "neg_line": [
            "-if f'{cls.__module__}.{cls.__name__}' == 'torch.nn.modules.rnn.LSTM':",
            "-module_name = cls.__module__",
            "-def get_full_class_name(cls, relocate_module=False):"
        ],
        "pos_line": [
            "+if module_name == '__main__':",
            "+warnings.warn('Callstack exhausted but main module still not found. This will probably cause issues that the '",
            "+'function/class cannot be imported.')",
            "+if f'{cls_or_func.__module__}.{cls_or_func.__name__}' == 'torch.nn.modules.rnn.LSTM':",
            "+module_name = cls_or_func.__module__",
            "+def get_importable_name(cls, relocate_module=False):"
        ],
        "core_change": "+if module_name == '__main__': +warnings.warn('Callstack exhausted but main module still not found. This will probably cause issues that the ' +'function/class cannot be imported.') -if f'{cls.__module__}.{cls.__name__}' == 'torch.nn.modules.rnn.LSTM': -module_name = cls.__module__ +if f'{cls_or_func.__module__}.{cls_or_func.__name__}' == 'torch.nn.modules.rnn.LSTM': +module_name = cls_or_func.__module__ -def get_full_class_name(cls, relocate_module=False): +def get_importable_name(cls, relocate_module=False):",
        "core_API": "warn"
    },
    {
        "commit_hash": "1b8ca8ece592faf0cdf15f3531fa0bc4929ded36",
        "index": "c6566c28..215bffb0 100644",
        "commit_message": "fix spelling mistake (#749)\n\n\n",
        "file": "DeepSpeed.txt.json",
        "label": "no",
        "comments": "doc update",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OnebitAdam(torch.optim.Optimizer):",
            "self.adam_freeze_key = False",
            "self.initialize = True",
            "print(",
            "-                f\"Finished the initialization step at rant {torch.distributed.get_rank()}\"",
            "+                f\"Finished the initialization step at rank {torch.distributed.get_rank()}\"",
            ")",
            "return loss"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"Finished the initialization step at rant {torch.distributed.get_rank()}\"), value='f\"Finished the initialization step at rank {torch.distributed.get_rank()}\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1131,
        "neg_line": [
            "-f\"Finished the initialization step at rant {torch.distributed.get_rank()}\""
        ],
        "pos_line": [
            "+f\"Finished the initialization step at rank {torch.distributed.get_rank()}\""
        ],
        "core_change": "-f\"Finished the initialization step at rant {torch.distributed.get_rank()}\" +f\"Finished the initialization step at rank {torch.distributed.get_rank()}\"",
        "core_API": "get_rank"
    },
    {
        "commit_hash": "555bf4d777211e5a4d03d13ebaca2970250378a9",
        "index": "dff08f8..510255f 100755",
        "commit_message": "üèã  Fix nan issue in training fastspeech/fastspeech2.\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "yes",
        "comments": "add param for type fix",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class TFFastSpeech(tf.keras.Model):",
            "== config.decoder_self_attention_params.hidden_size,",
            "name=\"decoder\",",
            ")",
            "-        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name=\"mel_before\")",
            "-        self.postnet = TFTacotronPostnet(config=config, name=\"postnet\")",
            "+        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, dtype=tf.float32, name=\"mel_before\")",
            "+        self.postnet = TFTacotronPostnet(config=config, dtype=tf.float32, name=\"postnet\")",
            "",
            "self.setup_inference_fn()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2548490)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2548491)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2548492)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2548493)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2548494)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2548495)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2548496)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2548497)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2548498)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2548499)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2548500)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2548501)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2548502)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2548503)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2548504)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2548505)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 1132,
        "neg_line": [
            "-self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name=\"mel_before\")",
            "-self.postnet = TFTacotronPostnet(config=config, name=\"postnet\")"
        ],
        "pos_line": [
            "+self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, dtype=tf.float32, name=\"mel_before\")",
            "+self.postnet = TFTacotronPostnet(config=config, dtype=tf.float32, name=\"postnet\")"
        ],
        "core_change": "-self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name=\"mel_before\") -self.postnet = TFTacotronPostnet(config=config, name=\"postnet\") +self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, dtype=tf.float32, name=\"mel_before\") +self.postnet = TFTacotronPostnet(config=config, dtype=tf.float32, name=\"postnet\")",
        "core_API": "Dense"
    },
    {
        "commit_hash": "ef78db3732d459848d865947e4373fb13628d52b",
        "index": "ad89422b..7306f6bf 100644",
        "commit_message": "Fix numerical instability in `GeneralConv` and `neighbor_sample` tests (#4754)\n\n* fix full testing\n\n* changelog\n\n* update\n\n* changelog\n\n* reset\n\n* update\n",
        "file": "pytorch_geometric.txt.json",
        "label": "no",
        "comments": "asset",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_neighbor_sampler_on_cora(get_dataset):",
            "_, n_id, adjs = next(iter(loader))",
            "out1 = model.batch(data.x[n_id], adjs)",
            "out2 = model.full(data.x, data.edge_index)[batch]",
            "-    assert torch.allclose(out1, out2)",
            "+    assert torch.allclose(out1, out2, atol=1e-7)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=979498)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=979499)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=979500)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=979501)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-7'), position=2, insert_id=979502)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1134,
        "neg_line": [
            "-assert torch.allclose(out1, out2)"
        ],
        "pos_line": [
            "+assert torch.allclose(out1, out2, atol=1e-7)"
        ],
        "core_change": "-assert torch.allclose(out1, out2) +assert torch.allclose(out1, out2, atol=1e-7)",
        "core_API": "batch"
    },
    {
        "commit_hash": "c84315ec35431bcfe4abcbcbdc589ee8adfc5e79",
        "index": "6f4f79e0f..7596298cc 100644",
        "commit_message": "model fixes + ipnb fixes\n\n",
        "file": "transformers.txt.json",
        "label": "no",
        "comments": "for loop",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "",
            "model.eval()",
            "with open(args.output_file, \"w\", encoding='utf-8') as writer:",
            "-        for input_ids, input_mask, segment_ids, example_indices in eval_dataloader:",
            "+        for input_ids, input_mask, example_indices in eval_dataloader:",
            "input_ids = input_ids.to(device)",
            "input_mask = input_mask.float().to(device)",
            "-            segment_ids = segment_ids.to(device)",
            "",
            "-            all_encoder_layers, _ = model(input_ids, segment_ids, input_mask)",
            "+            all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)",
            "",
            "for enc_layers, example_index in zip(all_encoder_layers, example_indices):",
            "feature = features[example_index.item()]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1252118)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=6, insert_id=1252119)",
            "Update(target_node=ASTNode(type=identifier, text=segment_ids), value='token_type_ids')",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=segment_ids), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1252120)",
            "Insert(target_node=IN(type=keyword_argument), node=('none', 'None'), position=2, insert_id=1252121)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'attention_mask'), position=0, insert_id=1252122)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1252123)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=input_mask), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=segment_ids))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=segment_ids))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=segment_ids))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 1136,
        "neg_line": [
            "-for input_ids, input_mask, segment_ids, example_indices in eval_dataloader:",
            "-segment_ids = segment_ids.to(device)",
            "-all_encoder_layers, _ = model(input_ids, segment_ids, input_mask)"
        ],
        "pos_line": [
            "+for input_ids, input_mask, example_indices in eval_dataloader:",
            "+all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)"
        ],
        "core_change": "-for input_ids, input_mask, segment_ids, example_indices in eval_dataloader: +for input_ids, input_mask, example_indices in eval_dataloader: -segment_ids = segment_ids.to(device) -all_encoder_layers, _ = model(input_ids, segment_ids, input_mask) +all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)",
        "core_API": "eval"
    },
    {
        "commit_hash": "a98fe2207d28d69be2d275e2473b165a47337a35",
        "index": "8d3452a7..376d7f14 100755",
        "commit_message": "Fix for restore from checkpoint\n\nResets the buffer index upon restore, ensuring the model is consistent\nwith the agent's behavior - starting a new episode.\n\nThis fixes the following error which occurs on attempt to restore:\n    InvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape + params.shape[1:], got updates.shape [30,1], indices.shape [21], params.shape [100,1]\n  \t[[Node: ppo/observe-timestep/store/ScatterUpdate = ScatterUpdate[T=DT_FLOAT, Tindices=DT_INT32, _class=[\"loc:@ppo/initialize/latest/initialize/state-state\"], use_locking=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ppo/initialize/latest/initialize/state-state, ppo/observe-timestep/store/mod, ppo/strided_slice, ^ppo/observe-timestep/store/AssignSub)]]\n\n",
        "file": "tensorforce.txt.json",
        "label": "yes",
        "comments": "add API call for state fix",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(object):",
            "#     raise TensorForceError(\"Invalid model directory/file.\")",
            "",
            "self.saver.restore(sess=self.session, save_path=file)",
            "+        self.session.run(self.buffer_index_reset_op)",
            "",
            "def get_components(self):",
            "\"\"\""
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2235174)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2235175)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2235176)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2235177)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2235178)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2235179)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'run'), position=2, insert_id=2235180)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2235181)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2235182)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2235183)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2235184)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2235185)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'session'), position=2, insert_id=2235186)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2235187)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2235188)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'buffer_index_reset_op'), position=2, insert_id=2235189)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 1140,
        "neg_line": [],
        "pos_line": [
            "+self.session.run(self.buffer_index_reset_op)"
        ],
        "core_change": "+self.session.run(self.buffer_index_reset_op)",
        "core_API": "restore"
    },
    {
        "commit_hash": "c26e81a22038a84a054c630a4435bcdb995c9e21",
        "index": "2bfc37d0..b2c6f567 100644",
        "commit_message": "enh: Implements `InferenceModule` as a pipelined module with separate preprocessor, predictor, and postprocessor modules (#2105)\n\n* Adding inference pipeline with seperate pre-processing, predict and post-processing modules\n\n* Update to flatten outputs from predict consistent to support triton\n\n* inference module refactor\n\n* add back InferenceLudwigModel\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* unify modules into inference.py\n\n* cleaned up inaccurate documentation\n\n* clean up\n\n* clean up type hints and update InferenceLudwigModel\n\n* clean up type hint; passes test_torchscript.py\n\n* added typing to inference module for clarity\n\n* remove inference_module_file_name constant\n\n* unified predict module with postproc\n\n* removed InferencePredictor entirely\n\n* add back the old inference module\n\n* add back training set metadata\n\n* revert change to predict module, move feature filtering to postproc\n\n* cleanup inference_module_v0\n\n* cleanup\n\n* adds device placement to InferenceLudwigModel\n\n* adds ability to save/load torchscript on particular devices\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* allows saving torchscript with dict of devices from api.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* correct device inputs\n\n* refactor to expose inference stages (prep for triton refactor)\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove magic 'cpu' string\n\n* remove extraneous constants\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add from_directory classmethod for e2e users\n\n* merge\n\n* merge InferenceModule and InferenceLudwigModel\n\n* add comment\n\n* revert small change\n\n* cleanup\n\n* add to_torchscript functionality\n\n* cleanup\n\n* pushes device logic down into inference stages\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* move device placement upstream to inference module to ensure stage modules are performant\n\n* adds logs for device placement experiments\n\n* removes logs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove stage_to_dict\n\n* clean up how we get input device in predictor_forward\n\n* first commit\n\n* wip\n\n* updated interfaces\n\n* postproc GPU\n\n* add intelligent device placement\n\n* clean up device api\n\n* revert flatten op in inference_module_v0\n\n* remove dtype workaround\n\n* benchmarking code\n\n* add DEVICE constant as good default for loading/saving\n\n* added helpful logging and style\n\n* cleanup\n\n* cleanup, adding docstrings\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* docstring\n\nCo-authored-by: Geoffrey Angus <geoffrey@predibase.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "no",
        "comments": "customize API",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class H3FeatureMixin(BaseFeatureMixin):",
            "):",
            "column = input_df[feature_config[COLUMN]]",
            "if column.dtype == object:",
            "-            column = column.map(int)",
            "-        column = column.map(H3FeatureMixin.h3_to_list)",
            "+            column = backend.df_engine.map_objects(column, int)",
            "+        column = backend.df_engine.map_objects(column, H3FeatureMixin.h3_to_list)",
            "",
            "proc_df[feature_config[PROC_COLUMN]] = backend.df_engine.map_objects(",
            "column, lambda x: np.array(x, dtype=np.uint8)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=601395)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=601396)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'map_objects'), position=2, insert_id=601397)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'column'), position=1, insert_id=601398)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=601399)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=601400)",
            "Update(target_node=ASTNode(type=identifier, text=column), value='backend')",
            "Update(target_node=ASTNode(type=identifier, text=map), value='df_engine')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=601401)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'map_objects'), position=2, insert_id=601402)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'column'), position=1, insert_id=601403)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=601404)",
            "Update(target_node=ASTNode(type=identifier, text=column), value='backend')",
            "Update(target_node=ASTNode(type=identifier, text=map), value='df_engine')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 1141,
        "neg_line": [
            "-column = column.map(int)",
            "-column = column.map(H3FeatureMixin.h3_to_list)"
        ],
        "pos_line": [
            "+column = backend.df_engine.map_objects(column, int)",
            "+column = backend.df_engine.map_objects(column, H3FeatureMixin.h3_to_list)"
        ],
        "core_change": "-column = column.map(int) -column = column.map(H3FeatureMixin.h3_to_list) +column = backend.df_engine.map_objects(column, int) +column = backend.df_engine.map_objects(column, H3FeatureMixin.h3_to_list)",
        "core_API": "map"
    },
    {
        "commit_hash": "01759d8ffb4899de1157398e4052f76a13c56527",
        "index": "9a58b84..9af8611 100644",
        "commit_message": "Texture Atlas sampling bug fix\n\nSummary: Fixes the index out of bound errors for texture sampling from a texture atlas: when barycentric coordinates are 1.0, the integer index into the (R, R) per face texture map is R (max can only be R-1).\n\nReviewed By: gkioxari\n\nDifferential Revision: D25543803\n\nfbshipit-source-id: 82d0935b981352b49c1d95d5a17f9cc88bad0a82\n\n",
        "file": "pytorch3d.txt.json",
        "label": "yes",
        "comments": "add API call for math fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TexturesAtlas(TexturesBase):",
            "# pyre-fixme[16]: `bool` has no attribute `__getitem__`.",
            "mask = (pix_to_face < 0)[..., None]",
            "bary_w01 = torch.where(mask, torch.zeros_like(bary_w01), bary_w01)",
            "-        w_xy = (bary_w01 * R).to(torch.int64)  # (N, H, W, K, 2)",
            "+        # If barycentric coordinates are > 1.0 (in the case of",
            "+        # blur_radius > 0.0), wxy might be > R. We need to clamp this",
            "+        # index to R-1 to index into the texture atlas.",
            "+        w_xy = (bary_w01 * R).to(torch.int64).clamp(max=R - 1)  # (N, H, W, K, 2)",
            "",
            "below_diag = (",
            "bary_w01.sum(dim=-1) * R - w_xy.float().sum(dim=-1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=921508)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=921509)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=921510)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clamp'), position=2, insert_id=921511)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=921512)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=921513)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=921514)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'max'), position=0, insert_id=921515)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=921516)",
            "Insert(target_node=IN(type=keyword_argument), node=('binary_operator', None), position=2, insert_id=921517)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'R'), position=0, insert_id=921518)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=921519)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=921520)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1142,
        "neg_line": [
            "-w_xy = (bary_w01 * R).to(torch.int64)  # (N, H, W, K, 2)"
        ],
        "pos_line": [
            "+# If barycentric coordinates are > 1.0 (in the case of",
            "+# blur_radius > 0.0), wxy might be > R. We need to clamp this",
            "+# index to R-1 to index into the texture atlas.",
            "+w_xy = (bary_w01 * R).to(torch.int64).clamp(max=R - 1)  # (N, H, W, K, 2)"
        ],
        "core_change": "-w_xy = (bary_w01 * R).to(torch.int64)  # (N, H, W, K, 2) +# If barycentric coordinates are > 1.0 (in the case of +# blur_radius > 0.0), wxy might be > R. We need to clamp this +# index to R-1 to index into the texture atlas. +w_xy = (bary_w01 * R).to(torch.int64).clamp(max=R - 1)  # (N, H, W, K, 2)",
        "core_API": "where"
    },
    {
        "commit_hash": "a49f6c295a467d7a533f74c08438c4b85f108c3c",
        "index": "70818b87..9a0c1289 100644",
        "commit_message": "Add binary neural networks. (#418)\n\n* add BinaryDenseLayer SignLayer etc\n\n* add example of binarynet cnn | add BinaryConv2d\n\n* rename scale layer\\\n\n* remove unused code\n\n* remove print params\n\n* rename function name in binarynet example\n\n* update all\n\n* rename sign act name\n\n* rename function\n\n* fix codacy;\n\n* rename sign\n\n* improve docs for sign\n\n* yapf\n\n",
        "file": "TensorLayer.txt.json",
        "label": "no",
        "comments": "rename",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def sign(x):  # https://github.com/AngusG/tensorflow-xnor-bnn/blob/master/models",
            "",
            "\"\"\"",
            "with tf.get_default_graph().gradient_override_map({\"sign\": \"QuantizeGrad\"}):",
            "-        return tf.sign(x, name='tl_sign')",
            "+        return tf.sign(x, name='sign')",
            "",
            "",
            "# if tf.__version__ > \"1.7\":"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1143,
        "neg_line": [
            "-return tf.sign(x, name='tl_sign')"
        ],
        "pos_line": [
            "+return tf.sign(x, name='sign')"
        ],
        "core_change": "-return tf.sign(x, name='tl_sign') +return tf.sign(x, name='sign')",
        "core_API": "get_default_graph"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "e1ccc43a..4acfab82 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "yes",
        "comments": "update API call for version fix",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VariationalSparseGP(GPModel):",
            "M = self.Xu.size(0)",
            "Kuu = self.kernel(self.Xu).contiguous()",
            "Kuu.view(-1)[::M + 1] += self.jitter  # add jitter to the diagonal",
            "-        Luu = Kuu.cholesky()",
            "+        Luu = torch.linalg.cholesky(Kuu)",
            "",
            "zero_loc = self.Xu.new_zeros(self.u_loc.shape)",
            "if self.whiten:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=676838)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=676839)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=Kuu), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=676840)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=676841)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1144,
        "neg_line": [
            "-Luu = Kuu.cholesky()"
        ],
        "pos_line": [
            "+Luu = torch.linalg.cholesky(Kuu)"
        ],
        "core_change": "-Luu = Kuu.cholesky() +Luu = torch.linalg.cholesky(Kuu)",
        "core_API": "size"
    },
    {
        "commit_hash": "0b8d26c148cd0cd6667060e10378367b4a271b98",
        "index": "6b7ad441..f68496ca 100644",
        "commit_message": "keras converter accepts fixed length sequence in embedding-flatten models\n\n",
        "file": "coremltools.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class NetGraph(object):",
            "self.remove_skip_layers(_KERAS_SKIP_LAYERS) # done 1 pass",
            "self.insert_1d_permute_layers()",
            "self.insert_permute_for_spatial_bn()",
            "+            self.insert_permute_for_embed_flatten()",
            "self.defuse_activation()",
            "self.remove_internal_input_layers()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2481932)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2481933)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2481934)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2481935)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2481936)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2481937)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'insert_permute_for_embed_flatten'), position=2, insert_id=2481938)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2481939)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2481940)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 9,
        "number": 1146,
        "neg_line": [],
        "pos_line": [
            "+self.insert_permute_for_embed_flatten()"
        ],
        "core_change": "+self.insert_permute_for_embed_flatten()",
        "core_API": "remove_skip_layers"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "3d9e4fba..e7a2fe06 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _matvecmul(x, y):",
            "",
            "",
            "def _cholesky(x):",
            "-    return x.sqrt() if x.dim() == 1 else x.cholesky()",
            "+    return x.sqrt() if x.dim() == 1 else torch.linalg.cholesky(x)",
            "",
            "",
            "def _transpose(x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=676943)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=676944)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=x), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=676945)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=676946)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1148,
        "neg_line": [
            "-return x.sqrt() if x.dim() == 1 else x.cholesky()"
        ],
        "pos_line": [
            "+return x.sqrt() if x.dim() == 1 else torch.linalg.cholesky(x)"
        ],
        "core_change": "-return x.sqrt() if x.dim() == 1 else x.cholesky() +return x.sqrt() if x.dim() == 1 else torch.linalg.cholesky(x)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "3c6d73bc5cca8295a2f7b17eb868f45327a89d08",
        "index": "9b2a8661e..c9c592779 100644",
        "commit_message": "Fix BERT/MobileBERT classifier dropout\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class MobileBertForMultipleChoice(MobileBertPreTrainedModel):",
            "super().__init__(config)",
            "",
            "self.mobilebert = MobileBertModel(config)",
            "-        self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "+        self.dropout = nn.Dropout(config.classifier_dropout)",
            "self.classifier = nn.Linear(config.hidden_size, 1)",
            "",
            "self.init_weights()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=hidden_dropout_prob), value='classifier_dropout')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1149,
        "neg_line": [
            "-self.dropout = nn.Dropout(config.hidden_dropout_prob)"
        ],
        "pos_line": [
            "+self.dropout = nn.Dropout(config.classifier_dropout)"
        ],
        "core_change": "-self.dropout = nn.Dropout(config.hidden_dropout_prob) +self.dropout = nn.Dropout(config.classifier_dropout)",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "0b04ac3117e44e9fcc0222b479a34048171462a7",
        "index": "29dd348f..e37ff68a 100644",
        "commit_message": "Fix TF RNN dynamic behavior\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def rnn(step_function, inputs, initial_states,",
            "new_states = []",
            "",
            "# all this circus is to recover the last vector in the sequence.",
            "-        begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))",
            "-        size = tf.pack([1] + [-1] * (ndim - 1))",
            "-        last_output = tf.slice(outputs, begin, size)",
            "+        slice_begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))",
            "+        slice_size = tf.pack([1] + [-1] * (ndim - 1))",
            "+        last_output = tf.slice(outputs, slice_begin, slice_size)",
            "last_output = tf.squeeze(last_output, [0])",
            "",
            "axes = [1, 0] + list(range(2, len(outputs.get_shape())))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=begin), value='slice_begin')",
            "Update(target_node=ASTNode(type=identifier, text=size), value='slice_size')",
            "Update(target_node=ASTNode(type=identifier, text=size), value='slice_size')",
            "Update(target_node=ASTNode(type=identifier, text=begin), value='slice_begin')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 4,
        "number": 1152,
        "neg_line": [
            "-begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))",
            "-size = tf.pack([1] + [-1] * (ndim - 1))",
            "-last_output = tf.slice(outputs, begin, size)"
        ],
        "pos_line": [
            "+slice_begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))",
            "+slice_size = tf.pack([1] + [-1] * (ndim - 1))",
            "+last_output = tf.slice(outputs, slice_begin, slice_size)"
        ],
        "core_change": "-begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1)) -size = tf.pack([1] + [-1] * (ndim - 1)) -last_output = tf.slice(outputs, begin, size) +slice_begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1)) +slice_size = tf.pack([1] + [-1] * (ndim - 1)) +last_output = tf.slice(outputs, slice_begin, slice_size)",
        "core_API": "pack"
    },
    {
        "commit_hash": "2d24ef0d3234867ac329b10ae3a11b9b7119d17b",
        "index": "91f56f2591..5185c1e80f 100644",
        "commit_message": "[RLlib] Add all simple learning tests as `framework=tf2`. (#19273)\n\n* Unpin gym and deprecate pendulum v0\n\nMany tests in rllib depended on pendulum v0,\nhowever in gym 0.21, pendulum v0 was deprecated\nin favor of pendulum v1. This may change reward\nthresholds, so will have to potentially rerun\nall of the pendulum v1 benchmarks, or use another\nenvironment in favor. The same applies to frozen\nlake v0 and frozen lake v1\n\nLastly, all of the RLlib tests and Tune tests have\nbeen moved to python 3.7\n\n* fix tune test_sampler::testSampleBoundsAx\n\n* fix re-install ray for py3.7 tests\n\nCo-authored-by: avnishn <avnishn@uw.edu>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def convert_to_numpy(x: TensorStructType, reduce_floats: bool = False):",
            "if torch and isinstance(item, torch.Tensor):",
            "ret = item.cpu().item() if len(item.size()) == 0 else \\",
            "item.detach().cpu().numpy()",
            "-        elif tf and isinstance(item, (tf.Tensor, tf.Variable)):",
            "+        elif tf and isinstance(item, (tf.Tensor, tf.Variable)) and \\",
            "+                hasattr(item, \"numpy\"):",
            "assert tf.executing_eagerly()",
            "ret = item.numpy()",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=boolean_operator), node=ASTNode(type=boolean_operator), position=0)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2139053)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('call', None), position=2, insert_id=2139054)",
            "Insert(target_node=IN(type=call), node=('identifier', 'hasattr'), position=0, insert_id=2139055)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2139056)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2139057)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'item'), position=1, insert_id=2139058)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2139059)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"numpy\"'), position=3, insert_id=2139060)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2139061)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1155,
        "neg_line": [
            "-elif tf and isinstance(item, (tf.Tensor, tf.Variable)):"
        ],
        "pos_line": [
            "+elif tf and isinstance(item, (tf.Tensor, tf.Variable)) and \\",
            "+hasattr(item, \"numpy\"):"
        ],
        "core_change": "-elif tf and isinstance(item, (tf.Tensor, tf.Variable)): +elif tf and isinstance(item, (tf.Tensor, tf.Variable)) and \\ +hasattr(item, \"numpy\"):",
        "core_API": "cpu"
    },
    {
        "commit_hash": "0be066fe56a141953f622eb8c142a2d2556c0526",
        "index": "3a303394..a816512c 100644",
        "commit_message": "small fix in staging\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class StagingInputWrapper(FeedfreeInput):",
            "",
            "def setup_staging_areas(self):",
            "for idx, device in enumerate(self._devices):",
            "-            inputs = self._input.get_input_tensors()",
            "-            dtypes = [x.dtype for x in inputs]",
            "with tf.device(device):",
            "-                stage = StagingArea(",
            "-                    dtypes, shapes=None)",
            "+                inputs = self._input.get_input_tensors()",
            "+                dtypes = [x.dtype for x in inputs]",
            "+                stage = StagingArea(dtypes, shapes=None)",
            "self._stage_ops.append(stage.put(inputs))",
            "self._areas.append(stage)",
            "outputs = stage.get()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('for_statement', None), position=2, insert_id=2302727)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=for, text=for), position=0)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=pattern_list), position=1)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=in, text=in), position=2)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=call), position=3)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=:, text=:), position=4)",
            "Insert(target_node=IN(type=for_statement), node=('block', ''), position=5, insert_id=2302728)",
            "Move(target_node=ASTNode(type=with_statement), node=ASTNode(type=block), position=3)",
            "Move(target_node=ASTNode(type=block), node=ASTNode(type=expression_statement), position=2)",
            "Delete(target_node=ASTNode(type=for_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 11,
        "number": 1156,
        "neg_line": [
            "-inputs = self._input.get_input_tensors()",
            "-dtypes = [x.dtype for x in inputs]",
            "-stage = StagingArea(",
            "-dtypes, shapes=None)"
        ],
        "pos_line": [
            "+inputs = self._input.get_input_tensors()",
            "+dtypes = [x.dtype for x in inputs]",
            "+stage = StagingArea(dtypes, shapes=None)"
        ],
        "core_change": "-inputs = self._input.get_input_tensors() -dtypes = [x.dtype for x in inputs] -stage = StagingArea( -dtypes, shapes=None) +inputs = self._input.get_input_tensors() +dtypes = [x.dtype for x in inputs] +stage = StagingArea(dtypes, shapes=None)",
        "core_API": "get_input_tensors"
    },
    {
        "commit_hash": "4ab335691506d5c0cb082e510e61f7e14c84e0e7",
        "index": "f258ce9..c8790c2 100644",
        "commit_message": "fix error when running with --show-mask-heatmaps (#274)\n\n\n",
        "file": "maskrcnn-benchmark.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class COCODemo(object):",
            "\"\"\"",
            "masks = predictions.get_field(\"mask\")",
            "masks_per_dim = self.masks_per_dim",
            "-        masks = torch.nn.functional.interpolate(",
            "+        masks = L.interpolate(",
            "masks.float(), scale_factor=1 / masks_per_dim",
            ").byte()",
            "height, width = masks.shape[-2:]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='L')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=functional))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1157,
        "neg_line": [
            "-masks = torch.nn.functional.interpolate("
        ],
        "pos_line": [
            "+masks = L.interpolate("
        ],
        "core_change": "-masks = torch.nn.functional.interpolate( +masks = L.interpolate(",
        "core_API": "get_field"
    },
    {
        "commit_hash": "f4d1c828b0ec73a11341d051d95a29992c220805",
        "index": "0673af9f..ef4d7d67 100644",
        "commit_message": "fix(speedup): make the tensor contiguous before randomizing (#5141)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelSpeedup:",
            "while not visit_queue.empty():",
            "curnode = visit_queue.get()",
            "self.update_indirect_sparsity(curnode)",
            "-            predecessors = self.torch_graph.find_predecessors(",
            "-                curnode.unique_name)",
            "+            predecessors = set(self.torch_graph.find_predecessors(",
            "+                curnode.unique_name))",
            "for predecessor in predecessors:",
            "out_degree[predecessor] -= 1",
            "if out_degree[predecessor] == 0:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'set'), position=0, insert_id=654918)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=654919)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=654920)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=654921)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 1159,
        "neg_line": [
            "-predecessors = self.torch_graph.find_predecessors(",
            "-curnode.unique_name)"
        ],
        "pos_line": [
            "+predecessors = set(self.torch_graph.find_predecessors(",
            "+curnode.unique_name))"
        ],
        "core_change": "-predecessors = self.torch_graph.find_predecessors( -curnode.unique_name) +predecessors = set(self.torch_graph.find_predecessors( +curnode.unique_name))",
        "core_API": "empty"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "4b8ae7d4..ab9546c9 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestLegacyAttention(AllenNlpTestCase):",
            "[[0.6, 0.8, 0.1], [0.15, 0.5, 0.2], [0.5, 0.3, 0.2]],",
            "]",
            ")",
            "-        mask = torch.FloatTensor([[1.0, 1.0, 0.0], [0.0, 0.0, 0.0]])",
            "+        mask = torch.BoolTensor([[True, True, False], [False, False, False]])",
            "result = attention(vector, matrix, mask).data.numpy()",
            "assert_almost_equal(result, numpy.array([[0.5, 0.5, 0.0], [0.0, 0.0, 0.0]]))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=FloatTensor), value='BoolTensor')",
            "Insert(target_node=ASTNode(type=list), node=('[', '['), position=0, insert_id=19699)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=5, insert_id=19700)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=[, text=[), position=0)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=19701)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=4, insert_id=19702)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=7, insert_id=19703)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=1, insert_id=19704)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=4, insert_id=19705)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=5, insert_id=19706)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=6, insert_id=19707)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=], text=]), position=7)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=float, text=1.0))",
            "Delete(target_node=ASTNode(type=float, text=1.0))",
            "Delete(target_node=ASTNode(type=float, text=0.0))",
            "Delete(target_node=ASTNode(type=float, text=0.0))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=float, text=0.0))",
            "Delete(target_node=ASTNode(type=float, text=0.0))",
            "Delete(target_node=ASTNode(type=], text=]))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 1161,
        "neg_line": [
            "-mask = torch.FloatTensor([[1.0, 1.0, 0.0], [0.0, 0.0, 0.0]])"
        ],
        "pos_line": [
            "+mask = torch.BoolTensor([[True, True, False], [False, False, False]])"
        ],
        "core_change": "-mask = torch.FloatTensor([[1.0, 1.0, 0.0], [0.0, 0.0, 0.0]]) +mask = torch.BoolTensor([[True, True, False], [False, False, False]])",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "f1e4db2aa80d72bfe9992476ccca52348a789db0",
        "index": "36c1560f1..12c0f82e6 100644",
        "commit_message": "Fix #1686\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OpenAIGPTDoubleHeadsModel(OpenAIGPTPreTrainedModel):",
            "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')",
            "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')",
            "tokenizer.add_special_tokens({'cls_token': '[CLS]'})  # Add a [CLS] to the vocabulary (we should train it also!)",
            "+        model.resize_token_embeddings(len(tokenizer))",
            "+",
            "choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]",
            "input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices",
            "-        mc_token_ids = torch.tensor([input_ids.size(-1), input_ids.size(-1)]).unsqueeze(0)  # Batch size 1",
            "+        mc_token_ids = torch.tensor([input_ids.size(-1)-1, input_ids.size(-1)-1]).unsqueeze(0)  # Batch size 1",
            "+",
            "outputs = model(input_ids, mc_token_ids=mc_token_ids)",
            "lm_prediction_scores, mc_prediction_scores = outputs[:2]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1245213)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1245214)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1245215)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1245216)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=0, insert_id=1245217)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1245218)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'resize_token_embeddings'), position=2, insert_id=1245219)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1245220)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1245221)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1245222)",
            "Insert(target_node=IN(type=call), node=('identifier', 'len'), position=0, insert_id=1245223)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1245224)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1245225)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'tokenizer'), position=1, insert_id=1245226)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1245227)",
            "Insert(target_node=ASTNode(type=list), node=('binary_operator', None), position=1, insert_id=1245228)",
            "Insert(target_node=ASTNode(type=list), node=('binary_operator', None), position=4, insert_id=1245229)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1245230)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1245231)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1245232)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1245233)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 1163,
        "neg_line": [
            "-mc_token_ids = torch.tensor([input_ids.size(-1), input_ids.size(-1)]).unsqueeze(0)  # Batch size 1"
        ],
        "pos_line": [
            "+model.resize_token_embeddings(len(tokenizer))",
            "+",
            "+mc_token_ids = torch.tensor([input_ids.size(-1)-1, input_ids.size(-1)-1]).unsqueeze(0)  # Batch size 1",
            "+"
        ],
        "core_change": "+model.resize_token_embeddings(len(tokenizer)) + -mc_token_ids = torch.tensor([input_ids.size(-1), input_ids.size(-1)]).unsqueeze(0)  # Batch size 1 +mc_token_ids = torch.tensor([input_ids.size(-1)-1, input_ids.size(-1)-1]).unsqueeze(0)  # Batch size 1 +",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "9a471f0a..95cd133e 100755",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "add_moving_summary(tf.reduce_mean(wrong, name='train_error'))",
            "",
            "# weight decay on all W of fc layers",
            "-        wd_cost = tf.mul(0.0004,",
            "-                         regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "-                         name='regularize_loss')",
            "+        wd_cost = regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss')",
            "add_moving_summary(cost, wd_cost)",
            "",
            "add_param_summary(('.*/W', ['histogram']))   # monitor W"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='regularize_cost')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tf), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', \"'fc.*/W'\"), position=1, insert_id=2307910)",
            "Update(target_node=ASTNode(type=identifier, text=regularize_cost), value='l2_regularizer')",
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '4e-4'), position=1, insert_id=2307911)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mul))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=float, text=0.0004))",
            "Delete(target_node=ASTNode(type=string, text='fc.*/W'))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=l2_loss))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 1164,
        "neg_line": [
            "-wd_cost = tf.mul(0.0004,",
            "-regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "-name='regularize_loss')"
        ],
        "pos_line": [
            "+wd_cost = regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss')"
        ],
        "core_change": "-wd_cost = tf.mul(0.0004, -regularize_cost('fc.*/W', tf.nn.l2_loss), -name='regularize_loss') +wd_cost = regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss')",
        "core_API": "reduce_mean"
    },
    {
        "commit_hash": "acb05c4f3ee130c1380e973a0d977e0b75d87f67",
        "index": "0dca2b22b5..1b3b27828d 100644",
        "commit_message": "fix `set` positional and keyword arguments (#2891)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def unique_inverse(",
            "",
            "def unique_values(",
            "x: Union[tf.Tensor, tf.Variable],",
            "+    /,",
            "*,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "ret = tf.unique(tf.reshape(x, [-1]))[0]",
            "return tf.sort(ret)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('positional_separator', None), position=4, insert_id=2001246)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=5, insert_id=2001247)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=9, insert_id=2001248)",
            "Insert(target_node=IN(type=positional_separator), node=('/', '/'), position=0, insert_id=2001249)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1166,
        "neg_line": [
            "-out: Optional[Union[tf.Tensor, tf.Variable]] = None"
        ],
        "pos_line": [
            "+/,",
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,"
        ],
        "core_change": "+/, -out: Optional[Union[tf.Tensor, tf.Variable]] = None +out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
        "core_API": "unique"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "4d75b350..035ce132 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestHighway(AllenNlpTestCase):",
            "",
            "def test_forward_works_on_nd_input(self):",
            "highway = Highway(2, 2)",
            "-        input_tensor = Variable(torch.ones(2, 2, 2))",
            "+        input_tensor = torch.ones(2, 2, 2)",
            "output = highway(input_tensor)",
            "assert output.size() == (2, 2, 2)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1167,
        "neg_line": [
            "-input_tensor = Variable(torch.ones(2, 2, 2))"
        ],
        "pos_line": [
            "+input_tensor = torch.ones(2, 2, 2)"
        ],
        "core_change": "-input_tensor = Variable(torch.ones(2, 2, 2)) +input_tensor = torch.ones(2, 2, 2)",
        "core_API": "ones"
    },
    {
        "commit_hash": "e153f12a4adccf441479de19d454f9639f0c2d3c",
        "index": "48c4f07851..1a9c64840d 100644",
        "commit_message": "fix concat + tensorflow out type hints\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def reshape(",
            "shape: Union[ivy.NativeShape, Sequence[int]],",
            "*,",
            "copy: Optional[bool] = None,",
            "-    out: Optional[tf.Tensor] = None,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if copy:",
            "newarr = tf.experimental.numpy.copy(x)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('subscript', None), position=2, insert_id=1985236)",
            "Insert(target_node=ASTNode(type=subscript), node=(']', ']'), position=3, insert_id=1985237)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Union'), position=0, insert_id=1985238)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1985239)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=1985240)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=4, insert_id=1985241)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=], text=]), position=5)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1985242)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1985243)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Variable'), position=2, insert_id=1985244)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 1168,
        "neg_line": [
            "-out: Optional[tf.Tensor] = None,"
        ],
        "pos_line": [
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,"
        ],
        "core_change": "-out: Optional[tf.Tensor] = None, +out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
        "core_API": "copy"
    },
    {
        "commit_hash": "a70e2709ab6ab43e3d439f8fe78db8abcca21b2c",
        "index": "baecfe1b..60de44f0 100644",
        "commit_message": "get action state dim fix\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TRPOUpdater(ValueFunction):",
            "",
            "action_means, action_log_stds = self.session.run([self.action_means,",
            "self.action_log_stds],",
            "-                                                         {self.state: state})",
            "+                                                         {self.state: [state]})",
            "",
            "action = action_means + np.exp(action_log_stds) * self.random.randn(*action_log_stds.shape)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=pair), node=('list', None), position=2, insert_id=2248577)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2248578)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=state), position=1)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=2248579)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1169,
        "neg_line": [
            "-{self.state: state})"
        ],
        "pos_line": [
            "+{self.state: [state]})"
        ],
        "core_change": "-{self.state: state}) +{self.state: [state]})",
        "core_API": "run"
    },
    {
        "commit_hash": "cce064403f0ac3fdea12007d31159533719c4f98",
        "index": "7f6b5971..87ca7273 100644",
        "commit_message": "Keras: Fix docstring for tf.keras.optimizer. -> tf.keras.optimizers.\n\nPiperOrigin-RevId: 481721074\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def convert_to_legacy_optimizer(optimizer):",
            "",
            "This function takes in a `tf.keras.optimizers.experimental.Optimizer`",
            "instance and converts it to the corresponding",
            "-    `tf.keras.optimizer.legacy.Optimizer` instance.",
            "+    `tf.keras.optimizers.legacy.Optimizer` instance.",
            "For example, `tf.keras.optimizers.experimental.Adam(...)` to",
            "`tf.keras.optimizers.legacy.Adam(...)`."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=`tf.keras.optimizer.legacy.Optimizer`), value='`tf.keras.optimizers.legacy.Optimizer`')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1171,
        "neg_line": [
            "-`tf.keras.optimizer.legacy.Optimizer` instance."
        ],
        "pos_line": [
            "+`tf.keras.optimizers.legacy.Optimizer` instance."
        ],
        "core_change": "-`tf.keras.optimizer.legacy.Optimizer` instance. +`tf.keras.optimizers.legacy.Optimizer` instance.",
        "core_API": "Adam"
    },
    {
        "commit_hash": "639d73d2e5ccc88c693a250b4fe553bdf9c63156",
        "index": "482879cfee..1c10dae531 100644",
        "commit_message": "backend fixes (excl. tensorflow) for floor_divide special cases | ivy floor_divide also altered\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def floor_divide(",
            "out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "x1, x2 = _cast_for_binary_op(x1, x2)",
            "-    return torch.div(x1, x2, rounding_mode=\"floor\", out=out)",
            "+    return torch.floor(torch.divide(x1, x2, out=out))",
            "",
            "",
            "def bitwise_or("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=div), value='floor')",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=351301)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=351302)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=351303)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=351304)",
            "Update(target_node=ASTNode(type=identifier, text=x1), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=x1), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=351305)",
            "Update(target_node=ASTNode(type=identifier, text=x2), value='divide')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=x2), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=351306)",
            "Update(target_node=ASTNode(type=identifier, text=rounding_mode), value='x1')",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=rounding_mode), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=351307)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x2'), position=3, insert_id=351308)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=keyword_argument), position=5)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=6)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"floor\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 1172,
        "neg_line": [
            "-return torch.div(x1, x2, rounding_mode=\"floor\", out=out)"
        ],
        "pos_line": [
            "+return torch.floor(torch.divide(x1, x2, out=out))"
        ],
        "core_change": "-return torch.div(x1, x2, rounding_mode=\"floor\", out=out) +return torch.floor(torch.divide(x1, x2, out=out))",
        "core_API": "div"
    },
    {
        "commit_hash": "17a7105058ec582c347fd003d9fc936a6ecf5584",
        "index": "daadc1bb..b8f173b3 100644",
        "commit_message": "fix mypy and lint checks\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def normalize(data: torch.Tensor, mean: torch.Tensor,",
            "mean = mean[..., :, None, None].to(data.device)",
            "std = std[..., :, None, None].to(data.device)",
            "",
            "-    out = data.sub(mean).div(std)",
            "+    out = (data - mean) / std",
            "",
            "return out",
            "-",
            "-# - denormalise"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=default_parameter), node=('binary_operator', None), position=2, insert_id=469833)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=469834)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=std), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=ERROR), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=469835)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'data'), position=0, insert_id=469836)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=469837)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=mean), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sub))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=div))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 1175,
        "neg_line": [
            "-out = data.sub(mean).div(std)",
            "-",
            "-# - denormalise"
        ],
        "pos_line": [
            "+out = (data - mean) / std"
        ],
        "core_change": "-out = data.sub(mean).div(std) +out = (data - mean) / std - -# - denormalise",
        "core_API": "sub"
    },
    {
        "commit_hash": "f96547708a889c09ca8a02ed7aadd8c5690503c5",
        "index": "6391994a4..538d7f4f8 100644",
        "commit_message": "Use soundfile for mp3 decoding instead of torchaudio (#5573)\n\n* use soundfile for mp3 decoding instead of torchaudio\n\n* fix some tests\n\n* remove torch and torchaudio from library's requirements\n\n* refactor audio decoding, decode everything with soundfile\n\n* remove torchaudio latest test ci stage, remove libsndfile and sox binaries installation\n\n* remove checks for libsndfile in tests since it's bundeled in python library\n\n* remove instructions about installing via package manager since it's misleading\n\n* pin soundfile version to the latest\n\n* update documentation\n\n* fix setup\n\n* Update docs/source/installation.md\n\nCo-authored-by: Mario ≈†a≈°ko <mariosasko777@gmail.com>\n\n* refactor decoding: move all the code under the main decode_example func\n\n* get audio format with os.path instead of string split\n\n* add module config variables for opus and mp3 support\n\n* apply steven's suggestion to installation docs\n\n* wrap torch.from_numpy in a func to avoid torch.from_numpy pickling error\n\n* Apply suggestions from code review\n\nCo-authored-by: Mario ≈†a≈°ko <mariosasko777@gmail.com>\n\n* fix code style\n\n* import xsplitext\n\n---------\n\nCo-authored-by: Mario ≈†a≈°ko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Pickler(dill.Pickler):",
            "",
            "@pklregister(obj_type)",
            "def _save_tensor(pickler, obj):",
            "+                        # `torch.from_numpy` is not picklable in `torch>=1.11.0`",
            "+                        def _create_tensor(np_array):",
            "+                            return torch.from_numpy(np_array)",
            "+",
            "dill_log(pickler, f\"To: {obj}\")",
            "args = (obj.detach().cpu().numpy(),)",
            "-                        pickler.save_reduce(torch.from_numpy, args, obj=obj)",
            "+                        pickler.save_reduce(_create_tensor, args, obj=obj)",
            "dill_log(pickler, \"# To\")",
            "return"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=4, insert_id=65775)",
            "Insert(target_node=IN(type=block), node=('function_definition', None), position=0, insert_id=65776)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', '_create_tensor'), position=1, insert_id=65777)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=65778)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', '_create_tensor'), position=1, insert_id=65779)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=65780)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=65781)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=65782)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=65783)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'np_array'), position=1, insert_id=65784)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=65785)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=65786)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=65787)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=65788)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=65789)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=65790)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'np_array'), position=1, insert_id=65791)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=65792)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 1186,
        "neg_line": [
            "-pickler.save_reduce(torch.from_numpy, args, obj=obj)"
        ],
        "pos_line": [
            "+# `torch.from_numpy` is not picklable in `torch>=1.11.0`",
            "+def _create_tensor(np_array):",
            "+return torch.from_numpy(np_array)",
            "+",
            "+pickler.save_reduce(_create_tensor, args, obj=obj)"
        ],
        "core_change": "+# `torch.from_numpy` is not picklable in `torch>=1.11.0` +def _create_tensor(np_array): +return torch.from_numpy(np_array) + -pickler.save_reduce(torch.from_numpy, args, obj=obj) +pickler.save_reduce(_create_tensor, args, obj=obj)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "42e2d02e44e1d10d7863eb18c5db581f049f7cb2",
        "index": "67b65faba..aed3c1e5e 100755",
        "commit_message": "[T5] Bug correction & Refactor (#8518)\n\n* fix bug\n\n* T5 refactor\n\n* refactor tf\n\n* apply sylvains suggestions\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, config_file, pytorch_du",
            "",
            "# Save pytorch-model",
            "print(\"Save PyTorch model to {}\".format(pytorch_dump_path))",
            "-    torch.save(model.state_dict(), pytorch_dump_path)",
            "+    model.save_pretrained(pytorch_dump_path)",
            "",
            "",
            "if __name__ == \"__main__\":"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=save), value='save_pretrained')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='model')",
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=state_dict))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 1187,
        "neg_line": [
            "-torch.save(model.state_dict(), pytorch_dump_path)"
        ],
        "pos_line": [
            "+model.save_pretrained(pytorch_dump_path)"
        ],
        "core_change": "-torch.save(model.state_dict(), pytorch_dump_path) +model.save_pretrained(pytorch_dump_path)",
        "core_API": "save"
    },
    {
        "commit_hash": "1caf0dafa3bc8d0bb309a46e2ccb12f714923260",
        "index": "2a6a9c25..486e7c43 100644",
        "commit_message": "Removes dependency on the overrides package (#5490)\n\n* Removes dependency on the overrides package\n\n* Changelog\n\n* Various fixes for mypy\n\n* Update cached_path dependency\n\n* What happened here?\n\n* Formatting\n\n* Fix more tests\n\n* One more missing overrides\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TorchCheckpointWrapper(CheckpointWrapper):",
            "#  --> https://github.com/facebookresearch/fairscale/blob/1e4a503cda8571851a68effd6e504a192838ab06/fairscale/nn/checkpoint/checkpoint_activations.py#L145-L153  # noqa: E501",
            "# We just patch the forward method to avoid having to proxy all the fields and other methods.",
            "# The use of weakref here is to prevent creating a ref cycle: m -> m.forward -> m.",
            "+",
            "+        assert len(kwargs) == 0  # This way of wrapping only works for positional arguments.",
            "+",
            "module.forward = functools.partial(  # type: ignore[assignment]",
            "_checkpointed_forward, type(module).forward, weakref.ref(module)",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1307857)",
            "Insert(target_node=IN(type=block), node=('assert_statement', None), position=0, insert_id=1307858)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=1307859)",
            "Insert(target_node=IN(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=1307860)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=1307861)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1307862)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '0'), position=2, insert_id=1307863)",
            "Insert(target_node=IN(type=call), node=('identifier', 'len'), position=0, insert_id=1307864)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1307865)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1307866)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'kwargs'), position=1, insert_id=1307867)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1307868)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 1190,
        "neg_line": [],
        "pos_line": [
            "+",
            "+assert len(kwargs) == 0  # This way of wrapping only works for positional arguments.",
            "+"
        ],
        "core_change": "+ +assert len(kwargs) == 0  # This way of wrapping only works for positional arguments. +",
        "core_API": "partial"
    },
    {
        "commit_hash": "20a9acca2636512522116601ae09c6be0408b486",
        "index": "310f02f1..afbfc39e 100644",
        "commit_message": "Speed-up warp_affine and fix bugs in RandomAffine [WIP: to add tests] (#474)\n\n* speed-up warp_affine, rotate, random_crop\n\n* added basic speed test for warp_affine\n\n* fixed centerization for random shear and bug (radians instead of degrees)\n\n* add test versus torchvision\n\n* added convert_affinematrix_to_homography function\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestInvertAffineTransform:",
            "assert_allclose(matrix_inv, expected)",
            "",
            "def test_gradcheck(self, device):",
            "-        matrix = torch.eye(2, 3).to(device)",
            "+        matrix = torch.eye(2, 3).to(device)[None]",
            "matrix = utils.tensor_to_gradcheck_var(matrix)  # to var",
            "assert gradcheck(kornia.invert_affine_transform, (matrix,),",
            "raise_exception=True)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('subscript', None), position=2, insert_id=443680)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=443681)",
            "Insert(target_node=IN(type=subscript), node=('none', 'None'), position=2, insert_id=443682)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=443683)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1191,
        "neg_line": [
            "-matrix = torch.eye(2, 3).to(device)"
        ],
        "pos_line": [
            "+matrix = torch.eye(2, 3).to(device)[None]"
        ],
        "core_change": "-matrix = torch.eye(2, 3).to(device) +matrix = torch.eye(2, 3).to(device)[None]",
        "core_API": "eye"
    },
    {
        "commit_hash": "093e901c979c0f8b0af92f79010b53db1ddcc972",
        "index": "56bb034..e2e6d96 100644",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def train(target, dataset, cluster_spec, ctx):",
            "# passing in None for summary_op to avoid a summary_thread being started.",
            "# Running summaries and training operations in parallel could run out of",
            "# GPU memory.",
            "-      summary_writer = tf.train.SummaryWriter(\"tensorboard_%d\" %(ctx.worker_num), graph=tf.get_default_graph())",
            "+      summary_writer = tf.summary.FileWriter(\"tensorboard_%d\" %(ctx.worker_num), graph=tf.get_default_graph())",
            "sv = tf.train.Supervisor(is_chief=is_chief,",
            "logdir=FLAGS.train_dir,",
            "init_op=init_op,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=SummaryWriter), value='FileWriter')",
            "Update(target_node=ASTNode(type=identifier, text=train), value='summary')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1192,
        "neg_line": [
            "-summary_writer = tf.train.SummaryWriter(\"tensorboard_%d\" %(ctx.worker_num), graph=tf.get_default_graph())"
        ],
        "pos_line": [
            "+summary_writer = tf.summary.FileWriter(\"tensorboard_%d\" %(ctx.worker_num), graph=tf.get_default_graph())"
        ],
        "core_change": "-summary_writer = tf.train.SummaryWriter(\"tensorboard_%d\" %(ctx.worker_num), graph=tf.get_default_graph()) +summary_writer = tf.summary.FileWriter(\"tensorboard_%d\" %(ctx.worker_num), graph=tf.get_default_graph())",
        "core_API": "SummaryWriter"
    },
    {
        "commit_hash": "7e4c7732ce52787c0ecb47f6e002792c56c8d79b",
        "index": "70b3d82c6..fec0be0d3 100644",
        "commit_message": "Fix encoders.py\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RNN(torch.nn.Module):",
            "def __init__(self, idim, elayers, cdim, hdim, dropout, typ=\"blstm\"):",
            "super(RNN, self).__init__()",
            "bidir = typ[0] == \"b\"",
            "-        self.nblstm = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,",
            "-                                    dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\",
            "+        self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,",
            "+                                   dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\",
            "else torch.nn.GRU(idim, cdim, elayers, batch_first=True, dropout=dropout,",
            "bidirectional=bidir)",
            "if bidir:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nblstm), value='nbrnn')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 1193,
        "neg_line": [
            "-self.nblstm = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,",
            "-dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\"
        ],
        "pos_line": [
            "+self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,",
            "+dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\"
        ],
        "core_change": "-self.nblstm = torch.nn.LSTM(idim, cdim, elayers, batch_first=True, -dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\ +self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True, +dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\",
        "core_API": "LSTM"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "7f2859e5..b60781cc 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PyroVAEImpl(VAE):",
            "",
            "def model(self, data):",
            "decoder = pyro.module('decoder', self.vae_decoder)",
            "-        z_mean, z_std = ng_zeros([data.size(0), 20]), ng_ones([data.size(0), 20])",
            "+        z_mean, z_std = torch.zeros([data.size(0), 20]), torch.ones([data.size(0), 20])",
            "with pyro.iarange('data', data.size(0)):",
            "z = pyro.sample('latent', Normal(z_mean, z_std).reshape(extra_event_dims=1))",
            "img = decoder.forward(z)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=750291)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=750292)",
            "Update(target_node=ASTNode(type=identifier, text=ng_zeros), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=ng_zeros), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=750293)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros'), position=2, insert_id=750294)",
            "Update(target_node=ASTNode(type=identifier, text=ng_ones), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=ng_ones), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=750295)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ones'), position=2, insert_id=750296)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1195,
        "neg_line": [
            "-z_mean, z_std = ng_zeros([data.size(0), 20]), ng_ones([data.size(0), 20])"
        ],
        "pos_line": [
            "+z_mean, z_std = torch.zeros([data.size(0), 20]), torch.ones([data.size(0), 20])"
        ],
        "core_change": "-z_mean, z_std = ng_zeros([data.size(0), 20]), ng_ones([data.size(0), 20]) +z_mean, z_std = torch.zeros([data.size(0), 20]), torch.ones([data.size(0), 20])",
        "core_API": "module"
    },
    {
        "commit_hash": "bc59c00277a30e5a03702773cc7b7b6dc17f6168",
        "index": "2c39d00c..18eb87d8 100644",
        "commit_message": "Fix docs links to PyTorch documentation (#856)\n\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "for _name, _Dist in torch.distributions.__dict__.items():",
            "locals()[_name] = _PyroDist",
            "",
            "_PyroDist.__doc__ = '''",
            "-    Wraps :class:`torch.distributions.{}` with",
            "+    Wraps :class:`{}.{}` with",
            ":class:`~pyro.distributions.torch_distribution.TorchDistributionMixin`.",
            "-    '''.format(_Dist.__name__)",
            "+    '''.format(_Dist.__module__, _Dist.__name__)",
            "",
            "__all__.append(_name)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='''\n    Wraps :class:`torch.distributions.{}` with\n:class:`~pyro.distributions.torch_distribution.TorchDistributionMixin`.\n    '''), value=\"'''\\n    Wraps :class:`{}.{}` with\\n:class:`~pyro.distributions.torch_distribution.TorchDistributionMixin`.\\n    '''\")",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=751894)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=751895)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_Dist'), position=0, insert_id=751896)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=751897)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__module__'), position=2, insert_id=751898)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 1196,
        "neg_line": [
            "-Wraps :class:`torch.distributions.{}` with",
            "-'''.format(_Dist.__name__)"
        ],
        "pos_line": [
            "+Wraps :class:`{}.{}` with",
            "+'''.format(_Dist.__module__, _Dist.__name__)"
        ],
        "core_change": "-Wraps :class:`torch.distributions.{}` with +Wraps :class:`{}.{}` with -'''.format(_Dist.__name__) +'''.format(_Dist.__module__, _Dist.__name__)",
        "core_API": "items"
    },
    {
        "commit_hash": "4dd784c32f76fb8285f205b94e2a6ebde731a1cd",
        "index": "363bbf3ff..5b4de28b7 100644",
        "commit_message": "Fix momentum and epsilon values (#19454)\n\nThe momentum value for PyTorch and TensorFlow batch normalization layers is not equivalent. The TensorFlow value should be (1 - pytorch_momentum) in order to ensure the correct updates are applied to the running mean and running variance calculations. We wouldn't observe a difference loading a pretrained model and performing inference, but evaluation outputs would change after some training steps.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TFData2VecVisionForSemanticSegmentation(TFData2VecVisionPreTrainedModel):",
            "# FPNs",
            "self.fpn1 = [",
            "tf.keras.layers.Conv2DTranspose(config.hidden_size, kernel_size=2, strides=2, name=\"fpn1.0\"),",
            "-            tf.keras.layers.BatchNormalization(name=\"fpn1.1\"),",
            "+            tf.keras.layers.BatchNormalization(name=\"fpn1.1\", momentum=0.9, epsilon=1e-5),",
            "tf.keras.layers.Activation(\"gelu\"),",
            "tf.keras.layers.Conv2DTranspose(config.hidden_size, kernel_size=2, strides=2, name=\"fpn1.3\"),",
            "]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2641192)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2641193)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2641194)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2641195)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'momentum'), position=0, insert_id=2641196)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2641197)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '0.9'), position=2, insert_id=2641198)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'epsilon'), position=0, insert_id=2641199)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2641200)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-5'), position=2, insert_id=2641201)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1197,
        "neg_line": [
            "-tf.keras.layers.BatchNormalization(name=\"fpn1.1\"),"
        ],
        "pos_line": [
            "+tf.keras.layers.BatchNormalization(name=\"fpn1.1\", momentum=0.9, epsilon=1e-5),"
        ],
        "core_change": "-tf.keras.layers.BatchNormalization(name=\"fpn1.1\"), +tf.keras.layers.BatchNormalization(name=\"fpn1.1\", momentum=0.9, epsilon=1e-5),",
        "core_API": "Conv2DTranspose"
    },
    {
        "commit_hash": "1ab2f1d30208a972fbb331da35de4f899580ea5d",
        "index": "7da22110..41619302 100644",
        "commit_message": "Tutorial fixing (#635)\n\n* TF bug fixing in Tutorials\n\n* Error fix in #476\n\n* Issue with Flags in Tutorials Fixed\n\n* Missing import fixed\n\n* Changelog Update\n\n* VGG19 import error fix\n\n* Error fixing in VGG tutorials\n\n* TFRecord Shape Error Fix\n\n* Sess Initialization Error Fix\n\n* Squeezenet model loading from \"models\" dir\n\n* PTB tutorials import issue fixed\n\n* mobilenet load from dir \"models\"\n\n* YAPF error fix\n\n* Missing Import fixed\n\n* Various Fixes on Tutorials\n\n* YAPF error correct\n\n* Update CHANGELOG.md\n\n* update VGG16 tutorial, auto download model\n\n* Python 3 Unicode Encoding Error\n\n* Deprecation Warning Fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main(_):",
            "optimizer = tf.train.GradientDescentOptimizer(lr)",
            "train_op = optimizer.apply_gradients(zip(grads, tvars))",
            "",
            "-    # sess.run(tf.initialize_all_variables())",
            "+    # sess.run(tf.global_variables_initializer())",
            "tl.layers.initialize_global_variables(sess)",
            "",
            "net.print_params()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1198,
        "neg_line": [
            "-# sess.run(tf.initialize_all_variables())"
        ],
        "pos_line": [
            "+# sess.run(tf.global_variables_initializer())"
        ],
        "core_change": "-# sess.run(tf.initialize_all_variables()) +# sess.run(tf.global_variables_initializer())",
        "core_API": "GradientDescentOptimizer"
    },
    {
        "commit_hash": "c62b3a2e7e9cb1665db1062d96cc13118350ad8d",
        "index": "d2652606..84493b1d 100644",
        "commit_message": "[Flax] Fix sample batch size DreamBooth  (#1129)\n\nfix sample batch size\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "logger.info(f\"Number of class images to sample: {num_new_images}.\")",
            "",
            "sample_dataset = PromptDataset(args.class_prompt, num_new_images)",
            "-            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)",
            "+            total_sample_batch_size = args.sample_batch_size * jax.local_device_count()",
            "+            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=total_sample_batch_size)",
            "",
            "for example in tqdm(",
            "sample_dataloader, desc=\"Generating class images\", disable=not jax.process_index() == 0"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=100372)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=100373)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'total_sample_batch_size'), position=0, insert_id=100374)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=100375)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=100376)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=100377)",
            "Insert(target_node=IN(type=binary_operator), node=('call', None), position=2, insert_id=100378)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=100379)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=100380)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'jax'), position=0, insert_id=100381)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=100382)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'local_device_count'), position=2, insert_id=100383)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=100384)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=100385)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'total_sample_batch_size'), position=2, insert_id=100386)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 1199,
        "neg_line": [
            "-sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)"
        ],
        "pos_line": [
            "+total_sample_batch_size = args.sample_batch_size * jax.local_device_count()",
            "+sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=total_sample_batch_size)"
        ],
        "core_change": "-sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size) +total_sample_batch_size = args.sample_batch_size * jax.local_device_count() +sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=total_sample_batch_size)",
        "core_API": "info"
    },
    {
        "commit_hash": "c22ed57a40340adefa08b5d06ac0a0253a3cb3ac",
        "index": "044936fe..53a05d89 100644",
        "commit_message": "Spacy token indexer (#3040)\n\n* add a tokenizer to ud\n\n* add spacy indexer\n\n* allow token_indexers to specify their own type\n\n* dumb hack to allow a whitespace spacy tokenizer...\n\n* pass through token embedder\n\n* add ndarray to TokenType, tests for pass through embedder\n\n* add doc\n\n* remove todo, test\n\n* fix docs\n\n* why is this test flaky\n\n* fix the correct test\n\n* add as_padded_tensor method\n\n* better place for depreciation stuff\n\n* add warning for calling inherited get_padding_token\n\n* ignore type for backward compatability\n\n* mattg comments\n\n* pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TokenCharactersIndexer(TokenIndexer[List[int]]):",
            "# Removes the \"dummy token\".",
            "padded_tokens.pop()",
            "# Truncates all the tokens to the desired length, and return the result.",
            "-        return {key: [list(token[:desired_token_length]) for token in padded_tokens]}",
            "+        return {key: torch.LongTensor([list(token[:desired_token_length])",
            "+                                       for token in padded_tokens])}"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=pair), node=('call', None), position=2, insert_id=24627)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=24628)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=24629)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=24630)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=24631)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'LongTensor'), position=2, insert_id=24632)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=24633)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=list_comprehension), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=24634)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1200,
        "neg_line": [
            "-return {key: [list(token[:desired_token_length]) for token in padded_tokens]}"
        ],
        "pos_line": [
            "+return {key: torch.LongTensor([list(token[:desired_token_length])",
            "+for token in padded_tokens])}"
        ],
        "core_change": "-return {key: [list(token[:desired_token_length]) for token in padded_tokens]} +return {key: torch.LongTensor([list(token[:desired_token_length]) +for token in padded_tokens])}",
        "core_API": "pop"
    },
    {
        "commit_hash": "16f5fa434902a3066ef11eb799c9900a824b4295",
        "index": "bc5633b6..4b1c6ec3 100644",
        "commit_message": "Fix tests failing on CUDA (#1834)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LKJCorrCholesky(TorchDistribution):",
            "Km1 = self._d - 1",
            "",
            "log_diagonals = x.diagonal(offset=0, dim1=-1, dim2=-2)[..., 1:].log()",
            "+        # TODO: Figure out why the `device` kwarg to torch.linspace seems to not work in certain situations,",
            "+        # and a seemingly redundant .to(x.device) is needed below.",
            "values = log_diagonals * torch.linspace(start=Km1 - 1, end=0, steps=Km1,",
            "dtype=x.dtype,",
            "-                                                device=x.device).expand_as(log_diagonals)",
            "+                                                device=x.device).expand_as(log_diagonals).to(x.device)",
            "",
            "values += log_diagonals.mul(eta.mul(2).add(-2.0))",
            "values = values.sum(-1) + lp"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=721197)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=721198)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=721199)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=721200)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=721201)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=721202)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=721203)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=721204)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=721205)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=721206)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 1201,
        "neg_line": [
            "-device=x.device).expand_as(log_diagonals)"
        ],
        "pos_line": [
            "+# TODO: Figure out why the `device` kwarg to torch.linspace seems to not work in certain situations,",
            "+# and a seemingly redundant .to(x.device) is needed below.",
            "+device=x.device).expand_as(log_diagonals).to(x.device)"
        ],
        "core_change": "+# TODO: Figure out why the `device` kwarg to torch.linspace seems to not work in certain situations, +# and a seemingly redundant .to(x.device) is needed below. -device=x.device).expand_as(log_diagonals) +device=x.device).expand_as(log_diagonals).to(x.device)",
        "core_API": "diagonal"
    },
    {
        "commit_hash": "bf0e094142309ffd2c287d40da047dfac9efd4ba",
        "index": "2e4a01109..7862e52f5 100644",
        "commit_message": "Fix redundant normalization of OWL-ViT text embeddings (#19712)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OwlViTModel(OwlViTPreTrainedModel):",
            "if return_base_image_embeds:",
            "last_hidden_state = vision_outputs[0]",
            "image_embeds = self.vision_model.post_layernorm(last_hidden_state)",
            "+        else:",
            "+            image_embeds = image_embeds_norm",
            "+            text_embeds = text_embeds_norm",
            "",
            "if not return_dict:",
            "output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1188317)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=1188318)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1188319)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1188320)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'else'), position=0, insert_id=1188321)",
            "Insert(target_node=IN(type=assignment), node=(':', ':'), position=1, insert_id=1188322)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=1188323)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=3, insert_id=1188324)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'image_embeds_norm'), position=4, insert_id=1188325)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'text_embeds'), position=0, insert_id=1188326)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1188327)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'text_embeds_norm'), position=2, insert_id=1188328)",
            "Insert(target_node=IN(type=type), node=('identifier', 'image_embeds'), position=0, insert_id=1188329)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 1202,
        "neg_line": [],
        "pos_line": [
            "+else:",
            "+image_embeds = image_embeds_norm",
            "+text_embeds = text_embeds_norm"
        ],
        "core_change": "+else: +image_embeds = image_embeds_norm +text_embeds = text_embeds_norm",
        "core_API": "post_layernorm"
    },
    {
        "commit_hash": "a6658185bbb6ba711558e63f58a69828a5bde128",
        "index": "e9813beae6..6f09734958 100644",
        "commit_message": "formatting fixes for Array API submodule in backend APIs.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def isnan(x: torch.Tensor)\\",
            "return torch.isnan(x)",
            "",
            "",
            "-def less(x1: torch.Tensor,x2: torch.Tensor):",
            "-    if hasattr(x1,'dtype') and hasattr(x2,'dtype'):",
            "-        promoted_type = torch.promote_types(x1.dtype,x2.dtype)",
            "+def less(x1: torch.Tensor, x2: torch.Tensor):",
            "+    if hasattr(x1, 'dtype') and hasattr(x2, 'dtype'):",
            "+        promoted_type = torch.promote_types(x1.dtype, x2.dtype)",
            "x1 = x1.to(promoted_type)",
            "x2 = x2.to(promoted_type)",
            "-    return torch.lt(x1,x2)",
            "+    return torch.lt(x1, x2)",
            "",
            "",
            "def cos(x: torch.Tensor)\\"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 1204,
        "neg_line": [
            "-def less(x1: torch.Tensor,x2: torch.Tensor):",
            "-if hasattr(x1,'dtype') and hasattr(x2,'dtype'):",
            "-promoted_type = torch.promote_types(x1.dtype,x2.dtype)",
            "-return torch.lt(x1,x2)"
        ],
        "pos_line": [
            "+def less(x1: torch.Tensor, x2: torch.Tensor):",
            "+if hasattr(x1, 'dtype') and hasattr(x2, 'dtype'):",
            "+promoted_type = torch.promote_types(x1.dtype, x2.dtype)",
            "+return torch.lt(x1, x2)"
        ],
        "core_change": "-def less(x1: torch.Tensor,x2: torch.Tensor): -if hasattr(x1,'dtype') and hasattr(x2,'dtype'): -promoted_type = torch.promote_types(x1.dtype,x2.dtype) +def less(x1: torch.Tensor, x2: torch.Tensor): +if hasattr(x1, 'dtype') and hasattr(x2, 'dtype'): +promoted_type = torch.promote_types(x1.dtype, x2.dtype) -return torch.lt(x1,x2) +return torch.lt(x1, x2)",
        "core_API": "isnan"
    },
    {
        "commit_hash": "4b99af0a44eabee91ec58f2ec4acd76d50fa8a55",
        "index": "6df05352..cc2b5145 100755",
        "commit_message": "Change rnn-cell to fix #103 (#104)\n\n* Change rnn-cell to fix #103\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "feat, labelidx, labelvalue, labelshape, seqlen = input_vars",
            "label = tf.SparseTensor(labelidx, labelvalue, labelshape)",
            "",
            "-        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=HIDDEN)",
            "-        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * NLAYER)",
            "+        cell = tf.contrib.rnn.BasicLSTMCell(num_units=HIDDEN)",
            "+        cell = tf.contrib.rnn.MultiRNNCell([cell] * NLAYER)",
            "",
            "initial = cell.zero_state(tf.shape(feat)[0], tf.float32)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rnn_cell), value='rnn')",
            "Update(target_node=ASTNode(type=identifier, text=rnn_cell), value='rnn')",
            "Update(target_node=ASTNode(type=identifier, text=nn), value='contrib')",
            "Update(target_node=ASTNode(type=identifier, text=nn), value='contrib')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 1205,
        "neg_line": [
            "-cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=HIDDEN)",
            "-cell = tf.nn.rnn_cell.MultiRNNCell([cell] * NLAYER)"
        ],
        "pos_line": [
            "+cell = tf.contrib.rnn.BasicLSTMCell(num_units=HIDDEN)",
            "+cell = tf.contrib.rnn.MultiRNNCell([cell] * NLAYER)"
        ],
        "core_change": "-cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=HIDDEN) -cell = tf.nn.rnn_cell.MultiRNNCell([cell] * NLAYER) +cell = tf.contrib.rnn.BasicLSTMCell(num_units=HIDDEN) +cell = tf.contrib.rnn.MultiRNNCell([cell] * NLAYER)",
        "core_API": "SparseTensor"
    },
    {
        "commit_hash": "a00fe9f1741ba7452b886b4d2fbd79eb9acd6a17",
        "index": "17a25608f2..a31368c59f 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "eigh.support_native_out = True",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"float16\", \"bfloat16\")}, backend_version)",
            "def eigvalsh(",
            "-    x: torch.Tensor,",
            "-    /,",
            "-    *,",
            "-    UPLO: Optional[str] = \"L\",",
            "-    out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "return torch.linalg.eigvalsh(x, UPLO=UPLO, out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 17,
        "number": 1208,
        "neg_line": [
            "-x: torch.Tensor,",
            "-/,",
            "-*,",
            "-UPLO: Optional[str] = \"L\",",
            "-out: Optional[torch.Tensor] = None"
        ],
        "pos_line": [
            "+x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None"
        ],
        "core_change": "-x: torch.Tensor, -/, -*, -UPLO: Optional[str] = \"L\", -out: Optional[torch.Tensor] = None +x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None",
        "core_API": "eigvalsh"
    },
    {
        "commit_hash": "6df9ee2305354e4270845ac784d591c27f74b4b0",
        "index": "e8175c9..a17cbd7 100644",
        "commit_message": "Fix broken centered mode in RMSProp and add tests.\n\nPiperOrigin-RevId: 256535597\nChange-Id: I00f58289a75683b3eb83159546e86f070bab3dea\n\n",
        "file": "sonnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class RMSProp(base.Module):",
            "ms.assign(tf.square(update) * (1. - decay) + ms * decay)",
            "if self.centered:",
            "mg.assign(update * (1. - decay) + mg * decay)",
            "-          denominator = ms - mg + epsilon",
            "+          denominator = ms - tf.square(mg) + epsilon",
            "else:",
            "denominator = ms + epsilon",
            "mom.assign(momentum * mom + ("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2183479)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2183480)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2183481)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2183482)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2183483)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'square'), position=2, insert_id=2183484)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2183485)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=mg), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2183486)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1209,
        "neg_line": [
            "-denominator = ms - mg + epsilon"
        ],
        "pos_line": [
            "+denominator = ms - tf.square(mg) + epsilon"
        ],
        "core_change": "-denominator = ms - mg + epsilon +denominator = ms - tf.square(mg) + epsilon",
        "core_API": "assign"
    },
    {
        "commit_hash": "55bd2836771721431dea3e02edc5f2ceedb5bc00",
        "index": "2fdf653..b9c201b 100644",
        "commit_message": "fix tf master 'is_sequence' util\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def _linear(args, output_size, bias, bias_start=0.0, weights_init=None,",
            "Raises:",
            "ValueError: if some of the arguments has unspecified or wrong shape.",
            "\"\"\"",
            "-    if args is None or (_rnn_cell._is_sequence(args) and not args):",
            "+    if args is None or (is_sequence(args) and not args):",
            "raise ValueError(\"`args` must be specified\")",
            "-    if not _rnn_cell._is_sequence(args):",
            "+    if not is_sequence(args):",
            "args = [args]",
            "",
            "# Calculate the total size of arguments on dimension 1."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_rnn_cell), value='is_sequence')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=_rnn_cell), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_is_sequence))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 1210,
        "neg_line": [
            "-if args is None or (_rnn_cell._is_sequence(args) and not args):",
            "-if not _rnn_cell._is_sequence(args):"
        ],
        "pos_line": [
            "+if args is None or (is_sequence(args) and not args):",
            "+if not is_sequence(args):"
        ],
        "core_change": "-if args is None or (_rnn_cell._is_sequence(args) and not args): +if args is None or (is_sequence(args) and not args): -if not _rnn_cell._is_sequence(args): +if not is_sequence(args):",
        "core_API": "_is_sequence"
    },
    {
        "commit_hash": "dcd32c30c1295ea2f24adf46f0f67a2701f1c69a",
        "index": "8f3c3b5b..a84188ac 100755",
        "commit_message": "Fixed problems with exploration. (#289)\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Moved `discount` into base Agent class.\n\n* Added LearningAgent class to hold all variables necessary for learning with an optimizable model.\nAll Agent child classes inherit from LearningAgent, except the non-learning ones (RandomAgent and ConstantAgent).\n\nIntroducing the new class (LearningAgent) removed lots of redundant c'tor and initialization code.\n\nObsoleted DDQNAgent (same as DQN with double_q_model==True). Gives an informative warning now (still backwards compatible).\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Moved `discount` into base Agent class.\n\n* Added LearningAgent class to hold all variables necessary for learning with an optimizable model.\nAll Agent child classes inherit from LearningAgent, except the non-learning ones (RandomAgent and ConstantAgent).\n\nIntroducing the new class (LearningAgent) removed lots of redundant c'tor and initialization code.\n\nObsoleted DDQNAgent (same as DQN with double_q_model==True). Gives an informative warning now (still backwards compatible).\n\n* Fixed travis import errors LearningAgent.\n\n* - Added LearningAgent properly to agents/__init__.py\n\n* Fixed various pytest failures.\nMoved 'scope' back into base Agent (needed by Random and ConstantAgents).\nRemoved  unnecessary parameters from Random and Constant (non-learning) Agents.\n\n* Fixed action_exploration in `Model` and `Exploration` classes.\n\n* Fixed action_exploration in `Model` and `Exploration` classes.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EpsilonDecay(Exploration):",
            "epsilon = self.final_epsilon + (2 ** (-half_life_ratio)) * (self.initial_epsilon - self.final_epsilon)",
            "return epsilon",
            "",
            "-        pred = tf.logical_or(x=(timestep < self.start_timestep), y=(timestep > self.start_timestep + self.timesteps))",
            "+        pred = tf.logical_or(x=(timestep < self.start_timestep),",
            "+                             y=(timestep > self.start_timestep + int(self.timesteps)))",
            "return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2237297)",
            "Insert(target_node=IN(type=call), node=('identifier', 'int'), position=0, insert_id=2237298)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2237299)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2237300)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2237301)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1212,
        "neg_line": [
            "-pred = tf.logical_or(x=(timestep < self.start_timestep), y=(timestep > self.start_timestep + self.timesteps))"
        ],
        "pos_line": [
            "+pred = tf.logical_or(x=(timestep < self.start_timestep),",
            "+y=(timestep > self.start_timestep + int(self.timesteps)))"
        ],
        "core_change": "-pred = tf.logical_or(x=(timestep < self.start_timestep), y=(timestep > self.start_timestep + self.timesteps)) +pred = tf.logical_or(x=(timestep < self.start_timestep), +y=(timestep > self.start_timestep + int(self.timesteps)))",
        "core_API": "logical_or"
    },
    {
        "commit_hash": "a0d169eb594798bb4fcb80ec1578e2b5b5b43b93",
        "index": "aa127bc..c4dfe12 100644",
        "commit_message": "fix efficientnet\n\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "ADAM = int(os.getenv(\"ADAM\", 0))",
            "if __name__ == \"__main__\":",
            "print(f\"NUM:{NUM} BS:{BS} CNT:{CNT}\")",
            "model = EfficientNet(NUM, classes=1000, has_se=False, track_running_stats=False)",
            "-  parameters = get_parameters(model)",
            "+  parameters = optim.get_parameters(model)",
            "for p in parameters: p.realize()",
            "if ADAM: optimizer = optim.Adam(parameters, lr=0.001)",
            "else: optimizer = optim.SGD(parameters, lr=0.001)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1618522)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'optim'), position=0, insert_id=1618523)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1618524)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=get_parameters), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1213,
        "neg_line": [
            "-parameters = get_parameters(model)"
        ],
        "pos_line": [
            "+parameters = optim.get_parameters(model)"
        ],
        "core_change": "-parameters = get_parameters(model) +parameters = optim.get_parameters(model)",
        "core_API": "getenv"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "6f70ce56..4c554a91 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTrain(AllenNlpTestCase):",
            "train_model(params(), serialization_dir=serialization_dir)",
            "archive = load_archive(str(serialization_dir / \"model.tar.gz\"))",
            "model = archive.model",
            "-        assert model.forward(torch.tensor([1, 2, 3]))[\"class\"] == torch.tensor(98) # pylint: disable=not-callable",
            "+        assert model.forward(torch.tensor([1, 2, 3]))[\"class\"] == torch.tensor(98)",
            "assert model.vocab.get_vocab_size() == 9"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1214,
        "neg_line": [
            "-assert model.forward(torch.tensor([1, 2, 3]))[\"class\"] == torch.tensor(98) # pylint: disable=not-callable"
        ],
        "pos_line": [
            "+assert model.forward(torch.tensor([1, 2, 3]))[\"class\"] == torch.tensor(98)"
        ],
        "core_change": "-assert model.forward(torch.tensor([1, 2, 3]))[\"class\"] == torch.tensor(98) # pylint: disable=not-callable +assert model.forward(torch.tensor([1, 2, 3]))[\"class\"] == torch.tensor(98)",
        "core_API": "forward"
    },
    {
        "commit_hash": "9f42d0ba538364a09d4f81b1c580f29d1904a87c",
        "index": "96e98cb7..ceec3daa 100644",
        "commit_message": "fix shape error on cuda (#1385)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EKFState(object):",
            "S = H.mm(P).mm(H.transpose(-1, -2)) + R  # innovation cov",
            "",
            "K_prefix = self._cov.mm(H.transpose(-1, -2))",
            "-        dx = K_prefix.mm(torch.gesv(dz, S)[0]).squeeze(1)  # K*dz",
            "+        dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz",
            "x = self._dynamic_model.geodesic_difference(x, -dx)",
            "",
            "I = eye_like(x, self._dynamic_model.dimension)  # noqa: E741"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=733981)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=5, insert_id=733982)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=733983)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=733984)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dz), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=733985)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'unsqueeze'), position=2, insert_id=733986)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=733987)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=1, insert_id=733988)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1215,
        "neg_line": [
            "-dx = K_prefix.mm(torch.gesv(dz, S)[0]).squeeze(1)  # K*dz"
        ],
        "pos_line": [
            "+dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz"
        ],
        "core_change": "-dx = K_prefix.mm(torch.gesv(dz, S)[0]).squeeze(1)  # K*dz +dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz",
        "core_API": "mm"
    },
    {
        "commit_hash": "fa826358f5299a37a1ab0161af1b4fba941ef908",
        "index": "343c735d..96d85e39 100644",
        "commit_message": "Warn on invalid node and edge type names in `HeteroData` (#5990)\n\nFixes #4150\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_my_conv():",
            "assert torch.allclose(conv((x1, x2), adj.t()), out1)",
            "assert torch.allclose(conv((x1, x2), torch_adj.t()), out1)",
            "assert torch.allclose(conv((x1, None), adj.t()), out2)",
            "-    assert torch.allclose(conv((x1, None), torch_adj.t()), out2)",
            "+    assert torch.allclose(conv((x1, None), torch_adj.t()), out2, atol=1e-6)",
            "conv.fuse = False",
            "assert torch.allclose(conv((x1, x2), adj.t()), out1)",
            "assert torch.allclose(conv((x1, x2), torch_adj.t()), out1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=950397)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=950398)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=950399)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=950400)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=950401)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1216,
        "neg_line": [
            "-assert torch.allclose(conv((x1, None), torch_adj.t()), out2)"
        ],
        "pos_line": [
            "+assert torch.allclose(conv((x1, None), torch_adj.t()), out2, atol=1e-6)"
        ],
        "core_change": "-assert torch.allclose(conv((x1, None), torch_adj.t()), out2) +assert torch.allclose(conv((x1, None), torch_adj.t()), out2, atol=1e-6)",
        "core_API": "allclose"
    },
    {
        "commit_hash": "b1347c956af4752560b53b891d352c48c6050305",
        "index": "0559f360e..0914134ad 100644",
        "commit_message": "[Metrics] AUROC error on multilabel + improved testing (#3350)\n\n* error on multilabel\n\n* fix tests\n\n* fix pep8\n\n* changelog\n\n* update doc test\n\n* fix doctest\n\n* fix doctest\n\n* update from suggestion\n\n* Apply suggestions from code review\n\nCo-authored-by: Adrian W√§lchli <aedu.waelchli@gmail.com>\n\n* Update test_classification.py\n\n* Update test_classification.py\n\n* retrigger test\n\n* 'pep8\n\nCo-authored-by: Adrian W√§lchli <aedu.waelchli@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_average_precision(pos_label):",
            "assert isinstance(ap, torch.Tensor)",
            "",
            "",
            "-@pytest.mark.parametrize('pos_label', [1, 2])",
            "+@pytest.mark.parametrize('pos_label', [0, 1])",
            "def test_auroc(pos_label):",
            "auroc = AUROC(pos_label=pos_label)",
            "assert auroc.name == 'auroc'",
            "",
            "-    pred, target = torch.tensor([1, 2, 3, 4]), torch.tensor([1, 2, 0, 1])",
            "+    pred, target = torch.tensor([1, 2, 3, 4]), torch.tensor([1, 1, 0, 1])",
            "area = auroc(pred=pred, target=target, sample_weight=[0.1, 0.2, 0.3, 0.4])",
            "assert isinstance(area, torch.Tensor)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=1), value='0')",
            "Update(target_node=ASTNode(type=integer, text=2), value='1')",
            "Update(target_node=ASTNode(type=integer, text=2), value='1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 1218,
        "neg_line": [
            "-@pytest.mark.parametrize('pos_label', [1, 2])",
            "-pred, target = torch.tensor([1, 2, 3, 4]), torch.tensor([1, 2, 0, 1])"
        ],
        "pos_line": [
            "+@pytest.mark.parametrize('pos_label', [0, 1])",
            "+pred, target = torch.tensor([1, 2, 3, 4]), torch.tensor([1, 1, 0, 1])"
        ],
        "core_change": "-@pytest.mark.parametrize('pos_label', [1, 2]) +@pytest.mark.parametrize('pos_label', [0, 1]) -pred, target = torch.tensor([1, 2, 3, 4]), torch.tensor([1, 2, 0, 1]) +pred, target = torch.tensor([1, 2, 3, 4]), torch.tensor([1, 1, 0, 1])",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "be08b41afe9241f1f206a5d3b1111fca7ef55133",
        "index": "a485d7893..00012ba1d 100644",
        "commit_message": "fix docstring\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransformerLM(nn.Module, LMInterface):",
            "m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)",
            "return ys_mask.unsqueeze(-2) & m",
            "",
            "-    def forward(self, x: torch.Tensor, t: torch.Tensor):",
            "+    def forward(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:",
            "xm = (x != 0)",
            "h, _ = self.encoder(x, self.target_mask(x))",
            "y = self.decoder(h)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('->', '->'), position=3, insert_id=168700)",
            "Insert(target_node=ASTNode(type=function_definition), node=('type', None), position=4, insert_id=168701)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=168702)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Tuple'), position=0, insert_id=168703)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=168704)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=168705)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=168706)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=4, insert_id=168707)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=5, insert_id=168708)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=6, insert_id=168709)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=7, insert_id=168710)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=168711)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=168712)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=168713)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=168714)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=168715)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=168716)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=168717)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=168718)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=168719)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 1219,
        "neg_line": [
            "-def forward(self, x: torch.Tensor, t: torch.Tensor):"
        ],
        "pos_line": [
            "+def forward(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:"
        ],
        "core_change": "-def forward(self, x: torch.Tensor, t: torch.Tensor): +def forward(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:",
        "core_API": "size"
    },
    {
        "commit_hash": "c2e70ca76872daf5c975bf8256a210938eaf3599",
        "index": "508e48bc..6f810c85 100644",
        "commit_message": "upgrade to pytorch 0.4.1 + make work with python 3.7 (but still 3.6 also) (#1543)\n\n* changes for pytorch 0.4.1\n\n* increase tolerance for srl test\n\n* update versions in setup.py\n\n* add script to check requirements.txt vs setup.py + fix setup.py\n\n* loosen bounds on pytorch version\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class NlvrDecoderStep(DecoderStep[NlvrDecoderState]):",
            "action_query = torch.cat([hidden_state, attended_sentence], dim=-1)",
            "# (group_size, action_embedding_dim)",
            "predicted_action_embedding = self._output_projection_layer(action_query)",
            "-        predicted_action_embedding = self._dropout(torch.nn.functional.tanh(predicted_action_embedding))",
            "+        predicted_action_embedding = self._dropout(torch.tanh(predicted_action_embedding))",
            "if state.checklist_state[0] is not None:",
            "embedding_addition = self._get_predicted_embedding_addition(state)",
            "addition = embedding_addition * self._checklist_embedding_multiplier"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=functional))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1222,
        "neg_line": [
            "-predicted_action_embedding = self._dropout(torch.nn.functional.tanh(predicted_action_embedding))"
        ],
        "pos_line": [
            "+predicted_action_embedding = self._dropout(torch.tanh(predicted_action_embedding))"
        ],
        "core_change": "-predicted_action_embedding = self._dropout(torch.nn.functional.tanh(predicted_action_embedding)) +predicted_action_embedding = self._dropout(torch.tanh(predicted_action_embedding))",
        "core_API": "cat"
    },
    {
        "commit_hash": "cef660ba5777114f0b495b9fed557008abf7ef20",
        "index": "71937fa..579905a 100644",
        "commit_message": "Bug fix\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class DistributedFusedAdam(torch.optim.Optimizer):",
            "grp = torch.distributed.new_group(ranks=ranks)",
            "if torch.distributed.get_rank() in ranks:",
            "self._ag_pg.append(grp)",
            "-            self._ag_st = [torch.cuda.Stream()]*self._num_ag_pg",
            "+            self._ag_st = [torch.cuda.Stream() for _ in range(self._num_ag_pg)]",
            "self._l2_grad_norm_st = torch.cuda.Stream() if self._compute_L2_grad_norm else None",
            "self._completion_st = torch.cuda.Stream()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('list_comprehension', None), position=2, insert_id=1812393)",
            "Move(target_node=IN(type=list_comprehension), node=ASTNode(type=[, text=[), position=0)",
            "Move(target_node=IN(type=list_comprehension), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=list_comprehension), node=('for_in_clause', None), position=2, insert_id=1812394)",
            "Insert(target_node=IN(type=list_comprehension), node=(']', ']'), position=3, insert_id=1812395)",
            "Insert(target_node=IN(type=for_in_clause), node=('for', 'for'), position=0, insert_id=1812396)",
            "Insert(target_node=IN(type=for_in_clause), node=('identifier', '_'), position=1, insert_id=1812397)",
            "Insert(target_node=IN(type=for_in_clause), node=('in', 'in'), position=2, insert_id=1812398)",
            "Insert(target_node=IN(type=for_in_clause), node=('call', None), position=3, insert_id=1812399)",
            "Insert(target_node=IN(type=call), node=('identifier', 'range'), position=0, insert_id=1812400)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1812401)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1812402)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1812403)",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 1223,
        "neg_line": [
            "-self._ag_st = [torch.cuda.Stream()]*self._num_ag_pg"
        ],
        "pos_line": [
            "+self._ag_st = [torch.cuda.Stream() for _ in range(self._num_ag_pg)]"
        ],
        "core_change": "-self._ag_st = [torch.cuda.Stream()]*self._num_ag_pg +self._ag_st = [torch.cuda.Stream() for _ in range(self._num_ag_pg)]",
        "core_API": "new_group"
    },
    {
        "commit_hash": "e540ee6cc331b373297e81c6d098346f3bde72c3",
        "index": "5909034..a3a3999 100644",
        "commit_message": "Fix notebooks.\n\nPiperOrigin-RevId: 305891180\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "\"#@title Load the Universal Sentence Encoder's TF Hub module\\n\",",
            "\"from absl import logging\\n\",",
            "\"\\n\",",
            "-        \"import tensorflow.compat.v1 as tf\\n\",",
            "-        \"tf.disable_v2_behavior()\\n\",",
            "+        \"import tensorflow as tf\\n\",",
            "\"\\n\",",
            "\"import tensorflow_hub as hub\\n\",",
            "\"import matplotlib.pyplot as plt\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=2)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1948921)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\\\\n\"'), position=0, insert_id=1948922)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1948923)",
            "Update(target_node=ASTNode(type=string, text=\"import tensorflow.compat.v1 as tf\\n\"), value='\"import tensorflow as tf\\\\n\"')",
            "Delete(target_node=ASTNode(type=string, text=\"tf.disable_v2_behavior()\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=string, text=\"\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 1227,
        "neg_line": [
            "-\"import tensorflow.compat.v1 as tf\\n\",",
            "-\"tf.disable_v2_behavior()\\n\","
        ],
        "pos_line": [
            "+\"import tensorflow as tf\\n\","
        ],
        "core_change": "-\"import tensorflow.compat.v1 as tf\\n\", -\"tf.disable_v2_behavior()\\n\", +\"import tensorflow as tf\\n\",",
        "core_API": "disable_v2_behavior"
    },
    {
        "commit_hash": "306db252a7c4d2ff3f66b30aae3b37a72737ee3b",
        "index": "cadce41d9..e1ae86695 100644",
        "commit_message": "fixes and clean-up\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class ESPnetASRTransducerModel(AbsESPnetModel):",
            "self.use_auxiliary_lm_loss = self.training and auxiliary_lm_loss_weight > 0",
            "",
            "if self.use_auxiliary_ctc:",
            "-            self.ctc_lin = torch.nn.Linear(encoder.output_size(), vocab_size)",
            "+            self.ctc_lin = torch.nn.Linear(encoder.dim_output, vocab_size)",
            "self.ctc_dropout_rate = auxiliary_ctc_dropout_rate",
            "",
            "if self.use_auxiliary_lm_loss:",
            "-            self.lm_lin = torch.nn.Linear(decoder.dunits, vocab_size)",
            "+            self.lm_lin = torch.nn.Linear(decoder.dim_output, vocab_size)",
            "",
            "self.lm_loss_smoothing = auxiliary_lm_loss_smoothing"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1824505)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1824506)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1824507)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Linear'), position=2, insert_id=1824508)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1824509)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1824510)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1824511)",
            "Update(target_node=ASTNode(type=identifier, text=output_size), value='dim_output')",
            "Update(target_node=ASTNode(type=identifier, text=dunits), value='dim_output')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Linear))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 1228,
        "neg_line": [
            "-self.ctc_lin = torch.nn.Linear(encoder.output_size(), vocab_size)",
            "-self.lm_lin = torch.nn.Linear(decoder.dunits, vocab_size)"
        ],
        "pos_line": [
            "+self.ctc_lin = torch.nn.Linear(encoder.dim_output, vocab_size)",
            "+self.lm_lin = torch.nn.Linear(decoder.dim_output, vocab_size)"
        ],
        "core_change": "-self.ctc_lin = torch.nn.Linear(encoder.output_size(), vocab_size) +self.ctc_lin = torch.nn.Linear(encoder.dim_output, vocab_size) -self.lm_lin = torch.nn.Linear(decoder.dunits, vocab_size) +self.lm_lin = torch.nn.Linear(decoder.dim_output, vocab_size)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "72983303c59bafecd4a7204850f275ca25170df3",
        "index": "6479d2b50..adc923260 100644",
        "commit_message": "Fix TFEncoderDecoderModelTest - Pytorch device (#15979)\n\n* fix device\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TFEncoderDecoderMixin:",
            "self.assertEqual(len(tf_outputs_loaded), len(pt_outputs), \"Output lengths differ between TF and PyTorch\")",
            "",
            "for tf_output_loaded, pt_output in zip(tf_outputs_loaded, pt_outputs):",
            "-            self.assert_almost_equals(tf_output_loaded.numpy(), pt_output.numpy(), 1e-3)",
            "+            self.assert_almost_equals(tf_output_loaded.numpy(), pt_output.detach().to(\"cpu\").numpy(), 1e-3)",
            "",
            "def check_equivalence_pt_to_tf(self, config, decoder_config, inputs_dict):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1205027)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1205028)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1205029)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1205030)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1205031)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1205032)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1205033)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1205034)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"cpu\"'), position=1, insert_id=1205035)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1205036)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1205037)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1205038)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=pt_output), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'detach'), position=2, insert_id=1205039)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1205040)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1205041)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1229,
        "neg_line": [
            "-self.assert_almost_equals(tf_output_loaded.numpy(), pt_output.numpy(), 1e-3)"
        ],
        "pos_line": [
            "+self.assert_almost_equals(tf_output_loaded.numpy(), pt_output.detach().to(\"cpu\").numpy(), 1e-3)"
        ],
        "core_change": "-self.assert_almost_equals(tf_output_loaded.numpy(), pt_output.numpy(), 1e-3) +self.assert_almost_equals(tf_output_loaded.numpy(), pt_output.detach().to(\"cpu\").numpy(), 1e-3)",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "b6bfe88d9ae11ba51eb8b4ba720826cf9274969c",
        "index": "f4ca9d2df..0f1c32f9c 100644",
        "commit_message": "Deprecate prepare_module (#3166)\n\n* Refactor prepare_module\n\n* Add deprecation warning in prepare_module\n\n* Remove prepare_module in inspect\n\n* Remove prepare_module in patching\n\n* Remove prepare_module in dummy_data\n\n* Remove prepare_module in run_beam\n\n* Remove prepare_module in test_dataset_common\n\n* Fix hash in run_beam\n\n* Remove prepare_module from test_load\n\n* Remove prepare_module from test_metric_common\n\n* Remove prepare_module from test_hf_gcp\n\n* Use deprecated function instead\n\n* Add deprecation to docstring\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class patch_submodule:",
            "Examples:",
            "",
            ">>> import importlib",
            "-        >>> from datasets.load import prepare_module",
            "+        >>> from datasets.load import dataset_module_factory",
            ">>> from datasets.streaming import patch_submodule, xjoin",
            ">>>",
            "-        >>> snli_module_path, _ = prepare_module(\"snli\")",
            "-        >>> snli_module = importlib.import_module(snli_module_path)",
            "+        >>> dataset_module = dataset_module_factory(\"snli\")",
            "+        >>> snli_module = importlib.import_module(dataset_module.module_path)",
            ">>> patcher = patch_submodule(snli_module, \"os.path.join\", xjoin)",
            ">>> patcher.start()",
            ">>> assert snli_module.os.path.join is xjoin"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=13)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=2, insert_id=1782147)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=0, insert_id=1782148)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=1, insert_id=1782149)",
            "Update(target_node=ASTNode(type=identifier, text=prepare_module), value='dataset_module_factory')",
            "Update(target_node=ASTNode(type=identifier, text=snli_module_path), value='dataset_module')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=snli_module_path), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=prepare_module), value='dataset_module_factory')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1782150)",
            "Update(target_node=ASTNode(type=identifier, text=snli_module_path), value='dataset_module')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=snli_module_path), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1782151)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'module_path'), position=2, insert_id=1782152)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 1230,
        "neg_line": [
            "->>> from datasets.load import prepare_module",
            "->>> snli_module_path, _ = prepare_module(\"snli\")",
            "->>> snli_module = importlib.import_module(snli_module_path)"
        ],
        "pos_line": [
            "+>>> from datasets.load import dataset_module_factory",
            "+>>> dataset_module = dataset_module_factory(\"snli\")",
            "+>>> snli_module = importlib.import_module(dataset_module.module_path)"
        ],
        "core_change": "->>> from datasets.load import prepare_module +>>> from datasets.load import dataset_module_factory ->>> snli_module_path, _ = prepare_module(\"snli\") ->>> snli_module = importlib.import_module(snli_module_path) +>>> dataset_module = dataset_module_factory(\"snli\") +>>> snli_module = importlib.import_module(dataset_module.module_path)",
        "core_API": "import_module"
    },
    {
        "commit_hash": "6f6f08c1bec5aceb60989b5d9b745a70740510d0",
        "index": "c3b85b51..78587777 100644",
        "commit_message": "feat(graphstore): support `num_nodes`, enabling `Tuple[FeatureStore, GraphStore]` in `LightningLinkData` (#5270)\n\n* init\n\n* test\n\n* fix\n\n* :(\n\n* return type\n\n* changelog\n\n* minor\n\n* major\n\n* Update torch_geometric/data/hetero_data.py\n\n* Update torch_geometric/data/hetero_data.py\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MyFeatureStore(FeatureStore):",
            "and attr.index == slice(None, None, None)):",
            "return tensor",
            "",
            "-        idx = torch.cat([(index == v).nonzero() for v in attr.index]).view(-1)",
            "+        idx = (torch.cat([(index == v).nonzero() for v in attr.index]).view(-1)",
            "+               if attr.index.numel() > 0 else [])",
            "return tensor[idx]",
            "",
            "def _remove_tensor(self, attr: TensorAttr) -> bool:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=967580)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=967581)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('conditional_expression', None), position=1, insert_id=967582)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=967583)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=967584)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=967585)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=967586)",
            "Insert(target_node=IN(type=conditional_expression), node=('list', None), position=4, insert_id=967587)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=967588)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=967589)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '0'), position=2, insert_id=967590)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=967591)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=1, insert_id=967592)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=967593)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=967594)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=967595)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=967596)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'numel'), position=2, insert_id=967597)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=967598)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=967599)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'attr'), position=0, insert_id=967600)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=967601)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'index'), position=2, insert_id=967602)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 1231,
        "neg_line": [
            "-idx = torch.cat([(index == v).nonzero() for v in attr.index]).view(-1)"
        ],
        "pos_line": [
            "+idx = (torch.cat([(index == v).nonzero() for v in attr.index]).view(-1)",
            "+if attr.index.numel() > 0 else [])"
        ],
        "core_change": "-idx = torch.cat([(index == v).nonzero() for v in attr.index]).view(-1) +idx = (torch.cat([(index == v).nonzero() for v in attr.index]).view(-1) +if attr.index.numel() > 0 else [])",
        "core_API": "cat"
    },
    {
        "commit_hash": "23d36f0cbecb6cd5ce2ced38831b7f1052dfe8de",
        "index": "2ca999a1..b586a4ad 100644",
        "commit_message": "GH-2882: fix trainling comma at the end of the sentence\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_transformer_jit_embeddings(results_base_path):",
            "",
            "tensors = base_embeddings.prepare_tensors([sentence])",
            "# ensure that the prepared tensors is what we expect",
            "-    assert sorted(tensors.keys()) == [\"attention_mask\", \"input_ids\", \"overflow_to_sample_mapping\", \"word_ids\"]",
            "+    assert sorted(tensors.keys()) == [",
            "+        \"attention_mask\",",
            "+        \"input_ids\",",
            "+        \"lengths\",",
            "+        \"overflow_to_sample_mapping\",",
            "+        \"word_ids\",",
            "+    ]",
            "",
            "wrapper = JitWrapper(base_embeddings)",
            "parameter_names, parameter_list = TransformerJitWordEmbeddings.parameter_to_list("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=list), position=5)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=233020)",
            "Insert(target_node=ASTNode(type=list), node=('string', '\"lengths\"'), position=6, insert_id=233021)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=7, insert_id=233022)"
        ],
        "plus_line": 7,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1233,
        "neg_line": [
            "-assert sorted(tensors.keys()) == [\"attention_mask\", \"input_ids\", \"overflow_to_sample_mapping\", \"word_ids\"]"
        ],
        "pos_line": [
            "+assert sorted(tensors.keys()) == [",
            "+\"attention_mask\",",
            "+\"input_ids\",",
            "+\"lengths\",",
            "+\"overflow_to_sample_mapping\",",
            "+\"word_ids\",",
            "+]"
        ],
        "core_change": "-assert sorted(tensors.keys()) == [\"attention_mask\", \"input_ids\", \"overflow_to_sample_mapping\", \"word_ids\"] +assert sorted(tensors.keys()) == [ +\"attention_mask\", +\"input_ids\", +\"lengths\", +\"overflow_to_sample_mapping\", +\"word_ids\", +]",
        "core_API": "prepare_tensors"
    },
    {
        "commit_hash": "5e6417e9887be8f02ab5b4f5c548dff7f3a4c8f6",
        "index": "8d105217..86ac074c 100644",
        "commit_message": "[Docs] Models (#416)\n\n* docs for attention\n\n* types for embeddings\n\n* unet2d docstrings\n\n* UNet2DConditionModel docstrings\n\n* fix typos\n\n* style and vq-vae docstrings\n\n* docstrings  for VAE\n\n* Update src/diffusers/models/unet_2d.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* make style\n\n* added inherits from sentence\n\n* docstring to forward\n\n* make style\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* finish model docs\n\n* up\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Timesteps(nn.Module):",
            "class GaussianFourierProjection(nn.Module):",
            "\"\"\"Gaussian Fourier embeddings for noise levels.\"\"\"",
            "",
            "-    def __init__(self, embedding_size=256, scale=1.0):",
            "+    def __init__(self, embedding_size: int = 256, scale: float = 1.0):",
            "super().__init__()",
            "self.weight = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=3, insert_id=104914)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=6, insert_id=104915)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=embedding_size), position=0)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=104916)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=104917)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type==, text==), position=3)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=integer, text=256), position=4)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=scale), position=0)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=104918)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=104919)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=104920)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=float, text=1.0), position=4)",
            "Insert(target_node=IN(type=type), node=('identifier', 'int'), position=0, insert_id=104921)",
            "Insert(target_node=IN(type=type), node=('identifier', 'float'), position=0, insert_id=104922)",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=default_parameter))",
            "Delete(target_node=ASTNode(type=default_parameter))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1235,
        "neg_line": [
            "-def __init__(self, embedding_size=256, scale=1.0):"
        ],
        "pos_line": [
            "+def __init__(self, embedding_size: int = 256, scale: float = 1.0):"
        ],
        "core_change": "-def __init__(self, embedding_size=256, scale=1.0): +def __init__(self, embedding_size: int = 256, scale: float = 1.0):",
        "core_API": "Parameter"
    },
    {
        "commit_hash": "ae914973df79cb45d8d42e7a8b36f0bb1c810b44",
        "index": "0a3c5686..7b7aa0c5 100644",
        "commit_message": "Fix tests on CUDA (#2098)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from kornia.testing import assert_close",
            "class TestOneHot:",
            "def test_smoke(self, device, dtype):",
            "num_classes = 4",
            "-        labels = torch.zeros(2, 2, 1, dtype=torch.int64)",
            "+        labels = torch.zeros(2, 2, 1, dtype=torch.int64, device=device)",
            "labels[0, 0, 0] = 0",
            "labels[0, 1, 0] = 1",
            "labels[1, 0, 0] = 2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=394304)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=9, insert_id=394305)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=394306)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=394307)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=394308)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1236,
        "neg_line": [
            "-labels = torch.zeros(2, 2, 1, dtype=torch.int64)"
        ],
        "pos_line": [
            "+labels = torch.zeros(2, 2, 1, dtype=torch.int64, device=device)"
        ],
        "core_change": "-labels = torch.zeros(2, 2, 1, dtype=torch.int64) +labels = torch.zeros(2, 2, 1, dtype=torch.int64, device=device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "afe5d42d8d1d80af911ed980c2936bfe887078f6",
        "index": "483dd095a..a4d13f0ef 100644",
        "commit_message": "Black preview (#17217)\n\n* Black preview\n\n* Fixup too!\n\n* Fix check copies\n\n* Use the same version as the CI\n\n* Bump black\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CanineModelTest(ModelTesterMixin, unittest.TestCase):",
            "torch.allclose(",
            "set_nan_tensor_to_zero(tuple_object), set_nan_tensor_to_zero(dict_object), atol=1e-5",
            "),",
            "-                            msg=f\"Tuple and dict output are not equal. Difference: {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`: {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\",",
            "+                            msg=(",
            "+                                \"Tuple and dict output are not equal. Difference:\"",
            "+                                f\" {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`:\"",
            "+                                f\" {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has\"",
            "+                                f\" `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\"",
            "+                            ),",
            ")",
            "",
            "recursive_check(tuple_output, dict_output)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_list), node=('parenthesized_expression', None), position=0, insert_id=1199399)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1199400)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('concatenated_string', None), position=1, insert_id=1199401)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1199402)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\"Tuple and dict output are not equal. Difference:\"'), position=0, insert_id=1199403)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', 'f\" {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`:\"'), position=1, insert_id=1199404)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', 'f\" {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has\"'), position=2, insert_id=1199405)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', 'f\" `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\"'), position=3, insert_id=1199406)",
            "Delete(target_node=ASTNode(type=string, text=f\"Tuple and dict output are not equal. Difference: {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`: {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\"))"
        ],
        "plus_line": 6,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1237,
        "neg_line": [
            "-msg=f\"Tuple and dict output are not equal. Difference: {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`: {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\","
        ],
        "pos_line": [
            "+msg=(",
            "+\"Tuple and dict output are not equal. Difference:\"",
            "+f\" {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`:\"",
            "+f\" {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has\"",
            "+f\" `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\"",
            "+),"
        ],
        "core_change": "-msg=f\"Tuple and dict output are not equal. Difference: {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`: {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\", +msg=( +\"Tuple and dict output are not equal. Difference:\" +f\" {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`:\" +f\" {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has\" +f\" `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\" +),",
        "core_API": "allclose"
    },
    {
        "commit_hash": "ca5819dec327be9e49412ce69909feea72f5d752",
        "index": "d291f5c..03d0eb1 100644",
        "commit_message": "revert padding bug fix for now\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def sample_autoregressive(partial_sequences,",
            "if has_partial_sequences and remove_partial_sequences:",
            "# remove partial sequences from outputs",
            "partial_length = mtf.reduce_sum(",
            "-            mtf.to_int32(mtf.not_equal(partial_sequences, padding_id)),",
            "+            mtf.to_int32(mtf.not_equal(partial_sequences, 0)),",
            "reduced_dim=length_dim)",
            "outputs = mtf.dynamic_shift(",
            "outputs, -partial_length, length_dim, wrap=False)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '0'), position=3, insert_id=1938434)",
            "Delete(target_node=ASTNode(type=identifier, text=padding_id))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1241,
        "neg_line": [
            "-mtf.to_int32(mtf.not_equal(partial_sequences, padding_id)),"
        ],
        "pos_line": [
            "+mtf.to_int32(mtf.not_equal(partial_sequences, 0)),"
        ],
        "core_change": "-mtf.to_int32(mtf.not_equal(partial_sequences, padding_id)), +mtf.to_int32(mtf.not_equal(partial_sequences, 0)),",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "948819f2c0c7a2e4d6851797f8d46553ab2acfa5",
        "index": "2028d224c..79bcf4993 100644",
        "commit_message": "Fix flake8 format problems.\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ConvolutionBlock(nn.Module):",
            "self.act = activation",
            "",
            "def forward(self, x):",
            "+        \"\"\"Compute Covolution Block",
            "+",
            "+        :param torch.Tensor x: (batch, time, size)",
            "+        :return torch.Tensor: convoluted `value` (batch, time, d_model)",
            "+        \"\"\"",
            "# exchange the temporal dimension and the feature dimension",
            "# pad the input from (batch, len, dim) to (batch, dim, len+(k-1))",
            "x = self.pad_left(x.transpose(1, 2))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=154522)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=154523)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Compute Covolution Block\\n\\n        :param torch.Tensor x: (batch, time, size)\\n        :return torch.Tensor: convoluted `value` (batch, time, d_model)\\n        \"\"\"'), position=0, insert_id=154524)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 1242,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Compute Covolution Block",
            "+",
            "+:param torch.Tensor x: (batch, time, size)",
            "+:return torch.Tensor: convoluted `value` (batch, time, d_model)",
            "+\"\"\""
        ],
        "core_change": "+\"\"\"Compute Covolution Block + +:param torch.Tensor x: (batch, time, size) +:return torch.Tensor: convoluted `value` (batch, time, d_model) +\"\"\"",
        "core_API": "pad_left"
    },
    {
        "commit_hash": "f2055cb1d4ce45d7aaacc49d8ab5bec7791a8f47",
        "index": "d17e730f..28548124 100644",
        "commit_message": "Add hypernetwork support to split cross attention v1\n\n* Add hypernetwork support to split_cross_attention_forward_v1\n* Fix device check in esrgan_model.py to use devices.device_esrgan instead of shared.device\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UpscalerESRGAN(Upscaler):",
            "print(\"Unable to load %s from %s\" % (self.model_path, filename))",
            "return None",
            "",
            "-        pretrained_net = torch.load(filename, map_location='cpu' if shared.device.type == 'mps' else None)",
            "+        pretrained_net = torch.load(filename, map_location='cpu' if devices.device_esrgan.type == 'mps' else None)",
            "crt_model = arch.RRDBNet(3, 3, 64, 23, gc=32)",
            "",
            "pretrained_net = fix_model_layers(crt_model, pretrained_net)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=shared), value='devices')",
            "Update(target_node=ASTNode(type=identifier, text=device), value='device_esrgan')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1243,
        "neg_line": [
            "-pretrained_net = torch.load(filename, map_location='cpu' if shared.device.type == 'mps' else None)"
        ],
        "pos_line": [
            "+pretrained_net = torch.load(filename, map_location='cpu' if devices.device_esrgan.type == 'mps' else None)"
        ],
        "core_change": "-pretrained_net = torch.load(filename, map_location='cpu' if shared.device.type == 'mps' else None) +pretrained_net = torch.load(filename, map_location='cpu' if devices.device_esrgan.type == 'mps' else None)",
        "core_API": "load"
    },
    {
        "commit_hash": "4177e0c3fb4c2c0909c8c390ee2d7adb2ef67b20",
        "index": "d084586..1acb0cb 100644",
        "commit_message": "fix test for v0.3\n\n",
        "file": "tensorboardX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "for epoch in range(num_epochs):",
            "if j % embedding_log == 0:",
            "print(\"loss_value:{}\".format(loss_value.data[0]))",
            "#we need 3 dimension for tensor to visualize it!",
            "-            out = torch.cat((out, torch.ones(len(out), 1)), 1)",
            "-            writer.add_embedding(out.data, metadata=label_batch.data, label_img=data_batch.data, global_step=n_iter)",
            "+            out = torch.cat((out.data, torch.ones(len(out), 1)), 1)",
            "+            writer.add_embedding(out, metadata=label_batch.data, label_img=data_batch.data, global_step=n_iter)",
            "",
            "writer.close()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'out'), position=1, insert_id=1156924)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=attribute), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=out))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 1244,
        "neg_line": [
            "-out = torch.cat((out, torch.ones(len(out), 1)), 1)",
            "-writer.add_embedding(out.data, metadata=label_batch.data, label_img=data_batch.data, global_step=n_iter)"
        ],
        "pos_line": [
            "+out = torch.cat((out.data, torch.ones(len(out), 1)), 1)",
            "+writer.add_embedding(out, metadata=label_batch.data, label_img=data_batch.data, global_step=n_iter)"
        ],
        "core_change": "-out = torch.cat((out, torch.ones(len(out), 1)), 1) -writer.add_embedding(out.data, metadata=label_batch.data, label_img=data_batch.data, global_step=n_iter) +out = torch.cat((out.data, torch.ones(len(out), 1)), 1) +writer.add_embedding(out, metadata=label_batch.data, label_img=data_batch.data, global_step=n_iter)",
        "core_API": "cat"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "0ea861551..821599afb 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .modeling_utils import PreTrainedModel",
            "logger = logging.getLogger(__name__)",
            "",
            "TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"transfo-xl-wt103\": \"https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin\",",
            "+    \"transfo-xl-wt103\": \"https://cdn.huggingface.co/transfo-xl-wt103-pytorch_model.bin\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/transfo-xl-wt103-pytorch_model.bin\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1245,
        "neg_line": [
            "-\"transfo-xl-wt103\": \"https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin\","
        ],
        "pos_line": [
            "+\"transfo-xl-wt103\": \"https://cdn.huggingface.co/transfo-xl-wt103-pytorch_model.bin\","
        ],
        "core_change": "-\"transfo-xl-wt103\": \"https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin\", +\"transfo-xl-wt103\": \"https://cdn.huggingface.co/transfo-xl-wt103-pytorch_model.bin\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "19a8504a..336c63e9 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class DeepSpeedZeRoOffload(object):",
            "self._prefetch_bucket_sz = int(prefetch_bucket_size)",
            "self._max_reuse_distance_in_numel = int(max_reuse_distance)",
            "self._max_available_parameters_in_numel = int(max_live_parameters)",
            "-        self.__allgather_stream = Stream(",
            "-        ) if overlap_comm else torch.cuda.default_stream()",
            "+        self.__allgather_stream = get_accelerator().Stream(",
            "+        ) if overlap_comm else get_accelerator().default_stream()",
            "",
            "self.forward_hooks = []",
            "self.backward_hooks = []"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('call', None), position=0, insert_id=1816589)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1816590)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1816591)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1816592)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1816593)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Stream'), position=2, insert_id=1816594)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816595)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816596)",
            "Insert(target_node=IN(type=call), node=('identifier', 'get_accelerator'), position=0, insert_id=1816597)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1816598)",
            "Update(target_node=ASTNode(type=identifier, text=Stream), value='get_accelerator')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816599)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816600)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 1248,
        "neg_line": [
            "-self.__allgather_stream = Stream(",
            "-) if overlap_comm else torch.cuda.default_stream()"
        ],
        "pos_line": [
            "+self.__allgather_stream = get_accelerator().Stream(",
            "+) if overlap_comm else get_accelerator().default_stream()"
        ],
        "core_change": "-self.__allgather_stream = Stream( -) if overlap_comm else torch.cuda.default_stream() +self.__allgather_stream = get_accelerator().Stream( +) if overlap_comm else get_accelerator().default_stream()",
        "core_API": "default_stream"
    },
    {
        "commit_hash": "8db068f8eedeb179dbb1866b691eecde46b5fb45",
        "index": "153ad56..55c8e83 100644",
        "commit_message": "Fix some comments and tokenizer outputs from P-Tuning to RLHF\n\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if __name__ == \"__main__\":",
            "rl_training = False",
            "actor_training = False",
            "",
            "-    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")",
            "+    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
            "+    # place here the path to the config.yaml file",
            "config_path = \"/home/pierpaolo/Documents/optimapi/ptuning/config.yaml\"",
            "",
            "if reward_training:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"cuda:1\"), value='\"cuda:0\"')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1249,
        "neg_line": [
            "-device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
        ],
        "pos_line": [
            "+device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
            "+# place here the path to the config.yaml file"
        ],
        "core_change": "-device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") +device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") +# place here the path to the config.yaml file",
        "core_API": "device"
    },
    {
        "commit_hash": "27ff6a1807757d86584817b629a496b1908f44fa",
        "index": "ba1dee93..5ad21d57 100755",
        "commit_message": "fix example about dropout\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "MIN_AFTER_DEQUEUE = int(50000 * 0.4)",
            "CAPACITY = MIN_AFTER_DEQUEUE + 3 * BATCH_SIZE",
            "",
            "def get_model(inputs, is_training):",
            "-    #keep_prob = tf.constant(0.5 if is_training else 1.0)",
            "+    #keep_prob = tf.constant(0.5 if is_training else 0.0)",
            "",
            "image, label = inputs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1250,
        "neg_line": [
            "-#keep_prob = tf.constant(0.5 if is_training else 1.0)"
        ],
        "pos_line": [
            "+#keep_prob = tf.constant(0.5 if is_training else 0.0)"
        ],
        "core_change": "-#keep_prob = tf.constant(0.5 if is_training else 1.0) +#keep_prob = tf.constant(0.5 if is_training else 0.0)",
        "core_API": "constant"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "d8717023..dfd351d3 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LDMPipeline(DiffusionPipeline):",
            "True, otherwise a `tuple. When returning a tuple, the first element is a list with the generated images.",
            "\"\"\"",
            "",
            "-        latents = torch.randn(",
            "+        latents = randn_tensor(",
            "(batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),",
            "generator=generator,",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1251,
        "neg_line": [
            "-latents = torch.randn("
        ],
        "pos_line": [
            "+latents = randn_tensor("
        ],
        "core_change": "-latents = torch.randn( +latents = randn_tensor(",
        "core_API": "randn"
    },
    {
        "commit_hash": "6302b7c830945f50aa9d1c62c0dbb7b57bbe2bb6",
        "index": "618f0c570..dcb290a0b 100644",
        "commit_message": "Release 0.1.0 (#609)\n\n* fix: download token file path (#556)\n\n* fix: new metrics usage in paramsearch and cross_validation (#553)\n\n* fix: new metrics usage in paramsearch and cross_validation\n\n* fix: remove incorrect import\n\n* feat: simple vocab refactored so it supports recursivity (#534)\n\n* feat: recursivity added to zero padding, check_str_batch added\n\n* feat: simple vocab refactored so it supports recursivity\n\n* fix: change check of type for numpy arrays to support uints\n\n* fix: fix get_dimensions for string batches\n\n* feat: add flatten_str_batch to utils\n\n* feat: add flattening to fit of the simple_vocab\n\n* fix: debug prints removed, chain bug fixed\n\n* fix: zero padding in recursion bug fixed\n\n* fix: remove char vocab from registry\n\n* fix: zero batch size issue fixed\n\n* fix: fix previous fix\n\n* feat: use simple_vocab in go_bot\n\n* fix: ner configs fix old char vocab\n\n* style: pep 8 imports order in core/data/utils.py\n\n* docs: remove outdated vocabs from docs\n\n* fix: remove outdated vocabs from registry\n\n* refactor: moved server/utils configs to designated folder (#546)\n\n* Moved configs to configs folder\n\n* Added default configs\n\n* Backup\n\n* Implemented cofig handling\n\n* Fix in paths.py\n\n* Added stateful and multi_instance params to MS Bot Framework config\n\n* Hotfix in default agent\n\n* Fixed utterances ids handling in default agent\n\n* Minor refactoring in MS BF server.py\n\n* Added MS Bot Framework app_id and app_secret params to server_config\n\n* Slighly fixed deep.py, stateful and multi_instance params moved to common_defaults section of config\n\n* Added https, key, cert params to server config\n\n* Added telegram token param to server config\n\n* Fixed config path retrival in test_quick_start.py\n\n* Changed download token file path to ~/.deeppavlov/token\n\n* Small fixes in get download token function\n\n* Fixed default server config\n\n* Minor fixes in paths.py\n\n* moved configs dir to utils/configs\n\n* Small fixes in configs\n\n* Updated readme and docs\n\n* Added ssh cert and key files check in rise api\n\n* Fixed configuration file path getting in alice.py\n\n* Changed naming of DP configuration to settings\n\n* Added https config params handling to alice.py\n\n* Fixed download token file path\n\n* fix: removed potential cycle dependencies at log.py (#563)\n\n* feat: notebook tutorial for classification (#558)\n\n* feature: notebook tutorial for classification\n\n* fix: fixes for classification tutorial\n\n* fix: fixes in classification notebook\n\n* fix: fixed some reviewed mistakes\n\n* feat: telegram agent (#561)\n\n* Moved configs to configs folder\n\n* Added default configs\n\n* Backup\n\n* Implemented cofig handling\n\n* Fix in paths.py\n\n* Added stateful and multi_instance params to MS Bot Framework config\n\n* Hotfix in default agent\n\n* Fixed utterances ids handling in default agent\n\n* Minor refactoring in MS BF server.py\n\n* Added MS Bot Framework app_id and app_secret params to server_config\n\n* Slighly fixed deep.py, stateful and multi_instance params moved to common_defaults section of config\n\n* Added https, key, cert params to server config\n\n* Added telegram token param to server config\n\n* Fixed config path retrival in test_quick_start.py\n\n* Changed download token file path to ~/.deeppavlov/token\n\n* Small fixes in get download token function\n\n* Fixed default server config\n\n* Minor fixes in paths.py\n\n* moved configs dir to utils/configs\n\n* Small fixes in configs\n\n* Updated readme and docs\n\n* Added ssh cert and key files check in rise api\n\n* Telegram wrapper (interactbot mode) now uses Agent entity. Also fixed multi model.out_params in default skill\n\n* Fixed configuration file path getting in alice.py\n\n* Changed naming of DP configuration to settings\n\n* Added https config params handling to alice.py\n\n* Fixed download token file path\n\n* refactor: python api (#551)\n\n* refactor: remove unnecessary utility functions\n\n* fix: inherit RedirectedPrints from contextlib.redirect_stdout\n\n* refactor: CosineSimilarityClassifier uses Serializable.__init__()\n\n* style: add typing for download.py\n\n* refactor: deep_download accepts config as dict\n\n* fix: fix errors found with code inspection\n\n* refactor: move find_config function away to core.common.file\n\n* feat: allow to download config's file requirements with build_model_from_config function\n\n* refactor: rename build_model_from_config to build_model\n\n* feat: add a config tree object\n\n* refactor: rename Sturct's _to_dict method to _asdict\nto conform with namedtuple's naming\n\n* fix: correct root directory for configs to search\n\n* feat: allow to import configs Struct and build_model fn from deeppavlov\n\n* docs: update python usage examples in model's docs\n\n* fix: do not raise in deeppavlov.__init__ because of absent requirements\n\n* fix: resolve merge conflict of recursive imports\n\n* feat: add pretty representation of Struct objects in iPython\n\n* feat: add download parameter to train_evaluate_model_from_config\n\n* feat: import train_evaluate_model_from_config in __init__ of deeppavlov as train_model\n\n* feat: add `train_model` sugar function\n\n* Fix quadratic complexity in ner_f1 (#564)\n\n* feat: iPython's tab-completion shows only directories and names for configs object (#566)\n\n* feat: iPython's tab-completion shows only directories and names for configs object\n\n* style: add typing to configs Struct methods\n\n* fix: show y_predicted in train logs examples instead of output names (#568)\n\n* feat: dialog logging (#560)\n\n* Moved configs to configs folder\n\n* Added default configs\n\n* Backup\n\n* Implemented cofig handling\n\n* Fix in paths.py\n\n* Added stateful and multi_instance params to MS Bot Framework config\n\n* Hotfix in default agent\n\n* Fixed utterances ids handling in default agent\n\n* Minor refactoring in MS BF server.py\n\n* Added MS Bot Framework app_id and app_secret params to server_config\n\n* Slighly fixed deep.py, stateful and multi_instance params moved to common_defaults section of config\n\n* Added https, key, cert params to server config\n\n* Added telegram token param to server config\n\n* Backup commit\n\n* Fixed config path retrival in test_quick_start.py\n\n* Implemented agent conversations logger\n\n* Changed download token file path to ~/.deeppavlov/token\n\n* Small fixes in get download token function\n\n* Fixed default server config\n\n* Minor fixes in paths.py\n\n* Added docstring and minor fixes to dialog logger\n\n* Added dialog logger apiref\n\n* moved configs dir to utils/configs\n\n* Small fixes in configs\n\n* Updated readme and docs\n\n* Added ssh cert and key files check in rise api\n\n* Moved dialog_logger configs\n\n* Added dialog logging to riseapi mode\n\n* Fixed configuration file path getting in alice.py\n\n* Changed naming of DP configuration to settings\n\n* Added https config params handling to alice.py\n\n* Fixed download token file path\n\n* Moved dialog logger config from utils/configs to utils/settings\n\n* Added Settings and dialog logging development guide\n\n* Changed typing Any -> Hashable in dialog_logger.py\n\n* Logging dir creation minor refactoring in dialog_logger.py\n\n* Small fixes in docs/devguides/settings.rst\n\n* Update docs/devguides/settings.rst\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* Updated settings devguide: made dialog logging defaults description clearer\n\n* Moved .deeppavlov file handling from dialog_logger.py to deeppavlov/__init__.py\n\n* Change naming of all DeepPavlov configuration tools: config -> settings\n\n* Fixed settings path handling in log.py\n\n* Fixed dialog_logging.py according PEP\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* Fixed dialog_logging.py according PEP\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* Fixed dialog_logging.py according PEP\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* Fixed typos in devguides/settings.rst\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* fix: making copy of config in read_data_by_config (#570)\n\n* fix: making copy of config in read_data_by_config\n\n* fix: get instead of pop\n\n* fix: reader_config is a copy\n\n* fix: remove unused import\n\n* Updated README.md with DeepPavlov dockerhub reference (#573)\n\n* Updated README.md with DeepPavlov dockerhub reference\n\n* docs: change header\n\n* fix: add fasttext to requirements to ranking configs instead of gensim (#571)\n\n* fix: add fasttext to requirements to ranking configs instead of gensim\n\n* fix: add fasttext to requirements to test ranking configs instead of gensim\n\n* Typo and grammar fix\n\n* feat: added ensure_ascii mode to dialog logging feature (#579)\n\n* Added ensure_ascii mode to dialog logging feature\n\n* Minor fixes in dialog logging fix\n\n* Update deeppavlov/core/agent/dialog_logger.py\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* feat: few shot SVM NER (#565)\n\n* fix: add svm tagger to registry\n\n* feat: template for ner-few shot iterator\n\n* feat: svm tagger added to ner models\n\n* chore: add svm ner to registry\n\n* fix: load loading in constructor, add checking existence of saved model\n\n* feat: bio converter added\n\n* fix: add bio restorer empty init\n\n* chore: add bio stuff to registry\n\n* feat: brand new few-shot iterator\n\n* fix: remove temporary acceleration slice\n\n* refactor: remove unnecessary utility functions\n\n* fix: inherit RedirectedPrints from contextlib.redirect_stdout\n\n* refactor: CosineSimilarityClassifier uses Serializable.__init__()\n\n* style: add typing for download.py\n\n* refactor: deep_download accepts config as dict\n\n* fix: fix errors found with code inspection\n\n* refactor: move find_config function away to core.common.file\n\n* feat: allow to download config's file requirements with build_model_from_config function\n\n* refactor: rename build_model_from_config to build_model\n\n* feat: add a config tree object\n\n* refactor: rename Sturct's _to_dict method to _asdict\nto conform with namedtuple's naming\n\n* fix: correct root directory for configs to search\n\n* feat: allow to import configs Struct and build_model fn from deeppavlov\n\n* docs: update python usage examples in model's docs\n\n* fix: do not raise in deeppavlov.__init__ because of absent requirements\n\n* fix: resolve merge conflict of recursive imports\n\n* feat: add pretty representation of Struct objects in iPython\n\n* feat: add download parameter to train_evaluate_model_from_config\n\n* feat: config for russian few-shot ner added\n\n* feat: import train_evaluate_model_from_config in __init__ of deeppavlov as train_model\n\n* feat: add `train_model` sugar function\n\n* docs: add few-shot ner doc\n\n* chore: setup paths, rename configs\n\n* docs: add few-shot ner doc\n\n* feat: add tests\n\n* Update docs/components/ner.rst\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* fix: add elmo requirements\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* docs: add docs for SVM ner\n\n* docs: add docs for FewShotNER iterator\n\n* refactor: parent init reused for ner-few-shot-iterator\n\n* fix: fix returned typing\n\n* fix: fix returned typing for gen_batches\n\n* chore: indents\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* chore: typing\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* refactor: clearer bio markup restorer\n\n* chore: import order\n\n* chore: add typing to svm model __call__\n\n* chore: return typing\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* chore: return save typing\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* chore: return typing\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* docs: lingua fix\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* fix: add requirements elmo\n\n* fix: add typings for svm ner\n\n* fix: add typings for bio\n\n* refactor: add path features usage\n\n* fix: fix shuffle, remove unnecessary imports\n\n* style: add blank ending line\n\n* chore: remove unnecessary import\n\n* fix: shuffle argument for gen_batches during the test\n\n* refactor: add typings change shuffle behaviour\n\n* style: typings\n\n* style: import order\n\n* fix: remove interact pretrained model from tests\n\n* fix: return predicted tags\n\n* feat: russian sentiment with elmo embeddings (#580)\n\n* feature: config for new rusentiment\n\n* feature: config for new rusentiment\n\n* docs: info about new elmo model\n\n* fix: add tests\n\n* fixed errors and updated docstrings\n\n* feat: skip downloads when files with the same hashes already exist (#582)\n\n* feat: add a utility function to generate .md5 files for archives\n\n* feat: add a function for computing archives hashes\n\n* feat: try and check md5 hashes before files downloads\n\n* tests: add a test for .md5 files existence on deeppavlov share\n\n* tests: remove urlib3 logs from tests of hashes existence\n\n* chore: fix imports order\n\n* chore: correct licence block\n\n* Apply suggestions from code review\n\nadd fastText import to docstring\n\nCo-Authored-By: Guitaricet <Guitaricet@gmail.com>\n\n* Update deeppavlov/models/embedders/tfidf_weighted_embedder.py\n\nCo-Authored-By: Guitaricet <Guitaricet@gmail.com>\n\n* feat: config variables (#574)\n\n* feat: first steps to using variables in configs\n\n* fix: remove set_deeppavlov_root function usage\n\n* feat: store metavariables in a single JSON-object\n\n* refactor: rename _parse_item to _parse_config_property\n\n* feat: parse subconfigs when building chainers\n\n* feat: parse_config before training\n\n* docs: update `expand_path`'s docstring\n\n* docs: add docstring to the `parse_config` function\n\n* style: pep8 in file.py\n\n* feat: use parse_config in every endpoint from deep.py\n\n* feat: adapted paramsearch to configs variables\n\n* feat: somewhat adapted evolution script to configs variables\n\n* chore: rename DEEPPAVLOV_ROOT to DEEPPAVLOV_PATH\n\n* feat: both evolution configs are conforming with config variables\n\n* feature: evolution use only MODELS_PATH\n\n* feature: replace MODELS_PATH to MODELS_SAVE_PATH and MODELS_LOAD_PATH\n\n* style: remove direct type comparison in favor of isinstance\n\n* fix: paths in evolve.py\n\n* style: remove excessive path casting\n\n* chore: rename result_table.csv in evolution to result_table.tsv\n\n* feat: adapt all classifiers configs to config variables\n\n* feat: adapt cross-validation config to config variables\n\n* feat: adapt all ranking configs to config variables\n\n* feat: adapt ecommerce config to config variables\n\n* fix: correct argument name in run_ms_bf_default_agent call\n\n* feat: adapt elmo configs to config variables\n\n* feat: adapt some faq configs to config variables\n\n* feat: adapt the rest of faq configs to config variables\n\n* feat: adapt gobot configs to config variables\n\n* feat: adapt morphotagger configs to config variables\n\n* fix: correct telegram_utils labeling for morphotagger configs\n\n* feat: adapt ner configs to config variables\n\n* feat: adapt odqa configs to config variables\n\n* feat: adapt odqa configs to config variables\n\n* feat: adapt paramsearch config to config variables\n\n* feat: adapt some ranking configs to config variables\n\n* feat: adapt ranking configs to config variables\n\n* feat: adapt bot_kvret configs to config variables\n\n* feat: adapt speller configs to config variables\n\n* feat: adapt squad configs to config variables\n\n* feat: adapt vectorizer config to config variables\n\n* chore: remove trailing / in download subdirs in configs\n\n* feat: create `CONFIGS_PATH` config variable for testing purposes\n\n* tests: adapt tests to config variables model\n\n* feat: merge `name` and `class` into `class_name`\n\n* test: correctly copy referenced configs\n\n* fix: small fixes in evolution\n\n* docs: classification, hypersearch docs for renamed ``class_name``\n\n* feat: merge `name` and `class` into `class_name` in test configs\n\n* fix: adapt to inplace config update in cross-validation\n\n* fix: keras_classification config description notebook\n\n* fix: remove keras_classification config description notebook\n\n* docs: replace `name` and `class` description with `class_name` description\n\n* docs: typing in evolution_param_generator.py\n\n* fix: upd chitchat notebook\n\n* docs: add readme block for config variables\n\n* docs: update config variables example in documentation to include usage of `DEEPPAVLOV_PATH`\n\n* fix: few_shot ner with new config format\n\n* fix: metrics import in ner notebook\n\n* docs: fix typos in MorphoTagger configs\n\n* feat: update gobot tutorial\n\n* feat: add links to collabs\n\n* feat: russian sentiment with elmo embeddings (#580)\n\n* feature: config for new rusentiment\n\n* feature: config for new rusentiment\n\n* docs: info about new elmo model\n\n* fix: add tests\n\n* fix: new rusentiment model is adapted to new config format\n\n* feat: language=python3 in tutorials\n\n* fix: sentiment config format\n\n* fix: add MODELS_PATH to the tensorboard log_dir\n\n* fix: MODELS_PATH for all tensorboard logdirs\n\n* fix: remove extra downloads from test classifiers configs\n\n* chore: add tests for classifiers projection layer\n\n* fix: do not load whole files from tar archives into memory when generationg .md5 files (#584)\n\n* feat: eCommerce bot with tfidf retrieve (#567)\n\n* fix: change agent's UI\n\n* fix: separate two bots\n\n* fix: change data reader\n\n* fix: add registry\n\n* fix: change def name\n\n* fix: code refactoring\n\n* fix: code refactoring\n\n* fix: add configs\n\n* fix: change config\n\n* fix: code refactoring\n\n* fix: change if\n\n* fix: edit docs\n\n* fix: fix name\n\n* feat: add tests\n\n* fix: change input\n\n* fix: add csr to list\n\n* fix: spaces\n\n* fix: minor changes\n\n* fix: omit testing the model\n\n* fix: simplify appearance\n\n* fix: change state\n\n* fix: types\n\n* fix: code refactoring\n\n* fix: code refactoring\n\n* fix: add all modes\n\n* fix: disable testing\n\n* fix: change def parameters\n\n* feat: improve naming\n\n* fix: docs\n\n* fix: clean files\n\n* fix: typo\n\n* fix: line length\n\n* fix: file name\n\n* fix: typo\n\n* fix: change input\n\n* fix: change input in config\n\n* fix: change format\n\n* fix: change input\n\n* fix: change docstring\n\n* doc: add resources note\n\n* doc: rename skill\n\n* doc: space requirement\n\n* fix: change test name\n\n* feat: adapt ecommerce configs to the new format\n\n* feat: allow validating every n batches (#589)\n\n* feat: allow validating every n batches\n\n* fix: tensorboard log for batches-validation logs batches instead epochs\n\n* feat: allow recursive training with a `--recursive` parameter (#590)\n\n* fix: change variables for save and load paths of main model in intents_snips (#594)\n\n* fix: utils.prepare.registry will not skip already imported packages (#595)\n\n* feat: Yahoo Conversational vs informational model (#593)\n\n* feature: basic dataset reader\n\n* fix: basic_dataset_reader for x and y lists\n\n* feature: data_sum_operator\n\n* chore: config\n\n* chore: config\n\n* feature: masking layer\n\n* feature: gru with masking for classification\n\n* fix: masking over outputs not hiddens\n\n* chore: adding maxpool over masking\n\n* chore: configs\n\n* feature: basic dataset reader\n\n* chore: configs\n\n* feature; dataset reader dealing with list input\n\n* chore: configs\n\n* chore: metrics\n\n* chore: yahoo answers config\n\n* chore: yahoo answers config for fulltext\n\n* fix: union to import, new metrics for yahoo\n\n* chore: config new model\n\n* chore: config for yahoo\n\n* feature: gru with self mult att and masking\n\n* chore: config for yahoo\n\n* chore: config experiment\n\n* chore: config experiment\n\n* chore: config experiment\n\n* chore: concat of max, aver and state\n\n* chore: concat of max, aver and state\n\n* fix: concat of max, aver and state\n\n* fix: concat of max, aver and state\n\n* chore: config\n\n* chore: config\n\n* chore: configs to new deeppavlov classification format\n\n* chore: configs\n\n* chore: config model_v5\n\n* chore: config model_v6 higher batch size\n\n* feature: notebook with pseudo-labeling for classification\n\n* fix: notebook fixes\n\n* chore: configs to compare embeddings\n\n* chore: number of epochs\n\n* chore: config with restore lr for pseudo labeling\n\n* chore: yahoo answers\n\n* fix: elmo version\n\n* chore: config for elmo classification\n\n* chore: config for elmo classification v14\n\n* chore: config for elmo classification v14\n\n* chore: config for elmo classification v15\n\n* chore: back to v14\n\n* chore: remove fasttext configs\n\n* chore: two models for yahoo elmo pretrained\n\n* chore: config for elmo pretrained final\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* fix: configs for experiment with elmo\n\n* chore:  configs for elmo google on cnn and gru\n\n* chore:  configs for elmo google on cnn and gru\n\n* chore:  configs for elmo google on cnn and gru\n\n* chore: config and notebook for pseudo-labelling\n\n* chore: config for next experiment on yahoo pseudo labeling\n\n* new run of pseudo-labeling\n\n* fix: change models downloads path to models path in main model paths\n\n* chore: remove masking from keras classification model\n\n* chore: config for convers_vs_info model\n\n* chore: convers_vs_info config\n\n* chore: new model params, pre-trained model prepared\n\n* chore: empty outputs for pseudo-labeling notebook\n\n* feature: config and docs for yahoo_L31 conversational vs informational\n\n* chore: another output of chainer\n\n* chore: tests, remove dataset_reader\n\n* fix: dir for downloading pre-trained model\n\n* fix: moved requirements\n\n* fix: rmeove basic dataset reader\n\n* chore: revert intents_snips fix. will be separate pull request\n\n* chore: remove dataset_iterator from yahoo model\n\n* chore: new links\n\n* chore: quick start in the beginning of docs\n\n* fix: remove masking function\n\n* fix: remove masking function\n\n* chore: remove train parameters except of metrics\n\n* fix: extra import\n\n* fix: example of usage\n\n* fix: units_gru instead of units_lstm\n\n* fix: do not load from saved when evaluating model without training (#592)\n\n* fix: do not load from saved when evaluating model without training\n\n* fix: new load_path logic in go_bot\n\n* fix: set matplotlib logging level to WARNING (#598)\n\n* feat: ODQA with squad noans (best ODQA) (#599)\n\n* feat: odqa with squad noans\n\n* docs: update scores for noans, add R3 scores\n\n* docs: update features.rst with new odqa scores\n\n* fix: correct url for morpho_en model (#601)\n\n* feat: allow recursive training even if the parent config has no reader (#597)\n\n* feat: Added a learning ELMo model (#569)\n\n* new(elmo): add  model\n\n* new(elmo): update  model\n\n* wip (elmo): all sub components\n\n* wip (elmo): pipline is  ready\n\n* wip (elmo): update docs, remove usless files.\n\n* wip (elmo): add download.\n\n* wip (elmo): add tests.\n\n* fix (elmo): fix the elmo embedder.\n\n* wip (elmo): cleaning after debug.\n\n* Change train.py a option of a start epoch\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* updated ELMO docs\nminor code style fixes\n\n* fixed rendering\n\n* typo fix\n\nCo-Authored-By: Guitaricet <Guitaricet@gmail.com>\n\n* wip (train): fix length of line\n\n* wip (simple_vocab): update simple_vocab and fix a path checking.\n\n* wip (file_paths_iterator): fix bugs\n\n* wip (iterator, reader, deep): fix style and doc\n\n* wip (chunk_generator): fix style, add chunk_generator to core utils\n\n* wip (file_paths_iterator): update docs\n\n* wip (file_paths_iterators): update docs,  add inheritance from file_paths_iterator\n\n* wip (deep): remove unused imports\n\n* wip (elmo): update style\n\n* wip (elmo): fix docs typo\n\n* Update file_paths_iterator.py\n\n* wip (elmo): fix bag of PR\n\n* wip (simple_vocab): fix style\n\n* Update deeppavlov/dataset_iterators/elmo_file_paths_iterator.py\n\nfix (elmo_file_paths_iterator): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/dataset_iterators/elmo_file_paths_iterator.py\n\nfix (style): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/dataset_iterators/file_paths_iterator.py\n\nfix (style): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/dataset_iterators/file_paths_iterator.py\n\nfix (style): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/elmo/train_utils.py\n\nfix (style): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/dataset_readers/file_paths_reader.py\n\nfix (style): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* fix (style): fix file_paths_iterator\n\n* fix (style): fix file_paths_iterator\n\n* fix (style): style correction\n\n* Update deeppavlov/models/elmo/elmo.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/preprocessors/str_token_reverser.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/elmo/elmo.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/elmo/elmo.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/elmo/bilm_model.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/preprocessors/str_utf8_encoder.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/elmo/bilm_model.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* wip (elmo): new elmo iterator\n\n* wip (elmo): impl 2 graphs\n\n* wip (elmo): ready 2 graph inside of elmo, honest ppl\n\n* wip (elmo): add a dumping\n\n* wip (elmo): update tfhub wrapper\n\n* fix (elmo): a bug of copy of options\n\n* fix (elmo): a bug of th_hub wrapper\n\n* wip (elmo): update configs and requirements\n\n* wip (elmo): update configs and requirements of elmo_embedder\n\n* wip (elmo): rm usless config\n\n* fix (elmo): a bug of the simple_vocab\n\n* wip (elmo): rm useless code of the simple_vocab\n\n* wip (elmo): change requirements of elmo configs.\n\n* wip (elmo): change dirs of elmo configs. Update the  registry.\n\n* wip (elmo): update simple_vocab.py and file_paths_iterator.py.\n\n* wip (elmo): fix style and docs\n\n* wip (elmo): replace weight_layers\n\n* wip (elmo): update a style\n\n* wip (elmo): update docs\n\n* wip (elmo): fix a bug of title of docs\n\n* wip (elmo): update docs, config paths, elmo2tfhub func signature.\n\n* wip (elmo): change download urls of config.\n\n* wip (elmo): fix configs after a merge\n\n* wip (elmo): fix doc typo\n\n* wip (elmo): update glob of file_paths_reader\n\n* wip (elmo): update a style\n\n* wip (elmo): fix loss and style\n\n* wip (elmo): rm debug print\n\n* wip (doc) update doc string\n\n* fix: padding keras model (#603)\n\n* fix: hot fix of padding type in keras_classification_model\n\n* fix: pre and post padding acceptable\n\n* fix: lear_rate to learning_rate\n\n* chore: config for yahoo with reader, iterator\n\n* docs: docstring on padding\n\n* docs: docs on yahoo in classifiers.rst\n\n* docs: add info about pre trained models classifiers.rst\n\n* chore: params for yahoo\n\n* chore: params for yahoo\n\n* chore: params for yahoo\n\n* chore: params for yahoo\n\n* chore: params for yahoo\n\n* fix: padding is optional parameter\n\n* fix: reinit of optional params\n\n* fix: new score for yahoo\n\n* fix: scores\n\n* chore: remove reader iterator from yahoo\n\n* fix: sort order (#604)\n\n* docs: usage examples and file sizes (#586)\n\n* chore: update version number\n\n* docs: remove obsolete note about odqa's GPU requirement\n\n* docs: add a warning in README about downloads location\n\n* docs: add a note about on disc space requirement for spelling correction models\n\n* doc: update memory notes and db size info\n\n* docs: add download sizes for classifiers\n\n* docs: add section about tf-idf ranking and update downloads info\n\n* docs: add note about disk space\n\n* docs: fix table in features.rst\n\n* docs: add download file sizes info\n\n* docs: fix config path in spellers usage example\n\n* fix: classifiers info about pre-trained in python\n\n* fix: change deep.py to deeppavlov/deep.py\n\n* fix: deep.py to -m deeppavlov\n\n* docs: fix indents in gobot's memory note\n\n* docs: replace `deep.py` call everywhere in docs with `-m deeppavlov`\n\n* docs: fix configs links in bash deeppavlov calls\n\n* docs: replace single graves with doubles\n\n* docs: add rst citations to NER documentation\n\n* docs: documents links as headers in features list\n\n* docs: add warning about config changes\n\n* docs: add memory requirement note for spelling correction in features\n\n* docs: add download file sizes for paraphraser and ranking\n\n* docs: add kvret_bot disk requirements info\n\n* docs: python api for go_bots\n\n* docs: update tables headers style in features list\n\n* docs: mock russian_tagsets package\n\n* docs: small fixes\n\n* docs: information about the model use in python api\n\n* fix: bugs in docs\n\n* doc: add \"quick start\" section to tfidf ranker and odqa\n\n* feat: rm extinguished run_model script\n\n* chore: merged docs for classifiers\n\n* doc: add space requirements\n\n* doc: add quick start section\n\n* docs: minor changes in neural_ranking doc\n\n* docs: add python example and download info to features docpage\n\n* docs: fix title underline\n\n* docs: ner docs updated\n\n* docs: update breaking changes warning\n\n* docs: move back few-shot docs\n\n* docs: prettify odqa scores table\n\n* docs: prettify ranker and squad results tables\n\n* docs: prettify classifiers table\n\n* docs: fix headers and beautify citations links in go-bot documentation\n\n* docs: beautify classification table in features list\n\n* docs: add a basic documentation for pattern_matching_skill\n\n* docs: update morphotagger docs\n\n* update: add processing of config variables in morphotagger 'predict' mode\n\n* refactor: update paths in morphotagger configs\n\n* fix: typo in morphotagger config\n\n* docs: update a basic documentation for pattern_matching_skill\n\n* docs: update headers in ecommerce.rst\n\n* examples: morphotagger Python API\n\n* docs: prettify morphotagger doc\n\n* wip (elmo): add tech requirements, update README\n\n* docs: seq2seq doc citation links\n\n* fix: properly return y_true for test in MorphotaggerDatasetReader\n\n* docs: add install instruction\n\n* docs: add installation of gobot and seq2seq_go_bot\n\n* docs: remove extra install command\n\n* docs: add an instruction for installing requirements for spelling\n\n* docs: add info about 'install' command\n\n* docs: add quick start info\n\n* fix: Unexpected indentation\n\n* docs: add install docs to classifiers\n\n* fix: indentation\n\n* fix: indentation\n\n* docs: minor updates in NER documentation\n\n* docs: add an instruction for installing requirements for morphotagger\n\n* docs: add an instruction for installing requirements for tfidf_ranking\n\n* docs: add an instruction for installing requirements for odqa\n\n* docs: add an instruction for installing requirements for squad\n\n* docs: add an instruction for installing requirements for ranking\n\n* docs: fix a minor typo\n\n* wip (docs): add install docs to elmo_model/elmo_embedder. (#607)\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GoalOrientedBotNetwork(TFModel):",
            "self.sess.run(tf.global_variables_initializer())",
            "",
            "super().__init__(**kwargs)",
            "-        if tf.train.checkpoint_exists(str(self.save_path.resolve())):",
            "+        if tf.train.checkpoint_exists(str(self.load_path.resolve())):",
            "log.info(\"[initializing `{}` from saved]\".format(self.__class__.__name__))",
            "self.load()",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=save_path), value='load_path')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1252,
        "neg_line": [
            "-if tf.train.checkpoint_exists(str(self.save_path.resolve())):"
        ],
        "pos_line": [
            "+if tf.train.checkpoint_exists(str(self.load_path.resolve())):"
        ],
        "core_change": "-if tf.train.checkpoint_exists(str(self.save_path.resolve())): +if tf.train.checkpoint_exists(str(self.load_path.resolve())):",
        "core_API": "run"
    },
    {
        "commit_hash": "e04bb2d59ec63f1b6549b485af1e1946376f723d",
        "index": "3efaf968..386a5acb 100755",
        "commit_message": "fix LinearWrap imports. use varreplace in DoReFa.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            ".BatchNorm('bnfc1')",
            ".apply(nonlin)",
            ".FullyConnected('fct', 1000, use_bias=True)())",
            "-        tf.get_variable = old_get_variable",
            "",
            "prob = tf.nn.softmax(logits, name='output')"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=2308311)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=prob), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_variable))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=old_get_variable))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 1253,
        "neg_line": [
            "-tf.get_variable = old_get_variable"
        ],
        "pos_line": [],
        "core_change": "-tf.get_variable = old_get_variable",
        "core_API": "softmax"
    },
    {
        "commit_hash": "f764b128d52bf2e90ef1ff63ee1b13018d527005",
        "index": "f03f44d0..b948c44f 100644",
        "commit_message": "[Torchscript] Add Vector preprocessing and postprocessing (#2160)\n\n* Adds torchscript-compatible Vector pre-/post- processing\n\n* finish merge\n\n* add vector input/output feature\n\n* type annotation\n\n* cleanup\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* workaround for py37\n\n* added return type hint\n\n* added return type hint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _SetPreprocessing(torch.nn.Module):",
            "self.unit_to_id = metadata[\"str2idx\"]",
            "self.is_bag = is_bag",
            "",
            "-    def forward(self, v: TorchscriptPreprocessingInput):",
            "+    def forward(self, v: TorchscriptPreprocessingInput) -> torch.Tensor:",
            "\"\"\"Takes a list of strings and returns a tensor of counts for each token.\"\"\"",
            "if not torch.jit.isinstance(v, List[str]):",
            "raise ValueError(f\"Unsupported input: {v}\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('->', '->'), position=3, insert_id=602251)",
            "Insert(target_node=ASTNode(type=function_definition), node=('type', None), position=4, insert_id=602252)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=602253)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=602254)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=602255)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=602256)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1254,
        "neg_line": [
            "-def forward(self, v: TorchscriptPreprocessingInput):"
        ],
        "pos_line": [
            "+def forward(self, v: TorchscriptPreprocessingInput) -> torch.Tensor:"
        ],
        "core_change": "-def forward(self, v: TorchscriptPreprocessingInput): +def forward(self, v: TorchscriptPreprocessingInput) -> torch.Tensor:",
        "core_API": "isinstance"
    },
    {
        "commit_hash": "65cdc240b63ad6872d6cd3126ca16ba4ea5b58aa",
        "index": "29b3cd9..6f0662d 100644",
        "commit_message": "Fix backprop bug when using non-zero hard loss weight.\n\nSummary:\nGot `RuntimeError: grad can be implicitly created only for scalar outputs` on f224839642.\n\nAfter some digging, found that we don't mean() the has_answer hard_loss so the final loss object becomes a tensor instead of scalar.\n\nAlso change cross_entropy loss to nll_loss per comment on Line 42 of loss.py\n\nReviewed By: stanvp\n\nDifferential Revision: D24349613\n\nfbshipit-source-id: 4109b72a6bab3d53fa5b93e6010f1ce7dcd06dde\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class KLDivergenceCELoss(Loss):",
            "soft_loss *= self.t ** 2  # See https://arxiv.org/pdf/1503.02531.pdf",
            "hard_loss = 0.0",
            "if self.hard_weight > 0.0:",
            "-            hard_loss = F.cross_entropy(",
            "-                logits,",
            "+            hard_loss = F.nll_loss(",
            "+                F.log_softmax(logits, 1, dtype=torch.float32),",
            "hard_targets,",
            "-                reduction=\"mean\" if reduce else \"none\",",
            "weight=self.weight,",
            "+                reduction=\"mean\" if reduce else \"none\",",
            ")",
            "",
            "return ("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=argument_list), position=5)",
            "Update(target_node=ASTNode(type=identifier, text=cross_entropy), value='nll_loss')",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=872470)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'hard_targets'), position=7, insert_id=872471)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=7, insert_id=872472)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=9, insert_id=872473)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=872474)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=872475)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'F'), position=0, insert_id=872476)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=872477)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'log_softmax'), position=2, insert_id=872478)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=872479)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=logits), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=872480)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=4)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=872481)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=872482)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=872483)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=872484)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=872485)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=872486)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=872487)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=872488)",
            "Delete(target_node=ASTNode(type=identifier, text=hard_targets))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 1256,
        "neg_line": [
            "-hard_loss = F.cross_entropy(",
            "-logits,",
            "-reduction=\"mean\" if reduce else \"none\","
        ],
        "pos_line": [
            "+hard_loss = F.nll_loss(",
            "+F.log_softmax(logits, 1, dtype=torch.float32),",
            "+reduction=\"mean\" if reduce else \"none\","
        ],
        "core_change": "-hard_loss = F.cross_entropy( -logits, +hard_loss = F.nll_loss( +F.log_softmax(logits, 1, dtype=torch.float32), -reduction=\"mean\" if reduce else \"none\", +reduction=\"mean\" if reduce else \"none\",",
        "core_API": "cross_entropy"
    },
    {
        "commit_hash": "1d691e1e7b579ef6146fbec85307d2192ef32adb",
        "index": "44e031d6..91591b58 100644",
        "commit_message": "added record-and-pretrain example, documentation improvements, fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Module(tf.Module):",
            "elif initializer == 'ones':",
            "initializer = tf_util.ones(shape=spec.shape, dtype=spec.type)",
            "elif initializer == 'constant':",
            "-            initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)",
            "+            initializer = tf.fill(",
            "+                dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type)",
            "+            )",
            "",
            "# Variable",
            "variable = tf.Variable("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf_util), value='tf')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=2224157)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2224158)",
            "Update(target_node=ASTNode(type=identifier, text=self), value='tf_util')",
            "Update(target_node=ASTNode(type=identifier, text=initialization_scale), value='constant')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2224159)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2224160)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2224161)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2224162)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2224163)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'value'), position=0, insert_id=2224164)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2224165)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'initialization_scale'), position=2, insert_id=2224166)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2224167)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2224168)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2224169)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'spec'), position=0, insert_id=2224170)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2224171)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=2224172)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 1257,
        "neg_line": [
            "-initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)"
        ],
        "pos_line": [
            "+initializer = tf.fill(",
            "+dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type)",
            "+)"
        ],
        "core_change": "-initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale) +initializer = tf.fill( +dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type) +)",
        "core_API": "ones"
    },
    {
        "commit_hash": "88d239631b9eb49527c21053d79d55e012f11a3c",
        "index": "1ec94ddf..d5ba3928 100644",
        "commit_message": "fixed unused code, error prone and code style  (#352)\n\n* raise not implemented error instead of pass\n\n* fix unused code\n\n* fix pylint issues.\n\n* fixed some assert error\n\n* fixed unused code\n\n* fixed all error prone\n\n* fixed most of the code style\n\n* yapf\n\n* yapf tests\n\n* fixed suggestion\n\n* yapf\n\n* remove unused code\n\n* remove unused code\n\n* fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def unstack_layer(layer, num=None, axis=0, name='unstack'):",
            "",
            "\"\"\"",
            "inputs = layer.outputs",
            "-    with tf.variable_scope(name) as vs:",
            "+    with tf.variable_scope(name):",
            "outputs = tf.unstack(inputs, num=num, axis=axis)",
            "",
            "logging.info(\"UnStackLayer %s: num: %s axis: %d, n_outputs: %d\" % (name, num, axis, len(outputs)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\ninputs = layer.outputs\n    with tf.variable_scope(name) as vs:\noutputs = tf.unstack(inputs, num=num, axis=axis)\n\nlogging.info(\"UnStackLayer %s: num: %s axis: %d, n_outputs: %d\"), value='\"\"\"\\ninputs = layer.outputs\\n    with tf.variable_scope(name):\\noutputs = tf.unstack(inputs, num=num, axis=axis)\\n\\nlogging.info(\"UnStackLayer %s: num: %s axis: %d, n_outputs: %d\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1258,
        "neg_line": [
            "-with tf.variable_scope(name) as vs:"
        ],
        "pos_line": [
            "+with tf.variable_scope(name):"
        ],
        "core_change": "-with tf.variable_scope(name) as vs: +with tf.variable_scope(name):",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "62e025ce79f66047f04c23938e33a6ead207043e",
        "index": "08ed411..6b56554 100644",
        "commit_message": "Fixed some test cases\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TrainTest(unittest.TestCase):",
            "'--lfw_nrof_folds', '2' ]",
            "args = facenet_train.parse_arguments(argv)",
            "model_dir = facenet_train.main(args)",
            "+",
            "+",
            "model_file = os.path.join(model_dir, 'model.ckpt-1')",
            "# Check that the trained model can be loaded",
            "+        tf.reset_default_graph()",
            "argv = ['--model_file', model_file,",
            "'--lfw_pairs', self.lfw_pairs_file,",
            "'--lfw_dir', self.dataset_dir,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=1930028)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1930029)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1930030)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1930031)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1930032)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1930033)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reset_default_graph'), position=2, insert_id=1930034)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1930035)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1930036)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 9,
        "number": 1259,
        "neg_line": [],
        "pos_line": [
            "+",
            "+",
            "+tf.reset_default_graph()"
        ],
        "core_change": "+ + +tf.reset_default_graph()",
        "core_API": "parse_arguments"
    },
    {
        "commit_hash": "ac65fcfb64df510764f8c4e55a63c7c7129856dd",
        "index": "d9563323..b20d0319 100644",
        "commit_message": "Reinforcement learning fix opt (#999)\n\n* change readme\n\n* Add files via upload\n\n* fix opt and make format\n\n* readme\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PPO(object):",
            "",
            "self.update_old_pi()",
            "adv = self.cal_adv(s, r)",
            "-        # adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful",
            "+        # adv = (adv - adv.mean())/(adv.std()+1e-6)  # sometimes helpful",
            "",
            "# update actor",
            "if METHOD['name'] == 'kl_pen':"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1262,
        "neg_line": [
            "-# adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful"
        ],
        "pos_line": [
            "+# adv = (adv - adv.mean())/(adv.std()+1e-6)  # sometimes helpful"
        ],
        "core_change": "-# adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful +# adv = (adv - adv.mean())/(adv.std()+1e-6)  # sometimes helpful",
        "core_API": "update_old_pi"
    },
    {
        "commit_hash": "6d01db93ad460a5e297a86c9cbe2e8bfa7d7ce53",
        "index": "e9437d48..5ed0abea 100644",
        "commit_message": "delaunay fix + planetoid mask is now a bool\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def edge_index_from_dict(graph_dict, num_nodes=None):",
            "",
            "",
            "def sample_mask(index, num_nodes):",
            "-    mask = torch.zeros((num_nodes, ), dtype=torch.uint8)",
            "+    mask = torch.zeros((num_nodes, ), dtype=torch.bool)",
            "mask[index] = 1",
            "return mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1263,
        "neg_line": [
            "-mask = torch.zeros((num_nodes, ), dtype=torch.uint8)"
        ],
        "pos_line": [
            "+mask = torch.zeros((num_nodes, ), dtype=torch.bool)"
        ],
        "core_change": "-mask = torch.zeros((num_nodes, ), dtype=torch.uint8) +mask = torch.zeros((num_nodes, ), dtype=torch.bool)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "1b59d8110ca29d311be9c2cd96488b2019b25ac9",
        "index": "71dd4dca..aa8e92ef 100644",
        "commit_message": "fix optimizer for restored model\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def main(args):",
            "checkpoint = torch.load(args.restore_path)",
            "model.load_state_dict(checkpoint['model'])",
            "optimizer.load_state_dict(checkpoint['optimizer'])",
            "-        print(\"\\n > Model restored from step %d\\n\" % checkpoint['step'])",
            "+        print(\" > Model restored from step %d\" % checkpoint['step'])",
            "start_epoch = checkpoint['step'] // len(train_loader)",
            "best_loss = checkpoint['linear_loss']",
            "start_epoch = 0",
            "args.restore_step = checkpoint['step']",
            "else:",
            "args.restore_step = 0",
            "-        print(\"\\n > Starting a new training\")",
            "+        print(\" > Starting a new training\")",
            "",
            "if use_cuda:",
            "-        model = nn.DataParallel(model.cuda())",
            "+        print(\" > Using CUDA.\")",
            "+        model = nn.DataParallel(model).cuda()",
            "",
            "num_params = count_parameters(model)",
            "print(\" | > Model has {} parameters\".format(num_params))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=1559429)",
            "Update(target_node=ASTNode(type=string, text=\"\\n > Starting a new training\"), value='\" > Starting a new training\"')",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1559430)",
            "Update(target_node=ASTNode(type=string, text=\"\\n > Model restored from step %d\\n\"), value='\" > Model restored from step %d\"')",
            "Insert(target_node=IN(type=call), node=('identifier', 'print'), position=0, insert_id=1559431)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1559432)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1559433)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\" > Using CUDA.\"'), position=1, insert_id=1559434)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1559435)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1559436)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1559437)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=cuda), position=2)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1559438)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=model), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1559439)",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 1264,
        "neg_line": [
            "-print(\"\\n > Model restored from step %d\\n\" % checkpoint['step'])",
            "-print(\"\\n > Starting a new training\")",
            "-model = nn.DataParallel(model.cuda())"
        ],
        "pos_line": [
            "+print(\" > Model restored from step %d\" % checkpoint['step'])",
            "+print(\" > Starting a new training\")",
            "+print(\" > Using CUDA.\")",
            "+model = nn.DataParallel(model).cuda()"
        ],
        "core_change": "-print(\"\\n > Model restored from step %d\\n\" % checkpoint['step']) +print(\" > Model restored from step %d\" % checkpoint['step']) -print(\"\\n > Starting a new training\") +print(\" > Starting a new training\") -model = nn.DataParallel(model.cuda()) +print(\" > Using CUDA.\") +model = nn.DataParallel(model).cuda()",
        "core_API": "load"
    },
    {
        "commit_hash": "64e6cd0c958a35bcf9a0af7cf9fed448498e2ea4",
        "index": "96b150e9..e8ba706a 100644",
        "commit_message": "Fix typos in torchscript tests.\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_transformer_conv():",
            "",
            "t = '(PairTensor, SparseTensor, NoneType) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(conv((x1, x2), adj.t()), out, atol=1e-6)",
            "+    assert torch.allclose(jit((x1, x2), adj.t()), out, atol=1e-6)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=conv), value='jit')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1265,
        "neg_line": [
            "-assert torch.allclose(conv((x1, x2), adj.t()), out, atol=1e-6)"
        ],
        "pos_line": [
            "+assert torch.allclose(jit((x1, x2), adj.t()), out, atol=1e-6)"
        ],
        "core_change": "-assert torch.allclose(conv((x1, x2), adj.t()), out, atol=1e-6) +assert torch.allclose(jit((x1, x2), adj.t()), out, atol=1e-6)",
        "core_API": "script"
    },
    {
        "commit_hash": "32525428e131c44f7237d05d676df9460f86bbfb",
        "index": "626c6816b..21920a33c 100644",
        "commit_message": "Fix doctest CI (#21166)\n\n* fix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "PT_SEQUENCE_CLASSIFICATION_SAMPLE = r\"\"\"",
            "...     \"{checkpoint}\", num_labels=num_labels, problem_type=\"multi_label_classification\"",
            "... )",
            "",
            "-    >>> labels = torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float)",
            "+    >>> labels = torch.sum(",
            "+    ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1",
            "+    ... ).to(torch.float)",
            ">>> loss = model(**inputs, labels=labels).loss",
            "```",
            "\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=r\"\"\"\n...     \"{checkpoint}\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n    >>> labels = torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```\n\"\"\"), value='r\"\"\"\\n...     \"{checkpoint}\", num_labels=num_labels, problem_type=\"multi_label_classification\"\\n... )\\n\\n    >>> labels = torch.sum(\\n    ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\\n    ... ).to(torch.float)\\n>>> loss = model(**inputs, labels=labels).loss\\n```\\n\"\"\"')"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1266,
        "neg_line": [
            "->>> labels = torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float)"
        ],
        "pos_line": [
            "+>>> labels = torch.sum(",
            "+...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1",
            "+... ).to(torch.float)"
        ],
        "core_change": "->>> labels = torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float) +>>> labels = torch.sum( +...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1 +... ).to(torch.float)",
        "core_API": "one_hot"
    },
    {
        "commit_hash": "34263ab196cbfab65f2afa1150e6cf6db5ffd8b6",
        "index": "01f741a70..13b4aa35d 100644",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_lecun_init_torch():",
            "torch.manual_seed(nseed)",
            "numpy.random.seed(nseed)",
            "os.environ[\"CHAINER_SEED\"] = str(nseed)",
            "-    import espnet.nets.pytorch.e2e_asr_th as m",
            "+    import espnet.nets.pytorch.e2e_asr as m",
            "model = m.Loss(m.E2E(40, 5, args), 0.5)",
            "b = model.predictor.ctc.ctc_lo.bias.data.numpy()",
            "assert numpy.all(b == 0.0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=e2e_asr_th), value='e2e_asr')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1267,
        "neg_line": [
            "-import espnet.nets.pytorch.e2e_asr_th as m"
        ],
        "pos_line": [
            "+import espnet.nets.pytorch.e2e_asr as m"
        ],
        "core_change": "-import espnet.nets.pytorch.e2e_asr_th as m +import espnet.nets.pytorch.e2e_asr as m",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "48a3208c39d9d4e5fcd5dd2942a9e8751cfe2674",
        "index": "5ca7da88..0cff50f4 100644",
        "commit_message": "Fixed nightly errors (#886)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestNormalizeLAF:",
            "laf = torch.tensor([[1, 0, 1], [0, 1, 1]]).float()",
            "laf = laf.view(1, 1, 2, 3)",
            "img = torch.rand(1, 3, h, w)",
            "-        expected = torch.tensor([[0.2, 0, 0.1], [0, 0.2, 0.2]]).float()",
            "+        expected = torch.tensor([[[[0.2, 0, 0.1], [0, 0.2, 0.2]]]]).float()",
            "lafn = kornia.feature.normalize_laf(laf, img)",
            "assert_allclose(lafn, expected)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('[', '['), position=0, insert_id=429659)",
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=1, insert_id=429660)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=2, insert_id=429661)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=429662)",
            "Move(target_node=IN(type=list), node=ASTNode(type=list), position=1)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=429663)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1270,
        "neg_line": [
            "-expected = torch.tensor([[0.2, 0, 0.1], [0, 0.2, 0.2]]).float()"
        ],
        "pos_line": [
            "+expected = torch.tensor([[[[0.2, 0, 0.1], [0, 0.2, 0.2]]]]).float()"
        ],
        "core_change": "-expected = torch.tensor([[0.2, 0, 0.1], [0, 0.2, 0.2]]).float() +expected = torch.tensor([[[[0.2, 0, 0.1], [0, 0.2, 0.2]]]]).float()",
        "core_API": "tensor"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "4e20d8ef..ddb3601b 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _PositiveDefinite_check(self, value):",
            "matrix_shape = value.shape[-2:]",
            "batch_shape = value.shape[:-2]",
            "flattened_value = value.reshape((-1,) + matrix_shape)",
            "-    return torch.stack([v.symeig(eigenvectors=False)[0][:1] > 0.0",
            "+    return torch.stack([torch.linalg.eigvalsh(v)[:1] > 0.0",
            "for v in flattened_value]).view(batch_shape)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=[, text=[), position=1)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=676922)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=676923)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'eigvalsh'), position=2, insert_id=676924)",
            "Update(target_node=ASTNode(type=identifier, text=eigenvectors), value='v')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=eigenvectors), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=v), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=symeig), value='linalg')",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=[, text=[))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1271,
        "neg_line": [
            "-return torch.stack([v.symeig(eigenvectors=False)[0][:1] > 0.0"
        ],
        "pos_line": [
            "+return torch.stack([torch.linalg.eigvalsh(v)[:1] > 0.0"
        ],
        "core_change": "-return torch.stack([v.symeig(eigenvectors=False)[0][:1] > 0.0 +return torch.stack([torch.linalg.eigvalsh(v)[:1] > 0.0",
        "core_API": "reshape"
    },
    {
        "commit_hash": "64e6cd0c958a35bcf9a0af7cf9fed448498e2ea4",
        "index": "764ad2bf..5a39a2df 100644",
        "commit_message": "Fix typos in torchscript tests.\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_dna_conv():",
            "",
            "t = '(Tensor, SparseTensor, OptTensor) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(conv(x, adj1.t()), out1, atol=1e-6)",
            "-    assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6)",
            "+    assert torch.allclose(jit(x, adj1.t()), out1, atol=1e-6)",
            "+    assert torch.allclose(jit(x, adj2.t()), out2, atol=1e-6)",
            "",
            "conv.cached = True",
            "conv(x, edge_index)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1010139)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1010140)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1010141)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'allclose'), position=2, insert_id=1010142)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1010143)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=5)",
            "Update(target_node=ASTNode(type=identifier, text=conv), value='jit')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=1010144)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1010145)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=1010146)",
            "Update(target_node=ASTNode(type=identifier, text=conv), value='jit')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=allclose))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=atol))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=float, text=1e-6))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 1272,
        "neg_line": [
            "-assert torch.allclose(conv(x, adj1.t()), out1, atol=1e-6)",
            "-assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6)"
        ],
        "pos_line": [
            "+assert torch.allclose(jit(x, adj1.t()), out1, atol=1e-6)",
            "+assert torch.allclose(jit(x, adj2.t()), out2, atol=1e-6)"
        ],
        "core_change": "-assert torch.allclose(conv(x, adj1.t()), out1, atol=1e-6) -assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6) +assert torch.allclose(jit(x, adj1.t()), out1, atol=1e-6) +assert torch.allclose(jit(x, adj2.t()), out2, atol=1e-6)",
        "core_API": "script"
    },
    {
        "commit_hash": "64e6cd0c958a35bcf9a0af7cf9fed448498e2ea4",
        "index": "62353a40..15fa5492 100644",
        "commit_message": "Fix typos in torchscript tests.\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_gat_conv():",
            "",
            "t = '(OptPairTensor, SparseTensor, Size, NoneType) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(conv((x1, x2), adj.t()), out1, atol=1e-6)",
            "-    assert torch.allclose(conv((x1, None), adj.t()), out2, atol=1e-6)",
            "+    assert torch.allclose(jit((x1, x2), adj.t()), out1, atol=1e-6)",
            "+    assert torch.allclose(jit((x1, None), adj.t()), out2, atol=1e-6)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1010159)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1010160)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1010161)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'allclose'), position=2, insert_id=1010162)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1010163)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=5)",
            "Update(target_node=ASTNode(type=identifier, text=conv), value='jit')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=1010164)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1010165)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=1010166)",
            "Update(target_node=ASTNode(type=identifier, text=conv), value='jit')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=allclose))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=atol))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=float, text=1e-6))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 1273,
        "neg_line": [
            "-assert torch.allclose(conv((x1, x2), adj.t()), out1, atol=1e-6)",
            "-assert torch.allclose(conv((x1, None), adj.t()), out2, atol=1e-6)"
        ],
        "pos_line": [
            "+assert torch.allclose(jit((x1, x2), adj.t()), out1, atol=1e-6)",
            "+assert torch.allclose(jit((x1, None), adj.t()), out2, atol=1e-6)"
        ],
        "core_change": "-assert torch.allclose(conv((x1, x2), adj.t()), out1, atol=1e-6) -assert torch.allclose(conv((x1, None), adj.t()), out2, atol=1e-6) +assert torch.allclose(jit((x1, x2), adj.t()), out1, atol=1e-6) +assert torch.allclose(jit((x1, None), adj.t()), out2, atol=1e-6)",
        "core_API": "script"
    },
    {
        "commit_hash": "e27aa15ed7fd4f3d735bf7a8a29b97fdede710bd",
        "index": "3b6ad73..474ab27 100644",
        "commit_message": "Add usage of batch norm in conv test and fix usage of is_training collection\n\n",
        "file": "skflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def batch_normalize(tensor_in, epsilon=1e-5, convnet=True, decay=0.9,",
            "\"\"\"Internal function that updates mean and variance during training\"\"\"",
            "with tf.control_dependencies([ema_assign_op]):",
            "return tf.identity(assign_mean), tf.identity(assign_var)",
            "-        IS_TRAINING = tf.get_collection(\"IS_TRAINING\")[-1]",
            "-        mean, variance = control_flow_ops.cond(IS_TRAINING,",
            "-                                               update_mean_var,",
            "-                                               lambda: (ema_mean, ema_var))",
            "+        is_training = tf.squeeze(tf.get_collection(\"IS_TRAINING\"))",
            "+        mean, variance = tf.python.control_flow_ops.cond(",
            "+            is_training, update_mean_var, lambda: (ema_mean, ema_var))",
            "return tf.nn.batch_norm_with_global_normalization(",
            "tensor_in, mean, variance, beta, gamma, epsilon,",
            "scale_after_normalization=scale_after_normalization)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=IS_TRAINING), value='is_training')",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2170525)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2170526)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2170527)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2170528)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2170529)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'squeeze'), position=2, insert_id=2170530)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2170531)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2170532)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2170533)",
            "Update(target_node=ASTNode(type=identifier, text=IS_TRAINING), value='is_training')",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2170534)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2170535)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=control_flow_ops), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2170536)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2170537)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'python'), position=2, insert_id=2170538)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 22,
        "number": 1274,
        "neg_line": [
            "-IS_TRAINING = tf.get_collection(\"IS_TRAINING\")[-1]",
            "-mean, variance = control_flow_ops.cond(IS_TRAINING,",
            "-update_mean_var,",
            "-lambda: (ema_mean, ema_var))"
        ],
        "pos_line": [
            "+is_training = tf.squeeze(tf.get_collection(\"IS_TRAINING\"))",
            "+mean, variance = tf.python.control_flow_ops.cond(",
            "+is_training, update_mean_var, lambda: (ema_mean, ema_var))"
        ],
        "core_change": "-IS_TRAINING = tf.get_collection(\"IS_TRAINING\")[-1] -mean, variance = control_flow_ops.cond(IS_TRAINING, -update_mean_var, -lambda: (ema_mean, ema_var)) +is_training = tf.squeeze(tf.get_collection(\"IS_TRAINING\")) +mean, variance = tf.python.control_flow_ops.cond( +is_training, update_mean_var, lambda: (ema_mean, ema_var))",
        "core_API": "control_dependencies"
    },
    {
        "commit_hash": "bc59c00277a30e5a03702773cc7b7b6dc17f6168",
        "index": "40012145..bcb25def 100644",
        "commit_message": "Fix docs links to PyTorch documentation (#856)\n\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "def torch_multinomial(input, num_samples, replacement=False):",
            "",
            "def torch_sign(value):",
            "\"\"\"",
            "-    Like ``torch.sign()`` but also works for numbers.",
            "+    Like :func:`torch.sign`` but also works for numbers.",
            "\"\"\"",
            "if isinstance(value, numbers.Number):",
            "return (value > 0) - (value < 0)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n    Like ``torch.sign()`` but also works for numbers.\n\"\"\"), value='\"\"\"\\n    Like :func:`torch.sign`` but also works for numbers.\\n\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1275,
        "neg_line": [
            "-Like ``torch.sign()`` but also works for numbers."
        ],
        "pos_line": [
            "+Like :func:`torch.sign`` but also works for numbers."
        ],
        "core_change": "-Like ``torch.sign()`` but also works for numbers. +Like :func:`torch.sign`` but also works for numbers.",
        "core_API": "sign"
    },
    {
        "commit_hash": "d2657cbf3a234114f8a70b40e3fc0031d3874309",
        "index": "e3f78176..aea6575d 100644",
        "commit_message": "bug fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LJSpeechDataset(Dataset):",
            "linear = torch.FloatTensor(linear)",
            "mel = torch.FloatTensor(mel)",
            "mel_lengths = torch.LongTensor(mel_lengths)",
            "-            stop_targets = torch.FloatTensor(stop_targets).squeeze()",
            "+            stop_targets = torch.FloatTensor(stop_targets)",
            "",
            "return text, text_lenghts, linear, mel, mel_lengths, stop_targets, item_idxs[0]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=squeeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1278,
        "neg_line": [
            "-stop_targets = torch.FloatTensor(stop_targets).squeeze()"
        ],
        "pos_line": [
            "+stop_targets = torch.FloatTensor(stop_targets)"
        ],
        "core_change": "-stop_targets = torch.FloatTensor(stop_targets).squeeze() +stop_targets = torch.FloatTensor(stop_targets)",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "55927127..d1177ac1 100755",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Synchronization(Optimizer):",
            "return deltas",
            "",
            "do_sync = (time - self.last_sync >= self.sync_frequency)",
            "-        return tf.cond(pred=do_sync, true_fn=sync, false_fn=no_sync)",
            "+        return self.cond(pred=do_sync, true_fn=sync, false_fn=no_sync)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1279,
        "neg_line": [
            "-return tf.cond(pred=do_sync, true_fn=sync, false_fn=no_sync)"
        ],
        "pos_line": [
            "+return self.cond(pred=do_sync, true_fn=sync, false_fn=no_sync)"
        ],
        "core_change": "-return tf.cond(pred=do_sync, true_fn=sync, false_fn=no_sync) +return self.cond(pred=do_sync, true_fn=sync, false_fn=no_sync)",
        "core_API": "cond"
    },
    {
        "commit_hash": "593dd8cf58fcd2fc0ba48aa81724cd82b904f7cf",
        "index": "e69b18cb..6ed94b25 100644",
        "commit_message": "Fix special values overflow/underflow for amp (#3901)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def multi_perspective_match_pairwise(",
            "norm_value = vector1_norm * vector2_norm.transpose(2, 3)",
            "",
            "# (batch, seq_len1, seq_len2, num_perspectives)",
            "-    return (mul_result / norm_value.clamp(min=eps)).permute(0, 2, 3, 1)",
            "+    return (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute(",
            "+        0, 2, 3, 1",
            "+    )",
            "",
            "",
            "class BiMpmMatching(nn.Module, FromParams):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=18682)",
            "Update(target_node=ASTNode(type=identifier, text=eps), value='tiny_value_of_dtype')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=eps), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=18683)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=18684)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=18685)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=18686)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'norm_value'), position=0, insert_id=18687)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=18688)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=18689)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1282,
        "neg_line": [
            "-return (mul_result / norm_value.clamp(min=eps)).permute(0, 2, 3, 1)"
        ],
        "pos_line": [
            "+return (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute(",
            "+0, 2, 3, 1",
            "+)"
        ],
        "core_change": "-return (mul_result / norm_value.clamp(min=eps)).permute(0, 2, 3, 1) +return (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute( +0, 2, 3, 1 +)",
        "core_API": "transpose"
    },
    {
        "commit_hash": "2a372e368282acb4387f0cdb40bce181c535ede4",
        "index": "fe4cd5cce..7492bcac7 100644",
        "commit_message": "Fix module dict in base finetuning (#8170)\n\n* Fix module dict in base finetuning\n\n* Update CHANGELOG.md\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def test_complex_nested_model():",
            "assert len(BaseFinetuning.flatten_modules(model)) == 10",
            "",
            "BaseFinetuning.freeze(model.encoder, train_bn=True)",
            "-    assert not model.encoder[0].conv.weight.requires_grad  # Validate a leaf module parameter is frozen",
            "+    assert not model.encoder[0].module_dict[\"conv\"].weight.requires_grad  # Validate a leaf module parameter is frozen",
            "assert not model.encoder[0].parent_param.requires_grad  # Validate the parent module parameter is frozen",
            "assert model.encoder[0].bn.weight.requires_grad"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('subscript', None), position=0, insert_id=1411815)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1411816)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"conv\"'), position=2, insert_id=1411817)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1411818)",
            "Update(target_node=ASTNode(type=identifier, text=conv), value='module_dict')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1283,
        "neg_line": [
            "-assert not model.encoder[0].conv.weight.requires_grad  # Validate a leaf module parameter is frozen"
        ],
        "pos_line": [
            "+assert not model.encoder[0].module_dict[\"conv\"].weight.requires_grad  # Validate a leaf module parameter is frozen"
        ],
        "core_change": "-assert not model.encoder[0].conv.weight.requires_grad  # Validate a leaf module parameter is frozen +assert not model.encoder[0].module_dict[\"conv\"].weight.requires_grad  # Validate a leaf module parameter is frozen",
        "core_API": "flatten_modules"
    },
    {
        "commit_hash": "8f90b432058584d16df2cb9d58e5243cf83014bf",
        "index": "0513537..2e023d3 100644",
        "commit_message": "fix attention bug. Attention needs to pass on encoder outputs\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTacotronLocationSensitiveAttention(tf.keras.layers.Layer):",
            "",
            "def get_initial_attention(self, batch_size):",
            "\"\"\"Get initial attention.\"\"\"",
            "-        return tf.zeros(shape=[batch_size, self.config.attention_dim], dtype=tf.float32)",
            "+        return tf.zeros(shape=[batch_size, self.config.encoder_lstm_units * 2], dtype=tf.float32)",
            "",
            "",
            "class TFTacotronPrenet(tf.keras.layers.Layer):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('binary_operator', None), position=3, insert_id=2217016)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=2217017)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=2217018)",
            "Update(target_node=ASTNode(type=identifier, text=attention_dim), value='encoder_lstm_units')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1289,
        "neg_line": [
            "-return tf.zeros(shape=[batch_size, self.config.attention_dim], dtype=tf.float32)"
        ],
        "pos_line": [
            "+return tf.zeros(shape=[batch_size, self.config.encoder_lstm_units * 2], dtype=tf.float32)"
        ],
        "core_change": "-return tf.zeros(shape=[batch_size, self.config.attention_dim], dtype=tf.float32) +return tf.zeros(shape=[batch_size, self.config.encoder_lstm_units * 2], dtype=tf.float32)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "09402a2174605b98c14c0050f8c521b8517e2c08",
        "index": "256560b..52ea927 100644",
        "commit_message": "torch.from_tensor() bug fix\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(hyp, opt, device, tb_writer=None):",
            "if rank != -1:",
            "indices = torch.zeros([dataset.n], dtype=torch.int)",
            "if rank == 0:",
            "-                    indices[:] = torch.from_tensor(dataset.indices, dtype=torch.int)",
            "+                    indices[:] = torch.tensor(dataset.indices, dtype=torch.int)",
            "dist.broadcast(indices, 0)",
            "if rank != 0:",
            "dataset.indices = indices.cpu().numpy()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=from_tensor), value='tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1290,
        "neg_line": [
            "-indices[:] = torch.from_tensor(dataset.indices, dtype=torch.int)"
        ],
        "pos_line": [
            "+indices[:] = torch.tensor(dataset.indices, dtype=torch.int)"
        ],
        "core_change": "-indices[:] = torch.from_tensor(dataset.indices, dtype=torch.int) +indices[:] = torch.tensor(dataset.indices, dtype=torch.int)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "15af8913dbdcdbceb4ee25395f696359dd8c9e32",
        "index": "95dca155b..67528e392 100644",
        "commit_message": "Fixed formatting\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BaseWorker(AbstractWorker, ObjectStorage):",
            "",
            "response = command(*args, **kwargs)",
            "",
            "-            #Temporary fix for websockets when returning a tuple of tensors from an LSTM cell",
            "+            # Temporary fix for websockets when returning a tuple of tensors from an LSTM cell",
            "if command_name == \"torch.lstm_cell\":",
            "response = torch.stack(response)",
            "",
            "-            #Temporary fix for websockets when returning a tuple of tensors from torch.sort()",
            "+            # Temporary fix for websockets when returning a tuple of tensors from torch.sort()",
            "if command_name == \"torch.sort\":",
            "Alpha_Tensor_Fixed = (response[0].float(), response[1].float())",
            "response = torch.stack(Alpha_Tensor_Fixed)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1292,
        "neg_line": [
            "-#Temporary fix for websockets when returning a tuple of tensors from an LSTM cell",
            "-#Temporary fix for websockets when returning a tuple of tensors from torch.sort()"
        ],
        "pos_line": [
            "+# Temporary fix for websockets when returning a tuple of tensors from an LSTM cell",
            "+# Temporary fix for websockets when returning a tuple of tensors from torch.sort()"
        ],
        "core_change": "-#Temporary fix for websockets when returning a tuple of tensors from an LSTM cell +# Temporary fix for websockets when returning a tuple of tensors from an LSTM cell -#Temporary fix for websockets when returning a tuple of tensors from torch.sort() +# Temporary fix for websockets when returning a tuple of tensors from torch.sort()",
        "core_API": "stack"
    },
    {
        "commit_hash": "9ebf4896eb16dff3fe24f99fe34c33f3639864a2",
        "index": "cb4d4b3..a9e9e6c 100644",
        "commit_message": "fix SCEModule.sSE (#113)\n\n\n",
        "file": "segmentation_models.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SCSEModule(nn.Module):",
            "nn.Conv2d(in_channels // reduction, in_channels, 1),",
            "nn.Sigmoid(),",
            ")",
            "-        self.sSE = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1), nn.Sigmoid())",
            "+        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())",
            "",
            "def forward(self, x):",
            "return x * self.cSE(x) + x * self.sSE(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=3, insert_id=1510012)",
            "Delete(target_node=ASTNode(type=identifier, text=in_channels))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1294,
        "neg_line": [
            "-self.sSE = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1), nn.Sigmoid())"
        ],
        "pos_line": [
            "+self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())"
        ],
        "core_change": "-self.sSE = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1), nn.Sigmoid()) +self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "a6b294fe0ccf1caa7def9a07a53bac9160f443b7",
        "index": "afeec46306..a8585d3b79 100644",
        "commit_message": "Fixed a problem with jax interpolate where it got into an infinite recursive loop\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def interpolate(",
            "size = [x.shape[0], *size, x.shape[1]]",
            "",
            "if align_corners or mode == \"area\":",
            "-        return ivy.interpolate(",
            "+        return ivy.functional.experimental.interpolate(",
            "x, size, mode=mode, align_corners=align_corners, antialias=antialias",
            ")",
            "x = jnp.transpose(x, (0, *range(2, dims + 2), 1))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1631703)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1631704)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1631705)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1631706)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'experimental'), position=2, insert_id=1631707)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=ivy), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1631708)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1296,
        "neg_line": [
            "-return ivy.interpolate("
        ],
        "pos_line": [
            "+return ivy.functional.experimental.interpolate("
        ],
        "core_change": "-return ivy.interpolate( +return ivy.functional.experimental.interpolate(",
        "core_API": "interpolate"
    },
    {
        "commit_hash": "e97ce38dad5af583763f836ccc87ba14ea52f813",
        "index": "9b16948d..8b177f7c 100755",
        "commit_message": "fix typo in last commit (fix #1202)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "logits = M(image)",
            "if ctx.is_main_training_tower:",
            "for op in M.updates:",
            "-                tf.add_to_collection(tf.GraphKeys.UPDATE_OPS)",
            "+                tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, op)",
            "",
            "# build cost function by tensorflow",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2275272)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'op'), position=3, insert_id=2275273)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1298,
        "neg_line": [
            "-tf.add_to_collection(tf.GraphKeys.UPDATE_OPS)"
        ],
        "pos_line": [
            "+tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, op)"
        ],
        "core_change": "-tf.add_to_collection(tf.GraphKeys.UPDATE_OPS) +tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, op)",
        "core_API": "add_to_collection"
    },
    {
        "commit_hash": "a61aa67e363aef99bd4ae43af92bc8a2b3378de2",
        "index": "7c2f1e80..01c19744 100644",
        "commit_message": "Moving allennlp.nn.decoding to allennlp.state_machines (#1714)\n\n* Moving allennlp.nn.decoding to allennlp.state_machines\n\n* Fixing docs\n\n* Updated TODO\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import numpy as np",
            "from numpy.testing import assert_almost_equal",
            "",
            "from allennlp.common.testing import AllenNlpTestCase",
            "-from allennlp.nn.decoding.decoder_trainers import ExpectedRiskMinimization",
            "-from ..simple_transition_system import SimpleDecoderState, SimpleDecoderStep",
            "+from allennlp.state_machines.trainers import ExpectedRiskMinimization",
            "+from ..simple_transition_system import SimpleState, SimpleTransitionFunction",
            "",
            "",
            "class TestExpectedRiskMinimization(AllenNlpTestCase):",
            "def setUp(self):",
            "super().setUp()",
            "-        self.initial_state = SimpleDecoderState([0], [[0]], [torch.Tensor([0.0])])",
            "-        self.decoder_step = SimpleDecoderStep()",
            "+        self.initial_state = SimpleState([0], [[0]], [torch.Tensor([0.0])])",
            "+        self.decoder_step = SimpleTransitionFunction()",
            "# Cost is the number of odd elements in the action history.",
            "self.supervision = lambda state: torch.Tensor([sum([x%2 != 0 for x in",
            "state.action_history[0]])])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nn), value='state_machines')",
            "Update(target_node=ASTNode(type=identifier, text=decoding), value='trainers')",
            "Update(target_node=ASTNode(type=identifier, text=SimpleDecoderState), value='SimpleState')",
            "Update(target_node=ASTNode(type=identifier, text=SimpleDecoderStep), value='SimpleTransitionFunction')",
            "Update(target_node=ASTNode(type=identifier, text=SimpleDecoderState), value='SimpleState')",
            "Update(target_node=ASTNode(type=identifier, text=SimpleDecoderStep), value='SimpleTransitionFunction')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=decoder_trainers))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 1300,
        "neg_line": [
            "-from allennlp.nn.decoding.decoder_trainers import ExpectedRiskMinimization",
            "-from ..simple_transition_system import SimpleDecoderState, SimpleDecoderStep",
            "-self.initial_state = SimpleDecoderState([0], [[0]], [torch.Tensor([0.0])])",
            "-self.decoder_step = SimpleDecoderStep()"
        ],
        "pos_line": [
            "+from allennlp.state_machines.trainers import ExpectedRiskMinimization",
            "+from ..simple_transition_system import SimpleState, SimpleTransitionFunction",
            "+self.initial_state = SimpleState([0], [[0]], [torch.Tensor([0.0])])",
            "+self.decoder_step = SimpleTransitionFunction()"
        ],
        "core_change": "-from allennlp.nn.decoding.decoder_trainers import ExpectedRiskMinimization -from ..simple_transition_system import SimpleDecoderState, SimpleDecoderStep +from allennlp.state_machines.trainers import ExpectedRiskMinimization +from ..simple_transition_system import SimpleState, SimpleTransitionFunction -self.initial_state = SimpleDecoderState([0], [[0]], [torch.Tensor([0.0])]) -self.decoder_step = SimpleDecoderStep() +self.initial_state = SimpleState([0], [[0]], [torch.Tensor([0.0])]) +self.decoder_step = SimpleTransitionFunction()",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "c466b9c1ff6cc86813972b7d63b1724d2e33a29a",
        "index": "0c3cff03..8aa0567d 100644",
        "commit_message": "fix memory bloat on restore\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args):  # pylint: disable=redefined-outer-name",
            "pos_weight=torch.tensor(10)) if c.stopnet else None",
            "",
            "if args.restore_path:",
            "-        checkpoint = torch.load(args.restore_path)",
            "+        checkpoint = torch.load(args.restore_path, map_location='cpu')",
            "try:",
            "# TODO: fix optimizer init, model.cuda() needs to be called before",
            "# optimizer restore"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1271244)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1271245)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=1271246)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1271247)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'cpu'\"), position=2, insert_id=1271248)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1302,
        "neg_line": [
            "-checkpoint = torch.load(args.restore_path)"
        ],
        "pos_line": [
            "+checkpoint = torch.load(args.restore_path, map_location='cpu')"
        ],
        "core_change": "-checkpoint = torch.load(args.restore_path) +checkpoint = torch.load(args.restore_path, map_location='cpu')",
        "core_API": "tensor"
    },
    {
        "commit_hash": "76ab31e18898d4c2aacb9725cfbe25b230bff974",
        "index": "7511e1dc..9a3d29d7 100644",
        "commit_message": "Fix wrong mps selection below MasOS 12.3\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_optimal_device():",
            "else:",
            "return torch.device(\"cuda\")",
            "",
            "-    if has_mps:",
            "+    if has_mps():",
            "return torch.device(\"mps\")",
            "",
            "return cpu"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('call', None), position=1, insert_id=1137423)",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=has_mps), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1137424)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1137425)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1137426)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1303,
        "neg_line": [
            "-if has_mps:"
        ],
        "pos_line": [
            "+if has_mps():"
        ],
        "core_change": "-if has_mps: +if has_mps():",
        "core_API": "device"
    },
    {
        "commit_hash": "03acade785cde366cc2e7f26b623aef61d3861ba",
        "index": "7297d64e8..b7692e7ef 100644",
        "commit_message": "[Train] Strip \"module.\" from state dict (#30705)\n\nThis PR adds logic to automatically strip the \"module.\" prefix from a user-saved state dict in TorchCheckpoint, which is present if a user obtains the state dict from a DistributedDataParallel module directly. We already obtain the underlying module if a user saves the model object, so this merely makes the logic consistent.\n\nThis PR also edits our examples to remove instances where this operation was conducted in the example itself. This led to issues if train.torch.prepare_model was used with num_workers=1 (eg. on Google Colab), as the module was not wrapped around, thus leading to the .module attribute being missing.\n\nSigned-off-by: Antoni Baum <antoni.baum@protonmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def test_torch_e2e_state_dict(ray_start_4_cpus):",
            "assert predictions.count() == 3",
            "",
            "",
            "+# We can't really test for prepare_model here as we can't detect what the user",
            "+# has saved without loading (and thus triggering the exception anyway)",
            "def test_torch_e2e_dir(ray_start_4_cpus, tmpdir):",
            "def train_func():",
            "model = torch.nn.Linear(3, 1)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 1305,
        "neg_line": [],
        "pos_line": [
            "+# We can't really test for prepare_model here as we can't detect what the user",
            "+# has saved without loading (and thus triggering the exception anyway)"
        ],
        "core_change": "+# We can't really test for prepare_model here as we can't detect what the user +# has saved without loading (and thus triggering the exception anyway)",
        "core_API": "count"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "c5953bd5..dd5ed606 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BiattentiveClassificationNetwork(Model):",
            "\"\"\"",
            "Parameters",
            "-        tokens : Dict[str, Variable], required",
            "+        tokens : Dict[str, torch.LongTensor], required",
            "The output of ``TextField.as_array()``.",
            "-        label : Variable, optional (default = None)",
            "+        label : torch.LongTensor, optional (default = None)",
            "A variable representing the label for each instance in the batch.",
            "Returns"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('attribute', None), position=0, insert_id=36774)",
            "Insert(target_node=ASTNode(type=subscript), node=('attribute', None), position=4, insert_id=36775)",
            "Update(target_node=ASTNode(type=identifier, text=Variable), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Variable), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=36776)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'LongTensor'), position=2, insert_id=36777)",
            "Update(target_node=ASTNode(type=identifier, text=Variable), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Variable), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=36778)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'LongTensor'), position=2, insert_id=36779)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 1307,
        "neg_line": [
            "-tokens : Dict[str, Variable], required",
            "-label : Variable, optional (default = None)"
        ],
        "pos_line": [
            "+tokens : Dict[str, torch.LongTensor], required",
            "+label : torch.LongTensor, optional (default = None)"
        ],
        "core_change": "-tokens : Dict[str, Variable], required +tokens : Dict[str, torch.LongTensor], required -label : Variable, optional (default = None) +label : torch.LongTensor, optional (default = None)",
        "core_API": "as_array"
    },
    {
        "commit_hash": "f0af771d891398fab987e481255a5705392d488f",
        "index": "fb18f1b549..c82b8d9f7e 100644",
        "commit_message": "[Test] Fix count_nonzero\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def count_nonzero(",
            "def _dtype_count_nonzero(a, axis, dtype):",
            "if dtype is None:",
            "return torch.count_nonzero(a, dim=axis)",
            "-        return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)",
            "+        return torch.tensor(torch.count_nonzero(a, dim=axis),",
            "+                            dtype=ivy.as_native_dtype(dtype))",
            "",
            "x = _dtype_count_nonzero(a, axis, dtype)",
            "if not keepdims:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=296218)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=3, insert_id=296219)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=296220)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=296221)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=296222)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=296223)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_native_dtype'), position=2, insert_id=296224)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=296225)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=dtype), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=296226)",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 1308,
        "neg_line": [
            "-return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)"
        ],
        "pos_line": [
            "+return torch.tensor(torch.count_nonzero(a, dim=axis),",
            "+dtype=ivy.as_native_dtype(dtype))"
        ],
        "core_change": "-return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype) +return torch.tensor(torch.count_nonzero(a, dim=axis), +dtype=ivy.as_native_dtype(dtype))",
        "core_API": "count_nonzero"
    },
    {
        "commit_hash": "76bb45964df1e62d1411b0a9e9fc673e9a791c9a",
        "index": "86bbee658..c340f2dac 100644",
        "commit_message": "Add Image feature (#3163)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Support str in encode_objects_to_image_dicts\n\n* Update src/datasets/features/image.py - objects_to_list_of_image_dicts type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class LibrispeechASR(datasets.GeneratorBasedBuilder):",
            "features=datasets.Features(",
            "{",
            "\"file\": datasets.Value(\"string\"),",
            "-                    \"audio\": datasets.features.Audio(sampling_rate=16_000),",
            "+                    \"audio\": datasets.Audio(sampling_rate=16_000),",
            "\"text\": datasets.Value(\"string\"),",
            "\"speaker_id\": datasets.Value(\"int64\"),",
            "\"chapter_id\": datasets.Value(\"int64\"),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=features))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1309,
        "neg_line": [
            "-\"audio\": datasets.features.Audio(sampling_rate=16_000),"
        ],
        "pos_line": [
            "+\"audio\": datasets.Audio(sampling_rate=16_000),"
        ],
        "core_change": "-\"audio\": datasets.features.Audio(sampling_rate=16_000), +\"audio\": datasets.Audio(sampling_rate=16_000),",
        "core_API": "Features"
    },
    {
        "commit_hash": "27c8143c671b25740a8a587b96e182bb7fa4ba6f",
        "index": "a921b5bd..dad1ad60 100644",
        "commit_message": "fix typing callable in load storage (#1768)\n\n* fix typing callable in load storage\n\n* missing import\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MKDDescriptor(nn.Module):",
            "",
            "",
            "def load_whitening_model(kernel_type: str, training_set: str) -> Dict:",
            "-    whitening_models = torch.hub.load_state_dict_from_url(urls[kernel_type], map_location=lambda storage, loc: storage)",
            "+    storage_fcn: Callable = lambda storage, loc: storage",
            "+    whitening_models = torch.hub.load_state_dict_from_url(",
            "+        urls[kernel_type], map_location=storage_fcn",
            "+    )",
            "whitening_model = whitening_models[training_set]",
            "return whitening_model"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=402658)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=402659)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'storage_fcn'), position=0, insert_id=402660)",
            "Insert(target_node=IN(type=assignment), node=(':', ':'), position=1, insert_id=402661)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=402662)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=3, insert_id=402663)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=lambda), position=4)",
            "Insert(target_node=IN(type=type), node=('identifier', 'Callable'), position=0, insert_id=402664)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'storage_fcn'), position=2, insert_id=402665)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1311,
        "neg_line": [
            "-whitening_models = torch.hub.load_state_dict_from_url(urls[kernel_type], map_location=lambda storage, loc: storage)"
        ],
        "pos_line": [
            "+storage_fcn: Callable = lambda storage, loc: storage",
            "+whitening_models = torch.hub.load_state_dict_from_url(",
            "+urls[kernel_type], map_location=storage_fcn",
            "+)"
        ],
        "core_change": "-whitening_models = torch.hub.load_state_dict_from_url(urls[kernel_type], map_location=lambda storage, loc: storage) +storage_fcn: Callable = lambda storage, loc: storage +whitening_models = torch.hub.load_state_dict_from_url( +urls[kernel_type], map_location=storage_fcn +)",
        "core_API": "load_state_dict_from_url"
    },
    {
        "commit_hash": "f1d55314dacb8300e2bd6d3c595883c0cb5336f6",
        "index": "43c654e07..24638764a 100644",
        "commit_message": "changed all asset to if/raise to prevent disable of assert during PYTHONOPTIMISE env (#4655)\n\n* changed all asset to if/raise to prevent disable of assert during PYTHONOPTIMISE env\n\n* minor fix for proper inverson of assert condition\n\n* Changed to AssertionError which is handled at multiple places\n\n* Changed to AssertionError which is handled at multiple places\n\n* added simple test case in test_string to increase test coverage\n\n* added simple test cases in test_string to increase test coverage\n\n* added simple test cases to increase test coverage\n\n* Either None OR More than One worker result found\n\n* changes for review comments\n\n* removed comments, minor changes\n\nCo-authored-by: Vivek Pothina <vivek.pothina@ninjacart.com>\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchTensor(AbstractTensor):",
            "",
            "\"\"\"",
            "",
            "-        assert isinstance(self.child, PointerTensor)",
            "+        if not isinstance(self.child, PointerTensor):",
            "+            raise TypeError(\"child should be a PointerTensor\")",
            "",
            "ps = list(pointers)",
            "ps.append(self)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=(')', ')'), position=1, insert_id=785657)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=785658)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=ps), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=assert, text=assert))",
            "Delete(target_node=ASTNode(type=identifier, text=isinstance))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=child))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=PointerTensor))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assert_statement))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 1312,
        "neg_line": [
            "-assert isinstance(self.child, PointerTensor)"
        ],
        "pos_line": [
            "+if not isinstance(self.child, PointerTensor):",
            "+raise TypeError(\"child should be a PointerTensor\")"
        ],
        "core_change": "-assert isinstance(self.child, PointerTensor) +if not isinstance(self.child, PointerTensor): +raise TypeError(\"child should be a PointerTensor\")",
        "core_API": "append"
    },
    {
        "commit_hash": "cb2d5e5a8086996e313a96d770ff665cb77ce756",
        "index": "cf5b31e3da..e44683a6c4 100644",
        "commit_message": "fix the unravel_index's torch backend as it returned a tuple rather than a torch.tensor\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def unravel_index(",
            "for dim in reversed(shape):",
            "output.append(temp % dim)",
            "temp = temp // dim",
            "-    return tuple(reversed(output))",
            "+    return torch.tensor(reversed(output))",
            "",
            "",
            "unravel_index.support_native_out = False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=264472)",
            "Update(target_node=ASTNode(type=identifier, text=tuple), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tuple), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=264473)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=264474)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1313,
        "neg_line": [
            "-return tuple(reversed(output))"
        ],
        "pos_line": [
            "+return torch.tensor(reversed(output))"
        ],
        "core_change": "-return tuple(reversed(output)) +return torch.tensor(reversed(output))",
        "core_API": "append"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "bb7b1492c..3b965bb9f 100644",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CanineSelfAttention(nn.Module):",
            "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for",
            "# masked positions, this operation will create a tensor which is 0.0 for",
            "# positions we want to attend and -10000.0 for masked positions.",
            "-                attention_mask = (1.0 - attention_mask.float()) * -10000.0",
            "+                attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min",
            "# Apply the attention mask (precomputed for all layers in CanineModel forward() function)",
            "attention_scores = attention_scores + attention_mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=2, insert_id=1195958)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1195959)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1195960)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1195961)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1195962)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1195963)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1195964)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1195965)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1195966)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1195967)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1195968)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1195969)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'attention_scores'), position=0, insert_id=1195970)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1195971)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1195972)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=float, text=10000.0))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 1315,
        "neg_line": [
            "-attention_mask = (1.0 - attention_mask.float()) * -10000.0"
        ],
        "pos_line": [
            "+attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min"
        ],
        "core_change": "-attention_mask = (1.0 - attention_mask.float()) * -10000.0 +attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min",
        "core_API": "float"
    },
    {
        "commit_hash": "ec0267475c16a1913e64cb4f81fd54d153e3d815",
        "index": "61ef9d8fc..a778c2f1e 100644",
        "commit_message": "Fix FlauBERT GPU test (#6142)\n\n* Fix GPU test\n\n* Remove legacy constructor\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FlaubertModel(XLMModel):",
            "# if self.is_decoder and src_enc is not None:",
            "#     src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]",
            "",
            "-        device = input_ids.device if input_ids is not None else inputs_embeds.device",
            "-",
            "# position_ids",
            "if position_ids is None:",
            "position_ids = torch.arange(slen, dtype=torch.long, device=device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=1235423)",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=input_ids))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=input_ids))",
            "Delete(target_node=ASTNode(type=is not, text=is))",
            "Delete(target_node=ASTNode(type=is not, text=not))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=identifier, text=inputs_embeds))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=conditional_expression))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 1317,
        "neg_line": [
            "-device = input_ids.device if input_ids is not None else inputs_embeds.device",
            "-"
        ],
        "pos_line": [],
        "core_change": "-device = input_ids.device if input_ids is not None else inputs_embeds.device -",
        "core_API": "arange"
    },
    {
        "commit_hash": "c02a7cf5460f2d51304a4049d15bcb3d9cca5b31",
        "index": "a1ef6425b5..3b7c9cb6f9 100644",
        "commit_message": "fix dtype, device in `sum`, `prod`, `to_dev` (#1358)\n\n* fix dtype, device in `sum`, `prod`, `to_dev`\n\n* make `device` have `None` as default\n\n* add `dtype = ivy.as_native_dtype(dtype)` and make `copy` positional\n\n* `to_dev` conform to array API\n\n* `astype` fixes to signature\n\n* black\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def prod(",
            "dtype = tf.int64",
            "elif x.dtype == tf.uint64:",
            "dtype = tf.uint64",
            "+    dtype = ivy.as_native_dtype(dtype)",
            "return tf.experimental.numpy.prod(x, axis, dtype, keepdims)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2008446)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2008447)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'dtype'), position=0, insert_id=2008448)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2008449)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2008450)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2008451)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2008452)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=2008453)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2008454)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_native_dtype'), position=2, insert_id=2008455)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2008456)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dtype'), position=1, insert_id=2008457)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2008458)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 1321,
        "neg_line": [],
        "pos_line": [
            "+dtype = ivy.as_native_dtype(dtype)"
        ],
        "core_change": "+dtype = ivy.as_native_dtype(dtype)",
        "core_API": "as_native_dtype"
    },
    {
        "commit_hash": "88acfde2320293a62d0f6541ff20d0b05f7c8099",
        "index": "c2b4456a2..4d30059cd 100644",
        "commit_message": "feat: models parameters check for ner\n\n* feat: parameters check added to ner\n\n* feat: parameters check added to slotfill\n\n* chore: minor clean-up\n\n* fix: fix conll-2003 model file names and archive names\n\n* refactor: remove blank line\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFModel(NNModel, metaclass=TfModelMeta):",
            "opt_scope = tf.variable_scope(optimizer_scope_name)",
            "with opt_scope:",
            "if learnable_scopes is None:",
            "-                variables_to_train = tf.trainable_variables()",
            "+                variables_to_train = tf.global_variables()",
            "else:",
            "variables_to_train = []",
            "for scope_name in learnable_scopes:",
            "-                    for var in tf.trainable_variables():",
            "+                    for var in tf.global_variables():",
            "if scope_name in var.name:",
            "variables_to_train.append(var)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1923371)",
            "Update(target_node=ASTNode(type=identifier, text=trainable_variables), value='global_variables')",
            "Update(target_node=ASTNode(type=identifier, text=trainable_variables), value='global_variables')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1923372)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1923373)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 1323,
        "neg_line": [
            "-variables_to_train = tf.trainable_variables()",
            "-for var in tf.trainable_variables():"
        ],
        "pos_line": [
            "+variables_to_train = tf.global_variables()",
            "+for var in tf.global_variables():"
        ],
        "core_change": "-variables_to_train = tf.trainable_variables() +variables_to_train = tf.global_variables() -for var in tf.trainable_variables(): +for var in tf.global_variables():",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "24b5e80667c8998d7e5e9689085fecc92a9506d3",
        "index": "0c06a13f..464d2c93 100644",
        "commit_message": "Fix axis specification in TF.\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def abs(x):",
            "",
            "",
            "def sqrt(x):",
            "-    x = tf.clip_by_value(x, _EPSILON, np.inf)",
            "+    x = tf.clip_by_value(x, 0., np.inf)",
            "return tf.sqrt(x)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '0.'), position=3, insert_id=2120291)",
            "Delete(target_node=ASTNode(type=identifier, text=_EPSILON))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1324,
        "neg_line": [
            "-x = tf.clip_by_value(x, _EPSILON, np.inf)"
        ],
        "pos_line": [
            "+x = tf.clip_by_value(x, 0., np.inf)"
        ],
        "core_change": "-x = tf.clip_by_value(x, _EPSILON, np.inf) +x = tf.clip_by_value(x, 0., np.inf)",
        "core_API": "clip_by_value"
    },
    {
        "commit_hash": "b1c261c1e66e589f271f9ec5504338aed2c3746f",
        "index": "46ea6ab39..90846eb84 100644",
        "commit_message": "[release] fix pytorch pbt failure test. (#31791)\n\nThe regression is introduced by #30705.\n\nAlso added some documentation into TorchTrainer so users know there is quite some magic happening :)\n\nTested manually in workspace.\nFollow-up PR to add more strict assertions to the test.\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train_func(config):",
            "checkpoint_epoch = checkpoint_dict[\"epoch\"]",
            "starting_epoch = checkpoint_epoch + 1",
            "",
            "+    model = train.torch.prepare_model(model)",
            "+",
            "# Load in training and validation data.",
            "transform_train = transforms.Compose(",
            "["
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1103992)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1103993)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'model'), position=0, insert_id=1103994)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1103995)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1103996)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1103997)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1103998)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1103999)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1104000)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'prepare_model'), position=2, insert_id=1104001)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1104002)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'model'), position=1, insert_id=1104003)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1104004)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train'), position=0, insert_id=1104005)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1104006)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=2, insert_id=1104007)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 1328,
        "neg_line": [],
        "pos_line": [
            "+model = train.torch.prepare_model(model)",
            "+"
        ],
        "core_change": "+model = train.torch.prepare_model(model) +",
        "core_API": "prepare_model"
    },
    {
        "commit_hash": "e3139ad3019cf1a5a67c3f469495e3c98ba82609",
        "index": "bf229faad..7ca249eac 100644",
        "commit_message": "fixed calculation of ctc loss in TFWav2Vec2ForCTC (#18014)\n\nCo-authored-by: Sreyan-G@NVIDIA <sreyang@nvidia.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFWav2Vec2ForCTC(TFWav2Vec2PreTrainedModel):",
            "loss = tf.reduce_sum(loss)",
            "if self.config.ctc_loss_reduction == \"mean\":",
            "loss = tf.reduce_mean(loss)",
            "+",
            "+            loss = tf.reshape(loss, (1,))",
            "else:",
            "loss = None"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2363224)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2363225)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'loss'), position=0, insert_id=2363226)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2363227)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2363228)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2363229)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2363230)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2363231)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2363232)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reshape'), position=2, insert_id=2363233)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2363234)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'loss'), position=1, insert_id=2363235)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2363236)",
            "Insert(target_node=IN(type=argument_list), node=('tuple', None), position=3, insert_id=2363237)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2363238)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2363239)",
            "Insert(target_node=IN(type=tuple), node=('integer', '1'), position=1, insert_id=2363240)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2363241)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=2363242)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 1329,
        "neg_line": [],
        "pos_line": [
            "+",
            "+loss = tf.reshape(loss, (1,))"
        ],
        "core_change": "+ +loss = tf.reshape(loss, (1,))",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "0eca74fa5f7bf82f3b93e3e38dd1d84cfedc5630",
        "index": "76330d8..9fe1829 100644",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestPointMeshDistance(TestCaseMixin, unittest.TestCase):",
            "self.assertClose(loss_op, loss_naive)",
            "",
            "# Compare backward pass",
            "-        rand_val = torch.rand((1)).item()",
            "+        rand_val = torch.rand(1).item()",
            "grad_dist = torch.tensor(rand_val, dtype=torch.float32, device=device)",
            "",
            "loss_naive.backward(grad_dist)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=1), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1331,
        "neg_line": [
            "-rand_val = torch.rand((1)).item()"
        ],
        "pos_line": [
            "+rand_val = torch.rand(1).item()"
        ],
        "core_change": "-rand_val = torch.rand((1)).item() +rand_val = torch.rand(1).item()",
        "core_API": "assertClose"
    },
    {
        "commit_hash": "2632d38f9bac93ed9f314a921b25e30a566e89cb",
        "index": "13df1c2..fccfa69 100644",
        "commit_message": "Fix //examples/text_embeddings_v2.\n\nDuring refactor to public APIs, it ended up using the wrong filename.\n\nPiperOrigin-RevId: 268192870\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TextEmbeddingModel(tf.train.Checkpoint):",
            "# Assign the table initializer to this instance to ensure the asset",
            "# it depends on is saved with the SavedModel.",
            "self._table_initializer = tf.lookup.TextFileInitializer(",
            "-        vocab_file_path, tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,",
            "+        write_vocabulary_file(self._vocabulary),",
            "+        tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,",
            "tf.int64, tf.lookup.TextFileIndex.LINE_NUMBER)",
            "self._table = tf.lookup.StaticVocabularyTable(",
            "self._table_initializer, num_oov_buckets=oov_buckets)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1950106)",
            "Update(target_node=ASTNode(type=identifier, text=vocab_file_path), value='write_vocabulary_file')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=vocab_file_path), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1950107)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1950108)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1950109)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1950110)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1950111)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1950112)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_vocabulary'), position=2, insert_id=1950113)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1332,
        "neg_line": [
            "-vocab_file_path, tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,"
        ],
        "pos_line": [
            "+write_vocabulary_file(self._vocabulary),",
            "+tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,"
        ],
        "core_change": "-vocab_file_path, tf.string, tf.lookup.TextFileIndex.WHOLE_LINE, +write_vocabulary_file(self._vocabulary), +tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,",
        "core_API": "TextFileInitializer"
    },
    {
        "commit_hash": "35fe3213ef5b1c566c4d73a9d90b0c2aace04f88",
        "index": "5888df9a..efd3e4fd 100644",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def decompose_essential_matrix(E_mat: torch.Tensor) -> Tuple[torch.Tensor, torch",
            "raise AssertionError(E_mat.shape)",
            "",
            "# decompose matrix by its singular values",
            "-    U, _, V = torch.svd(E_mat)",
            "+    U, _, V = _torch_svd_cast(E_mat)",
            "Vt = V.transpose(-2, -1)",
            "",
            "mask = torch.ones_like(E_mat)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_svd_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=svd))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1333,
        "neg_line": [
            "-U, _, V = torch.svd(E_mat)"
        ],
        "pos_line": [
            "+U, _, V = _torch_svd_cast(E_mat)"
        ],
        "core_change": "-U, _, V = torch.svd(E_mat) +U, _, V = _torch_svd_cast(E_mat)",
        "core_API": "svd"
    },
    {
        "commit_hash": "539ee456d49e1073ce0bf4655bca4e24a60df8c2",
        "index": "7a68a333c..4dea38d5c 100755",
        "commit_message": "[Examples] Replicates the new --log_level feature to all trainer-based pytorch (#12359)\n\n* added log_level\n\n* fix comment\n\n* fixed log_level\n\n* Trigger CI\n\n* Unfied logging\n\n* simplified args for log_level\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def main():",
            "",
            "model.resize_token_embeddings(len(tokenizer))",
            "",
            "-    # Preprocessing the raw_datasets.",
            "+    # Preprocessing the datasets.",
            "# First we tokenize all the texts.",
            "padding = \"max_length\" if args.pad_to_max_length else False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1335,
        "neg_line": [
            "-# Preprocessing the raw_datasets."
        ],
        "pos_line": [
            "+# Preprocessing the datasets."
        ],
        "core_change": "-# Preprocessing the raw_datasets. +# Preprocessing the datasets.",
        "core_API": "resize_token_embeddings"
    },
    {
        "commit_hash": "a0d99fdb2cfcc418809dde975f51097c3d6010ca",
        "index": "01cbdd7..f835566 100644",
        "commit_message": "Fixed weight init for fused weight matrices in fused MHA by adding correct gain factor.\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class EncdecMultiheadAttn(nn.Module):",
            "",
            "def reset_parameters(self):",
            "nn.init.xavier_uniform_(self.in_proj_weight_q)",
            "-        nn.init.xavier_uniform_(self.in_proj_weight_kv)",
            "+        # in_proj_weight_kv has shape [2 * hidden, hidden] but it should be",
            "+        # initialized like a [hidden, hidden] matrix.",
            "+        # sqrt(6 / (hidden + hidden)) / sqrt(6 / (2 * hidden + hidden)) = sqrt(1.5)",
            "+        # therefore xavier_uniform gain should be set to sqrt(1.5).",
            "+        nn.init.xavier_uniform_(self.in_proj_weight_kv, gain=math.sqrt(1.5))",
            "nn.init.xavier_uniform_(self.out_proj_weight)",
            "if self.bias:",
            "nn.init.constant_(self.in_proj_bias_q, 0.)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1314472)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1314473)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1314474)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'gain'), position=0, insert_id=1314475)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1314476)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=1314477)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1314478)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1314479)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=0, insert_id=1314480)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1314481)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sqrt'), position=2, insert_id=1314482)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1314483)",
            "Insert(target_node=IN(type=argument_list), node=('float', '1.5'), position=1, insert_id=1314484)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1338,
        "neg_line": [
            "-nn.init.xavier_uniform_(self.in_proj_weight_kv)"
        ],
        "pos_line": [
            "+# in_proj_weight_kv has shape [2 * hidden, hidden] but it should be",
            "+# initialized like a [hidden, hidden] matrix.",
            "+# sqrt(6 / (hidden + hidden)) / sqrt(6 / (2 * hidden + hidden)) = sqrt(1.5)",
            "+# therefore xavier_uniform gain should be set to sqrt(1.5).",
            "+nn.init.xavier_uniform_(self.in_proj_weight_kv, gain=math.sqrt(1.5))"
        ],
        "core_change": "-nn.init.xavier_uniform_(self.in_proj_weight_kv) +# in_proj_weight_kv has shape [2 * hidden, hidden] but it should be +# initialized like a [hidden, hidden] matrix. +# sqrt(6 / (hidden + hidden)) / sqrt(6 / (2 * hidden + hidden)) = sqrt(1.5) +# therefore xavier_uniform gain should be set to sqrt(1.5). +nn.init.xavier_uniform_(self.in_proj_weight_kv, gain=math.sqrt(1.5))",
        "core_API": "xavier_uniform_"
    },
    {
        "commit_hash": "f91febfbe58d566f86d8e979126687f3149618b4",
        "index": "d3374238b..2111c8d03 100644",
        "commit_message": "fixed to already implemented fucntion\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FeedForwardTransformer(TTSInterface, torch.nn.Module):",
            "spembs = None",
            "",
            "# get option",
            "-        alpha = getattr(inference_args, \"fastspeech_alpha\", None)",
            "+        alpha = getattr(inference_args, \"fastspeech_alpha\", 1.0)",
            "",
            "# inference",
            "_, outs, _ = self._forward("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '1.0'), position=5, insert_id=157250)",
            "Delete(target_node=ASTNode(type=none, text=None))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1341,
        "neg_line": [
            "-alpha = getattr(inference_args, \"fastspeech_alpha\", None)"
        ],
        "pos_line": [
            "+alpha = getattr(inference_args, \"fastspeech_alpha\", 1.0)"
        ],
        "core_change": "-alpha = getattr(inference_args, \"fastspeech_alpha\", None) +alpha = getattr(inference_args, \"fastspeech_alpha\", 1.0)",
        "core_API": "_forward"
    },
    {
        "commit_hash": "8064f2d087a197b513d6615a478f48d2ceeb4198",
        "index": "7a189a28f9..fe8558ca1e 100644",
        "commit_message": "small bug fixes for torch gradients module.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def adam_update(ws, dcdws, lr, mw, vw, step, beta1=0.9, beta2=0.999, epsilon=1e-",
            "",
            "def stop_gradient(x, preserve_type=True):",
            "is_var = is_variable(x)",
            "-    # ToDo: work out why _torch.tensor() wrapping is necessary in certain cases, presumably .detach() should be enough.",
            "-    x = _torch.tensor(x.detach())",
            "+    x = x.detach()",
            "if is_var and preserve_type:",
            "-        return variable(x)",
            "+        return x.requires_grad_()",
            "return x"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=381835)",
            "Update(target_node=ASTNode(type=identifier, text=variable), value='x')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=variable), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=381836)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'requires_grad_'), position=2, insert_id=381837)",
            "Delete(target_node=ASTNode(type=identifier, text=_torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=x))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 15,
        "number": 1343,
        "neg_line": [
            "-# ToDo: work out why _torch.tensor() wrapping is necessary in certain cases, presumably .detach() should be enough.",
            "-x = _torch.tensor(x.detach())",
            "-return variable(x)"
        ],
        "pos_line": [
            "+x = x.detach()",
            "+return x.requires_grad_()"
        ],
        "core_change": "-# ToDo: work out why _torch.tensor() wrapping is necessary in certain cases, presumably .detach() should be enough. -x = _torch.tensor(x.detach()) +x = x.detach() -return variable(x) +return x.requires_grad_()",
        "core_API": "tensor"
    },
    {
        "commit_hash": "2e56a4793e2cf9ee39226c1daf1f54bef1005ce7",
        "index": "7e5bcf1..96fc6f9 100644",
        "commit_message": "rename log_softmax, support dim, fix onnx Softmax\n\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpeedyResNet(nn.Module):",
            "])",
            "self.lin = nn.Linear(512, num_classes, bias=False)",
            "",
            "-  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of logsoftmax",
            "+  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of log_softmax",
            "def forward(self, x):",
            "x = self.ic(x)",
            "x = self.ib(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1344,
        "neg_line": [
            "-# note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of logsoftmax"
        ],
        "pos_line": [
            "+# note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of log_softmax"
        ],
        "core_change": "-# note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of logsoftmax +# note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of log_softmax",
        "core_API": "Linear"
    },
    {
        "commit_hash": "7a9a08c5d3ca4699fb439f691c40e1320b37507a",
        "index": "860f9357e..3d5548b7b 100644",
        "commit_message": "Drop torch 1.6 testing (#10390)\n\n* Drop torch 1.6 support\n\n* Drop 1.6 support\n\n* Update CHANGELOG\n\n* Fixes\n\n* Split change\n\n* Undo change\n\n* 1.7 -> 1.7.1\n\nhttps://github.com/pytorch/pytorch/issues/47354\n\n* Force trigger nightly\n\n* Update .github/workflows/events-nightly.yml\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n\n* Revert 1.7.1 change - try wildcard\n\n* Update adjust versions and test it\n\n* Undo test changes\n\n* Revert \"Undo test changes\"\n\nThis reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def reset_deterministic_algorithm():",
            "yield",
            "if _TORCH_GREATER_EQUAL_1_8:",
            "torch.use_deterministic_algorithms(False)",
            "-    elif _TORCH_GREATER_EQUAL_1_7:",
            "+    else:",
            "torch.set_deterministic(False)",
            "-    else:  # the minimum version Lightning supports is PyTorch 1.6",
            "-        torch._set_deterministic(False)",
            "",
            "",
            "@pytest.fixture"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=elif), value='else')",
            "Delete(target_node=ASTNode(type=identifier, text=_TORCH_GREATER_EQUAL_1_7))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_set_deterministic))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 1346,
        "neg_line": [
            "-elif _TORCH_GREATER_EQUAL_1_7:",
            "-else:  # the minimum version Lightning supports is PyTorch 1.6",
            "-torch._set_deterministic(False)"
        ],
        "pos_line": [
            "+else:"
        ],
        "core_change": "-elif _TORCH_GREATER_EQUAL_1_7: +else: -else:  # the minimum version Lightning supports is PyTorch 1.6 -torch._set_deterministic(False)",
        "core_API": "use_deterministic_algorithms"
    },
    {
        "commit_hash": "202e7bf19a34fc18839ce29515273e321ecd04af",
        "index": "e2281f6636..a0e9a1aeb2 100644",
        "commit_message": "fix (#1174)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LocalSyncParallelOptimizer(object):",
            "",
            "# Then setup the per-device loss graphs that use the shared weights",
            "self._batch_index = tf.placeholder(tf.int32)",
            "-        data_splits = zip(",
            "-            *[tf.split(ph, len(devices)) for ph in input_placeholders])",
            "+",
            "+        # Split on the CPU in case the data doesn't fit in GPU memory.",
            "+        with tf.device(\"/cpu:0\"):",
            "+            data_splits = zip(",
            "+                *[tf.split(ph, len(devices)) for ph in input_placeholders])",
            "+",
            "self._towers = []",
            "for device, device_placeholders in zip(self.devices, data_splits):",
            "self._towers.append(self._setup_device(device,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=2, insert_id=2155474)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=2155475)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=2155476)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=2155477)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=2155478)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=2155479)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=2155480)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2155481)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2155482)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2155483)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2155484)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=2155485)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2155486)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"/cpu:0\"'), position=1, insert_id=2155487)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2155488)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 1347,
        "neg_line": [
            "-data_splits = zip(",
            "-*[tf.split(ph, len(devices)) for ph in input_placeholders])"
        ],
        "pos_line": [
            "+",
            "+# Split on the CPU in case the data doesn't fit in GPU memory.",
            "+with tf.device(\"/cpu:0\"):",
            "+data_splits = zip(",
            "+*[tf.split(ph, len(devices)) for ph in input_placeholders])",
            "+"
        ],
        "core_change": "-data_splits = zip( -*[tf.split(ph, len(devices)) for ph in input_placeholders]) + +# Split on the CPU in case the data doesn't fit in GPU memory. +with tf.device(\"/cpu:0\"): +data_splits = zip( +*[tf.split(ph, len(devices)) for ph in input_placeholders]) +",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "0519ff024992c87c346d5c2fdf27d917d1b6b962",
        "index": "71f1368e3..d46b2fc2c 100644",
        "commit_message": "refactor BaseWorker.send_command to accept explicit arguments. (#3487)\n\n* syft: refactor BaseWorker.send_command to accept explicit arguments.\n\nsyft: remove unused variables\n\n* syft: fix linter issues.\n\n* syft: fix argument type of args_ and update docstring for send_command of BaseWoker class.\n\n* syft, pointers: fix int type to tuple conversion.\n\nCo-authored-by: Shubham Gupta <shubamgupta3121@gmail.com>\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchHook(FrameworkHook):",
            "@wraps(attr)",
            "def overloaded_attr(self_torch, *args, **kwargs):",
            "ptr = hook_self.local_worker.send_command(",
            "-                recipient=self_torch.worker(), message=(f\"{'torch'}.{attr}\", None, args, kwargs)",
            "+                recipient=self_torch.worker(),",
            "+                cmd_name=f\"{'torch'}.{attr}\",",
            "+                args_=args,",
            "+                kwargs_=kwargs,",
            ")",
            "",
            "return ptr.wrap()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=793596)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=,, text=,), position=4)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=793597)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=,, text=,), position=6)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=793598)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=793599)",
            "Update(target_node=ASTNode(type=identifier, text=message), value='cmd_name')",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=message), position=0)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=string, text=f\"{'torch'}.{attr}\"), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'args_'), position=0, insert_id=793600)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=793601)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=args), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'kwargs_'), position=0, insert_id=793602)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=793603)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=kwargs), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 1349,
        "neg_line": [
            "-recipient=self_torch.worker(), message=(f\"{'torch'}.{attr}\", None, args, kwargs)"
        ],
        "pos_line": [
            "+recipient=self_torch.worker(),",
            "+cmd_name=f\"{'torch'}.{attr}\",",
            "+args_=args,",
            "+kwargs_=kwargs,"
        ],
        "core_change": "-recipient=self_torch.worker(), message=(f\"{'torch'}.{attr}\", None, args, kwargs) +recipient=self_torch.worker(), +cmd_name=f\"{'torch'}.{attr}\", +args_=args, +kwargs_=kwargs,",
        "core_API": "send_command"
    },
    {
        "commit_hash": "13acec6d7c78dacd5e1fe9b0b4a325e1d39abc15",
        "index": "776573b..7b6c231 100644",
        "commit_message": "word_language_model: Fix Transformer init_weights\n\nModel was not getting initialized property since it was using the\ndecoder object instead of decoder weight to initialize zeros.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TransformerModel(nn.Module):",
            "def init_weights(self):",
            "initrange = 0.1",
            "nn.init.uniform_(self.encoder.weight, -initrange, initrange)",
            "-        nn.init.zeros_(self.decoder)",
            "+        nn.init.zeros_(self.decoder.weight)",
            "nn.init.uniform_(self.decoder.weight, -initrange, initrange)",
            "",
            "def forward(self, src, has_mask=True):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1346336)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'weight'), position=2, insert_id=1346337)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 1352,
        "neg_line": [
            "-nn.init.zeros_(self.decoder)"
        ],
        "pos_line": [
            "+nn.init.zeros_(self.decoder.weight)"
        ],
        "core_change": "-nn.init.zeros_(self.decoder) +nn.init.zeros_(self.decoder.weight)",
        "core_API": "uniform_"
    },
    {
        "commit_hash": "70bd1a4225f18e283254cc307d1314b4aa203b27",
        "index": "b150d714..03e2c28e 100644",
        "commit_message": "Implement RGB to LUV (#442)\n\n* add rgb <-> luv and doc update\n\n* add gamma nonlinearity in luv\n\n* increase precision on xyz->rgb coeff\n\n* finish tests\n\n* update docs and add citation\n\n* add comments\n\n* fix docs\n\n* update test and correct docs\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def xyz_to_rgb(image: torch.Tensor) -> torch.Tensor:",
            "y: torch.Tensor = image[..., 1, :, :]",
            "z: torch.Tensor = image[..., 2, :, :]",
            "",
            "-    r: torch.Tensor = 3.240479 * x + -1.53715 * y + -0.498535 * z",
            "-    g: torch.Tensor = -0.969256 * x + 1.875991 * y + 0.041556 * z",
            "-    b: torch.Tensor = 0.055648 * x + -0.204043 * y + 1.057311 * z",
            "+    r: torch.Tensor = 3.2404813432005266 * x + -1.5371515162713185 * y + -0.4985363261688878 * z",
            "+    g: torch.Tensor = -0.9692549499965682 * x + 1.8759900014898907 * y + 0.0415559265582928 * z",
            "+    b: torch.Tensor = 0.0556466391351772 * x + -0.2040413383665112 * y + 1.0573110696453443 * z",
            "",
            "out: torch.Tensor = torch.stack((r, g, b), dim=-3)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=2, insert_id=443849)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=type), position=2)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=443850)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=443851)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=443852)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=443853)",
            "Update(target_node=ASTNode(type=float, text=0.041556), value='0.0415559265582928')",
            "Update(target_node=ASTNode(type=float, text=1.057311), value='1.0573110696453443')",
            "Update(target_node=ASTNode(type=float, text=3.240479), value='3.2404813432005266')",
            "Update(target_node=ASTNode(type=float, text=0.498535), value='0.4985363261688878')",
            "Update(target_node=ASTNode(type=float, text=1.875991), value='1.8759900014898907')",
            "Update(target_node=ASTNode(type=float, text=0.055648), value='0.0556466391351772')",
            "Update(target_node=ASTNode(type=float, text=1.53715), value='1.5371515162713185')",
            "Update(target_node=ASTNode(type=float, text=0.969256), value='0.9692549499965682')",
            "Update(target_node=ASTNode(type=float, text=0.204043), value='0.2040413383665112')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 20,
        "number": 1354,
        "neg_line": [
            "-r: torch.Tensor = 3.240479 * x + -1.53715 * y + -0.498535 * z",
            "-g: torch.Tensor = -0.969256 * x + 1.875991 * y + 0.041556 * z",
            "-b: torch.Tensor = 0.055648 * x + -0.204043 * y + 1.057311 * z"
        ],
        "pos_line": [
            "+r: torch.Tensor = 3.2404813432005266 * x + -1.5371515162713185 * y + -0.4985363261688878 * z",
            "+g: torch.Tensor = -0.9692549499965682 * x + 1.8759900014898907 * y + 0.0415559265582928 * z",
            "+b: torch.Tensor = 0.0556466391351772 * x + -0.2040413383665112 * y + 1.0573110696453443 * z"
        ],
        "core_change": "-r: torch.Tensor = 3.240479 * x + -1.53715 * y + -0.498535 * z -g: torch.Tensor = -0.969256 * x + 1.875991 * y + 0.041556 * z -b: torch.Tensor = 0.055648 * x + -0.204043 * y + 1.057311 * z +r: torch.Tensor = 3.2404813432005266 * x + -1.5371515162713185 * y + -0.4985363261688878 * z +g: torch.Tensor = -0.9692549499965682 * x + 1.8759900014898907 * y + 0.0415559265582928 * z +b: torch.Tensor = 0.0556466391351772 * x + -0.2040413383665112 * y + 1.0573110696453443 * z",
        "core_API": "stack"
    },
    {
        "commit_hash": "9612c15087ac36e0405ab5d606acf36c32d805ad",
        "index": "03be4fc..25ed9d3 100644",
        "commit_message": "[MRG] Image regressor Added (#92)\n\n* move softmax to loss\n\n* image regressor class created\n\n* metric loss implemented\n\n* util bug fixed\n\n* regressor tested\n\n* bug fix\n\n* refactor bo\n\n* rename\n\n* test fixed\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def set_stub_weight_to_torch(stub_layer, torch_layer):",
            "",
            "",
            "def set_stub_weight_to_keras(stub_layer, keras_layer):",
            "-    stub_layer.export_weights_keras(keras_layer)",
            "\\ No newline at end of file",
            "+    stub_layer.export_weights_keras(keras_layer)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2561879)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=call), position=0)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1356,
        "neg_line": [
            "-stub_layer.export_weights_keras(keras_layer)"
        ],
        "pos_line": [
            "+stub_layer.export_weights_keras(keras_layer)"
        ],
        "core_change": "-stub_layer.export_weights_keras(keras_layer) +stub_layer.export_weights_keras(keras_layer)",
        "core_API": "export_weights_keras"
    },
    {
        "commit_hash": "ca8c7c0ad5a807398a998eb1889f92813c5864f3",
        "index": "904991c..42844f9 100644",
        "commit_message": "fix reshape error\n\n",
        "file": "attention-is-all-you-need-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiHeadAttention(nn.Module):",
            "# perform attention, result size = (n_head * mb_size) x len_q x d_v",
            "outputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))",
            "",
            "-        # back to original mb_size batch",
            "-        outputs = outputs.view(mb_size, len_q, -1)            # mb_size x len_q x (n_head*d_v)",
            "+        # back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v)",
            "+        outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1)",
            "",
            "# project back to residual size",
            "outputs = self.proj(outputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'torch'), position=0, insert_id=59152)",
            "Update(target_node=ASTNode(type=identifier, text=view), value='cat')",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=59153)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=59154)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=59155)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=59156)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=59157)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=59158)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=59159)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=59160)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=unary_operator, text=-1), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=59161)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=59162)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'split'), position=2, insert_id=59163)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=59164)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=outputs), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=mb_size), position=3)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=4)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=59165)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=6)",
            "Update(target_node=ASTNode(type=identifier, text=len_q), value='dim')",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=len_q), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=59166)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '0'), position=2, insert_id=59167)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 1357,
        "neg_line": [
            "-# back to original mb_size batch",
            "-outputs = outputs.view(mb_size, len_q, -1)            # mb_size x len_q x (n_head*d_v)"
        ],
        "pos_line": [
            "+# back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v)",
            "+outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1)"
        ],
        "core_change": "-# back to original mb_size batch -outputs = outputs.view(mb_size, len_q, -1)            # mb_size x len_q x (n_head*d_v) +# back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v) +outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1)",
        "core_API": "attention"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "dfb3b85e..fb203f79 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TestTokenCharactersEncoder(AllenNlpTestCase):",
            "",
            "def test_forward_applies_embedding_then_encoder(self):",
            "numpy_tensor = numpy.random.randint(6, size=(3, 4, 7))",
            "-        inputs = Variable(torch.from_numpy(numpy_tensor))",
            "+        inputs = torch.from_numpy(numpy_tensor)",
            "encoder_output = self.encoder(inputs)",
            "reshaped_input = inputs.view(12, 7)",
            "embedded = self.embedding(reshaped_input)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1360,
        "neg_line": [
            "-inputs = Variable(torch.from_numpy(numpy_tensor))"
        ],
        "pos_line": [
            "+inputs = torch.from_numpy(numpy_tensor)"
        ],
        "core_change": "-inputs = Variable(torch.from_numpy(numpy_tensor)) +inputs = torch.from_numpy(numpy_tensor)",
        "core_API": "randint"
    },
    {
        "commit_hash": "573bdb0a5d2897ff6c7520ebb38693c7acfbf17e",
        "index": "193545eea..11b853509 100644",
        "commit_message": "Add tests to Trainer (#6605)\n\n* Add tests to Trainer\n\n* Test if removing long breaks everything\n\n* Remove ugly hack\n\n* Fix distributed test\n\n* Use float for number of epochs\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def default_data_collator(features: List[InputDataClass]) -> Dict[str, torch.Ten",
            "if isinstance(v, torch.Tensor):",
            "batch[k] = torch.stack([f[k] for f in features])",
            "else:",
            "-                batch[k] = torch.tensor([f[k] for f in features], dtype=torch.long)",
            "+                batch[k] = torch.tensor([f[k] for f in features])",
            "",
            "return batch"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=long))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1361,
        "neg_line": [
            "-batch[k] = torch.tensor([f[k] for f in features], dtype=torch.long)"
        ],
        "pos_line": [
            "+batch[k] = torch.tensor([f[k] for f in features])"
        ],
        "core_change": "-batch[k] = torch.tensor([f[k] for f in features], dtype=torch.long) +batch[k] = torch.tensor([f[k] for f in features])",
        "core_API": "stack"
    },
    {
        "commit_hash": "9ebaea545ffddf2e9079994f2ea657a7fa5f358c",
        "index": "86ac074c..06b814e2 100644",
        "commit_message": "Optimize Stable Diffusion (#371)\n\n* initial commit\n\n* make UNet stream capturable\n\n* try to fix noise_pred value\n\n* remove cuda graph and keep NB\n\n* non blocking unet with PNDMScheduler\n\n* make timesteps np arrays for pndm scheduler\nbecause lists don't get formatted to tensors in `self.set_format`\n\n* make max async in pndm\n\n* use channel last format in unet\n\n* avoid moving timesteps device in each unet call\n\n* avoid memcpy op in `get_timestep_embedding`\n\n* add `channels_last` kwarg to `DiffusionPipeline.from_pretrained`\n\n* update TODO\n\n* replace `channels_last` kwarg with `memory_format` for more generality\n\n* revert the channels_last changes to leave it for another PR\n\n* remove non_blocking when moving input ids to device\n\n* remove blocking from all .to() operations at beginning of pipeline\n\n* fix merging\n\n* fix merging\n\n* model can run in other precisions without autocast\n\n* attn refactoring\n\n* Revert \"attn refactoring\"\n\nThis reverts commit 0c70c0e189cd2c4d8768274c9fcf5b940ee310fb.\n\n* remove restriction to run conv_norm in fp32\n\n* use `baddbmm` instead of `matmul`for better in attention for better perf\n\n* removing all reshapes to test perf\n\n* Revert \"removing all reshapes to test perf\"\n\nThis reverts commit 006ccb8a8c6bc7eb7e512392e692a29d9b1553cd.\n\n* add shapes comments\n\n* hardcore whats needed for jitting\n\n* Revert \"hardcore whats needed for jitting\"\n\nThis reverts commit 2fa9c698eae2890ac5f8e367ca80532ecf94df9a.\n\n* Revert \"remove restriction to run conv_norm in fp32\"\n\nThis reverts commit cec592890c32da3d1b78d38b49e4307aedf459b9.\n\n* revert using baddmm in attention's forward\n\n* cleanup comment\n\n* remove restriction to run conv_norm in fp32. no quality loss was noticed\n\nThis reverts commit cc9bc1339c998ebe9e7d733f910c6d72d9792213.\n\n* add more optimizations techniques to docs\n\n* Revert \"add shapes comments\"\n\nThis reverts commit 31c58eadb8892f95478cdf05229adf678678c5f4.\n\n* apply suggestions\n\n* make quality\n\n* apply suggestions\n\n* styling\n\n* `scheduler.timesteps` are now arrays so we dont need .to()\n\n* remove useless .type()\n\n* use mean instead of max in `test_stable_diffusion_inpaint_pipeline_k_lms`\n\n* move scheduler timestamps to correct device if tensors\n\n* add device to `set_timesteps` in LMSD scheduler\n\n* `self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\n\n* quick fix\n\n* styling\n\n* remove kwargs from schedulers `set_timesteps`\n\n* revert to using max in K-LMS inpaint pipeline test\n\n* Revert \"`self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\"\n\nThis reverts commit 00d5a51e5c20d8d445c8664407ef29608106d899.\n\n* move timesteps to correct device before loop in SD pipeline\n\n* apply previous fix to other SD pipelines\n\n* UNet now accepts tensor timesteps even on wrong device, to avoid errors\n- it shouldnt affect performance if timesteps are alrdy on correct device\n- it does slow down performance if they're on the wrong device\n\n* fix pipeline when timesteps are arrays with strides\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_timestep_embedding(",
            "assert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"",
            "",
            "half_dim = embedding_dim // 2",
            "-    exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32)",
            "+    exponent = -math.log(max_period) * torch.arange(",
            "+        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device",
            "+    )",
            "exponent = exponent / (half_dim - downscale_freq_shift)",
            "",
            "-    emb = torch.exp(exponent).to(device=timesteps.device)",
            "+    emb = torch.exp(exponent)",
            "emb = timesteps[:, None].float() * emb[None, :]",
            "",
            "# scale embeddings"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=104173)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=7)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 1363,
        "neg_line": [
            "-exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32)",
            "-emb = torch.exp(exponent).to(device=timesteps.device)"
        ],
        "pos_line": [
            "+exponent = -math.log(max_period) * torch.arange(",
            "+start=0, end=half_dim, dtype=torch.float32, device=timesteps.device",
            "+)",
            "+emb = torch.exp(exponent)"
        ],
        "core_change": "-exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32) +exponent = -math.log(max_period) * torch.arange( +start=0, end=half_dim, dtype=torch.float32, device=timesteps.device +) -emb = torch.exp(exponent).to(device=timesteps.device) +emb = torch.exp(exponent)",
        "core_API": "log"
    },
    {
        "commit_hash": "beefe6403e1969c9224f2082fa5ebfd0cef68605",
        "index": "2be3745bf..acdc67d47 100644",
        "commit_message": "PaillierTensor and Arg Type Matching (experimental) (#2740)\n\n* Cleanup directory names\n\nWith syft.frameworks.torch, the directory names have becomee disorganized\nand don't seem to follow any set standard. Also, some are simply too long\nand cumbersome to be typing out all the time (such as differential_privacy).\n\nI have replaced long names with their shorthand options, federated leearning\nto simply fl, differential_privacy to dp. I also changed crypto to mpc\nsince all of the algorithms contained within the folder exclusively\nrelated to secure multi-party computation. If and when we implement\na wider varitey of crypto algorithms, we can reconsider this name. However,\nat present, I would like for us to use a more descriptive directory name.\n\n* Revert changed notbooks\n\nI testeed all the notebooks to ensure that the work with the\nrenamd directories but don't wish to actually change them\n\n* Revert changed notbooks\n\nI testeed all the notebooks to ensure that the work with the\nrenamd directories but don't wish to actually change them\n\n* Revert setup.py change\n\n* Revert docs change\n\n* Revert docs change\n\n* Revert docs change\n\n* Init boilerplate example tensor\n\nI am beginning this project by copy-pasting LoggingTensor as a boilerplate\nexample of how to create a custom tensor type in PySyft. I then changed\nall instances of LoggingTensor to PaillierTensor\n\n* Init experimental notebook\n\nI'll be doing my experimenting and development/testing in this notebook\n\n* Add PaillierTensor to hook_args\n\nIn this commit, I added PaillierTensor to the hook_args\nconfiguration file\n\n* Add support for encrypt, decrypt, and __add__\n\nI added support for basic encryption, decryption, and addition to PaillierTensor.\nData within the tensor is stored as a numpy array of phe scalars, where phe\nis the python package for paillier homomorphic encryption. This will allow\nus to use many of the desirable numpy operations as needed\n\n* Remove unused boilerplate\n\n* Parallelize homomorphic encryption and decryption\n\nSince the encryption and decryption step was quite slow, I parallelizd it\nusing the multiprocessing library. I'm pretty sure I'll be able to do this\nfor all of the paillier operations, greatly increasing the speed of the lib.\n\n* Move thread pool to syft.\n\nI don't want to have to re-initialize the thread pool over and over, so I'm\nadding it as a global syft variable\n\n* Hook args automatically\n\nSince PyTorch has a lot of similar methods to Numpy, by defaeult se can use\nthe normal hooking logic for most methods\n\n* Add experimental support for mis-matching args\n\nOne thing we want to be able to do is to add a tensor of scalars to a tensor of\nencrypted valus and vise versa. This was very tricky to do and we need to\nlook closer to make sure I didn't add any security issues but I think\nit looks right.\n\n* Add comment\n\n* Remove unused code\n\n* Add support for encrypted matrix multiplication\n\n* Fix bug in arithmetic\n\n* Fix bug in matmul\n\n* Fix bug in test\n\n* Remove verbose comments\n\n* Run black\n\n* Add phe to requirements.txt\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_section_1_differential_privacy():",
            "query_result = np.argmax(counts)",
            "query_result",
            "",
            "-    from syft.frameworks.torch.differential_privacy import pate",
            "+    from syft.frameworks.torch.dp import pate",
            "",
            "num_teachers, num_examples, num_labels = (100, 100, 10)",
            "preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int)  # fake preds"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=differential_privacy), value='dp')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1364,
        "neg_line": [
            "-from syft.frameworks.torch.differential_privacy import pate"
        ],
        "pos_line": [
            "+from syft.frameworks.torch.dp import pate"
        ],
        "core_change": "-from syft.frameworks.torch.differential_privacy import pate +from syft.frameworks.torch.dp import pate",
        "core_API": "argmax"
    },
    {
        "commit_hash": "f735050c4fc662698cbf9fccce54119167fd7668",
        "index": "c6a2edc3..3fc1a9a0 100644",
        "commit_message": "[ENHANCE] removed deprecated codes for v6.0 (#1281)\n\n* removed deprecation warning and deprecated codes\n\n* Update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestNormalize:",
            "f = kornia.enhance.Normalize(mean=mean, std=std)",
            "data = torch.ones(2, 3, 256, 313)",
            "if isinstance(mean, float):",
            "-            expected = (data - torch.tensor(mean)) / torch.tensor(std)",
            "+            expected = (data - torch.as_tensor(mean)) / torch.as_tensor(std)",
            "else:",
            "-            expected = (data - torch.tensor(mean[0])) / torch.tensor(std[0])",
            "+            expected = (data - torch.as_tensor(mean[0])) / torch.as_tensor(std[0])",
            "assert_close(f(data), expected)",
            "",
            "@staticmethod"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='as_tensor')",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='as_tensor')",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='as_tensor')",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='as_tensor')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 1365,
        "neg_line": [
            "-expected = (data - torch.tensor(mean)) / torch.tensor(std)",
            "-expected = (data - torch.tensor(mean[0])) / torch.tensor(std[0])"
        ],
        "pos_line": [
            "+expected = (data - torch.as_tensor(mean)) / torch.as_tensor(std)",
            "+expected = (data - torch.as_tensor(mean[0])) / torch.as_tensor(std[0])"
        ],
        "core_change": "-expected = (data - torch.tensor(mean)) / torch.tensor(std) +expected = (data - torch.as_tensor(mean)) / torch.as_tensor(std) -expected = (data - torch.tensor(mean[0])) / torch.tensor(std[0]) +expected = (data - torch.as_tensor(mean[0])) / torch.as_tensor(std[0])",
        "core_API": "Normalize"
    },
    {
        "commit_hash": "129350f37c96be5717d9c195b701da57692fd24e",
        "index": "cab27ff4..2b61c284 100755",
        "commit_message": "fixed problem with multi-state/action networks and scoping\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(Distribution):",
            "logits = [log(prob) for _ in range(util.prod(shape)) for prob in probabilities]",
            "action_size = util.prod(self.shape) * self.num_actions",
            "",
            "-        with tf.name_scope(name=scope):",
            "-            self.logits = Linear(size=action_size, bias=logits, scope='logits')",
            "+        self.logits = Linear(size=action_size, bias=logits, scope='logits')",
            "",
            "super(Categorical, self).__init__(scope, summary_labels)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=3)",
            "Delete(target_node=ASTNode(type=with, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=name_scope))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=scope))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=with_clause))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=with_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 1368,
        "neg_line": [
            "-with tf.name_scope(name=scope):",
            "-self.logits = Linear(size=action_size, bias=logits, scope='logits')"
        ],
        "pos_line": [
            "+self.logits = Linear(size=action_size, bias=logits, scope='logits')"
        ],
        "core_change": "-with tf.name_scope(name=scope): -self.logits = Linear(size=action_size, bias=logits, scope='logits') +self.logits = Linear(size=action_size, bias=logits, scope='logits')",
        "core_API": "prod"
    },
    {
        "commit_hash": "9da6a07ac9594a8f94cd2d6ab68309ff514c75c1",
        "index": "7000e5d2..df633567 100644",
        "commit_message": "lint fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class Data(object):",
            "return self",
            "",
            "def cuda(self, props=None):",
            "-        func = lambda x: x.cuda() if torch.cuda.is_available() else x  # noqa",
            "+        def func(x):",
            "+            return x.cuda() if torch.cuda.is_available() else x",
            "+",
            "return self._transer(func, props)",
            "",
            "def cpu(self, props=None):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('function_definition', None), position=0, insert_id=1868443)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=1868444)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=identifier, text=func), position=1)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=1868445)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=:, text=:), position=3)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=1868446)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=1868447)",
            "Move(target_node=IN(type=parameters), node=ASTNode(type=identifier, text=x), position=1)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=1868448)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=1868449)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1868450)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=conditional_expression), position=1)",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=lambda, text=lambda))",
            "Delete(target_node=ASTNode(type=lambda_parameters))",
            "Delete(target_node=ASTNode(type=lambda))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 1371,
        "neg_line": [
            "-func = lambda x: x.cuda() if torch.cuda.is_available() else x  # noqa"
        ],
        "pos_line": [
            "+def func(x):",
            "+return x.cuda() if torch.cuda.is_available() else x",
            "+"
        ],
        "core_change": "-func = lambda x: x.cuda() if torch.cuda.is_available() else x  # noqa +def func(x): +return x.cuda() if torch.cuda.is_available() else x +",
        "core_API": "cuda"
    },
    {
        "commit_hash": "d2c3c251abb28e0cdc0f653bcb4c2116a011f653",
        "index": "15161458..e5851f3e 100644",
        "commit_message": "GH-48: fix warning: flatten parameters in language model\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LanguageModel(nn.Module):",
            "encoded = self.encoder(input)",
            "emb = self.drop(encoded)",
            "",
            "+        self.rnn.flatten_parameters()",
            "+",
            "output, hidden = self.rnn(emb, hidden)",
            "",
            "if self.proj is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1363198)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1363199)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1363200)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1363201)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1363202)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1363203)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flatten_parameters'), position=2, insert_id=1363204)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1363205)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1363206)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1363207)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1363208)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'rnn'), position=2, insert_id=1363209)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 1372,
        "neg_line": [],
        "pos_line": [
            "+self.rnn.flatten_parameters()",
            "+"
        ],
        "core_change": "+self.rnn.flatten_parameters() +",
        "core_API": "encoder"
    },
    {
        "commit_hash": "09e46066eb05f08da27211119e128096afb1cdd7",
        "index": "187e16a1f..042f10500 100644",
        "commit_message": "[Train] Fix accuracy calculation for CIFAR example (#22292)\n\nSame as #21689 except for cifar\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "def train_func(config):",
            "train_dataset = Subset(train_dataset, list(range(64)))",
            "validation_dataset = Subset(validation_dataset, list(range(64)))",
            "",
            "-    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])",
            "-    validation_loader = DataLoader(validation_dataset, batch_size=config[\"batch_size\"])",
            "+    worker_batch_size = config[\"batch_size\"] // train.world_size()",
            "+",
            "+    train_loader = DataLoader(train_dataset, batch_size=worker_batch_size)",
            "+    validation_loader = DataLoader(validation_dataset, batch_size=worker_batch_size)",
            "",
            "train_loader = train.torch.prepare_data_loader(train_loader)",
            "validation_loader = train.torch.prepare_data_loader(validation_loader)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1772460)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1772461)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'worker_batch_size'), position=0, insert_id=1772462)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1772463)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=1772464)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('//', '//'), position=1, insert_id=1772465)",
            "Insert(target_node=IN(type=binary_operator), node=('call', None), position=2, insert_id=1772466)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1772467)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1772468)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train'), position=0, insert_id=1772469)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1772470)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'world_size'), position=2, insert_id=1772471)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1772472)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1772473)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'worker_batch_size'), position=2, insert_id=1772474)",
            "Update(target_node=ASTNode(type=identifier, text=config), value='worker_batch_size')",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=identifier, text=config), position=2)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text=\"batch_size\"))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 1373,
        "neg_line": [
            "-train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])",
            "-validation_loader = DataLoader(validation_dataset, batch_size=config[\"batch_size\"])"
        ],
        "pos_line": [
            "+worker_batch_size = config[\"batch_size\"] // train.world_size()",
            "+",
            "+train_loader = DataLoader(train_dataset, batch_size=worker_batch_size)",
            "+validation_loader = DataLoader(validation_dataset, batch_size=worker_batch_size)"
        ],
        "core_change": "-train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"]) -validation_loader = DataLoader(validation_dataset, batch_size=config[\"batch_size\"]) +worker_batch_size = config[\"batch_size\"] // train.world_size() + +train_loader = DataLoader(train_dataset, batch_size=worker_batch_size) +validation_loader = DataLoader(validation_dataset, batch_size=worker_batch_size)",
        "core_API": "world_size"
    },
    {
        "commit_hash": "f0395cf58e487d5fb01df5ca7e4ae66c88aef45f",
        "index": "900b425b3..3c01286c6 100755",
        "commit_message": "Fix test_model_parallelization (#17249)\n\n* Fix test_model_parallelization\n\n* Modify\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class ModelTesterMixin:",
            "memory_after_parallelization = get_current_gpu_memory_use()",
            "",
            "# Assert that the memory use on all devices is higher than it was when loaded only on CPU",
            "-            for n in range(torch.cuda.device_count()):",
            "+            for n in range(len(model.device_map.keys())):",
            "self.assertGreater(memory_after_parallelization[n], memory_at_start[n])",
            "",
            "# Assert that the memory use of device 0 is lower than it was when the entire model was loaded on it"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'len'), position=0, insert_id=1875849)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1875850)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1875851)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1875852)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1875853)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=device_count), value='keys')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='model')",
            "Update(target_node=ASTNode(type=identifier, text=cuda), value='device_map')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1374,
        "neg_line": [
            "-for n in range(torch.cuda.device_count()):"
        ],
        "pos_line": [
            "+for n in range(len(model.device_map.keys())):"
        ],
        "core_change": "-for n in range(torch.cuda.device_count()): +for n in range(len(model.device_map.keys())):",
        "core_API": "device_count"
    },
    {
        "commit_hash": "f6bcaffe4a67012b4b067038bdbc5d43b807540b",
        "index": "a3d161e5..af7c3a29 100644",
        "commit_message": "Style fixes\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def deconv2d(x, kernel, output_shape, strides=(1, 1),",
            "x = _preprocess_conv2d_input(x, dim_ordering)",
            "output_shape = _preprocess_deconv_output_shape(output_shape, dim_ordering)",
            "kernel = _preprocess_conv2d_kernel(kernel, dim_ordering)",
            "-    kernel = tf.transpose(kernel, (0, 1, 3, 2))  # tranpose kernel chanels",
            "+    kernel = tf.transpose(kernel, (0, 1, 3, 2))",
            "padding = _preprocess_border_mode(border_mode)",
            "strides = (1,) + strides + (1,)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1377,
        "neg_line": [
            "-kernel = tf.transpose(kernel, (0, 1, 3, 2))  # tranpose kernel chanels"
        ],
        "pos_line": [
            "+kernel = tf.transpose(kernel, (0, 1, 3, 2))"
        ],
        "core_change": "-kernel = tf.transpose(kernel, (0, 1, 3, 2))  # tranpose kernel chanels +kernel = tf.transpose(kernel, (0, 1, 3, 2))",
        "core_API": "transpose"
    },
    {
        "commit_hash": "72a6bf33c00662b7094186bf566f21b60a37fd0e",
        "index": "76aa1aa50..9eb17f2c6 100755",
        "commit_message": "[Bert, et al] fix early device assignment (#14447)\n\n* fix early device assignment\n\n* more models\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FNetEmbeddings(nn.Module):",
            "if version.parse(torch.__version__) > version.parse(\"1.6.0\"):",
            "self.register_buffer(",
            "\"token_type_ids\",",
            "-                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),",
            "+                torch.zeros(self.position_ids.size(), dtype=torch.long),",
            "persistent=False,",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=position_ids))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 1378,
        "neg_line": [
            "-torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),"
        ],
        "pos_line": [
            "+torch.zeros(self.position_ids.size(), dtype=torch.long),"
        ],
        "core_change": "-torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device), +torch.zeros(self.position_ids.size(), dtype=torch.long),",
        "core_API": "parse"
    },
    {
        "commit_hash": "aca5f0812cd44813d6703e6a13a075ada6b99e81",
        "index": "5e28a5357..0d37ba8a1 100644",
        "commit_message": "fixing bug for #822\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Conv2dSubsampling(torch.nn.Module):",
            "torch.nn.ReLU()",
            ")",
            "self.out = torch.nn.Sequential(",
            "-            torch.nn.Linear(odim * (idim // 4), odim),",
            "+            torch.nn.Linear(odim * ((idim - 1)// 4), odim),",
            "PositionalEncoding(odim, dropout_rate)",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('parenthesized_expression', None), position=0, insert_id=1341603)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1341604)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=1341605)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1341606)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=idim), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1341607)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1341608)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1380,
        "neg_line": [
            "-torch.nn.Linear(odim * (idim // 4), odim),"
        ],
        "pos_line": [
            "+torch.nn.Linear(odim * ((idim - 1)// 4), odim),"
        ],
        "core_change": "-torch.nn.Linear(odim * (idim // 4), odim), +torch.nn.Linear(odim * ((idim - 1)// 4), odim),",
        "core_API": "ReLU"
    },
    {
        "commit_hash": "73cae2e3aaed5d66eff0e745bcc49312b8ce5252",
        "index": "ae338909..46dade6c 100644",
        "commit_message": "[Feat] Affine scale with non-isotropic values (#646)\n\n* Updated non-isotropic scaling\n\n* Fixed tracerWarning\n\n* Updated 3D non-isotropic scaling\n\n* Refined code\n\n* Added some comments\n\n* Fixed a weird module import error\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestInvertAffineTransform:",
            "",
            "def test_rot90_batch(self, device):",
            "angle = torch.tensor([90.]).to(device)",
            "-        scale = torch.tensor([1.]).to(device)",
            "+        scale = torch.tensor([[1., 1.]]).to(device)",
            "center = torch.tensor([[0., 0.]]).to(device)",
            "expected = torch.tensor([[",
            "[0., -1., 0.],"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=1, insert_id=437571)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=2, insert_id=437572)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=437573)",
            "Move(target_node=IN(type=list), node=ASTNode(type=float, text=1.), position=1)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=437574)",
            "Insert(target_node=IN(type=list), node=('float', '1.'), position=3, insert_id=437575)",
            "Move(target_node=IN(type=list), node=ASTNode(type=], text=]), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1383,
        "neg_line": [
            "-scale = torch.tensor([1.]).to(device)"
        ],
        "pos_line": [
            "+scale = torch.tensor([[1., 1.]]).to(device)"
        ],
        "core_change": "-scale = torch.tensor([1.]).to(device) +scale = torch.tensor([[1., 1.]]).to(device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "6ba2231d7227825dfb76e9df161824d04234e69f",
        "index": "89356f85..b7c54f01 100644",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LDMSuperResolutionPipelineIntegrationTests(unittest.TestCase):",
            "ldm.to(torch_device)",
            "ldm.set_progress_bar_config(disable=None)",
            "",
            "-        generator = torch.Generator(device=torch_device).manual_seed(0)",
            "+        generator = torch.manual_seed(0)",
            "image = ldm(image=init_image, generator=generator, num_inference_steps=20, output_type=\"numpy\").images",
            "",
            "image_slice = image[0, -3:, -3:, -1]",
            "",
            "assert image.shape == (1, 256, 256, 3)",
            "-        expected_slice = np.array([0.7418, 0.7472, 0.7424, 0.7422, 0.7463, 0.726, 0.7382, 0.7248, 0.6828])",
            "+        expected_slice = np.array([0.7644, 0.7679, 0.7642, 0.7633, 0.7666, 0.7560, 0.7425, 0.7257, 0.6907])",
            "+",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=float, text=0.7418), value='0.7644')",
            "Update(target_node=ASTNode(type=float, text=0.7472), value='0.7679')",
            "Update(target_node=ASTNode(type=float, text=0.7424), value='0.7642')",
            "Update(target_node=ASTNode(type=float, text=0.7422), value='0.7633')",
            "Update(target_node=ASTNode(type=float, text=0.7463), value='0.7666')",
            "Update(target_node=ASTNode(type=float, text=0.726), value='0.7560')",
            "Update(target_node=ASTNode(type=float, text=0.7382), value='0.7425')",
            "Update(target_node=ASTNode(type=float, text=0.7248), value='0.7257')",
            "Update(target_node=ASTNode(type=float, text=0.6828), value='0.6907')",
            "Delete(target_node=ASTNode(type=identifier, text=Generator))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch_device))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 1384,
        "neg_line": [
            "-generator = torch.Generator(device=torch_device).manual_seed(0)",
            "-expected_slice = np.array([0.7418, 0.7472, 0.7424, 0.7422, 0.7463, 0.726, 0.7382, 0.7248, 0.6828])"
        ],
        "pos_line": [
            "+generator = torch.manual_seed(0)",
            "+expected_slice = np.array([0.7644, 0.7679, 0.7642, 0.7633, 0.7666, 0.7560, 0.7425, 0.7257, 0.6907])",
            "+"
        ],
        "core_change": "-generator = torch.Generator(device=torch_device).manual_seed(0) +generator = torch.manual_seed(0) -expected_slice = np.array([0.7418, 0.7472, 0.7424, 0.7422, 0.7463, 0.726, 0.7382, 0.7248, 0.6828]) +expected_slice = np.array([0.7644, 0.7679, 0.7642, 0.7633, 0.7666, 0.7560, 0.7425, 0.7257, 0.6907]) +",
        "core_API": "to"
    },
    {
        "commit_hash": "595aca27eab60bc899ac7c2283763e56534d5473",
        "index": "b4b3643..16248db 100644",
        "commit_message": "use assertClose\n\nSummary: use assertClose in some tests, which enforces shape equality. Fixes some small problems, including graph_conv on an empty graph.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D20556912\n\nfbshipit-source-id: 60a61eafe3c03ce0f6c9c1a842685708fb10ac5b\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestMeshEdgeLoss(unittest.TestCase):",
            "loss = mesh_edge_loss(meshes, target_length=target_length)",
            "",
            "predloss = TestMeshEdgeLoss.mesh_edge_loss_naive(meshes, target_length)",
            "-        self.assertTrue(torch.allclose(loss, predloss))",
            "+        self.assertClose(loss, predloss)",
            "",
            "@staticmethod",
            "def mesh_edge_loss("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=allclose), value='assertClose')",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=assertTrue))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1386,
        "neg_line": [
            "-self.assertTrue(torch.allclose(loss, predloss))"
        ],
        "pos_line": [
            "+self.assertClose(loss, predloss)"
        ],
        "core_change": "-self.assertTrue(torch.allclose(loss, predloss)) +self.assertClose(loss, predloss)",
        "core_API": "mesh_edge_loss_naive"
    },
    {
        "commit_hash": "94d20f71a3b9be4ce6764240fa34767114ad616b",
        "index": "b8462b62..70f7ea8a 100644",
        "commit_message": "Remove tl.layers.initialize_global_variables(sess) (#931)\n\n* update sampling layers\n\n* upadte zoom\n\n* fix bug zoom\n\n* typo\n\n* fix bug affine_transform_cv2 x and y\n\n* fix bug crop when crop size equal to image size\n\n* fix file docs typo\n\n* fix bug instance norm\n\n* fix docs\n\n* update examples , init variables\n\n* changelog\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "lambd = .99  # decay factor",
            "e = 0.1  # e-Greedy Exploration, the larger the more random",
            "num_episodes = 10000",
            "with tf.Session() as sess:",
            "-    tl.layers.initialize_global_variables(sess)",
            "+    sess.run(tf.global_variables_initializer())",
            "for i in range(num_episodes):",
            "## Reset environment and get first new observation",
            "episode_time = time.time()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2633288)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=sess), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=layers), value='run')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2633289)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2633290)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2633291)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2633292)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=initialize_global_variables), value='global_variables_initializer')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=initialize_global_variables), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=1, insert_id=2633293)",
            "Delete(target_node=ASTNode(type=identifier, text=tl))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 1387,
        "neg_line": [
            "-tl.layers.initialize_global_variables(sess)"
        ],
        "pos_line": [
            "+sess.run(tf.global_variables_initializer())"
        ],
        "core_change": "-tl.layers.initialize_global_variables(sess) +sess.run(tf.global_variables_initializer())",
        "core_API": "Session"
    },
    {
        "commit_hash": "b2e6cccd53bd6c076c32421b8c4d562a96437524",
        "index": "a9a633d4..4296b105 100644",
        "commit_message": "[IMPORTANT] release test script for layers | basic layer update (#389)\n\n* release test script for layers\n* update basic layer \n* fixed averaged embedding layer for python2\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "sess = tf.InteractiveSession()",
            "",
            "batch_size = 128",
            "x = tf.placeholder(tf.float32, shape=[None, 784])",
            "-y_ = tf.placeholder(",
            "-    tf.int64, shape=[",
            "-        None,",
            "-    ])",
            "+y_ = tf.placeholder(tf.int64, shape=[None])",
            "",
            "",
            "def keras_block(x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 1390,
        "neg_line": [
            "-y_ = tf.placeholder(",
            "-tf.int64, shape=[",
            "-None,",
            "-])"
        ],
        "pos_line": [
            "+y_ = tf.placeholder(tf.int64, shape=[None])"
        ],
        "core_change": "-y_ = tf.placeholder( -tf.int64, shape=[ -None, -]) +y_ = tf.placeholder(tf.int64, shape=[None])",
        "core_API": "InteractiveSession"
    },
    {
        "commit_hash": "34fb9c48350e314b13e3c685cded9b57efbe1b08",
        "index": "a0aad3cc89..7b2ea0da30 100644",
        "commit_message": "fix lint and docstring failures\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def ceil(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor",
            "return torch.ceil(x, out=out)",
            "",
            "",
            "-def floor(x: torch.Tensor,",
            "-          *,",
            "-          out: Optional[torch.Tensor] = None",
            "-          ) -> torch.Tensor:",
            "+def floor(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "if \"int\" in str(x.dtype):",
            "if ivy.exists(out):",
            "return ivy.inplace_update(out, x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=block), node=ASTNode(type=ERROR), position=1)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=351765)",
            "Insert(target_node=IN(type=assignment), node=('pattern_list', None), position=0, insert_id=351766)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=type), position=2)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type==, text==), position=3)",
            "Insert(target_node=IN(type=assignment), node=('ERROR', None), position=4, insert_id=351767)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=assignment), position=5)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=,, text=,), position=1)",
            "Insert(target_node=IN(type=pattern_list), node=('list_splat_pattern', None), position=2, insert_id=351768)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=none, text=None), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=->, text=->), position=2)",
            "Move(target_node=IN(type=list_splat_pattern), node=ASTNode(type=*, text=*), position=0)",
            "Insert(target_node=IN(type=list_splat_pattern), node=('ERROR', None), position=1, insert_id=351769)",
            "Move(target_node=IN(type=list_splat_pattern), node=ASTNode(type=identifier, text=out), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=,, text=,), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=))",
            "Delete(target_node=ASTNode(type=list_splat_pattern))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 1392,
        "neg_line": [
            "-def floor(x: torch.Tensor,",
            "-*,",
            "-out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def floor(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "core_change": "-def floor(x: torch.Tensor, -*, -out: Optional[torch.Tensor] = None -) -> torch.Tensor: +def floor(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
        "core_API": "ceil"
    },
    {
        "commit_hash": "c7c7b008d92a7d22cf7d994c1ca0bb39fa696826",
        "index": "f834967f6..d9bcece88 100644",
        "commit_message": "Squashed commit of the following:\n\ncommit 047d0c474c18a87c205e566948410be16787e477\nMerge: 9396ed37d bfe7bca3a\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 19 09:50:02 2022 -0400\n\n    Merge pull request #4378 from akreal/fix-check_short_utt\n\n    Fix minimum input length for Conv2dSubsampling2 in check_short_utt\n\ncommit bfe7bca3a98da52714e1c45906cf826704464b7c\nAuthor: Pavel Denisov <pavel.denisov@ims.uni-stuttgart.de>\nDate:   Thu May 19 13:41:59 2022 +0200\n\n    Fix minimum input length for Conv2dSubsampling2 in check_short_utt\n\ncommit 9396ed37deb8b101fd064d46c85975ad9047bf87\nMerge: c54b585c1 e047156ec\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 14:50:56 2022 +0900\n\n    Merge pull request #4376 from kamo-naoyuki/libsndfile\n\n    Remove the restriction for libsndfile version\n\ncommit c54b585c1ca6693ae7ba7e299a48af762eda6adf\nMerge: 9ca49caed 88465607c\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Thu May 19 12:29:02 2022 +0900\n\n    Merge pull request #4374 from YosukeHiguchi/master\n\n    Minor fixes for the intermediate loss usage and Mask-CTC decoding\n\ncommit e047156ec8df3266259aed03742ac798e365f648\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 10:11:08 2022 +0900\n\n    remove version restiction for libsndfile\n\ncommit 9ca49caed98410cd7d2c71e4781819a1e92b35d9\nMerge: b008ac7d5 2952c3bca\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 09:38:33 2022 +0900\n\n    Merge pull request #4375 from espnet/kamo-naoyuki-patch-1\n\n    Update .mergify.yml\n\ncommit 88465607cf5e899b8ce1b93c5c9fe09b69a2ab83\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 07:05:29 2022 +0900\n\n    fix for test\n\ncommit 2952c3bca26a70723094d5a160387b7936f71769\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 06:59:02 2022 +0900\n\n    Update .mergify.yml\n\ncommit b008ac7d58e9ced1a9f8c89cc85ee69d9e9461ab\nMerge: 3c96908ed 4203c9c9c\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 06:32:44 2022 +0900\n\n    Merge pull request #4372 from kamo-naoyuki/isort\n\n    Add isort checking to the CI tests\n\ncommit 4de7aa562f74c596e5b616fd8278a50a707d0198\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 06:19:20 2022 +0900\n\n    fix for test\n\ncommit 9c83ddb46404334914764a8e4356ea8a4c3c806c\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 05:05:01 2022 +0900\n\n    support gpu decoding for mask-ctc\n\ncommit 49100e4f1b3fc389c5672dc2ca17973525c4bf02\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 05:03:29 2022 +0900\n\n    fix bug for returning intermediate states\n\ncommit 4203c9c9c9d5a68cd13d464290cead3738ed003d\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 17:47:22 2022 +0900\n\n    apply isort\n\ncommit d0f2eac70a5521adf59618ba3ce6603e2863f0c5\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 17:46:47 2022 +0900\n\n    modified for isort options\n\ncommit 8f73b73d23d34bf5f3e8ed2f625dca1916ea8683\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:38:34 2022 +0900\n\n    apply black\n\ncommit 6974dd4efc11e465d4a3d1a34190c7ed782dacee\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:35:15 2022 +0900\n\n    Add descriptions for isort\n\ncommit 24c3676a8d4c2e60d2726e9bcd9bdbed740610e0\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:16:53 2022 +0900\n\n    Apply isort\n\ncommit 3c96908edc5c592c9c99bba0640428613dc7c3cb\nMerge: c173c3093 aa5d6ffff\nAuthor: Jiatong <728307998@qq.com>\nDate:   Tue May 17 18:00:40 2022 -0700\n\n    Merge pull request #4341 from chintu619/st_bugfix\n\n    bug fixes in ST recipes\n\ncommit c173c30930631731e6836c274a591ad571749741\nMerge: e0e0620ac d38188cc3\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 17 15:20:31 2022 +0900\n\n    Merge pull request #4371 from espnet/kamo-naoyuki-patch-1\n\n    Update .mergify.yml\n\ncommit d38188cc30af6cffc4ad0233e7e705e93511c11d\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 17 13:43:40 2022 +0900\n\n    Update .mergify.yml\n\ncommit e0e0620acca0df345cf317a13c839d7d4d5c773f\nMerge: df053b8c1 2cfbbd337\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 13:01:02 2022 +0900\n\n    Merge pull request #4369 from kan-bayashi/minor_fix_jets\n\ncommit 2cfbbd337d64f68e1f937e37feeb544d972c4e0b\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 11:06:00 2022 +0900\n\n    updated jets test\n\ncommit 17ab7747fe7e0d4d6885847f2c738253a859dedf\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 11:05:52 2022 +0900\n\n    updated README\n\ncommit 6ec8c27815c6fded4c13b01b8d2707016e9e8e95\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 09:25:41 2022 +0900\n\n    updated README\n\ncommit b1e6c752b0d94f3209593e0cdbd5b43d79e8076d\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 09:19:54 2022 +0900\n\n    shorten jets test\n\ncommit df053b8c13c26fe289fc882751801fd781e9d43e\nMerge: afa8f8ec5 5aa543a9f\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 08:13:36 2022 +0900\n\n    Merge pull request #4364 from imdanboy/master\n\n    add e2e tts model: JETS\n\ncommit 5aa543a9ff6c329f5fc601f3aa053ffd4afb19ba\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Mon May 16 21:13:30 2022 +0900\n\n    minor fix of docstrings and comments\n\ncommit a82e78d18aca9c00bcf8f378c42e78a0de24940e\nAuthor: imdanboy <imdanboy@gmail.com>\nDate:   Fri May 13 22:28:31 2022 +0900\n\n    JETS; e2e tts model\n\ncommit afa8f8ec5b8ec77deb1a3c1531915ebbee7b80e6\nMerge: fffb3444f cd77501a8\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Fri May 13 17:36:30 2022 -0400\n\n    Merge pull request #4349 from pyf98/quantization\n\n    Add quantization in ESPnet2 for asr inference\n\ncommit fffb3444fe4d8ef2630a22dd145d6f1fb0caab46\nMerge: f840b8114 5331890e6\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 20:36:39 2022 +0900\n\n    Merge pull request #4361 from espnet/kamo-naoyuki-patch-1\n\n    Update README.md\n\ncommit aa5d6ffff67079f2cbe6a7e1eba852e459f0f6a4\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 05:15:32 2022 -0400\n\n    fix lm tag names\n\ncommit 3cac7bb7f732a694f4b87007271d394a9ee3838e\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 05:07:55 2022 -0400\n\n    resolve conflicts and fix lm_train filenames\n\ncommit ea44663e8a24ebfcaa03f3bba149e561e970fdf3\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 04:43:18 2022 -0400\n\n    review suggested changes\n\ncommit 650c733437da32627f88fe369555ce1955536087\nMerge: 6d1bd3a8e f840b8114\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 03:18:08 2022 -0400\n\n    Merge branch 'espnet_master' into st_bugfix\n\ncommit 5331890e6a6a61a3006e5e2c13d47172f5587a29\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 13:15:40 2022 +0900\n\n    Update README.md\n\ncommit f840b8114452b4803b8fb25c1f22a93da146e9ba\nMerge: 1b1241040 9cfd6af64\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 13:13:34 2022 +0900\n\n    Merge pull request #4348 from kamo-naoyuki/1.11.0\n\n    Add pytorch=1.10.2 and 1.11.0 to ci configurations\n\ncommit 9cfd6af64a28237019196cd495fbd2943790ce21\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 09:58:04 2022 +0900\n\n    fix\n\ncommit 2625be71a722e7eb030dff4f71d8dc9599a33844\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 03:46:24 2022 +0900\n\n    remove warning\n\ncommit 9a2001fac56dddf5ba1c2eaec092cb420f83f7c9\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 03:44:11 2022 +0900\n\n    fix for pytorch1.11 (+= became inplace op)\n\ncommit 5518b6ba0af0bba9e9d59d6c47607656f49c9988\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 22:04:42 2022 +0900\n\n    fix import order\n\ncommit 98689a5f0bfd88efffdbbcdd5d924e186d563a91\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 21:17:35 2022 +0900\n\n    change to show the error logs when jobs are failed\n\ncommit bb0d0aaa9e9f9076ac88aad425ad2f2caef369a7\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:40:39 2022 +0900\n\n    fix code style\n\ncommit 934b161f1f714637c3d7d47c14f8c810a9df6fe2\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:33:58 2022 +0900\n\n    change to show the error logs when jobs are failed\n\ncommit 5c474b96c543c3d26e95b432355bcfd2bf8dc116\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:20:18 2022 +0900\n\n    remove verbosity options\n\ncommit 005aad11b37acf388c6b70143ab40a5231bc7a39\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:04:57 2022 +0900\n\n    fix\n\ncommit 5c4b966a957062e4de298bcb69fe8cf6f1365fd1\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:36:11 2022 +0900\n\n    remove tests for python=3.10.0 temporary\n\ncommit 809ac3741814b7d9ebdd351b9e0e9343e236977c\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:27:20 2022 +0900\n\n    fix\n\ncommit 86186b744fb2bfc259909c49cc906fb0856d15bf\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:10:18 2022 +0900\n\n    add installation for packaging\n\ncommit 8fbac77268906075043cbecfb3e1c5625b145fce\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:59:17 2022 +0900\n\n    fix\n\ncommit b0050d97da3d0545b62a5d21b029ddd016ce6ca1\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:56:52 2022 +0900\n\n    fix\n\ncommit 6e9035d42eea31cad87a7c8b87fc79635a6df7c2\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:32:33 2022 +0900\n\n    fix\n\ncommit 1c344a95ceb83b4b44675aee5326afeb9284d8e8\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:25:35 2022 +0900\n\n    change LooseVersion to parse\n\ncommit f899a05768436cc38fb432d6f002ab667983abbd\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:09:33 2022 +0900\n\n    fix\n\ncommit 7d5242212403e740c4d5b8ebd9a346a991ea50a9\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:09:15 2022 +0900\n\n    fix\n\ncommit b7cfdd9a70559271e45de103e242228f94e837ff\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:05:41 2022 +0900\n\n    Change LooseVersion to parse\n\ncommit d234b9ab30bbc2bb6fd42d6335421a6f8a9ed637\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 17:10:40 2022 +0900\n\n    fix\n\ncommit 1b1241040e1e30e575a182b6be8b8e4602badeb8\nMerge: 39bae01e4 52c238d02\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Wed May 11 13:00:13 2022 -0400\n\n    Merge pull request #4352 from espnetUser/master\n\n    Add unit test to streaming ASR inference\n\ncommit 52c238d02d50fcfb2c4e2a5058c743c7db913eec\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 16:10:04 2022 +0200\n\n    Applied black formating to test_asr_inference.py for PR\n\ncommit 87c7573874aeec096dd1e902478d3dd6e2c83ad2\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 15:43:01 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Fix CI error on mismatch in Tensor dtypes\n\ncommit 39bae01e4a132da69b9b0d025da8c579a5f38b77\nMerge: dd24d7d41 71f3c8813\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 17:53:04 2022 +0900\n\n    Merge pull request #4355 from kan-bayashi/fix_lid_in_gan_tts\n\ncommit dd24d7d41517202b308afb186f466c8006ae4c14\nMerge: 2dde7734b f7b390582\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 17:52:09 2022 +0900\n\n    Merge pull request #4206 from WeiGodHorse/master\n\ncommit 2dde7734bade874d4f8cfe7df4be069e64259fd5\nMerge: beb336027 ec7e2b07b\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 16:27:55 2022 +0900\n\n    Merge pull request #4356 from kan-bayashi/fix_mixed_precision_vits\n\n    fix loss = NaN in VITS with mixed precision\n\ncommit 7a590ccd0da4897ef283486776f134eabe865ce0\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 09:25:03 2022 +0200\n\n    Applied black formating to test_asr_inference.py for PR\n\ncommit ec7e2b07bfa85c8a2292de7a2edbf1c2cd956d99\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 14:48:36 2022 +0900\n\n    fixed black\n\ncommit 2be9ddc5a2c0a7c4aad2b155fa1450222ca0c7a3\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 14:28:05 2022 +0900\n\n    fixed mixed_precision NaN (#4236)\n\ncommit 71f3c88133c7a29db54baa7eaa3b4fdf329cbdf5\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 13:39:59 2022 +0900\n\n    fixed optional data names for TTS\n\ncommit ee57ff94dfa2c3ced30c1b103076b4ae18fa9199\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 22:37:18 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Fix dtype CI error\n\ncommit 272d5d015f89f1520c82c31bd309fdce89d88f50\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 21:52:21 2022 +0200\n\n    Update test_asr_inference.py\n\n    Remove streaming=true parameter\n\ncommit c96e0d7f79e6e94e568b22156eb61004d5d8cf8c\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 21:25:57 2022 +0200\n\n    Aplied black formating to test_asr_inference.py for PR\n\ncommit cd77501a8f09b5b11bf5422b0e24b8316820af77\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Tue May 10 12:02:07 2022 -0400\n\n    fix error for rnn encoders flatten_parameters\n\ncommit 3aafdb9d92c8c61d62be72f0907da957d177aa8c\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 17:05:48 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Bugfix in streaming inference #4216\n\ncommit 61b50138b7e8828506a18067cc2f482e745e83d7\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 16:58:14 2022 +0200\n\n    Update test_asr_inference.py\n\n    Added edge test case for streaming asr unit test and increased execution time out\n\ncommit 052dd603900362048675f65058b7a6f4bd94bc7d\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 23:27:41 2022 -0400\n\n    fix ci\n\ncommit 06e2a7a16a06cda326035d03c84734d18c852cd3\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 23:10:14 2022 -0400\n\n    apply black\n\ncommit a48423fda5ab75d1205396ca5f744dc8ca98df00\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 22:59:57 2022 -0400\n\n    add test for espnet2 quantization\n\ncommit acb24c886f47fec7a00063cb66423e7bd52ea0bc\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 22:59:39 2022 -0400\n\n    add quantization to asr_inference\n\ncommit b98fc861939310b73b50f959bc45176da10ef493\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:52:27 2022 +0900\n\n    fix\n\ncommit 3428f032d58c73902b5e6fe80307eb08cfc64ff6\nMerge: 4ff2ce124 beb336027\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:42:23 2022 +0900\n\n    Merge branch 'master' into 1.11.0\n\ncommit 4ff2ce1244e0af72439deaa59226eba434a70618\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:34:31 2022 +0900\n\n    add pytorch=1.10.1, 1.11.0 to ci configurations\n\ncommit beb3360276aa9ff65fe84f4c5e99c0c063c2a6be\nMerge: 537f9b6c1 79cda74ba\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Mon May 9 16:27:37 2022 -0400\n\n    Merge pull request #4347 from YosukeHiguchi/espnet2_maskctc2\n\n    Minor fix for Mask-CTC forward function\n\ncommit 79cda74ba20f0b795251e23a9cb9fd624e2be02d\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Mon May 9 22:43:29 2022 +0900\n\n    add kwargs in forward argument\n\ncommit 537f9b6c14ab195cdcd21c404656c8534295f15d\nMerge: 793b999a5 9e8e75315\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Sun May 8 17:34:55 2022 -0400\n\n    Merge pull request #4343 from Emrys365/complex_support\n\n    Fix a bug in stats aggregation when PITSolver is used\n\ncommit 9e8e753154f5f71c9cb26217483427adb278759c\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Sat May 7 13:16:35 2022 +0800\n\n    Apply black\n\ncommit 5ea4e087a311ab7c798950e68ae92e10b1bb41d8\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Sat May 7 12:05:49 2022 +0800\n\n    Fix a bug in stats aggregation when PITSolver is used\n\ncommit 6d1bd3a8ef695a75358d019cc1b33100817c0dad\nMerge: eb6dc2d55 793b999a5\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 10:51:14 2022 -0400\n\n    Merge branch 'espnet:master' into st_bugfix\n\ncommit eb6dc2d55faac7e62742d0b7791d8f3a991e91d1\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 10:08:19 2022 -0400\n\n    typo fix\n\ncommit 8c56ee817867358f2a8130372fd914c136bd7a5b\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 08:59:26 2022 -0400\n\n    bug fixes in ST recipes\n\n    * Change sampling frequency in `fbank.conf` and `pitch.conf` in Covost2 recipe\n    * In `run.sh`, if language is low resource, then have more speed perturbations. Fix typos for test sets\n    * In `st.sh`\n      * fix directory naming issues to avoid replacement for different language pairs\n      * Replace `>>` with `>` to replace previous inference results\n      * Fix removing of empty text in stage 4\n      * When removing utterance-ID in `ref.trn.org` or `hyp.trn.org`, the current implementation removes all words in parenthesis instead of removing just the utterance-ID from the end of each line. Fixed this by changing `perl -pe 's/\\([^\\)]+\\)//g;'` to `perl -pe 's/\\([^\\)]+\\)$//g;'`\n\ncommit f7b390582d2d77b113a92a5e52f907d5832d6f04\nAuthor: È≠èÂÆ™Ë±™ <weixianhao@bytedance.com>\nDate:   Fri May 6 20:18:05 2022 +0800\n\n    change a test file to conform new pypinyin package\n\ncommit b83128fafc913e775a49d37a5cad24a893718020\nAuthor: È≠èÂÆ™Ë±™ <weixianhao@bytedance.com>\nDate:   Fri May 6 17:54:20 2022 +0800\n\n    Fix missing punctuation\n\ncommit 931fd226babe69b35c6e3a6a288e5e0c901736a1\nAuthor: È≠èÂÆ™Ë±™ <weixianhao@bytedance.com>\nDate:   Fri May 6 16:54:31 2022 +0800\n\n    reformat\n\ncommit 793b999a50af484a5eaf6227ef7556b48514ef15\nMerge: 4f41a1a06 6d0672882\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:54:27 2022 -0400\n\n    Merge pull request #4330 from pyf98/show_translation_result\n\n    Update show_translation_result.sh to show all decoding results under the given exp directory\n\ncommit 4f41a1a06ecd96af567bc73d1d6734531dd3cb44\nMerge: a49cc60cd f0d7cc2bf\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:53:10 2022 -0400\n\n    Merge pull request #4329 from roshansh-cmu/wandb\n\n    Wandb Minor Fix for Model Resume\n\ncommit a49cc60cda690e448d925c3e2bfdc5a85b3f5cd3\nMerge: de624ed58 21fba33c6\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:51:43 2022 -0400\n\n    Merge pull request #4338 from espnet/ftshijt-patch-1\n\n    Fix typo\n\ncommit 21fba33c69d9199c6897ffc6da8433ab94b7051d\nAuthor: Jiatong <728307998@qq.com>\nDate:   Thu May 5 21:25:10 2022 -0400\n\n    Fix typo\n\ncommit de624ed58953d17907fb241c5cb6514f27510162\nMerge: b757b89d4 fe288000d\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 16:10:44 2022 -0400\n\n    Merge pull request #4332 from simpleoier/chime6\n\n    add chime6 recipe\n\ncommit c504336661fa3cefa60b2214da39fbf0118fce49\nMerge: 50269e8b4 b757b89d4\nAuthor: È≠èÂÆ™Ë±™ <weixianhao@bytedance.com>\nDate:   Wed May 4 21:58:43 2022 +0800\n\n    Merge remote-tracking branch 'upstream/master'\n\ncommit fe288000dbde339b4c386408af488af4bac423b6\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Tue May 3 17:51:36 2022 -0400\n\n    add egs2/chime6/asr1 recipe\n\ncommit 6d06728820576ed96a729b3477a29ccab12542f1\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 20:53:52 2022 -0400\n\n    fix ci\n\ncommit 72333a892d16ef913633111120f159008812795e\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 20:34:06 2022 -0400\n\n    fix ci\n\ncommit f15e6adaafaca380ea152cf2b38d604eea3603d3\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:54:37 2022 -0400\n\n    quote expansion\n\ncommit f6731cd97565bf4108f1064a83f1fffea4ca351b\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:43:49 2022 -0400\n\n    update mt.sh\n\ncommit 552060a1d5670d0fd838bd8e10fc9e47a1122346\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:41:41 2022 -0400\n\n    update show translation result\n\ncommit f0d7cc2bfbc8f68c42820262a8ca6e4906f3818b\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Fri Apr 29 20:57:18 2022 -0400\n\n    Delete resnet.py\n\ncommit 79c071e9ecd268a1963e8ca3863a2f5eaf34a525\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Fri Apr 29 20:54:37 2022 -0400\n\n    Wandb minor fix for model resume\n\ncommit ffe7c58ac8a255769f6952b8c7225a5158a00068\nMerge: 835033c70 b757b89d4\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Fri Apr 29 20:45:47 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit b757b89d45d5574cebf44e225cbe32e3e9e4f522\nMerge: 930b380de 664414c8f\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Fri Apr 29 16:11:56 2022 +0900\n\n    Merge pull request #4320 from cadia-lvl/add-progress-bar\n\ncommit 930b380de02b31f8d2da4144d471e60ed41d70fc\nMerge: 2a48371b8 de81cf979\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 16:30:34 2022 -0400\n\n    Merge pull request #4316 from simpleoier/enh_s2t\n\n    add egs2/chime4/enh_asr1 recipe and results\n\ncommit de81cf979fd61ab13e0ab0fe0432fbbaa4776be3\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Thu Apr 28 11:54:10 2022 -0400\n\n    update egs2/chime4/enh_asr1/README.md and related enh1, asr1 configs.\n\ncommit 664414c8f27d5148377ffa733c7f8369eaf7ebd4\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Thu Apr 28 21:31:45 2022 +0900\n\n    fixed flake8\n\ncommit 2a48371b8ceffd4899dc08f2fc5df092ed1d8a93\nMerge: 72c1d8f2b 5a9178236\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:40:31 2022 -0400\n\n    Merge pull request #4243 from D-Keqi/master\n\n    Add streaming ST/SLU\n\ncommit 72c1d8f2bde996febde895c603722dba1634cf20\nMerge: b7f0a5a6f 406656cdc\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:37:23 2022 -0400\n\n    Merge pull request #4110 from earthmanylf/dpclanddan\n\n    Merge Deep Clustering and Deep Attractor Network to enh separator\n\ncommit b7f0a5a6fc227049c1b8735d8ac4362c27333022\nMerge: 44971ff96 2d950f962\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:33:11 2022 -0400\n\n    Merge pull request #4328 from Emrys365/egs2_aishell4\n\n    Rename egs2/clarity21/enh_2021 to egs2/clarity21/enh1\n\ncommit 2d950f96223fd4823203b6a4e9afdc86b2357e7e\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Thu Apr 28 16:58:26 2022 +0800\n\n    Rename egs2/clarity21/enh_2021/\n\ncommit 2b663318cd1773fb8685b1e03295b6bc6889c283\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Thu Apr 28 00:59:22 2022 -0400\n\n    fix small bugs and add CHiME4 enh_asr1 recipe & results\n\ncommit 406656cdcb668a77910074b4382b557b6f845c54\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Thu Apr 28 11:10:11 2022 +0800\n\n    Add custom name in __init__ in tf_domain.py; Merge test_dpcl_loss.py to test_tf_domain.py\n\ncommit 5a9178236bc1a7a4a5db82ad84773d9c43199c81\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:31:29 2022 +0800\n\n    use the another st_inference\n\ncommit 9e4bb7fa88e8c63e69712e77c5b783c64181fbc2\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:13:59 2022 +0800\n\n    fix conflict\n\ncommit 21d2ac6331ec0779b8ec2d3265ccdfabfaacbd61\nMerge: b801ddc96 44971ff96\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:12:15 2022 +0800\n\n    Merge pull request #17 from espnet/master\n\n    merge the latest espnet\n\ncommit b801ddc96aedd2a9b4e63d2e3612c3cf7417799a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:11:11 2022 +0800\n\n    Add files via upload\n\ncommit 316cf02340a627548b71317ba04afac457f68101\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:04:29 2022 +0800\n\n    fix conflict\n\ncommit 9b33b791d7c7b509f514b7540a8ec5dd7fff9d0b\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 23:22:22 2022 +0800\n\n    Fix format\n\ncommit 346a42467881e5bbd9414200dd3c915935eb56dd\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 22:37:22 2022 +0800\n\n    Fix format\n\ncommit 44971ff962aae30c962226f1ba3d87de057ac00e\nMerge: 0ae377389 c4b93e8fd\nAuthor: Jiatong <728307998@qq.com>\nDate:   Wed Apr 27 10:13:03 2022 -0400\n\n    Merge pull request #4324 from ftshijt/master\n\n    Add Test Functions for ST Train and Inference\n\ncommit 0d3be31602306650fee44c367cbc788e0b0462db\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 22:09:12 2022 +0800\n\n    Fix format\n\ncommit b24d108b0d7d501b2faa1971feca5a281198d351\nMerge: 4c679c061 f1312a8b2\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 21:29:33 2022 +0800\n\n    Fix conflict\n\ncommit 4c679c061c1a0be411f613bdbdeb7849af19edf4\nMerge: a90e2ecef 0ae377389\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 21:15:33 2022 +0800\n\n    Fix conflict\n\ncommit 10e6c7ea2e5783442631213dfc20dd7b9543839d\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Wed Apr 27 09:30:47 2022 +0000\n\n    split docstring to conform with linter\n\ncommit c4b93e8fd870954ec2649abc3fc6172d78d92166\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:49:00 2022 -0400\n\n    apply black\n\ncommit 04d0cd84878701a0ff5e09933581c98ef7e0adac\nMerge: 72b6b21d5 4a12ab320\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:27:36 2022 -0400\n\n    Merge branch 'master' of https://github.com/ftshijt/espnet\n\ncommit 72b6b21d509a26d30a454525811c3530ee6b297b\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:27:09 2022 -0400\n\n    add st unit test\n\ncommit d1e8ac3d8717f8717fb645592c25ee8cafc4060c\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:15:18 2022 -0400\n\n    update test\n\ncommit 5fb7dd619293dcd1cc02c6371c4079c22a40a23b\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 00:53:46 2022 -0400\n\n    remove requirement for src_token_list\n\ncommit 4118b1b21f25fc7d8aa56658cd7ff691684884be\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:31:42 2022 +0800\n\n    fix conflict\n\ncommit 5436784241eaa4f60e0990627758a841e7927651\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:06:19 2022 +0800\n\n    Update test_integration_espnet2.sh\n\ncommit 469168b4451b4922306b3393598d199a514acd50\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:04:56 2022 +0800\n\n    fix issue\n\ncommit 06ddfe19a346f1ea8b620e4eb5bf61bfdcfc3309\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:01:38 2022 +0800\n\n    fix conflict\n\ncommit 5a81f91ce6734745272e6d960261797cfcb3dd41\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 09:57:18 2022 +0800\n\n    fix conflict\n\ncommit 91d48d920c229af3902fc05c361ba1b5f1636c67\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Tue Apr 26 22:21:13 2022 +0000\n\n    applied black\n\ncommit ec518ccc74b85e3b50304ab70ae5a1f069df0038\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Wed Feb 23 11:31:56 2022 +0000\n\n    Add progress bar to phonemization\n\ncommit f1312a8b2eeecf57f740b963b832dc4a806ac5f8\nAuthor: earthmanylf <43513215+earthmanylf@users.noreply.github.com>\nDate:   Mon Apr 25 10:37:19 2022 +0800\n\n    Update README.md\n\n    Co-authored-by: Wangyou Zhang <C0me_On@163.com>\n\ncommit a90e2ecef4854884dc525345a466f33fce79bd0a\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 22:55:54 2022 +0800\n\n    Fix format problems\n\ncommit be0112bf99c7caf787feba50c7dbc47a1879dbfb\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 22:06:45 2022 +0800\n\n    Fix format problems\n\ncommit 16acdadb6dba56d0f91a3132b540a01c9bd25c89\nMerge: feb28baf9 f6a2522ad\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 21:14:02 2022 +0800\n\n    Fix conflict\n\ncommit 95be28ab0e48415922677a92639833d648f3844c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:47:11 2022 +0800\n\n    Fix CI\n\ncommit a0966f61701041228c96924359b8e6678960a31a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:46:10 2022 +0800\n\n    Fix CI\n\ncommit 1daecd4570f477da905e4365ff30e4c0be53ca44\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:44:21 2022 +0800\n\n    fix CI\n\ncommit 7261735b82173ae5ac377844fad2f3b9289e08ec\nMerge: 809106e2a f6a2522ad\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:21:06 2022 +0800\n\n    Merge pull request #15 from espnet/master\n\n    Merging the latest ESPnet\n\ncommit 809106e2a512990b30fd1afcf2c7bf897d185d58\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 12:33:18 2022 +0800\n\n    show the log result\n\ncommit 65b53563cac0fdc09d653112f85dd735313cb650\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 11:10:41 2022 +0800\n\n    show the error report in the log\n\ncommit 36bdfcbfd0731e543db130b6fb756e140f9f2cb2\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 21 15:21:07 2022 +0800\n\n    fix ci\n\ncommit c8e05efd90ea4c9f775b149916d05f0f74092157\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 21 11:30:54 2022 +0800\n\n    fix ci\n\ncommit 4831a6671728e52f0b2a0766a7c4cb60dd3d470f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 20:34:26 2022 +0800\n\n    fix CI\n\ncommit 26fc7e1b41c57dc5c6a6882fe20a8847ee5a055c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 16:37:29 2022 +0800\n\n    Add files via upload\n\ncommit b7c7bf13f9df6d9c09888c21c5c071c15f1023bc\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 15:19:37 2022 +0800\n\n    fix ci\n\ncommit 2b1b6bbef15553a11862a9c74352bed95412337d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 11:33:40 2022 +0800\n\n    fix fbank_pitch issue\n\ncommit 0d5736fc393332465ae49a620392735a22312c97\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 11:33:21 2022 +0800\n\n    fix fbank_pitch issue\n\ncommit 835033c70cb2821340481b6e3f695d3afe6cbcd0\nMerge: fcf13c412 42eb3108a\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Apr 19 07:36:09 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit 70c1980b7c8d396bd5d05d8eba50bf90a84bff55\nAuthor: D-Keqi <462975470@qq.com>\nDate:   Tue Apr 19 19:01:41 2022 +0800\n\n    fix CI\n\ncommit fabb3a1fd17b10cbcf252240e0c40243a8c2f971\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:39:39 2022 +0800\n\n    update the test_integration_espnet2\n\ncommit c08e023e429ad90399f3722d825ccaa33c84b291\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:36:09 2022 +0800\n\n    Update and rename tmp to path.sh\n\ncommit 838d2ecfa767585a3df0161388f5dd5de426695a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:35:08 2022 +0800\n\n    Add files via upload\n\ncommit 62162ae8938d71f0f9040ee1e27eb40c83882808\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:33:31 2022 +0800\n\n    Create tmp\n\ncommit 9a5585e282b68d44921879385f5a3796bacd1fdb\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:33:00 2022 +0800\n\n    Delete t\n\ncommit 349f4ab3498bc296d46ad4b42a77fda25d5e2286\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:31:43 2022 +0800\n\n    add conf\n\ncommit e3486d24210cb53491518d913df2268a2f03eded\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:28:12 2022 +0800\n\n    Create t\n\ncommit 652cf1774dd442d55082652713bbadbc4b6946a6\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:27:47 2022 +0800\n\n    Delete tmp\n\ncommit 48fcab7a8d8b0ad1a97798fa823d315aa7708d3d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:27:12 2022 +0800\n\n    add st1 of mini_an4\n\ncommit 1800b0be298111842ab2a3cf5f39a9ac79c3a86f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:25:21 2022 +0800\n\n    Create tmp\n\ncommit 0a1d05b61d611ca8a7b7ca1815ae089781cbdfde\nMerge: 73ca6e4e4 952a70a70\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 13 10:20:46 2022 +0800\n\n    Merge pull request #14 from espnet/master\n\n    Merge the latest ESPnet\n\ncommit 73ca6e4e4baddd5f3fb6075788ed3e902021b9c8\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:59:52 2022 +0800\n\n    fix ci\n\n    fix ci\n\ncommit acd3e0acdc4d4c6eadfa531711906aa29ffb01a0\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:58:34 2022 +0800\n\n    fix CI\n\n    fix CI\n\ncommit e6da9baea12c6383282bdb716745060be5011a08\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:16:45 2022 +0800\n\n    Add files via upload\n\ncommit fc45fa368bc55b92f94e9ae6f9a6953728f3c894\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:13:53 2022 +0800\n\n    Delete README.md\n\ncommit 5b8c0b567f6b172e2112c5460c45e44b934478a6\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:13:11 2022 +0800\n\n    Delete egs2/chime4/asr1/exp/asr_train_asr_streaming_transformer_raw_en_char/decode_asr_streamindt05_real_beamformit_2micsg_lm_lm_train_lm_en_char_valid.loss.ave_asr_model_valid.acc.ave directory\n\ncommit 87ac110aaf70e2c339bac6ed7c5b60a856acc535\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:10:14 2022 +0800\n\n    streaming slu\n\ncommit 7b7fde9752cd9cd4905d642996215a158bf8d026\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:09:27 2022 +0800\n\n    streaming slu\n\ncommit fcd129620bbbc063dd918b83961d568ad694e45a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:08:55 2022 +0800\n\n    streaming st\n\ncommit 17fe79ca89b496e4f9b6b4caaa2497816d4855b3\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:07:28 2022 +0800\n\n    streaming st\n\ncommit 812a527bb836a2fbd12ceb6d3bcabcc728d88427\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:06:31 2022 +0800\n\n    streaming st\n\ncommit e69a6d8efcd1ae57aca6315d70a20e484d360f7f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:05:25 2022 +0800\n\n    streaming st\n\ncommit e488037b8d9b3e46476874f62b095ae5b7323e19\nMerge: 9fb445053 189e1593d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 15:32:57 2022 +0800\n\n    Merge pull request #13 from espnet/master\n\n    Update lastest espnet\n\ncommit fcf13c412842d57cf48580dd89ff0d1fc5e6c3e0\nMerge: 39700a054 c4aba12f9\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Apr 6 13:35:13 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit feb28baf9dd6af564fe30920c1c6e70c2258e0de\nMerge: 3e6167c51 c4aba12f9\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 6 19:24:06 2022 +0800\n\n    Add deep clustering end-to-end training method\n\ncommit 50269e8b4dd0696d02e5da9f70c2d7952a26f392\nAuthor: WeiGodHorse <weigodhorse@gmail.com>\nDate:   Fri Mar 25 22:58:41 2022 +0800\n\n    fix a bug in Mandarin pypinyin_g2p_phone\n\ncommit 39700a054ac5ed718a1eb74cef9b64b2144b727c\nMerge: aa706c512 14c635069\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Thu Mar 24 17:42:11 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit aa706c5122391feee57d4db121a403dfd8ea0ab0\nMerge: ab2fa25af 350af365f\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Mar 23 23:34:17 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit ab2fa25af6dffce3ecdf3e92adaa171d3d156d50\nMerge: de5e7139b cb8181a99\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 16:03:38 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit de5e7139b65549adfcac58cb0ee23c32c50634ea\nMerge: 5ef36bcae 1bac0f080\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 15:09:20 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 5ef36bcae3fac1792ccc2aae6b7dbab715f094fe\nMerge: 597cd7bd8 0c246e23c\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 13:35:27 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 597cd7bd8a0efbe82733d19774297ab90f5c659f\nMerge: 6625f9056 f16e579e2\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Mar 7 21:54:06 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 6625f9056b5087aeb13a2214c770d586c067f5e3\nMerge: 5f237866b 5e070668e\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Mar 7 13:35:03 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 3e6167c51df23b7629d7830e81e8cf4ea52032fc\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 20:03:31 2022 +0800\n\n    Fixed format in some files\n\ncommit 294373a121cf0766efe623dc56b12d0990a77c93\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 18:26:49 2022 +0800\n\n    Update code and add comments in separator\n\ncommit 5f86c1104cbce4275043e11050b69191834ddbc0\nMerge: 7aa90b584 6f429608b\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 18:06:10 2022 +0800\n\n    Add experiment result in egs2/wsj0_2mix/enh1/README.md; Update code in some files\n\ncommit 5f237866b360028676c7b9e903d15839cdaa0113\nMerge: 66c1a798d 6f429608b\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Sun Mar 6 19:26:35 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 66c1a798d15f531b4c4b4c1e02cfd1eda6813f92\nMerge: 5c5eb0292 a04a98c98\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Thu Mar 3 18:14:47 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 7aa90b5844ba1d0050cfd737b2a2fabe9abd5d62\nMerge: 5f7e2e714 b274c4ea6\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Thu Mar 3 16:20:25 2022 +0800\n\n    Merge branch 'master' of github.com:espnet/espnet into dpclanddan\n\ncommit 5c5eb0292e28c19345fc71d456348f6353f2e2a4\nMerge: bd8e400fa 9863980d2\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Mar 2 12:13:35 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit bd8e400fa37ebc1b77f7a938ae9275bb18de6fe5\nMerge: 58aec432d 7999009d5\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Feb 28 20:37:32 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 5f7e2e7140cc7204acecda90a6ff1d5379967da6\nMerge: d3acdcc3b 637d8c333\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Feb 27 13:19:45 2022 +0800\n\n    Merge branch 'master' of github.com:espnet/espnet into dpclanddan\n\ncommit d3acdcc3bd537cf3f50c8d5c4642dfc488daa656\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 18:32:30 2022 +0800\n\n    fix bugs of test_dan_separator.py\n\ncommit c54d9a4087106b56ab5ce4ec9758aeb74bca0b4c\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 16:00:30 2022 +0800\n\n    add subs to the abs_separator.py\n\ncommit c1d9be5f4f9eb32bc75fb7a8b2fe406aa997946c\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 15:30:46 2022 +0800\n\n    update for dpcl and dan\n\ncommit 58aec432d97300ec12494676a19900a08a950827\nMerge: 23a537e2a 9c24b3add\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Feb 23 16:17:09 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 23a537e2ad1ee9af7e8016054208d5ce1cc572fd\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Tue Feb 22 06:50:03 2022 -0500\n\n    black fix\n\ncommit 8572a57af47ef72e9f010601483b31eb96baf03f\nMerge: 969b333d9 650472b45\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Mon Feb 21 22:35:49 2022 -0500\n\n    Mergefix\n\ncommit ee20e18a5f0eef55c8b0709e1e6b9bcddf10e4e6\nMerge: 63f88c02b a3e1543e9\nAuthor: earthmanylf <43513215+earthmanylf@users.noreply.github.com>\nDate:   Wed Feb 16 14:29:36 2022 +0800\n\n    Merge pull request #1 from espnet/master\n\n    Merge from upstream\n\ncommit 9fb445053f999b64350e5e7a56a1699a727ed125\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Sep 15 00:30:05 2021 +0800\n\n    Update README.md\n\ncommit 8c6d3e1614a247b78f1b17ff2c6ef3b3725b166a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Sep 15 00:29:31 2021 +0800\n\n    Update README.md\n\ncommit 2411dbb82b08aee182df0738a47d7f6f44bdcea8\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Mon Sep 13 13:08:52 2021 +0800\n\n    Update README.md\n\ncommit 3edc1a6d816428b3e4e099271dc51c117b9c8d3b\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Mon Sep 13 13:08:25 2021 +0800\n\n    Update README.md\n\ncommit d4d4b7e450992867bc0ee91ffb467ec38ad6981c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 11 23:11:39 2021 +0800\n\n    Update README.md\n\ncommit 885ab0552dc26076b0b581eb88813f426179fdcb\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:48:05 2021 +0800\n\n    add results\n\ncommit dfba960da5e60cd9d78c439b7fa0e400332fbe46\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:43:36 2021 +0800\n\n    create exp\n\ncommit 391d7c78f310313ca78abc1b3341183a15336579\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:40:23 2021 +0800\n\n    steaming results\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CTC(torch.nn.Module):",
            "self.probs = None  # for visualization",
            "",
            "# In case of Pytorch >= 1.7.0, CTC will be always builtin",
            "-        self.ctc_type = (",
            "-            ctc_type",
            "-            if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")",
            "-            else \"builtin\"",
            "-        )",
            "+        self.ctc_type = ctc_type if V(torch.__version__) < V(\"1.7.0\") else \"builtin\"",
            "",
            "if ctc_type != self.ctc_type:",
            "logging.warning(f\"CTC was set to {self.ctc_type} due to PyTorch version.\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=conditional_expression), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=LooseVersion), value='V')",
            "Update(target_node=ASTNode(type=identifier, text=LooseVersion), value='V')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 6,
        "number": 1393,
        "neg_line": [
            "-self.ctc_type = (",
            "-ctc_type",
            "-if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")",
            "-else \"builtin\"",
            "-)"
        ],
        "pos_line": [
            "+self.ctc_type = ctc_type if V(torch.__version__) < V(\"1.7.0\") else \"builtin\""
        ],
        "core_change": "-self.ctc_type = ( -ctc_type -if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\") -else \"builtin\" -) +self.ctc_type = ctc_type if V(torch.__version__) < V(\"1.7.0\") else \"builtin\"",
        "core_API": "warning"
    },
    {
        "commit_hash": "1caf0dafa3bc8d0bb309a46e2ccb12f714923260",
        "index": "a25945e7..8e38c9d3 100644",
        "commit_message": "Removes dependency on the overrides package (#5490)\n\n* Removes dependency on the overrides package\n\n* Changelog\n\n* Various fixes for mypy\n\n* Update cached_path dependency\n\n* What happened here?\n\n* Formatting\n\n* Fix more tests\n\n* One more missing overrides\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BLEU(Metric):",
            "self._prediction_lengths += dist_reduce_sum(_prediction_lengths)",
            "self._reference_lengths += dist_reduce_sum(_reference_lengths)",
            "",
            "-    @overrides",
            "def get_metric(self, reset: bool = False) -> Dict[str, float]:",
            "",
            "brevity_penalty = self._get_brevity_penalty()"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=function_definition), position=3)",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=overrides))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type=decorated_definition))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1396,
        "neg_line": [
            "-@overrides"
        ],
        "pos_line": [],
        "core_change": "-@overrides",
        "core_API": "_get_brevity_penalty"
    },
    {
        "commit_hash": "81c260fb483e92c39195a1ca36e65bed33868157",
        "index": "2042fd28..e6017722 100644",
        "commit_message": "Fixed audio/bag/binary feature tests to work on GPU (#1600)\n\n* Fixed audio/bag/binary feature tests to work on GPU\n\n* Added timeseries feature test (#1601)\n\n* Removed device transfer calls from encoders to features\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def binary_config():",
            "def test_binary_input_feature(binary_config: Dict, encoder: str) -> None:",
            "binary_config.update({\"encoder\": encoder})",
            "binary_input_feature = BinaryInputFeature(binary_config)",
            "-    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32)",
            "+    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)",
            "encoder_output = binary_input_feature(binary_tensor)",
            "assert encoder_output[\"encoder_output\"].shape[1:] == binary_input_feature.output_shape"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=608183)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=608184)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=608185)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=608186)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=608187)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'DEVICE'), position=1, insert_id=608188)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=608189)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1397,
        "neg_line": [
            "-binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32)"
        ],
        "pos_line": [
            "+binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)"
        ],
        "core_change": "-binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32) +binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)",
        "core_API": "update"
    },
    {
        "commit_hash": "6d4f83cae02129b7f49acf022561711cd937e8b8",
        "index": "f7a9c5d8..a01cdeba 100644",
        "commit_message": "Drop JIT support for `core.check`, `Boxes`, and others (#2219)\n\n* Drop JIT support for `core.check` API\n\n- Consequently for this, we drop support of JIT on the following items: (in of dynamo)\n  - enhance\n    - AdjustSigmoid\n    - AdjustLog\n    - AddWeighted\n  - geometry\n    - UndistortPoints\n    - bbox and Boxes - follow up on #2218\n    - EuclideanDistance\n    - TransformPoints\n    - HomographyWarper\n    - WarpPerspective\n    - UpscaleDouble\n  - losses\n\n* Update typing with pyupgrade\n* drop all jit related from bbox and boxes\n\nfrom #2218\n* fix/skip failing dynamo tests\n* fix loss hd\n* fix typing\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestEulerFromQuaternion(BaseTester):",
            "def test_module(self, device, dtype):",
            "pass",
            "",
            "-    def test_jit(self, device, dtype):",
            "+    def test_dynamo(self, device, dtype, torch_optimizer):",
            "q = Quaternion.random(batch_size=1)",
            "q = q.to(device, dtype)",
            "op = euler_from_quaternion",
            "-        op_jit = torch.jit.script(op)",
            "-        assert_close(op(q.w, q.x, q.y, q.z), op_jit(q.w, q.x, q.y, q.z))",
            "+        op_optimized = torch_optimizer(op)",
            "+        assert_close(op(q.w, q.x, q.y, q.z), op_optimized(q.w, q.x, q.y, q.z))",
            "",
            "def test_forth_and_back(self, device, dtype):",
            "q = Quaternion.random(batch_size=2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=test_jit), value='test_dynamo')",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=388590)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'torch_optimizer'), position=7, insert_id=388591)",
            "Update(target_node=ASTNode(type=identifier, text=op_jit), value='op_optimized')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='torch_optimizer')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=op_jit), value='op_optimized')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=jit))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=script))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 1399,
        "neg_line": [
            "-def test_jit(self, device, dtype):",
            "-op_jit = torch.jit.script(op)",
            "-assert_close(op(q.w, q.x, q.y, q.z), op_jit(q.w, q.x, q.y, q.z))"
        ],
        "pos_line": [
            "+def test_dynamo(self, device, dtype, torch_optimizer):",
            "+op_optimized = torch_optimizer(op)",
            "+assert_close(op(q.w, q.x, q.y, q.z), op_optimized(q.w, q.x, q.y, q.z))"
        ],
        "core_change": "-def test_jit(self, device, dtype): +def test_dynamo(self, device, dtype, torch_optimizer): -op_jit = torch.jit.script(op) -assert_close(op(q.w, q.x, q.y, q.z), op_jit(q.w, q.x, q.y, q.z)) +op_optimized = torch_optimizer(op) +assert_close(op(q.w, q.x, q.y, q.z), op_optimized(q.w, q.x, q.y, q.z))",
        "core_API": "random"
    },
    {
        "commit_hash": "c402309470740e17973eeee22a7b90123f85b310",
        "index": "7914e36b9..caca5ff6a 100644",
        "commit_message": "fix CI error\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AdadeltaFactory(OptimizerFactoryInterface):",
            "",
            "\"\"\"",
            "return torch.optim.Adadelta(",
            "-            target,",
            "-            rho=args.rho,",
            "-            eps=args.eps,",
            "-            weight_decay=args.weight_decay,",
            "+            target, rho=args.rho, eps=args.eps, weight_decay=args.weight_decay",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 1,
        "number": 1401,
        "neg_line": [
            "-target,",
            "-rho=args.rho,",
            "-eps=args.eps,",
            "-weight_decay=args.weight_decay,"
        ],
        "pos_line": [
            "+target, rho=args.rho, eps=args.eps, weight_decay=args.weight_decay"
        ],
        "core_change": "-target, -rho=args.rho, -eps=args.eps, -weight_decay=args.weight_decay, +target, rho=args.rho, eps=args.eps, weight_decay=args.weight_decay",
        "core_API": "Adadelta"
    },
    {
        "commit_hash": "91d8f09e03c0e462ceaf5166a690dde0e6e2b4b3",
        "index": "b6a6a06fd..b25fbcd8e 100644",
        "commit_message": "fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ESPnetSVSModel(AbsESPnetModel):",
            "midi_score_lengths = torch.tensor([len(midi_score)])",
            "tempo_score_lengths = torch.tensor([len(tempo_score)])",
            "beat_score_phn_lengths = torch.tensor([len(beat_score_phn)])",
            "-        beat_score_syb_lengths = torch.tensor([len(beat_score_syb)])",
            "assert (",
            "label_score_lengths == midi_score_lengths",
            "and label_score_lengths == tempo_score_lengths"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=beat_score_syb_lengths))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=len))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=beat_score_syb))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 1402,
        "neg_line": [
            "-beat_score_syb_lengths = torch.tensor([len(beat_score_syb)])"
        ],
        "pos_line": [],
        "core_change": "-beat_score_syb_lengths = torch.tensor([len(beat_score_syb)])",
        "core_API": "tensor"
    },
    {
        "commit_hash": "1d73566e4e2fe9a4f5e799b80f8506a0f92768e3",
        "index": "01f90697..a49b14a2 100644",
        "commit_message": "bugfix in GST\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class StyleTokenLayer(nn.Module):",
            "self.key_dim = embedding_dim // num_heads",
            "self.style_tokens = nn.Parameter(",
            "torch.FloatTensor(num_style_tokens, self.key_dim))",
            "-        nn.init.orthogonal_(self.style_tokens)",
            "+        nn.init.normal_(self.style_tokens, mean=0, std=0.5)",
            "self.attention = MultiHeadAttention(",
            "query_dim=self.query_dim,",
            "key_dim=self.key_dim,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=orthogonal_), value='normal_')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1555670)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1555671)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1555672)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1555673)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'mean'), position=0, insert_id=1555674)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1555675)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '0'), position=2, insert_id=1555676)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'std'), position=0, insert_id=1555677)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1555678)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '0.5'), position=2, insert_id=1555679)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 1403,
        "neg_line": [
            "-nn.init.orthogonal_(self.style_tokens)"
        ],
        "pos_line": [
            "+nn.init.normal_(self.style_tokens, mean=0, std=0.5)"
        ],
        "core_change": "-nn.init.orthogonal_(self.style_tokens) +nn.init.normal_(self.style_tokens, mean=0, std=0.5)",
        "core_API": "Parameter"
    },
    {
        "commit_hash": "d669a74623f273f74213a88b5233964d1ab3ea08",
        "index": "011e7b8..d3bec8e 100644",
        "commit_message": "Detect.py supports running against a Triton container (#9228)\n\n* update coco128-seg comments\n\n* Enables detect.py to use Triton for inference\n\nTriton Inference Server is an open source inference serving software\nthat streamlines AI inferencing.\nhttps://github.com/triton-inference-server/server\n\nThe user can now provide a \"--triton-url\" argument to detect.py to use\na local or remote Triton server for inference.\nFor e.g., http://localhost:8000 will use http over port 8000\nand grpc://localhost:8001 will use grpc over port 8001.\nNote, it is not necessary to specify a weights file to use Triton.\n\nA Triton container can be created by first exporting the Yolov5 model\nto a Triton supported runtime. Onnx, Torchscript, TensorRT are\nsupported by both Triton and the export.py script.\n\nThe exported model can then be containerized via the OctoML CLI.\nSee https://github.com/octoml/octo-cli#getting-started for a guide.\n\n* added triton client to requirements\n\n* fixed support for TFSavedModels in Triton\n\n* reverted change\n\n* Test CoreML update\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update ci-testing.yml\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Use pathlib\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Refacto DetectMultiBackend to directly accept triton url as --weights http://...\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Deploy category\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update detect.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add printout and requirements check\n\n* Cleanup\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* triton fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed triton model query over grpc\n\n* Update check_requirements('tritonclient[all]')\n\n* group imports\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix likely remote URL bug\n\n* update comment\n\n* Update is_url()\n\n* Fix 2x download attempt on http://path/to/model.pt\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: glennjocher <glenn.jocher@ultralytics.com>\nCo-authored-by: Gaz Iqbal <giqbal@octoml.ai>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run(",
            "seen, windows, dt = 0, [], (Profile(), Profile(), Profile())",
            "for path, im, im0s, vid_cap, s in dataset:",
            "with dt[0]:",
            "-            im = torch.Tensor(im).to(device)",
            "+            im = torch.Tensor(im).to(model.device)",
            "im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32",
            "if len(im.shape) == 3:",
            "im = im[None]  # expand for batch dim"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1291895)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=0, insert_id=1291896)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1291897)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=device), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1405,
        "neg_line": [
            "-im = torch.Tensor(im).to(device)"
        ],
        "pos_line": [
            "+im = torch.Tensor(im).to(model.device)"
        ],
        "core_change": "-im = torch.Tensor(im).to(device) +im = torch.Tensor(im).to(model.device)",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "de67c1546d065c86d5bd5cecf022925bb01d0ec7",
        "index": "c7766c1c..2e125bbe 100644",
        "commit_message": "Add Cuda tests for pyro.distributions; fix errors (#297)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Gamma(Distribution):",
            "alpha = alpha.expand_as(x)",
            "beta = beta.expand_as(x)",
            "ll_1 = - beta * x",
            "-        ll_2 = (alpha - pyro.ones(x.size())) * torch.log(x)",
            "+        ll_2 = (alpha - 1.0) * torch.log(x)",
            "ll_3 = alpha * torch.log(beta)",
            "ll_4 = - log_gamma(alpha)",
            "return ll_1 + ll_2 + ll_3 + ll_4"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '1.0'), position=2, insert_id=763130)",
            "Delete(target_node=ASTNode(type=identifier, text=pyro))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ones))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=size))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1407,
        "neg_line": [
            "-ll_2 = (alpha - pyro.ones(x.size())) * torch.log(x)"
        ],
        "pos_line": [
            "+ll_2 = (alpha - 1.0) * torch.log(x)"
        ],
        "core_change": "-ll_2 = (alpha - pyro.ones(x.size())) * torch.log(x) +ll_2 = (alpha - 1.0) * torch.log(x)",
        "core_API": "expand_as"
    },
    {
        "commit_hash": "8ed08e4270fdcb889c3a62e016328d7f14171d3c",
        "index": "670082c2..cc67182b 100644",
        "commit_message": "[Deterministic torch randn] Allow tensors to be generated on CPU (#1902)\n\n* [Deterministic torch randn] Allow tensors to be generated on CPU\n\n* fix more\n\n* up\n\n* fix more\n\n* up\n\n* Update src/diffusers/utils/torch_utils.py\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\n* Apply suggestions from code review\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UnCLIPPipelineIntegrationTests(unittest.TestCase):",
            "pipeline = pipeline.to(torch_device)",
            "pipeline.set_progress_bar_config(disable=None)",
            "",
            "-        generator = torch.Generator(device=torch_device).manual_seed(0)",
            "+        generator = torch.Generator(device=\"cpu\").manual_seed(0)",
            "output = pipeline(",
            "\"horse\",",
            "num_images_per_prompt=1,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', '\"cpu\"'), position=2, insert_id=92011)",
            "Delete(target_node=ASTNode(type=identifier, text=torch_device))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1409,
        "neg_line": [
            "-generator = torch.Generator(device=torch_device).manual_seed(0)"
        ],
        "pos_line": [
            "+generator = torch.Generator(device=\"cpu\").manual_seed(0)"
        ],
        "core_change": "-generator = torch.Generator(device=torch_device).manual_seed(0) +generator = torch.Generator(device=\"cpu\").manual_seed(0)",
        "core_API": "to"
    },
    {
        "commit_hash": "eafa4837fceff97ae6b732713775ee6152fd7b9b",
        "index": "a3c03995b..9566a5f5a 100644",
        "commit_message": "Black fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class Reporter:",
            "if LooseVersion(torch.__version__) >= LooseVersion(\"1.4.0\"):",
            "if torch.cuda.is_initialized():",
            "stats[\"gpu_max_cached_mem_GB\"] = (",
            "-                    torch.cuda.max_memory_reserved() / 2**30",
            "+                    torch.cuda.max_memory_reserved() / 2 ** 30",
            ")",
            "else:",
            "if torch.cuda.is_available() and torch.cuda.max_memory_cached() > 0:",
            "-                stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2**30",
            "+                stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2 ** 30",
            "",
            "self.stats.setdefault(self.epoch, {})[sub_reporter.key] = stats",
            "sub_reporter.finished()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1412,
        "neg_line": [
            "-torch.cuda.max_memory_reserved() / 2**30",
            "-stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2**30"
        ],
        "pos_line": [
            "+torch.cuda.max_memory_reserved() / 2 ** 30",
            "+stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2 ** 30"
        ],
        "core_change": "-torch.cuda.max_memory_reserved() / 2**30 +torch.cuda.max_memory_reserved() / 2 ** 30 -stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2**30 +stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2 ** 30",
        "core_API": "is_initialized"
    },
    {
        "commit_hash": "74bbd6fd76742466bc28134c9b8dfb99e4a677af",
        "index": "966b9ff..2f50278 100644",
        "commit_message": "Fix returning a proper rotation in levelling; supporting batches and default centroid\n\nSummary:\n`get_rotation_to_best_fit_xy` is useful to expose externally, however there was a bug (which we probably did not care about for our use case): it could return a rotation matrix with det(R) == ‚àí1.\nThe diff fixes that, and also makes centroid optional (it can be computed from points).\n\nReviewed By: bottler\n\nDifferential Revision: D39926791\n\nfbshipit-source-id: 5120c7892815b829f3ddcc23e93d4a5ec0ca0013\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def fit_circle_in_3d(",
            "Circle3D object",
            "\"\"\"",
            "centroid = points.mean(0)",
            "-    r = _get_rotation_to_best_fit_xy(points, centroid)",
            "+    r = get_rotation_to_best_fit_xy(points, centroid)",
            "normal = r[:, 2]",
            "rotated_points = (points - centroid) @ r",
            "result_2d = fit_circle_in_2d("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_get_rotation_to_best_fit_xy), value='get_rotation_to_best_fit_xy')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1413,
        "neg_line": [
            "-r = _get_rotation_to_best_fit_xy(points, centroid)"
        ],
        "pos_line": [
            "+r = get_rotation_to_best_fit_xy(points, centroid)"
        ],
        "core_change": "-r = _get_rotation_to_best_fit_xy(points, centroid) +r = get_rotation_to_best_fit_xy(points, centroid)",
        "core_API": "mean"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "3059e0d1..1d236bd7 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SwapBufferManager(object):",
            "self.count = count",
            "self.dtype = dtype",
            "self.all_buffers = [",
            "-            torch.zeros(num_elems,",
            "-                        device='cpu',",
            "-                        dtype=dtype).pin_memory() for _ in range(count)",
            "+            get_accelerator().pin_memory(",
            "+                torch.zeros(num_elems,",
            "+                            device='cpu',",
            "+                            dtype=dtype)) for _ in range(count)",
            "]",
            "self.free_buffer_index = [i for i in range(count)]",
            "self.used_buffer_index = {}"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=75136)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=75137)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=75138)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=75139)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pin_memory'), position=2, insert_id=75140)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=75141)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=75142)",
            "Insert(target_node=IN(type=call), node=('identifier', 'get_accelerator'), position=0, insert_id=75143)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=pin_memory))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 1415,
        "neg_line": [
            "-torch.zeros(num_elems,",
            "-device='cpu',",
            "-dtype=dtype).pin_memory() for _ in range(count)"
        ],
        "pos_line": [
            "+get_accelerator().pin_memory(",
            "+torch.zeros(num_elems,",
            "+device='cpu',",
            "+dtype=dtype)) for _ in range(count)"
        ],
        "core_change": "-torch.zeros(num_elems, -device='cpu', -dtype=dtype).pin_memory() for _ in range(count) +get_accelerator().pin_memory( +torch.zeros(num_elems, +device='cpu', +dtype=dtype)) for _ in range(count)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "ee209d4d016e2ef1b2e73c4be64ad43895bc7e27",
        "index": "5f257dd61..473ccd14f 100755",
        "commit_message": "Fix PT TF ViTMAE (#16766)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ViTMAEDecoder(nn.Module):",
            "[ViTMAELayer(decoder_config) for _ in range(config.decoder_num_hidden_layers)]",
            ")",
            "",
            "-        self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size)",
            "+        self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size, eps=config.layer_norm_eps)",
            "self.decoder_pred = nn.Linear(",
            "config.decoder_hidden_size, config.patch_size**2 * config.num_channels, bias=True",
            ")  # encoder to decoder"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1535629)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1535630)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=1535631)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1535632)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1535633)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=1535634)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1535635)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'layer_norm_eps'), position=2, insert_id=1535636)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1416,
        "neg_line": [
            "-self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size)"
        ],
        "pos_line": [
            "+self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size, eps=config.layer_norm_eps)"
        ],
        "core_change": "-self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size) +self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size, eps=config.layer_norm_eps)",
        "core_API": "LayerNorm"
    },
    {
        "commit_hash": "fcc98f50981a27fc816e3fcc8086b4697b743da0",
        "index": "f9aecd7b6..a707796b1 100755",
        "commit_message": "fixed a bug\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "n_vocab = len(char_list)",
            "",
            "# for debug, small data",
            "-    # train = train[:100000]",
            "-    # valid = valid[:100]",
            "+    train = train[:100000]",
            "+    valid = valid[:100]",
            "",
            "# for debug, ptb data",
            "# train, valid, _ = chainer.datasets.get_ptb_words()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=186408)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=186409)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=186410)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=186411)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'train'), position=0, insert_id=186412)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=186413)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=2, insert_id=186414)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'valid'), position=0, insert_id=186415)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=186416)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=2, insert_id=186417)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'train'), position=0, insert_id=186418)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=186419)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=186420)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=186421)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'valid'), position=0, insert_id=186422)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=186423)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=186424)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=186425)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=186426)",
            "Insert(target_node=IN(type=slice), node=('integer', '100000'), position=1, insert_id=186427)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=186428)",
            "Insert(target_node=IN(type=slice), node=('integer', '100'), position=1, insert_id=186429)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 1421,
        "neg_line": [
            "-# train = train[:100000]",
            "-# valid = valid[:100]"
        ],
        "pos_line": [
            "+train = train[:100000]",
            "+valid = valid[:100]"
        ],
        "core_change": "-# train = train[:100000] -# valid = valid[:100] +train = train[:100000] +valid = valid[:100]",
        "core_API": "get_ptb_words"
    },
    {
        "commit_hash": "b214442e745e9400e4f842d406939b4e2cd33390",
        "index": "9fce99ffb..59f01184b 100644",
        "commit_message": "New logger connector code (#7882)\n\n* New logger connector code\n\n* Update CHANGELOG\n\n* Update requirements\n\n* Fix import path\n\n* Add new suffix\n\n* Tests\n\n* Minor changes\n\n* Rename and reorder\n\n* Fix import\n\n* Formatting\n\n* Fix with seed_everything?\n\n* Fix test?\n\n* Fix test?\n\n* Minor change\n\n* Minor changes\n\n* Minor changes\n\n* Force float\n\n* Fix minimal bug\n\n* Fix minimal bug\n\n* Update with latest changes\n\n* Fix import\n\n* bad merge\n\n* update typing\n\nCo-authored-by: tchaton <thomas@grid.ai>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _setup_ddp(rank, worldsize):",
            "def _ddp_test_fn(rank, worldsize):",
            "_setup_ddp(rank, worldsize)",
            "tensor = torch.tensor([1.0])",
            "-    actual = LightningModule._LightningModule__sync(tensor, sync_dist=True, sync_dist_op=torch.distributed.ReduceOp.SUM)",
            "+    sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM)",
            "+    actual = sync(tensor)",
            "assert actual.item() == dist.get_world_size(), \"Result-Log does not work properly with DDP and Tensors\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=533060)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=533061)",
            "Update(target_node=ASTNode(type=identifier, text=actual), value='sync')",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'actual'), position=0, insert_id=533062)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=533063)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=533064)",
            "Update(target_node=ASTNode(type=identifier, text=LightningModule), value='_Sync')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=LightningModule), position=0)",
            "Insert(target_node=IN(type=call), node=('identifier', 'sync'), position=0, insert_id=533065)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=533066)",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='sync_ddp_if_available')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=533067)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'tensor'), position=1, insert_id=533068)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=533069)",
            "Update(target_node=ASTNode(type=identifier, text=sync_dist), value='should')",
            "Update(target_node=ASTNode(type=identifier, text=sync_dist_op), value='op')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_LightningModule__sync))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 1422,
        "neg_line": [
            "-actual = LightningModule._LightningModule__sync(tensor, sync_dist=True, sync_dist_op=torch.distributed.ReduceOp.SUM)"
        ],
        "pos_line": [
            "+sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM)",
            "+actual = sync(tensor)"
        ],
        "core_change": "-actual = LightningModule._LightningModule__sync(tensor, sync_dist=True, sync_dist_op=torch.distributed.ReduceOp.SUM) +sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM) +actual = sync(tensor)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "f62c0ca906262a950dd4254c29062fc8ec06712d",
        "index": "8e18457b1..860c20d39 100644",
        "commit_message": "fixed\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ESPnetTTSModel(AbsESPnetModel):",
            ")[0][0]",
            "if self.energy_normalize is not None:",
            "energy = self.energy_normalize(energy[None])[0][0]",
            "-            kwargs[\"energy\"] = energy",
            "+            if energy is not None:",
            "+                kwargs[\"energy\"] = energy",
            "",
            "if spembs is not None:",
            "kwargs[\"spembs\"] = spembs"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=5, insert_id=150421)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=150422)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=150423)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=150424)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=150425)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'energy'), position=0, insert_id=150426)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=150427)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=150428)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=150429)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1425,
        "neg_line": [
            "-kwargs[\"energy\"] = energy"
        ],
        "pos_line": [
            "+if energy is not None:",
            "+kwargs[\"energy\"] = energy"
        ],
        "core_change": "-kwargs[\"energy\"] = energy +if energy is not None: +kwargs[\"energy\"] = energy",
        "core_API": "energy_normalize"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "c6f43a4ce..4ba4c4d09 100644",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TFFlaubertMainLayer(tf.keras.layers.Layer):",
            "# encoder attention (for decoder only)",
            "# if self.is_decoder and src_enc is not None:",
            "#     attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)",
            "-            #     attn = F.dropout(attn, p=self.dropout, training=self.training)",
            "+            #     attn = nn.functional.dropout(attn, p=self.dropout, training=self.training)",
            "#     tensor = tensor + attn",
            "#     tensor = self.layer_norm15[i](tensor)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1426,
        "neg_line": [
            "-#     attn = F.dropout(attn, p=self.dropout, training=self.training)"
        ],
        "pos_line": [
            "+#     attn = nn.functional.dropout(attn, p=self.dropout, training=self.training)"
        ],
        "core_change": "-#     attn = F.dropout(attn, p=self.dropout, training=self.training) +#     attn = nn.functional.dropout(attn, p=self.dropout, training=self.training)",
        "core_API": "dropout"
    },
    {
        "commit_hash": "64e6cd0c958a35bcf9a0af7cf9fed448498e2ea4",
        "index": "4612b6e5..f2748a13 100644",
        "commit_message": "Fix typos in torchscript tests.\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_gcn2_conv():",
            "",
            "t = '(Tensor, Tensor, SparseTensor, OptTensor) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(conv(x, x_0, adj1.t()), out1, atol=1e-6)",
            "-    assert torch.allclose(conv(x, x_0, adj2.t()), out2, atol=1e-6)",
            "+    assert torch.allclose(jit(x, x_0, adj1.t()), out1, atol=1e-6)",
            "+    assert torch.allclose(jit(x, x_0, adj2.t()), out2, atol=1e-6)",
            "",
            "conv.cached = True",
            "conv(x, x_0, edge_index)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1010179)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1010180)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1010181)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'allclose'), position=2, insert_id=1010182)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1010183)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=5)",
            "Update(target_node=ASTNode(type=identifier, text=conv), value='jit')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=1010184)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1010185)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=1010186)",
            "Update(target_node=ASTNode(type=identifier, text=conv), value='jit')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=allclose))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=atol))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=float, text=1e-6))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 1427,
        "neg_line": [
            "-assert torch.allclose(conv(x, x_0, adj1.t()), out1, atol=1e-6)",
            "-assert torch.allclose(conv(x, x_0, adj2.t()), out2, atol=1e-6)"
        ],
        "pos_line": [
            "+assert torch.allclose(jit(x, x_0, adj1.t()), out1, atol=1e-6)",
            "+assert torch.allclose(jit(x, x_0, adj2.t()), out2, atol=1e-6)"
        ],
        "core_change": "-assert torch.allclose(conv(x, x_0, adj1.t()), out1, atol=1e-6) -assert torch.allclose(conv(x, x_0, adj2.t()), out2, atol=1e-6) +assert torch.allclose(jit(x, x_0, adj1.t()), out1, atol=1e-6) +assert torch.allclose(jit(x, x_0, adj2.t()), out2, atol=1e-6)",
        "core_API": "script"
    },
    {
        "commit_hash": "8067efa41cd3a6fa67d1a7495ccceb27682bc7e5",
        "index": "3004038c..1c3748ed 100644",
        "commit_message": "Fix NameError: name 'dist' is not defined (#763)\n\n\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def mpi_discovery(distributed_port=TORCH_DISTRIBUTED_DEFAULT_PORT, verbose=True)",
            "os.environ['MASTER_PORT']))",
            "",
            "if torch.distributed.is_initialized():",
            "-        assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank())",
            "+        assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(",
            "+            rank, torch.distributed.get_rank())",
            "assert torch.distributed.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format(",
            "world_size, torch.distributed.get_world_size())"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=84029)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=84030)",
            "Update(target_node=ASTNode(type=identifier, text=dist), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dist), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=84031)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1428,
        "neg_line": [
            "-assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank())"
        ],
        "pos_line": [
            "+assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(",
            "+rank, torch.distributed.get_rank())"
        ],
        "core_change": "-assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank()) +assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format( +rank, torch.distributed.get_rank())",
        "core_API": "is_initialized"
    },
    {
        "commit_hash": "84790b78f60221fe6819b5b01ed27c65da12e37a",
        "index": "95cd133e..11b9530d 100755",
        "commit_message": "re-organize predict/; fix TF incompatibile change of sparse_softmax_cross_entropy_loss\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            ".FullyConnected('fc1', 512, nl=tf.nn.relu) \\",
            ".FullyConnected('linear', out_dim=self.cifar_classnum, nl=tf.identity)()",
            "",
            "-        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)",
            "+        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "wrong = symbf.prediction_incorrect(logits, label)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2307851)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=2307852)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=logits), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2307853)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'logits'), position=2, insert_id=2307854)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'labels'), position=0, insert_id=2307855)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2307856)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=label), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1430,
        "neg_line": [
            "-cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)"
        ],
        "pos_line": [
            "+cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)"
        ],
        "core_change": "-cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label) +cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
        "core_API": "sparse_softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "c87568cebaef3bd75fb41538bc1fcfaaa08ac368",
        "index": "7e72cdca7..ead2a969c 100644",
        "commit_message": "remove unused variables and fix typo\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_wordlm():",
            "char_dict = {x: i for i, x in enumerate(char_list)}",
            "word_dict = {x: i for i, x in enumerate(word_list)}",
            "",
            "-    rnnlm = lm_pytorch.ClassifierWithState(",
            "+    word_rnnlm = lm_pytorch.ClassifierWithState(",
            "lm_pytorch.RNNLM(len(word_list), n_layers, n_units)",
            ")",
            "word_rnnlm = lm_pytorch.ClassifierWithState("
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rnnlm), value='word_rnnlm')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1431,
        "neg_line": [
            "-rnnlm = lm_pytorch.ClassifierWithState("
        ],
        "pos_line": [
            "+word_rnnlm = lm_pytorch.ClassifierWithState("
        ],
        "core_change": "-rnnlm = lm_pytorch.ClassifierWithState( +word_rnnlm = lm_pytorch.ClassifierWithState(",
        "core_API": "ClassifierWithState"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "f8b5859ee9..94565286f6 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFModelV2(ModelV2):",
            "name,",
            "framework=\"tf\")",
            "self.var_list = []",
            "-        if tf.executing_eagerly():",
            "+        if tf1.executing_eagerly():",
            "self.graph = None",
            "else:",
            "-            self.graph = tf.get_default_graph()",
            "+            self.graph = tf1.get_default_graph()",
            "",
            "def context(self):",
            "\"\"\"Returns a contextmanager for the current TF graph.\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2145738)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2145739)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2145740)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 1433,
        "neg_line": [
            "-if tf.executing_eagerly():",
            "-self.graph = tf.get_default_graph()"
        ],
        "pos_line": [
            "+if tf1.executing_eagerly():",
            "+self.graph = tf1.get_default_graph()"
        ],
        "core_change": "-if tf.executing_eagerly(): +if tf1.executing_eagerly(): -self.graph = tf.get_default_graph() +self.graph = tf1.get_default_graph()",
        "core_API": "executing_eagerly"
    },
    {
        "commit_hash": "4fd8977eafd658056c92ffa6294027e77cdc46c0",
        "index": "c0d4158c01..8c2d9146af 100644",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def stats(policy, train_batch):",
            "\"cur_lr\": tf.cast(policy.cur_lr, tf.float64),",
            "\"policy_loss\": policy.loss.pi_loss,",
            "\"policy_entropy\": policy.loss.entropy,",
            "-        \"var_gnorm\": tf.global_norm(list(policy.model.trainable_variables())),",
            "+        \"var_gnorm\": tf.linalg.global_norm(",
            "+            list(policy.model.trainable_variables())),",
            "\"vf_loss\": policy.loss.vf_loss,",
            "}",
            "",
            "",
            "def grad_stats(policy, train_batch, grads):",
            "return {",
            "-        \"grad_gnorm\": tf.global_norm(grads),",
            "+        \"grad_gnorm\": tf.linalg.global_norm(grads),",
            "\"vf_explained_var\": explained_variance(",
            "train_batch[Postprocessing.VALUE_TARGETS],",
            "policy.model.value_function()),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145816)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145817)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=string, text=\"var_gnorm\"), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=ERROR), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=3, insert_id=2145818)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145819)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145820)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=2145821)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 1434,
        "neg_line": [
            "-\"var_gnorm\": tf.global_norm(list(policy.model.trainable_variables())),",
            "-\"grad_gnorm\": tf.global_norm(grads),"
        ],
        "pos_line": [
            "+\"var_gnorm\": tf.linalg.global_norm(",
            "+list(policy.model.trainable_variables())),",
            "+\"grad_gnorm\": tf.linalg.global_norm(grads),"
        ],
        "core_change": "-\"var_gnorm\": tf.global_norm(list(policy.model.trainable_variables())), +\"var_gnorm\": tf.linalg.global_norm( +list(policy.model.trainable_variables())), -\"grad_gnorm\": tf.global_norm(grads), +\"grad_gnorm\": tf.linalg.global_norm(grads),",
        "core_API": "cast"
    },
    {
        "commit_hash": "b15b476a988b603119a674a1eb42dc3c8731855a",
        "index": "74c9cb535..ad3a148f1 100644",
        "commit_message": "Add load_dataset_builder (#2500)\n\n* Add load_dataset_builder\n\n* Fix\n\n* Add docstring\n\n* Remove _return_resolved_file_path arg\n\n* Improve camel-case/snake-case conversion\n\n* Add test\n\n* Fix test\n\n* Fix packaged_modules\n\n* Improve test\n\n* Fix prepare_module test\n\n* Doc improvement\n\n* Mention load_dataset_builder in docs\n\n* Remove replacements in packaged_module\n\n* Try to trigger CI\n\n* mention dataset_builder.info in the docs\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_load_dataset_streaming(dataset_loading_script_dir, data_dir):",
            "def test_loading_from_the_datasets_hub():",
            "with tempfile.TemporaryDirectory() as tmp_dir:",
            "dataset = load_dataset(SAMPLE_DATASET_IDENTIFIER, cache_dir=tmp_dir)",
            "-        assert len(dataset[\"train\"]), 2",
            "-        assert len(dataset[\"validation\"]), 3",
            "+        assert len(dataset[\"train\"]) == 2",
            "+        assert len(dataset[\"validation\"]) == 3",
            "del dataset"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=1784066)",
            "Insert(target_node=ASTNode(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=1784067)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1784068)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=integer, text=2), position=2)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1784069)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=integer, text=3), position=2)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 1435,
        "neg_line": [
            "-assert len(dataset[\"train\"]), 2",
            "-assert len(dataset[\"validation\"]), 3"
        ],
        "pos_line": [
            "+assert len(dataset[\"train\"]) == 2",
            "+assert len(dataset[\"validation\"]) == 3"
        ],
        "core_change": "-assert len(dataset[\"train\"]), 2 -assert len(dataset[\"validation\"]), 3 +assert len(dataset[\"train\"]) == 2 +assert len(dataset[\"validation\"]) == 3",
        "core_API": "TemporaryDirectory"
    },
    {
        "commit_hash": "1c39505d4eb0d6e4c81c38849338b79bb11efe2f",
        "index": "c388fb2..191bd77 100644",
        "commit_message": "leaf Variable inplace bug fix (#1619)\n\n\n",
        "file": "yolov3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Model(nn.Module):",
            "m = self.model[-1]  # Detect() module",
            "for mi, s in zip(m.m, m.stride):  # from",
            "b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)",
            "-            b[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)",
            "-            b[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls",
            "+            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)",
            "+            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls",
            "mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)",
            "",
            "def _print_biases(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('attribute', None), position=0, insert_id=1279682)",
            "Insert(target_node=ASTNode(type=subscript), node=('slice', None), position=3, insert_id=1279683)",
            "Insert(target_node=ASTNode(type=subscript), node=('attribute', None), position=0, insert_id=1279684)",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=slice), position=3)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=b), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1279685)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=1279686)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=1279687)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=b), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1279688)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=1279689)",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 1436,
        "neg_line": [
            "-b[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)",
            "-b[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls"
        ],
        "pos_line": [
            "+b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)",
            "+b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls"
        ],
        "core_change": "-b[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image) -b[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls +b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image) +b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls",
        "core_API": "view"
    },
    {
        "commit_hash": "a3eb64311e4b0ca507e841049b0bac3be4dbf249",
        "index": "1c59c68b2..b3414976d 100644",
        "commit_message": "[sgd] Fixes TrainingOperator wrap model incorrect (#14353)\n\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "class TrainingOperator:",
            "",
            "logger.debug(\"Registering optimizers.\")",
            "self._optimizers = optimizers",
            "-        if not isinstance(self._optimizers, Iterable):",
            "+        if isinstance(self._optimizers, torch.optim.Optimizer):",
            "self._optimizers = [self._optimizers]",
            "",
            "if schedulers:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1615498)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1615499)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1615500)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Optimizer'), position=2, insert_id=1615501)",
            "Update(target_node=ASTNode(type=identifier, text=Iterable), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Iterable), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1615502)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'optim'), position=2, insert_id=1615503)",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=not_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 1438,
        "neg_line": [
            "-if not isinstance(self._optimizers, Iterable):"
        ],
        "pos_line": [
            "+if isinstance(self._optimizers, torch.optim.Optimizer):"
        ],
        "core_change": "-if not isinstance(self._optimizers, Iterable): +if isinstance(self._optimizers, torch.optim.Optimizer):",
        "core_API": "debug"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "d53c2750..02136db2 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CnnHighwayEncoder(Seq2VecEncoder):",
            "``encoding``:",
            "Shape ``(batch_size, projection_dim)`` tensor with context-insensitive token representations.",
            "\"\"\"",
            "-        # pylint: disable=arguments-differ",
            "-",
            "# convolutions want (batch_size, embedding_dim, num_characters)",
            "inputs = inputs.transpose(1, 2)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1441,
        "neg_line": [
            "-# pylint: disable=arguments-differ",
            "-"
        ],
        "pos_line": [],
        "core_change": "-# pylint: disable=arguments-differ -",
        "core_API": "transpose"
    },
    {
        "commit_hash": "b6a18cb39bc3b8a51a66d3dad28cdd6bde34f470",
        "index": "3d4c43132c..126b7610ef 100644",
        "commit_message": "[rllib] Also refactor DQN to use shared RLlib models (#730)\n\n* wip\n\n* works with cartpole\n\n* lint\n\n* fix pg\n\n* comment\n\n* action dist rename\n\n* preprocessor\n\n* fix test\n\n* typo\n\n* fix the action[0] nonsense\n\n* revert\n\n* satisfy the lint\n\n* wip\n\n* works with cartpole\n\n* lint\n\n* fix pg\n\n* comment\n\n* action dist rename\n\n* preprocessor\n\n* fix test\n\n* typo\n\n* fix the action[0] nonsense\n\n* revert\n\n* satisfy the lint\n\n* Minor indentation changes.\n\n* fix merge\n\n* add humanoid\n\n* initial dqn refactor\n\n* remove tfutil\n\n* fix calls\n\n* fix tf errors 1\n\n* closer\n\n* runs now\n\n* lint\n\n* tensorboard graph\n\n* fix linting\n\n* more 4 space\n\n* fix\n\n* fix linT\n\n* more lint\n\n* oops\n\n* es parity\n\n* remove example.py\n\n* fix training bug\n\n* add cartpole demo\n\n* try fixing cartpole\n\n* allow model options, configure cartpole\n\n* debug\n\n* simplify\n\n* no dueling\n\n* avoid out of file handles\n\n* Test dqn in jenkins.\n\n* Minor formatting.\n\n* fix issue\n\n* fix another\n\n* Fix problem in which we log to a directory that hasn't been created.\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class VisionNetwork(Model):",
            "conv2, 512, [10, 10], padding=\"VALID\", scope=\"fc1\")",
            "fc2 = slim.conv2d(fc1, num_outputs, [1, 1], activation_fn=None,",
            "normalizer_fn=None, scope=\"fc2\")",
            "-            return tf.squeeze(fc2, [1, 2])",
            "+            return tf.squeeze(fc2, [1, 2]), tf.squeeze(fc1, [1, 2])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('expression_list', None), position=1, insert_id=2156260)",
            "Move(target_node=IN(type=expression_list), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=2156261)",
            "Insert(target_node=IN(type=expression_list), node=('call', None), position=2, insert_id=2156262)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2156263)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2156264)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2156265)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2156266)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'squeeze'), position=2, insert_id=2156267)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2156268)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'fc1'), position=1, insert_id=2156269)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2156270)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=3, insert_id=2156271)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2156272)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2156273)",
            "Insert(target_node=IN(type=list), node=('integer', '1'), position=1, insert_id=2156274)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=2156275)",
            "Insert(target_node=IN(type=list), node=('integer', '2'), position=3, insert_id=2156276)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=2156277)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 1442,
        "neg_line": [
            "-return tf.squeeze(fc2, [1, 2])"
        ],
        "pos_line": [
            "+return tf.squeeze(fc2, [1, 2]), tf.squeeze(fc1, [1, 2])"
        ],
        "core_change": "-return tf.squeeze(fc2, [1, 2]) +return tf.squeeze(fc2, [1, 2]), tf.squeeze(fc1, [1, 2])",
        "core_API": "conv2d"
    },
    {
        "commit_hash": "28d0048218ad7bce69510b16024510afba0daed2",
        "index": "25c9db5d5..2a48ba5f4 100644",
        "commit_message": "Fx support for multiple model architectures (#17393)\n\n* Support for Bart and LayoutLM, and partial support for XLNet\n\n* Support for mbart\n\n* A lot of new models supported\n\n* Support for other models\n\n* LayoutLM fix\n\n* Use strings instead of classes\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LayoutLMModel(LayoutLMPreTrainedModel):",
            "token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)",
            "",
            "if bbox is None:",
            "-            bbox = torch.zeros(tuple(list(input_shape) + [4]), dtype=torch.long, device=device)",
            "+            bbox = torch.zeros(input_shape + (4,), dtype=torch.long, device=device)",
            "",
            "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=binary_operator), position=1)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=input_shape), position=0)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('tuple', None), position=3, insert_id=1199023)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=1199024)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=4), position=1)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=1199025)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=1199026)",
            "Delete(target_node=ASTNode(type=identifier, text=tuple))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=list))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 1443,
        "neg_line": [
            "-bbox = torch.zeros(tuple(list(input_shape) + [4]), dtype=torch.long, device=device)"
        ],
        "pos_line": [
            "+bbox = torch.zeros(input_shape + (4,), dtype=torch.long, device=device)"
        ],
        "core_change": "-bbox = torch.zeros(tuple(list(input_shape) + [4]), dtype=torch.long, device=device) +bbox = torch.zeros(input_shape + (4,), dtype=torch.long, device=device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "4c06893610ee148f6645b7fee21f382de5d53023",
        "index": "3e409cfb7..1e31b5c40 100644",
        "commit_message": "Fix nn.DataParallel compatibility in PyTorch 1.5 (#4300)\n\n* Test case for #3936\n\n* multigpu tests pass on pytorch 1.4.0\n\n* Fixup\n\n* multigpu tests pass on pytorch 1.5.0\n\n* Update src/transformers/modeling_utils.py\n\n* Update src/transformers/modeling_utils.py\n\n* rename multigpu to require_multigpu\n\n* mode doc\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BertModel(BertPreTrainedModel):",
            "",
            "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
            "# ourselves in which case we just need to make it broadcastable to all heads.",
            "-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(",
            "-            attention_mask, input_shape, self.device",
            "-        )",
            "+        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)",
            "",
            "# If a 2D ou 3D attention mask is provided for the cross-attention",
            "# we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=3)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1237982)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=device), position=4)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 7,
        "number": 1445,
        "neg_line": [
            "-extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(",
            "-attention_mask, input_shape, self.device",
            "-)"
        ],
        "pos_line": [
            "+extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)"
        ],
        "core_change": "-extended_attention_mask: torch.Tensor = self.get_extended_attention_mask( -attention_mask, input_shape, self.device -) +extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)",
        "core_API": "get_extended_attention_mask"
    },
    {
        "commit_hash": "2912ed996c1ca1583e89f67d90b55f6dac00bbef",
        "index": "01be5120..5b492974 100644",
        "commit_message": "fixed example\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def batch_average(input, slice):",
            "\"\"\"Averages ``input`` features in the node dimension. Batch information is",
            "given by ``slice``.",
            "",
            "-    Example::",
            "+    Example:",
            "",
            "-        >>>> input = torch.FloatTensor([[1, 2], [3, 4], [5, 6], [7, 8]])",
            "-        >>>> slice = torch.LongTensor([2, 4])",
            "-        >>>> output = batch_average(input, slice)",
            "-        >>>> # [[2, 3], [6, 7]]",
            "+        >>> input = torch.FloatTensor([[1, 2], [3, 4], [5, 6], [7, 8]])",
            "+        >>> slice = torch.LongTensor([2, 4])",
            "+        >>> output = batch_average(input, slice)",
            "+        >>> # [[2, 3], [6, 7]]",
            "\"\"\"",
            "",
            "last_index = 0"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Averages ``input`` features in the node dimension. Batch information is\ngiven by ``slice``.\n\n    Example::\n\n        >>>> input = torch.FloatTensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n        >>>> slice = torch.LongTensor([2, 4])\n        >>>> output = batch_average(input, slice)\n        >>>> # [[2, 3], [6, 7]]\n\"\"\"), value='\"\"\"Averages ``input`` features in the node dimension. Batch information is\\ngiven by ``slice``.\\n\\n    Example:\\n\\n        >>> input = torch.FloatTensor([[1, 2], [3, 4], [5, 6], [7, 8]])\\n        >>> slice = torch.LongTensor([2, 4])\\n        >>> output = batch_average(input, slice)\\n        >>> # [[2, 3], [6, 7]]\\n\"\"\"')"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 1,
        "number": 1446,
        "neg_line": [
            "-Example::",
            "->>>> input = torch.FloatTensor([[1, 2], [3, 4], [5, 6], [7, 8]])",
            "->>>> slice = torch.LongTensor([2, 4])",
            "->>>> output = batch_average(input, slice)",
            "->>>> # [[2, 3], [6, 7]]"
        ],
        "pos_line": [
            "+Example:",
            "+>>> input = torch.FloatTensor([[1, 2], [3, 4], [5, 6], [7, 8]])",
            "+>>> slice = torch.LongTensor([2, 4])",
            "+>>> output = batch_average(input, slice)",
            "+>>> # [[2, 3], [6, 7]]"
        ],
        "core_change": "-Example:: +Example: ->>>> input = torch.FloatTensor([[1, 2], [3, 4], [5, 6], [7, 8]]) ->>>> slice = torch.LongTensor([2, 4]) ->>>> output = batch_average(input, slice) ->>>> # [[2, 3], [6, 7]] +>>> input = torch.FloatTensor([[1, 2], [3, 4], [5, 6], [7, 8]]) +>>> slice = torch.LongTensor([2, 4]) +>>> output = batch_average(input, slice) +>>> # [[2, 3], [6, 7]]",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "aed653a1..2b5f8813 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class SpatialTransformer2dAffineLayer(Layer):",
            "# 4. Get all parameters",
            "variables = tf.get_collection(TF_GRAPHKEYS_VARIABLES, scope=vs.name)",
            "",
            "-        # # fixed",
            "-        # self.all_layers = list(layer.all_layers)",
            "-        # self.all_params = list(layer.all_params)",
            "-        # self.all_drop = dict(layer.all_drop)",
            "-        #",
            "# # theta_layer",
            "# self.all_layers.extend(theta_layer.all_layers)",
            "# self.all_params.extend(theta_layer.all_params)",
            "# self.all_drop.update(theta_layer.all_drop)",
            "",
            "-        # this layer",
            "self.all_layers.append(self.outputs)",
            "self.all_params.extend(variables)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 1447,
        "neg_line": [
            "-# # fixed",
            "-# self.all_layers = list(layer.all_layers)",
            "-# self.all_params = list(layer.all_params)",
            "-# self.all_drop = dict(layer.all_drop)",
            "-#",
            "-# this layer"
        ],
        "pos_line": [],
        "core_change": "-# # fixed -# self.all_layers = list(layer.all_layers) -# self.all_params = list(layer.all_params) -# self.all_drop = dict(layer.all_drop) -# -# this layer",
        "core_API": "get_collection"
    },
    {
        "commit_hash": "98291f0d33b704d1e4d14197278f2996ac8f4b4a",
        "index": "bac25201..d2618c9a 100644",
        "commit_message": "[WIP] - Documentation Cleaning and Coding Style (#516)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* tl.layers.pooling YAPF reformat\n\n* yapf updated\n\n* gitignore updated\n\n* YAPF Style Fixing Attempt\n\n* Space Error Fix\n\n* Style Correction\n\n* Assertion Codacy Errors Corrected\n\n* Error Fix\n\n* Assertion Refactored\n\n* YAPF Style Applied to Master\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "import tensorlayer as tl",
            "def model(x, is_train=True, reuse=False, name_scope=\"env1\"):",
            "with tf.variable_scope(name_scope, reuse=reuse):",
            "net = tl.layers.InputLayer(x, name='input')",
            "-        net = tl.layers.TimeDistributedLayer(net, layer_class=tl.layers.DenseLayer, args={'n_units': 50, 'name': 'dense'}, name='time_dense')",
            "+        net = tl.layers.TimeDistributedLayer(",
            "+            net, layer_class=tl.layers.DenseLayer, args={",
            "+                'n_units': 50,",
            "+                'name': 'dense'",
            "+            }, name='time_dense'",
            "+        )",
            "return net",
            "",
            "",
            "class Layer_Time_Distributed_Test(CustomTestCase):",
            "+",
            "@classmethod",
            "def setUpClass(cls):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1448,
        "neg_line": [
            "-net = tl.layers.TimeDistributedLayer(net, layer_class=tl.layers.DenseLayer, args={'n_units': 50, 'name': 'dense'}, name='time_dense')"
        ],
        "pos_line": [
            "+net = tl.layers.TimeDistributedLayer(",
            "+net, layer_class=tl.layers.DenseLayer, args={",
            "+'n_units': 50,",
            "+'name': 'dense'",
            "+}, name='time_dense'",
            "+)",
            "+"
        ],
        "core_change": "-net = tl.layers.TimeDistributedLayer(net, layer_class=tl.layers.DenseLayer, args={'n_units': 50, 'name': 'dense'}, name='time_dense') +net = tl.layers.TimeDistributedLayer( +net, layer_class=tl.layers.DenseLayer, args={ +'n_units': 50, +'name': 'dense' +}, name='time_dense' +) +",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "8f9e908b72f0402f981fefc5865fe6c03a81c1c1",
        "index": "d29b95e77..f03306e1f 100644",
        "commit_message": "embed sized setting in default lm bug fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RNNLM(nn.Module):",
            "super(RNNLM, self).__init__()",
            "self.embed = nn.Embedding(n_vocab, n_embed)",
            "if typ == \"lstm\":",
            "-            self.rnn = nn.ModuleList( [nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)] )",
            "+            self.rnn = nn.ModuleList(",
            "+                [nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "+                )",
            "else:",
            "-            self.rnn = nn.ModuleList( [nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)] )",
            "-",
            "+            self.rnn = nn.ModuleList(",
            "+                [nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "+                )",
            "+",
            "self.dropout = nn.ModuleList(",
            "[nn.Dropout(dropout_rate) for _ in range(n_layers + 1)])",
            "self.lo = nn.Linear(n_units, n_vocab)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1449,
        "neg_line": [
            "-self.rnn = nn.ModuleList( [nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)] )",
            "-self.rnn = nn.ModuleList( [nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)] )",
            "-"
        ],
        "pos_line": [
            "+self.rnn = nn.ModuleList(",
            "+[nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "+)",
            "+self.rnn = nn.ModuleList(",
            "+[nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "+)",
            "+"
        ],
        "core_change": "-self.rnn = nn.ModuleList( [nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)] ) +self.rnn = nn.ModuleList( +[nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)] +) -self.rnn = nn.ModuleList( [nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)] ) - +self.rnn = nn.ModuleList( +[nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)] +) +",
        "core_API": "Embedding"
    },
    {
        "commit_hash": "72ab10399f6b2d6e338aca4ac0ebb2f16a0cfb2e",
        "index": "32ae35b74..8d8c1aeed 100644",
        "commit_message": "Fix loss\n\nPlease review @thomwolf but i think this is equivqlent (and it mimics the loss computation of the original loss)\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BertForQuestionAnswering(nn.Module):",
            "",
            "def compute_loss(logits, positions):",
            "max_position = positions.max().item()",
            "-                one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1, device=input_ids.device).zero_()",
            "+                one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1).zero_()",
            "one_hot = one_hot.scatter(1, positions.cpu(), 1) # Second argument need to be LongTensor and not cuda.LongTensor",
            "-                one_hot = one_hot[:, :seq_length]",
            "+                one_hot = one_hot[:, :seq_length].to(input_ids.device)",
            "log_probs = nn.functional.log_softmax(logits, dim = -1).view(batch_size, seq_length)",
            "loss = -torch.mean(torch.sum(one_hot*log_probs), dim = -1)",
            "return loss"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1251925)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1251926)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1251927)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1251928)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1251929)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1251930)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1251931)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 1450,
        "neg_line": [
            "-one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1, device=input_ids.device).zero_()",
            "-one_hot = one_hot[:, :seq_length]"
        ],
        "pos_line": [
            "+one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1).zero_()",
            "+one_hot = one_hot[:, :seq_length].to(input_ids.device)"
        ],
        "core_change": "-one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1, device=input_ids.device).zero_() +one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1).zero_() -one_hot = one_hot[:, :seq_length] +one_hot = one_hot[:, :seq_length].to(input_ids.device)",
        "core_API": "max"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "e5b67897..3f2ba222 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BagOfEmbeddingsEncoder(Seq2VecEncoder):",
            "summed = summed / lengths.unsqueeze(-1).float()",
            "",
            "if length_mask is not None:",
            "-                summed = summed * (length_mask > 0).float().unsqueeze(-1)",
            "+                summed = summed * (length_mask > 0).unsqueeze(-1)",
            "",
            "return summed"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='unsqueeze')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=unsqueeze))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1451,
        "neg_line": [
            "-summed = summed * (length_mask > 0).float().unsqueeze(-1)"
        ],
        "pos_line": [
            "+summed = summed * (length_mask > 0).unsqueeze(-1)"
        ],
        "core_change": "-summed = summed * (length_mask > 0).float().unsqueeze(-1) +summed = summed * (length_mask > 0).unsqueeze(-1)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "f8798c1c13bece96a0fd92d9b7e7bd69576fa34d",
        "index": "f601f3be..c480bc31 100644",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def vec_like(n: int, tensor: torch.Tensor, shared_memory: bool = False):",
            "if len(tensor.shape) < 1:",
            "raise AssertionError(tensor.shape)",
            "",
            "-    vec = torch.zeros(n, 1, device=tensor.device, dtype=tensor.dtype)",
            "+    vec = zeros(n, 1, device=tensor.device, dtype=tensor.dtype)",
            "return vec[None].expand(tensor.shape[0], n, 1) if shared_memory else vec[None].repeat(tensor.shape[0], 1, 1)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=zeros), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1459,
        "neg_line": [
            "-vec = torch.zeros(n, 1, device=tensor.device, dtype=tensor.dtype)"
        ],
        "pos_line": [
            "+vec = zeros(n, 1, device=tensor.device, dtype=tensor.dtype)"
        ],
        "core_change": "-vec = torch.zeros(n, 1, device=tensor.device, dtype=tensor.dtype) +vec = zeros(n, 1, device=tensor.device, dtype=tensor.dtype)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "a115b0f4..a4ca03be 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LanguageModel(Model):",
            "return_dict = {}",
            "",
            "# If we have target tokens, calculate the loss.",
            "-        token_ids = source.get(\"tokens\")",
            "-        if token_ids is not None:",
            "+        token_id_dict = source.get(\"tokens\")",
            "+        if token_id_dict is not None:",
            "+            token_ids = token_id_dict[\"tokens\"]",
            "assert isinstance(contextual_embeddings, torch.Tensor)",
            "",
            "# Use token_ids to compute targets"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=22524)",
            "Update(target_node=ASTNode(type=identifier, text=token_ids), value='token_id_dict')",
            "Update(target_node=ASTNode(type=identifier, text=token_ids), value='token_id_dict')",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=22525)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=22526)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'token_ids'), position=0, insert_id=22527)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=22528)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=2, insert_id=22529)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'token_id_dict'), position=0, insert_id=22530)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=22531)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"tokens\"'), position=2, insert_id=22532)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=22533)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 1462,
        "neg_line": [
            "-token_ids = source.get(\"tokens\")",
            "-if token_ids is not None:"
        ],
        "pos_line": [
            "+token_id_dict = source.get(\"tokens\")",
            "+if token_id_dict is not None:",
            "+token_ids = token_id_dict[\"tokens\"]"
        ],
        "core_change": "-token_ids = source.get(\"tokens\") -if token_ids is not None: +token_id_dict = source.get(\"tokens\") +if token_id_dict is not None: +token_ids = token_id_dict[\"tokens\"]",
        "core_API": "get"
    },
    {
        "commit_hash": "6e0b27d6c934d6beca0c4b3c2972c21bd86f5499",
        "index": "1bdc350..ecfd2bf 100644",
        "commit_message": "Fix SummaryWritter error and similar deprecated warnings (#248)\n\nTF1.0 advises to use FileWriter and similarly pathed functions to replace deprecated ones.\n  All changes here are related to fixing deprecated errors and warnings.\n",
        "file": "tensorflow-wavenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main():",
            "",
            "# Save the result as an audio summary.",
            "datestring = str(datetime.now()).replace(' ', 'T')",
            "-    writer = tf.train.SummaryWriter(logdir)",
            "-    tf.audio_summary('generated', decode, wavenet_params['sample_rate'])",
            "-    summaries = tf.merge_all_summaries()",
            "+    writer = tf.summary.FileWriter(logdir)",
            "+    tf.summary.audio('generated', decode, wavenet_params['sample_rate'])",
            "+    summaries = tf.summary.merge_all()",
            "summary_out = sess.run(summaries,",
            "feed_dict={samples: np.reshape(waveform, [-1, 1])})",
            "writer.add_summary(summary_out)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2207160)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2207161)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'audio'), position=2, insert_id=2207162)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2207163)",
            "Update(target_node=ASTNode(type=identifier, text=SummaryWriter), value='FileWriter')",
            "Update(target_node=ASTNode(type=identifier, text=audio_summary), value='summary')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2207164)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'merge_all'), position=2, insert_id=2207165)",
            "Update(target_node=ASTNode(type=identifier, text=train), value='summary')",
            "Update(target_node=ASTNode(type=identifier, text=merge_all_summaries), value='summary')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 12,
        "number": 1463,
        "neg_line": [
            "-writer = tf.train.SummaryWriter(logdir)",
            "-tf.audio_summary('generated', decode, wavenet_params['sample_rate'])",
            "-summaries = tf.merge_all_summaries()"
        ],
        "pos_line": [
            "+writer = tf.summary.FileWriter(logdir)",
            "+tf.summary.audio('generated', decode, wavenet_params['sample_rate'])",
            "+summaries = tf.summary.merge_all()"
        ],
        "core_change": "-writer = tf.train.SummaryWriter(logdir) -tf.audio_summary('generated', decode, wavenet_params['sample_rate']) -summaries = tf.merge_all_summaries() +writer = tf.summary.FileWriter(logdir) +tf.summary.audio('generated', decode, wavenet_params['sample_rate']) +summaries = tf.summary.merge_all()",
        "core_API": "now"
    },
    {
        "commit_hash": "514486739cc732ad05549d81bd48c0aa9e03a0f3",
        "index": "66420b4c0..a57238df4 100644",
        "commit_message": "Fix CI with change of name of nlp (#7054)\n\n* nlp -> datasets\n\n* More nlp -> datasets\n\n* Woopsie\n\n* More nlp -> datasets\n\n* One last\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def load_indexes():",
            "",
            "@st.cache(allow_output_mutation=True)",
            "def load_train_data():",
            "-    eli5 = nlp.load_dataset(\"eli5\", name=\"LFQA_reddit\")",
            "+    eli5 = datasets.load_dataset(\"eli5\", name=\"LFQA_reddit\")",
            "eli5_train = eli5[\"train_eli5\"]",
            "eli5_train_q_reps = np.memmap(",
            "\"eli5_questions_reps.dat\", dtype=\"float32\", mode=\"r\", shape=(eli5_train.num_rows, 128)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1465,
        "neg_line": [
            "-eli5 = nlp.load_dataset(\"eli5\", name=\"LFQA_reddit\")"
        ],
        "pos_line": [
            "+eli5 = datasets.load_dataset(\"eli5\", name=\"LFQA_reddit\")"
        ],
        "core_change": "-eli5 = nlp.load_dataset(\"eli5\", name=\"LFQA_reddit\") +eli5 = datasets.load_dataset(\"eli5\", name=\"LFQA_reddit\")",
        "core_API": "cache"
    },
    {
        "commit_hash": "4e518ba3ea27822896794ae76167f503c028d8c0",
        "index": "49bd192de7..db2c09274a 100644",
        "commit_message": "Fixed failing test for depthwise_conv2d (#9229)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def depthwise_conv2d(",
            "dilations: Optional[Union[int, Tuple[int, int]]] = 1,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    x = torch.tensor(x)",
            "-    filters = torch.tensor(filters)",
            "+    x = torch.as_tensor(x)",
            "+    filters = torch.as_tensor(filters)",
            "strides = [strides] * 2 if isinstance(strides, int) else strides",
            "strides = [strides[1], strides[2]] if len(strides) == 4 else strides",
            "dilations = [dilations] * 2 if isinstance(dilations, int) else dilations",
            "-    filters = ivy.squeeze(filters, 3) if filters.ndim == 4 else filters",
            "+    filters = ivy.squeeze(filters, 3).to_native() if filters.ndim == 4 else filters",
            "",
            "f_w_after_dilation = filters.shape[1] + (",
            "(dilations[1] - 1) * (filters.shape[1] - 1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('call', None), position=0, insert_id=278505)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=278506)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=278507)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=278508)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to_native'), position=2, insert_id=278509)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=278510)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=278511)",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='as_tensor')",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='as_tensor')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 10,
        "number": 1466,
        "neg_line": [
            "-x = torch.tensor(x)",
            "-filters = torch.tensor(filters)",
            "-filters = ivy.squeeze(filters, 3) if filters.ndim == 4 else filters"
        ],
        "pos_line": [
            "+x = torch.as_tensor(x)",
            "+filters = torch.as_tensor(filters)",
            "+filters = ivy.squeeze(filters, 3).to_native() if filters.ndim == 4 else filters"
        ],
        "core_change": "-x = torch.tensor(x) -filters = torch.tensor(filters) +x = torch.as_tensor(x) +filters = torch.as_tensor(filters) -filters = ivy.squeeze(filters, 3) if filters.ndim == 4 else filters +filters = ivy.squeeze(filters, 3).to_native() if filters.ndim == 4 else filters",
        "core_API": "tensor"
    },
    {
        "commit_hash": "777b0b125a9192468187249f00771ec1c4f7e610",
        "index": "6f78f7ed..deb8ce8e 100644",
        "commit_message": "small fix\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceTagger(flair.nn.Model):",
            "lengths: List[int] = [len(sentence.tokens) for sentence in sentences]",
            "longest_token_sequence_in_batch: int = max(lengths)",
            "",
            "-        pre_allocated_zero_tensor = t = torch.zeros(",
            "+        pre_allocated_zero_tensor = torch.zeros(",
            "self.embeddings.embedding_length * longest_token_sequence_in_batch,",
            "dtype=torch.float,",
            "device=flair.device,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=t))",
            "Delete(target_node=ASTNode(type==, text==))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1468,
        "neg_line": [
            "-pre_allocated_zero_tensor = t = torch.zeros("
        ],
        "pos_line": [
            "+pre_allocated_zero_tensor = torch.zeros("
        ],
        "core_change": "-pre_allocated_zero_tensor = t = torch.zeros( +pre_allocated_zero_tensor = torch.zeros(",
        "core_API": "zeros"
    },
    {
        "commit_hash": "35c65b028714f58978d50ef1a5cb2fe4e5be8fa0",
        "index": "c4fc83bf9..0589ac64d 100644",
        "commit_message": "Fix test suite when running on MPS-enabled hardware (#14708)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_lite_dataloader_device_placement(src_device_str, dest_device_str):",
            "iterator = iter(lite_dataloader)",
            "",
            "batch0 = next(iterator)",
            "-    # TODO: This should be torch.equal, but not supported on MPS at this time (torch 1.12)",
            "-    assert torch.allclose(batch0, torch.tensor([0, 1], device=dest_device))",
            "+    # TODO: torch.equal is not supported on MPS at this time (torch 1.12)",
            "+    assert torch.equal(batch0, torch.tensor([0, 1], device=dest_device))",
            "",
            "batch1 = next(iterator)",
            "-    # TODO: This should be torch.equal, but not supported on MPS at this time (torch 1.12)",
            "-    assert torch.allclose(batch1[\"data\"], torch.tensor([2, 3], device=dest_device))",
            "+    # TODO: torch.equal is not supported on MPS at this time (torch 1.12)",
            "+    assert torch.equal(batch1[\"data\"], torch.tensor([2, 3], device=dest_device))",
            "",
            "",
            "def test_lite_optimizer_wraps():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=allclose), value='equal')",
            "Update(target_node=ASTNode(type=identifier, text=allclose), value='equal')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 2,
        "number": 1469,
        "neg_line": [
            "-# TODO: This should be torch.equal, but not supported on MPS at this time (torch 1.12)",
            "-assert torch.allclose(batch0, torch.tensor([0, 1], device=dest_device))",
            "-# TODO: This should be torch.equal, but not supported on MPS at this time (torch 1.12)",
            "-assert torch.allclose(batch1[\"data\"], torch.tensor([2, 3], device=dest_device))"
        ],
        "pos_line": [
            "+# TODO: torch.equal is not supported on MPS at this time (torch 1.12)",
            "+assert torch.equal(batch0, torch.tensor([0, 1], device=dest_device))",
            "+# TODO: torch.equal is not supported on MPS at this time (torch 1.12)",
            "+assert torch.equal(batch1[\"data\"], torch.tensor([2, 3], device=dest_device))"
        ],
        "core_change": "-# TODO: This should be torch.equal, but not supported on MPS at this time (torch 1.12) -assert torch.allclose(batch0, torch.tensor([0, 1], device=dest_device)) +# TODO: torch.equal is not supported on MPS at this time (torch 1.12) +assert torch.equal(batch0, torch.tensor([0, 1], device=dest_device)) -# TODO: This should be torch.equal, but not supported on MPS at this time (torch 1.12) -assert torch.allclose(batch1[\"data\"], torch.tensor([2, 3], device=dest_device)) +# TODO: torch.equal is not supported on MPS at this time (torch 1.12) +assert torch.equal(batch1[\"data\"], torch.tensor([2, 3], device=dest_device))",
        "core_API": "allclose"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "4ddadcf05..f38d05142 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "Returns:",
            "\"\"\"",
            "",
            "",
            "-class Sacrebleu(nlp.Metric):",
            "+class Sacrebleu(datasets.Metric):",
            "def _info(self):",
            "-        return nlp.MetricInfo(",
            "+        return datasets.MetricInfo(",
            "description=_DESCRIPTION,",
            "citation=_CITATION,",
            "homepage=\"https://github.com/mjpost/sacreBLEU\",",
            "inputs_description=_KWARGS_DESCRIPTION,",
            "-            features=nlp.Features(",
            "+            features=datasets.Features(",
            "{",
            "-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-                    \"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+                    \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "}",
            "),",
            "codebase_urls=[\"https://github.com/mjpost/sacreBLEU\"],"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 1470,
        "neg_line": [
            "-class Sacrebleu(nlp.Metric):",
            "-return nlp.MetricInfo(",
            "-features=nlp.Features(",
            "-\"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-\"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"),"
        ],
        "pos_line": [
            "+class Sacrebleu(datasets.Metric):",
            "+return datasets.MetricInfo(",
            "+features=datasets.Features(",
            "+\"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+\"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),"
        ],
        "core_change": "-class Sacrebleu(nlp.Metric): +class Sacrebleu(datasets.Metric): -return nlp.MetricInfo( +return datasets.MetricInfo( -features=nlp.Features( +features=datasets.Features( -\"predictions\": nlp.Value(\"string\", id=\"sequence\"), -\"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"), +\"predictions\": datasets.Value(\"string\", id=\"sequence\"), +\"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),",
        "core_API": "MetricInfo"
    }
]